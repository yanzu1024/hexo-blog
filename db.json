{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-butterfly/source/css/index.styl","path":"css/index.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/css/var.styl","path":"css/var.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/img/404.jpg","path":"img/404.jpg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/img/butterfly-icon.png","path":"img/butterfly-icon.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/img/error-page.png","path":"img/error-page.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/img/favicon.ico","path":"img/favicon.ico","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/img/friend_404.gif","path":"img/friend_404.gif","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/js/tw_cn.js","path":"js/tw_cn.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/js/search/algolia.js","path":"js/search/algolia.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-butterfly/source/js/search/local-search.js","path":"js/search/local-search.js","modified":0,"renderable":1},{"_id":"source/images/1.png","path":"images/1.png","modified":0,"renderable":0},{"_id":"source/images/bg.png","path":"images/bg.png","modified":0,"renderable":0},{"_id":"source/images/about_bg.png","path":"images/about_bg.png","modified":0,"renderable":0},{"_id":"source/images/avatar.jpg","path":"images/avatar.jpg","modified":0,"renderable":0},{"_id":"source/images/archive_bg.png","path":"images/archive_bg.png","modified":0,"renderable":0},{"_id":"source/images/categories_bg.jpg","path":"images/categories_bg.jpg","modified":0,"renderable":0},{"_id":"source/images/conan.png","path":"images/conan.png","modified":0,"renderable":0},{"_id":"source/images/gallery_bg.png","path":"images/gallery_bg.png","modified":0,"renderable":0},{"_id":"source/images/home_bg.png","path":"images/home_bg.png","modified":0,"renderable":0},{"_id":"source/images/link_bg.jpg","path":"images/link_bg.jpg","modified":0,"renderable":0},{"_id":"source/images/music_bg.png","path":"images/music_bg.png","modified":0,"renderable":0},{"_id":"source/images/nav_logo.png","path":"images/nav_logo.png","modified":0,"renderable":0},{"_id":"source/images/tag_bg.jpg","path":"images/tag_bg.jpg","modified":0,"renderable":0}],"Cache":[{"_id":"source/_data/link.yml","hash":"6e519b3a2b367ef96be0e0eb5754634a9fe25608","modified":1733324204688},{"_id":"source/about/index.md","hash":"3a639cd3995a1e0d059226987baad57210951079","modified":1735181784797},{"_id":"source/css/nav.css","hash":"5dee1d21f8c400c73a9c1ec90f369c224fc70ca2","modified":1733665736021},{"_id":"source/css/universe.css","hash":"c6d2b707984d0b892a25cfa06d3361c5b2aaae55","modified":1733666627463},{"_id":"source/css/info_card.css","hash":"a25b3bc69aef45c95f4ad311ee3eeb4f03947681","modified":1733667574254},{"_id":"source/css/custom.css","hash":"d1199916f62dce07ef4009ba70adf800a0771485","modified":1733835196174},{"_id":"source/css/neon_light.css","hash":"e4ef220dceb7f45b3374bb16a742fb3b2b1c3fb9","modified":1733666294254},{"_id":"source/_posts/02.md","hash":"8f335a0998aeeeb36fb145e8ec196ee4517975e6","modified":1736248166315},{"_id":"source/_posts/03.md","hash":"50133187dabee622ba1481e1776002b98f8ce4c5","modified":1736248176993},{"_id":"source/_posts/04.md","hash":"9509804353604565692ca73fd92842dc2a9e8de2","modified":1736248193124},{"_id":"source/_posts/01.md","hash":"2859454464f5bd92b5a053674e39b2933edc21d6","modified":1736249217575},{"_id":"source/_posts/05.md","hash":"b0a35e36fb8a4f7f626e6a2b2b1453d031cab2c4","modified":1736248212737},{"_id":"source/_posts/06.md","hash":"958ea23a9ef3fb6272d7b55cb80822a6f2714309","modified":1739796421147},{"_id":"source/_posts/07.md","hash":"c21f749ea4ce98c46724824c50617ef5ed070128","modified":1739796640467},{"_id":"source/_posts/10.md","hash":"4e90a2881e5594be62a37e754621171985de9842","modified":1740052016472},{"_id":"source/_posts/08.md","hash":"9548182936991e8ed31d8577df536e0da9b4efd5","modified":1739866969877},{"_id":"source/_posts/09.md","hash":"688cde261958c1e7a06f277a94fa57d1b3df9d39","modified":1740052016462},{"_id":"source/_posts/12.md","hash":"01c9cf569a7103e7e402aa66a668fad1f93eb1d0","modified":1740293658121},{"_id":"source/_posts/11.md","hash":"0e242371a170714e29c57b3c01fc6ce80a044ee8","modified":1740053167726},{"_id":"source/_posts/15.md","hash":"bcee64f9da5e5b8c14763e99bd2eb5668c767a4b","modified":1752637595332},{"_id":"source/_posts/14.md","hash":"fd2dc6bda005df2d46b8dfee8c528aeda2098a85","modified":1745674819549},{"_id":"source/_posts/13.md","hash":"aa9b1a141e167031d9629d82e1f1e40388270273","modified":1744126237676},{"_id":"source/_posts/16.md","hash":"0f8a529569d0530697aa5e5eceef16cc6d14df97","modified":1747915257950},{"_id":"source/_posts/17.md","hash":"d6289313d7cba4c4c9ae3f4b10bb2cae5493059f","modified":1748261417876},{"_id":"source/_posts/18.md","hash":"a192077c43902ec694d4a5b8bd4623020361ffdc","modified":1748393191168},{"_id":"source/_posts/19.md","hash":"ffd478ccbc36fbe5fc60ec6f59b8a178ad920b34","modified":1748575477895},{"_id":"source/_posts/20.md","hash":"aca31340efc497b6edaa429320c9bc17a100601d","modified":1749631376672},{"_id":"source/_posts/21.md","hash":"1792169bad5cda0e2ccfa7210c46d1b05e1c0b78","modified":1749782645833},{"_id":"source/_posts/23.md","hash":"33754e219970036b0208747cb5e69caf8e33c90e","modified":1750081348314},{"_id":"source/_posts/22.md","hash":"6ed107acc838d1004af69eee631a4205491059ff","modified":1750071489358},{"_id":"source/_posts/24.md","hash":"e4b95de9b08b7d15427b9937d49880cd555a0038","modified":1750404371662},{"_id":"source/_posts/25.md","hash":"26275437e89dc81fe9fe27b80864326d6ee4812a","modified":1752305290200},{"_id":"source/js/title.js","hash":"18f8579d35e6d3485daf2374687759ff22393eab","modified":1735181543402},{"_id":"source/js/fps.js","hash":"1cb7e8c3a1086b74b7125b350cb20656d2d856e9","modified":1733832959459},{"_id":"source/_posts/26.md","hash":"ad3fde2775bf553f87e058cb88c4ca3bb57d5bf4","modified":1752586945896},{"_id":"source/js/universe.js","hash":"ea22ec4b15698ad46493cf57cb3809cfae4bbf3d","modified":1733666555813},{"_id":"source/images/avatar.jpg","hash":"9d74b16ac2beec2f5034cb31849b4b0b18e39d51","modified":1733413252357},{"_id":"source/music/index.md","hash":"9a022e0f8ab3826f5986d9a9de1db2121395679c","modified":1733464993701},{"_id":"source/tags/index.md","hash":"11ed8a2f3ae5bcc9ed72a697e3f074707f0ca398","modified":1733416411350},{"_id":"source/gallery/index.md","hash":"109a6e732c04ec00a26efae235c1f74526a86311","modified":1735739457412},{"_id":"source/images/categories_bg.jpg","hash":"ad0b14a6467bfb5028eb1b4d142092f14f43791b","modified":1733464242923},{"_id":"source/images/nav_logo.png","hash":"9445e8dd5aa96f9129497b97c33130cb3f498ae7","modified":1733466298339},{"_id":"source/images/tag_bg.jpg","hash":"dca1df73e8b7700a28cd39449c8f21f207b910ae","modified":1733464287415},{"_id":"source/images/link_bg.jpg","hash":"6dd3656f62eeb4033e316739a1b403ee32f2f8d6","modified":1733464323574},{"_id":"source/images/archive_bg.png","hash":"c19805508ba63ba503c79d5dd49ae37766a75f67","modified":1733464682156},{"_id":"node_modules/hexo-theme-butterfly/README.md","hash":"310bf423097f1cefb6121ce3f115b2ef68aacc44","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/LICENSE","hash":"1128f8f91104ba9ef98d37eea6523a888dcfa5de","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/package.json","hash":"4d8e943931d7fe6bb542475591ee82414218b23a","modified":1733322969104},{"_id":"node_modules/hexo-theme-butterfly/README_CN.md","hash":"6e79be6f188bd671eb6ef1084ea7dd757fa2b3e9","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/_config.yml","hash":"30b0e274e266a2c4566bd5122681b41f7a7affea","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/archive.pug","hash":"bb32c9c476372de747dfa563b83f77d7a917a77d","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/category.pug","hash":"bf07d9624ac7285214c7f1d505da3a813c164c8c","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/.github/FUNDING.yml","hash":"da5e77f5e0cdb7e11b36546fb6796d10e3dfbe5d","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/index.pug","hash":"6aba5fd01aba75fd38ef2e2c1d406d951b8c5560","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/page.pug","hash":"a59b90dd3f845ef6bac18091c55e8e019f4fe7a3","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/tag.pug","hash":"98c0084a4c62415775ca9e261e3d4eeb7668f35b","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/plugins.yml","hash":"a244af3b578de079666bd87c3055917220e63c25","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/post.pug","hash":"3869163174958bef2b68033f21e9bce1c74e3a78","modified":1735709006613},{"_id":"node_modules/hexo-theme-butterfly/languages/default.yml","hash":"f46a8e362641f7a7c432d68e3984e739d5fe6b44","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/languages/en.yml","hash":"f46a8e362641f7a7c432d68e3984e739d5fe6b44","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/languages/ja.yml","hash":"c978fbf9421af4d96978161cef828ea88ae06037","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/languages/zh-CN.yml","hash":"e91ac1a94e1fc84cd75bc6b0a8b7a62acfb2a516","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/languages/ko.yml","hash":"ca8d31350e60b5849bb609a4d33f90d3dd8905a2","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/.github/workflows/publish.yml","hash":"05857c2f265246d8de00e31037f2720709540c09","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/languages/zh-HK.yml","hash":"c3e863ad5b5aa618873ad4370cf0335a9cc67b39","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/languages/zh-TW.yml","hash":"9fecee5467e9c5034515fea73f1901c3c67e8ef9","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/.github/ISSUE_TEMPLATE/config.yml","hash":"7b4831ae8f8f8c55dd1b856781210c517c63e6dd","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/.github/ISSUE_TEMPLATE/bug_report.yml","hash":"df07add8fca55a7a12173af53201d306000fd9e0","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/.github/ISSUE_TEMPLATE/feature_request.yml","hash":"996640605ed1e8e35182f0fd9a60a88783b24b03","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/footer.pug","hash":"78cd51877c72a66de13ad88af6687627d4743b03","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/.github/workflows/stale.yml","hash":"ac62b989b5550c756e1986fcc68f243170705383","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/additional-js.pug","hash":"cd2e0141d7bbd2bd348703ead52c8060904ab338","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/layout.pug","hash":"345cb07213cdf66cfef7d9699c60f5f3297e9585","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/pagination.pug","hash":"eb31f49a826c1f95890078745e55c9a2ccb319f9","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/sidebar.pug","hash":"d4f3a798cdded0c0744efab3b1397b00cdb0c6e5","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/common/postDesc.js","hash":"b69c137c06791db58e347474b88803e112d100dd","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/events/404.js","hash":"a196e0d2eb40d1643bcf53a467ad37104f877ca5","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/events/cdn.js","hash":"21fb5aabe043486d095c4c8cce361ed85ba88a26","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head.pug","hash":"82548510310fb44587371fcc9afd7f0bbdb673b1","modified":1733833711942},{"_id":"node_modules/hexo-theme-butterfly/scripts/events/comment.js","hash":"0e59424100fc3138b9096b38a0a2384a6411f8d9","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/events/init.js","hash":"428b94c7b9e83f7ea36227dee66bfe3c23aee4a8","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/rightside.pug","hash":"7d3581a446285b0cd9362266d54b59fe8768af9e","modified":1733833701699},{"_id":"node_modules/hexo-theme-butterfly/scripts/events/merge_config.js","hash":"072f042e069e7a0523fa439569c0c672f389b5c4","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/events/stylus.js","hash":"1ac9527c510ca757724f15dec84b0ead032411b2","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/events/welcome.js","hash":"8ad9911b755cba13dde2cc055c3f857a6b0dd20e","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/helpers/aside_archives.js","hash":"24a88d138ccabf29698a6cfe399b2e10f126367f","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/helpers/aside_categories.js","hash":"5edf400514843e28b329c3fd3f12f66ed5faca66","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/helpers/getArchiveLength.js","hash":"1b37a484cbcc81f51953ea093644b70499c26314","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/helpers/inject_head_js.js","hash":"7397db8012c27a4a1da75dd659745676654a0c65","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/helpers/series.js","hash":"806e2e5843df23e7eb94709056676f5ce6bfefa7","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/helpers/page.js","hash":"39325366b622cfd2089cb1c9344d89f2edd79816","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/helpers/related_post.js","hash":"9c2ee5868879529f7cef7d93edca5b755f45e7e9","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/filters/post_lazyload.js","hash":"860f967ecf3c6a6ea785b560a7aae4d0757cd18a","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/filters/random_cover.js","hash":"a8eef3f37428436554f58a2b6bac7c255fbdf38d","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/chartjs.js","hash":"a26402edc63cbe8faa6a99647f039ffd9616e1f6","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/flink.js","hash":"555f7e4a2a7d8c4420ca467e87625bf96db11a70","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/button.js","hash":"132d1e7757d0dac42d6b0957a25484613d7ed873","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/gallery.js","hash":"e0abf9f0ec51a78258bbf0101632e7dd2c54db25","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/hide.js","hash":"365db87ddfc582bf8c15cb440c48bed95106e4b1","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/inlineImg.js","hash":"512c68a22ae4a58d6a6b24b368a0c00c2ccb4fcb","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/label.js","hash":"22bc3a24c3610b5a3c3a2043b7d10e26d00018b8","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/mermaid.js","hash":"5c2a07df5874b5377540884e4da14dd21489378f","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/note.js","hash":"1acefc59ead75ebd8cafee36efc7da4fa426d088","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/score.js","hash":"5cb273e95846874e3a58074074c501df23c5e912","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/series.js","hash":"f4507d1527cc0d5fe9fdd89eb769e15814a18fec","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/timeline.js","hash":"67c1aad5ddfe56deae672146c2774379aa65af95","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/scripts/tag/tabs.js","hash":"3a9a28f6833e9cea60761e52ab8990fcb597444f","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/index.styl","hash":"755490867fd8afe47d5cce24faea2ca172b0c4dd","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/var.styl","hash":"8cc7b245da61e35cfcc80cafb4e553b5fa968111","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/img/error-page.png","hash":"d2519710498a871ca3e913c57e2ba20a805b6430","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/img/favicon.ico","hash":"455ac256580bf31a45813dbbdb87219bfc8bfb04","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/js/main.js","hash":"9fb88eb196f9368768aaa554c679129e4969f069","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/Open_Graph.pug","hash":"2beb68bdd43b09a539c8f4ed0cb7c0838c03409a","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/analytics.pug","hash":"e5a26d2e2ca789dffc6bd9cfc13ff9e530c9911c","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/config.pug","hash":"fbe602e22ee4a60d711485ad925dfe06b92a4c76","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/js/tw_cn.js","hash":"7ef59df188ea523da89f4caf69c5c0f14e78da69","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/google_adsense.pug","hash":"95a37e92b39c44bcbea4be7e29ddb3921c5b8220","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/pwa.pug","hash":"0e301fe266b3260257b5a619dc8eedf2c1955e7e","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/js/utils.js","hash":"fd3c26366c78dd82bc87d4ddebe76c582122e1b7","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/site_verification.pug","hash":"e2e8d681f183f00ce5ee239c42d2e36b3744daad","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/config_site.pug","hash":"c681697eac3657c2c3e921c2774d67bb93d47331","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/head/preconnect.pug","hash":"5208fe1e75d97a05fd9bdd6cc53c59d8b741b94b","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/header/post-info.pug","hash":"605809f97b2f0da0e2e84dd2365fc8ccee1de6bd","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/header/nav.pug","hash":"a564d250a66af1f011ee453f206b21bdcd723e1e","modified":1733666168577},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/header/menu_item.pug","hash":"95316827e12ec1a2cee18b7d5f49971e9a88b138","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/header/social.pug","hash":"b7608dca3d361001d661779fe8be03c370231c41","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/header/index.pug","hash":"84b460638b19d95f670262a3312ae0d552f88523","modified":1733463690851},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/loading/index.pug","hash":"8b12fa52bd522dded8b7f1ae3f580f339d75aecf","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/page/404.pug","hash":"ca9cc03a51561413728d7211f0a544654c5d4e36","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/page/categories.pug","hash":"5276a8d2835e05bd535fedc9f593a0ce8c3e8437","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/loading/pace.pug","hash":"38b85f46c8e1bcbc43d2a4875d94ea201518eeeb","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/page/default-page.pug","hash":"4912beecdef7af33f0704ef4d6605e1ac69fbb11","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/loading/fullpage-loading.pug","hash":"55090eee8076af761be7741b4e330ceea07c27d7","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/mixins/indexPostUI.pug","hash":"77eca4f5dbf38c2c805b732ff8d7d9c1560b6f16","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/page/tags.pug","hash":"38c0781ac7544d2010d5bfe7ae521125f3672975","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/post/outdate-notice.pug","hash":"1661f8a53334789099efffa92246158cb3d933b3","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/page/flink.pug","hash":"a6c3a4a608f540d05105a7ae19e2f5719c2a3f71","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/mixins/article-sort.pug","hash":"10aa44e0216dbde863e2bd41267b613dc220e517","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/page/shuoshuo.pug","hash":"7e7cd9dfd832c8f3e829ad445eccf5cb5dc8ff18","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/post/reward.pug","hash":"2dca79799c566ffa68b21ecefa9c63432b5088e1","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/pangu.pug","hash":"0c1affa498b28e79e5a465f3f8f18035993bcb88","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/post/post-copyright.pug","hash":"2bef558fa87986805b3397319b4c088564ba7c44","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/aplayer.pug","hash":"0595d60dbf701e2ffa92181923861b1457a02112","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/effect.pug","hash":"1d39670ee6225f85f5c53bf5c84f3fd6e19290e8","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/pjax.pug","hash":"5c7605a0353ef2709f75bad27794f985c7c78907","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/prismjs.pug","hash":"512111475060798925eb1a7bc7617c34b8bd32ab","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_ad.pug","hash":"60dc48a7b5d89c2a49123c3fc5893ab9c57dd225","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_archives.pug","hash":"86897010fe71503e239887fd8f6a4f5851737be9","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/umami_analytics.pug","hash":"e2fa3804c0fef70bb6fa884f1055c978725ccef2","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/subtitle.pug","hash":"21d42f43f26a2075abf2e6d13d35fb1aaf53a8b4","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_announcement.pug","hash":"ae392459ad401a083ca51ee0b27526b3c1e1faed","modified":1733834575343},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_bottom_self.pug","hash":"13dc8ce922e2e2332fe6ad5856ebb5dbf9ea4444","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_newest_comment.pug","hash":"ba95646b78688bcdcf77166407c4799e5e3c3e6d","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_author.pug","hash":"4ae59a48f8984ce8316cfc82756180dc496774eb","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_categories.pug","hash":"d1a416d0a8a7916d0b1a41d73adc66f8c811e493","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_post_toc.pug","hash":"a658a274c5f7896ee5122725bee45548693bdd66","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_tags.pug","hash":"eceb4420a64c720f0d2741e89d6229bbb3d87353","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_post_series.pug","hash":"bd5ad01277f8c6ddf8a3a29af1518e5fe6eed23f","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_top_self.pug","hash":"ae67c6d4130a6c075058a9c1faea1648bcc6f83e","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_recent_post.pug","hash":"e5aac7b28ed4123d75797263c64e74ac547945bc","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/card_webinfo.pug","hash":"5da5f11a0f7f0fc06732df412bb77a7dddc41429","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_global/function.styl","hash":"15f321aee7876f2366677914a7cd2ab8b071b162","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/widget/index.pug","hash":"66f7a8b0cebc05c575ec3cb70b08d6854029d87a","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/chat.styl","hash":"1a864887fb1bc9d24caa6b8b63a9bfc251762ffd","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/highlight.styl","hash":"79e95511d8d51e8944092b9596994d9779c1f4e4","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_global/index.styl","hash":"506bbff9c954d5f92a6d97145be2d732571fae39","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/aside.styl","hash":"3312e3bc1076ac8ba9dd902a671602c78a5e1479","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/theme.styl","hash":"4765d72ab300910437d64991d3f70a82b41d4e3a","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/comments.styl","hash":"134811b2d696f9ed2c0cd578f3886f1c60770c0a","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/footer.styl","hash":"4382ecffdb778de9fa05edf24765b68ae6da1220","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/head.styl","hash":"dbd4af28bbf2fe9a7d75a75fdf460dfb3317311a","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/relatedposts.styl","hash":"517361bf999e41fdea2c57e0af6e82d310f76edd","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/pagination.styl","hash":"316447b94933af2cc580cc9c2b8b42e3875b8c08","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/post.styl","hash":"0ba6e4f5252d1a95e9dfa20b4a17207b6713947d","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/loading.styl","hash":"ac2aeee9926f75b2a0098efe1c114126987430f2","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/reward.styl","hash":"0d0ef8a9260b494e50ef545e5f395d649418f0a9","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/rightside.styl","hash":"743f9dd5585ecac812f3aa908eede4093d84fbd7","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/sidebar.styl","hash":"be7ee2c63cedbc6b7f03cb3993ec533d9ae9d8bb","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_layout/third-party.styl","hash":"a96513acd7f51ffa01531ef3a24fdf5391ae93ae","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_mode/darkmode.styl","hash":"11a0fde4e784b0af5bb813c9e9ca9e480e5db99a","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/hexo.styl","hash":"d76c38adf1d9c1279ef4241835667789f5b736e0","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/gallery.styl","hash":"5ded9ddc66777e93cc4f0e1e4349540c0249346d","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_mode/readmode.styl","hash":"e2761a2515b34d2d41dea00551abc44d4ce72aeb","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/inlineImg.styl","hash":"f7415acfee7adb5ba01fd278b1e16b636a6b20a1","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/button.styl","hash":"2c71b5bfb873d57eb6532144f66e86183dd0dcf6","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/label.styl","hash":"4a651e47d658340443bb63b7d15a1e9e34f76aca","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/hide.styl","hash":"7e5ca6a899c66b26cebc027db530d54018e2c5c0","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/series.styl","hash":"cf24d72ea16ef253a438efe95b9a0a8fdfbb6d01","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/note.styl","hash":"909bb5079b26b6ee68177919f522566503654058","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_search/algolia.styl","hash":"44d6b9fa9f0cb9026722f7dda15368d41d3fed20","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/tabs.styl","hash":"e76655e699154c3d39043c1a5bb8400dc1658399","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_search/local-search.styl","hash":"1ce084ed240d0e998bcdf1416ada598637878398","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_tags/timeline.styl","hash":"62d03f6c429c2de256a204399610a041cab23b04","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_search/index.styl","hash":"e2046eed9503ac506d5ca120b60203dd2cd968af","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_third-party/normalize.min.css","hash":"2c18a1c9604af475b4749def8f1959df88d8b276","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/categories.styl","hash":"643ff8c79d033947da312b70847b1b45850d7b29","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/404.styl","hash":"354902d03f13068cf8a3617fb0ea92669e6b2a86","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/archives.styl","hash":"c08432ecceaa187005c55144cf6f2b092ca50820","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/flink.styl","hash":"b02e88bb5dafcef11be4669da6cd9cddbf9a852a","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/common.styl","hash":"8aa755a3f588e6e598e4fb6510faa6e766d59a94","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/homepage.styl","hash":"3fa5aa012c62da3b5220654426c0708cab24e9af","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/tags.styl","hash":"e765323537d174c7dcb88614100ccdc3e2d020e1","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_page/shuoshuo.styl","hash":"855af8f97daad51b0161d96d50abd71edec87142","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/abcjs/abcjs.pug","hash":"cb77a73a2b47156d18afb9f2376b3d502d0d12d7","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/abcjs/index.pug","hash":"cabb3a06f8ef297a1ea3d91ced8abeaa0831aa14","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/artalk.pug","hash":"71af0b679e00290b0854384368b3c7e9b3e5f26a","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/disqus.pug","hash":"c5f7081ca29db8cc80f808dfc29e36d5fa22fd7e","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/js/search/local-search.js","hash":"4e11d033fb58563f5e1b497f1a6f1c62d3501ee6","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/js/search/algolia.js","hash":"e5821f78381af9f0f646952a7dd118daab2a79a6","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/fb.pug","hash":"a35c21e9ec2bef465c945408da515bfb5d848d6d","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/index.pug","hash":"b2d274db84ef22fbd6d5ea8f4404821898934209","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/remark42.pug","hash":"001e8be47854b891efe04013c240c38fed4185eb","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/waline.pug","hash":"3a5ccfc69bd8ccb4b8f3ce3502023f7914f2a022","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/valine.pug","hash":"5715fc2dc75808af0a434fe66b81d0f651d03ed3","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/chat/index.pug","hash":"5aaef147ae31e1ffc49152acd43dabc5f15b39ba","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/card-post-count/twikoo.pug","hash":"007bb96fd84f38852b7ab5f761e6867b6058422f","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/chat/chatra.pug","hash":"8b21906b1b3f3faa19bdc16f4167b0b5d487c9ac","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/chat/crisp.pug","hash":"2c6eb1b0b3586ce6a92228f809be37642a2010b0","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/chat/tidio.pug","hash":"9ed621742714de6de6593490e3d4aaa3f30a4791","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/artalk.pug","hash":"f71d8acf5b0f3ece91ae1e018962c73b50941a45","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/disqusjs.pug","hash":"8f3d4c33ec3d1be4bd203ba2461db9ccfad63f23","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/giscus.pug","hash":"966ef931a9545cdde8c270322e8650b54b965935","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/disqus.pug","hash":"be40f20273c31e8b37353d9705ad9b9c693e16a2","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/gitalk.pug","hash":"dd5c7e808654aefeb3fcc81c7b0911cff1ee8776","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/js.pug","hash":"00ed91c52939b9675b316137f854d13684c895a6","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/facebook_comments.pug","hash":"d040c00332e83c985dbc0e76e3242336edbcd764","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/index.pug","hash":"a9709905593d960954e2dd572f09f48a6c2b1ef7","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/livere.pug","hash":"6ddd9dce1a553a0d24509d7f9c78b9f6742be030","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/twikoo.pug","hash":"8869bac6ec822a70dfe432d7bd5e599750441ddb","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/utterances.pug","hash":"46c177643a8e65f036cb08c29387d4ab29485859","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/math/index.pug","hash":"bd87dc207f1cab66e48496548b18f0dd9021eb91","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/remark42.pug","hash":"830bafcf50180fed4fcf76f0da277d702e66a5b3","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/math/katex.pug","hash":"1c2190c9bac26992de8d52be4f8522309d566750","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/valine.pug","hash":"5227663ccbe86c830447e4dcc594202b1582595d","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/comments/waline.pug","hash":"bfc6e6690e70c5bd646bdff354b750ebd782f4ad","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/artalk.pug","hash":"86cf7ba9cae6eb8bee0ca792df7a82f58c8451ff","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/math/chartjs.pug","hash":"a3f409a0c1ed3df8c920f7d32b86c3832459f3bf","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/math/mathjax.pug","hash":"49aece5be2a0a8e414257ecbfc2abd7d0d5f8c64","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/math/mermaid.pug","hash":"2e82651607326e7f999fc7891480c74a06cbb416","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/github-issues.pug","hash":"b82c936cb73c325fcac69832a3e475ed732e06f9","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/disqus-comment.pug","hash":"619c41729691a54871b49da54ea27657d1b58e7e","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/common.pug","hash":"7e3b4e1124c9917b820e96bd0c915c08da68aecd","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/index.pug","hash":"4ec0642f2d5444acfab570a6f8c7868e7ff43fde","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/valine.pug","hash":"988f75833dcb18d4a02c34cdea51e39434ea5723","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/remark42.pug","hash":"eee2c6b5c73ea280c7b683a1d117cab5a2ea5f66","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/waline.pug","hash":"393a327a7f41de070abd53cd95149d3e7e1bbf61","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/share/addtoany.pug","hash":"85c92f8a7e44d7cd1c86f089a05be438535e5362","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/share/share-js.pug","hash":"393c94702fd65c585e37bcf9a4f33c638d8bac72","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/search/algolia.pug","hash":"9c3c109a12d2b6916e8b4965cca12f521510ead9","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/share/index.pug","hash":"8a7d5cdca6a87897d435fc88ebcb7ec0b7ec8591","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/search/index.pug","hash":"11a4e7decb634fbd6af454aa5ec230945837b6a9","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/search/local-search.pug","hash":"f11c6db285f57a475db638e74920efb2d0e5a4d0","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/newest-comments/twikoo-comment.pug","hash":"16796729df739a47508999b5467a57037ee6e9df","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/layout/includes/third-party/search/docsearch.pug","hash":"18b492731f67d449bb32d7fac321deb57a92e75f","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/prismjs/index.styl","hash":"1997713a8722391c8b5c8c1598ec19adee993fa5","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/highlight/index.styl","hash":"cc272f7d612a52f4f1d2a95456cc89bc709fe0e5","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/prismjs/diff.styl","hash":"dcd3af96d8be1a6358a88daf2c66165cc9b792d9","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/prismjs/line-number.styl","hash":"ed77729c26cf815c75efd3a6889b6ac805471765","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/css/_highlight/highlight/diff.styl","hash":"fdcc5a88505fcbf0593aa04ecb4ac0c8ad4f8fa3","modified":499162500000},{"_id":"node_modules/hexo-theme-butterfly/source/img/butterfly-icon.png","hash":"f5dd732fed5c3bcd4aa76bac3441bac8485fb432","modified":499162500000},{"_id":"source/images/bg.png","hash":"cef5563ef113e052ca349327c802d2ba6e3f2558","modified":1733485424523},{"_id":"source/images/conan.png","hash":"435647abe572cf3e7c5102860635a8da449a30dc","modified":1733485562160},{"_id":"source/images/1.png","hash":"6cb2b484bc71add06a7df4456a2074a019cc149c","modified":1733485612857},{"_id":"source/images/gallery_bg.png","hash":"4517f57516b56a4eb9ab123e2e106eaee26d97ed","modified":1733465045829},{"_id":"source/images/music_bg.png","hash":"1f633f5279ea4987b4e7990b5207225e08c43b03","modified":1733465078566},{"_id":"source/images/home_bg.png","hash":"8a3f6f5fbc5db031dd6fe9046490cd851ac24d71","modified":1733463916541},{"_id":"source/images/about_bg.png","hash":"d96a6b64db514828be8458213f7bf82adbb59baf","modified":1733464566810},{"_id":"public/css/info_card.css","hash":"515adb0aabe46e04934fd7186b0c867701276dbf","modified":1752637643934},{"_id":"public/css/nav.css","hash":"519f3731a98cf59b08f3435fabaf672433577a1b","modified":1752637643934},{"_id":"public/css/custom.css","hash":"5195491de23991ef027082a06e16b7f4e53f7e94","modified":1752637643934},{"_id":"public/css/universe.css","hash":"6dd59c5851adeeb3831a64d8ce216d95dba2a6b0","modified":1752637643934},{"_id":"public/css/neon_light.css","hash":"21c8bfa36b1510a464370dc07e4f481739f1caba","modified":1752637643934},{"_id":"public/js/fps.js","hash":"cd922033dac84ac1de4f1e2ec8b712ec4e830be5","modified":1752637643934},{"_id":"public/search.xml","hash":"023828e9000e9f2668cc87694a5b0356a9111c2e","modified":1752637643934},{"_id":"public/js/title.js","hash":"8a9cc9bbf2e909fa4b90e3b1a08daa6302f6eaa7","modified":1752637643934},{"_id":"public/js/universe.js","hash":"fb0de0b3d7842d41b46dd113723dc77f82a4d530","modified":1752637643934},{"_id":"public/about/index.html","hash":"e3aacc29e8e281718bef6d3ca896f2bfe55088c6","modified":1752637643934},{"_id":"public/tags/index.html","hash":"e492cc75cd340393358c7eb036ea1e32f070b857","modified":1752637643934},{"_id":"public/music/index.html","hash":"0751ced7cd823badbe81ace90459c838376305df","modified":1752637643934},{"_id":"public/gallery/index.html","hash":"34b88045bdc31f34c710ebf8347da643bdd1b850","modified":1752637643934},{"_id":"public/2025/07/16/15/index.html","hash":"0df9dec023128c14e9d79495274b6bd1dd5850d3","modified":1752637643934},{"_id":"public/2025/07/15/26/index.html","hash":"c1c957ae44b0d3329c3f4a1f729424b0039b2311","modified":1752637643934},{"_id":"public/2025/07/12/25/index.html","hash":"f1c83c5a0af909bb21367622428f5393b6461aa9","modified":1752637643934},{"_id":"public/2025/06/20/24/index.html","hash":"274e850abc38bab7831dcb909aced9fcd5a67f2f","modified":1752637643934},{"_id":"public/2025/06/16/23/index.html","hash":"94f1b765c2ab6f594ff6e18bc297eb58348b4dbe","modified":1752637643934},{"_id":"public/2025/06/15/22/index.html","hash":"836a26605d9479c57c3ef91bc22663d9a5313b02","modified":1752637643934},{"_id":"public/2025/06/11/21/index.html","hash":"60ec18f967debc2a0c1ddd31af4a73cdcace4865","modified":1752637643934},{"_id":"public/2025/06/11/20/index.html","hash":"10cdc9baf6b125ff1f685f30831001ae5043f67c","modified":1752637643934},{"_id":"public/2025/05/30/19/index.html","hash":"ca8440d8f998232ce2b8a2e2450b6ede0964456d","modified":1752637643934},{"_id":"public/2025/05/28/18/index.html","hash":"2926afeedb6fdc69d5c378a30931bec3f25d62e5","modified":1752637643934},{"_id":"public/2025/05/26/17/index.html","hash":"d884eaace86230b433f1070c14c22b71f78b8d51","modified":1752637643934},{"_id":"public/2025/05/22/16/index.html","hash":"fe59bb3bf473ceaaa91a101e51786803e599cd39","modified":1752637643934},{"_id":"public/2025/04/26/14/index.html","hash":"ece118552a8bdd39542740baa8d4f7216f9ad478","modified":1752637643934},{"_id":"public/2025/04/08/13/index.html","hash":"75786907ff0a7fefc91eb2b3fbb4706d417478d6","modified":1752637643934},{"_id":"public/2025/02/23/12/index.html","hash":"d7b1c0b1af01959b81769d842cdfcf16e21fb65f","modified":1752637643934},{"_id":"public/2025/02/20/11/index.html","hash":"428ed5f24e95a683b31089439b69f4f7e37e6d50","modified":1752637643934},{"_id":"public/2025/02/20/10/index.html","hash":"c0effe0b6f361ca8a15a5c366a8e035711766a91","modified":1752637643934},{"_id":"public/2025/02/19/09/index.html","hash":"6d14d9fb2400cf7fbe110a53105380fff890009e","modified":1752637643934},{"_id":"public/2025/02/18/08/index.html","hash":"cffe3ffc2080194187fc315399e552062cc27eae","modified":1752637643934},{"_id":"public/2025/02/17/07/index.html","hash":"bbf6f049a515e1fd3491d164bc2e1607da716e39","modified":1752637643934},{"_id":"public/2025/02/17/06/index.html","hash":"8dfbfab2f7f8a17e3f3bb0fb4a32b59b9716ae53","modified":1752637643934},{"_id":"public/2025/01/03/05/index.html","hash":"379ffb86fdd414b4045014f1650e1ba988c18694","modified":1752637643934},{"_id":"public/2025/01/02/04/index.html","hash":"ba1246e78279318e53915525287ffe807edca1b0","modified":1752637643934},{"_id":"public/2025/01/01/03/index.html","hash":"32f81332b9b82398372ae53c1733bc2e810992a2","modified":1752637643934},{"_id":"public/2025/01/01/02/index.html","hash":"cea31537b2cc646251ba026ec9050b43e87b29ef","modified":1752637643934},{"_id":"public/2024/12/04/01/index.html","hash":"30c0cb032440fae3545b28a186dad76dcce65a19","modified":1752637643934},{"_id":"public/archives/index.html","hash":"4a7cd131a2bdf5b5ecc3eb4f58e590610f47066b","modified":1752637643934},{"_id":"public/archives/page/2/index.html","hash":"972c3eb622e4f2a3e01040f3326e08cd2a9eeba9","modified":1752637643934},{"_id":"public/archives/page/3/index.html","hash":"555763c309d4f3b54aabc27e2bf419b8aa18dc68","modified":1752637643934},{"_id":"public/archives/2024/index.html","hash":"17d3cc025d0ace8074409288d4cf16bb38187bb3","modified":1752637643934},{"_id":"public/archives/2024/12/index.html","hash":"1cddfb1e875b4f5cab166ea0b30fa6350d28cdbf","modified":1752637643934},{"_id":"public/archives/2025/index.html","hash":"63a7d0189aba8f40ab1e41f782455e51442b8425","modified":1752637643934},{"_id":"public/archives/2025/page/2/index.html","hash":"c7c693be0f0a893900c460ee7e0968525f2ce5ce","modified":1752637643934},{"_id":"public/archives/2025/page/3/index.html","hash":"a731fdc587524d27d5569cffa0ee3b98088453f2","modified":1752637643934},{"_id":"public/archives/2025/01/index.html","hash":"21682636b0a154c161cca6b4b0b97ab2172fcc7f","modified":1752637643934},{"_id":"public/archives/2025/02/index.html","hash":"277ea2d170a75385723d31606885122f39cf8fa8","modified":1752637643934},{"_id":"public/archives/2025/04/index.html","hash":"7a6a017b733809e3ecf8f45bedd8bf7c9525f3ce","modified":1752637643934},{"_id":"public/archives/2025/05/index.html","hash":"d86885d5b1dd60c3cc5e631811d4bf626edfbd14","modified":1752637643934},{"_id":"public/index.html","hash":"47e94a6708bd9516a2bd54293d336b77446b130b","modified":1752637643934},{"_id":"public/archives/2025/06/index.html","hash":"80c56779d46f8b568fd2ed2d7c24559a3fc4c81d","modified":1752637643934},{"_id":"public/archives/2025/07/index.html","hash":"26b9a41c4152927d1729c96d8b31b9818d19ffc5","modified":1752637643934},{"_id":"public/page/2/index.html","hash":"2c14cd891168fc344e85e084da819b3585c6fd48","modified":1752637643934},{"_id":"public/page/4/index.html","hash":"402a3e77944a615a9053b6f1dccd6c72402b3a2d","modified":1752637643934},{"_id":"public/page/3/index.html","hash":"8a35cfce4ced172c388c2a08f0e5d796c8c57d74","modified":1752637643934},{"_id":"public/page/5/index.html","hash":"a51bb3e37cefe1c76353e1ab415b5d23eeda6229","modified":1752637643934},{"_id":"public/tags/ROS2/index.html","hash":"8ce190ce48530cc471be2f8bd74334965f2ebb5d","modified":1752637643934},{"_id":"public/tags/ROS2/page/2/index.html","hash":"3435ed7b06e2c98b3193fc7419cf51a56f2c5843","modified":1752637643934},{"_id":"public/tags/Learning/index.html","hash":"11f8e1ea3a49335218d05bdaf18dc1da73fdb8ff","modified":1752637643934},{"_id":"public/tags/Learning/page/2/index.html","hash":"e80b12d68377416742f98130bd1901f09ccc829d","modified":1752637643934},{"_id":"public/tags/Learning/page/3/index.html","hash":"a140904d62599ca34fac94baf8ee7f38a5755416","modified":1752637643934},{"_id":"public/tags/Ubuntu/index.html","hash":"ef1a6e5105983de49cb6e536fc7b2b8ecfaf92fa","modified":1752637643934},{"_id":"public/tags/安装教程/index.html","hash":"04f47cc3608b5ac34115f5952eb8d10c8a939b1e","modified":1752637643934},{"_id":"public/tags/ROS1/index.html","hash":"2b80eb3c2a92b13c9525ef5ac8c22144b3801b82","modified":1752637643934},{"_id":"public/tags/autoware/index.html","hash":"07177e6a5fec3ee517cd498b4a102d0e66a08f51","modified":1752637643934},{"_id":"public/tags/DL/index.html","hash":"e7ff40ff6f742a4e8cecebab97c7e701b3a7b5e6","modified":1752637643934},{"_id":"public/tags/Pytorch/index.html","hash":"a8d5d3a8404fd2cdc05c29893d5511a332110739","modified":1752637643934},{"_id":"public/tags/RL/index.html","hash":"b063f4219f711c9b5f94e8e48dd18548074b3867","modified":1752637643934},{"_id":"public/tags/math-theory/index.html","hash":"1ca530fe64dfd50e228da1bb8bf72273207e946b","modified":1752637643934},{"_id":"public/tags/上网/index.html","hash":"69b944e7a6a4762171edb51aa4c5db1be791084b","modified":1752637643934},{"_id":"public/tags/安装/index.html","hash":"f36a8613c3645fafea7f2a2fc8dce3bec02f68a6","modified":1752637643934},{"_id":"public/tags/踩坑/index.html","hash":"41a3780ca2e53a2404af4871f82dfc32ff4f3d0d","modified":1752637643934},{"_id":"public/tags/paper-reading/index.html","hash":"8e3248f9820f214c8e96badf5c717d164db40316","modified":1752637643934},{"_id":"public/tags/gnaw-book/index.html","hash":"e25a8ccb5b36c1bb5e458ebe904f172f07c82e41","modified":1752637643934},{"_id":"public/tags/title-tattle/index.html","hash":"3e3c86c27538a31398918763375a5af87957f654","modified":1752637643934},{"_id":"public/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1752637643934},{"_id":"public/img/error-page.png","hash":"d2519710498a871ca3e913c57e2ba20a805b6430","modified":1752637643934},{"_id":"public/img/favicon.ico","hash":"455ac256580bf31a45813dbbdb87219bfc8bfb04","modified":1752637643934},{"_id":"public/images/avatar.jpg","hash":"9d74b16ac2beec2f5034cb31849b4b0b18e39d51","modified":1752637643934},{"_id":"public/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1752637643934},{"_id":"public/images/categories_bg.jpg","hash":"ad0b14a6467bfb5028eb1b4d142092f14f43791b","modified":1752637643934},{"_id":"public/images/tag_bg.jpg","hash":"dca1df73e8b7700a28cd39449c8f21f207b910ae","modified":1752637643934},{"_id":"public/images/nav_logo.png","hash":"9445e8dd5aa96f9129497b97c33130cb3f498ae7","modified":1752637643934},{"_id":"public/css/var.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1752637643934},{"_id":"public/js/utils.js","hash":"fd3c26366c78dd82bc87d4ddebe76c582122e1b7","modified":1752637643934},{"_id":"public/js/search/algolia.js","hash":"e5821f78381af9f0f646952a7dd118daab2a79a6","modified":1752637643934},{"_id":"public/js/search/local-search.js","hash":"4e11d033fb58563f5e1b497f1a6f1c62d3501ee6","modified":1752637643934},{"_id":"public/css/index.css","hash":"2434b990e706bf43ecd32086928165e5d565e61b","modified":1752637643934},{"_id":"public/js/main.js","hash":"9fb88eb196f9368768aaa554c679129e4969f069","modified":1752637643934},{"_id":"public/js/tw_cn.js","hash":"7ef59df188ea523da89f4caf69c5c0f14e78da69","modified":1752637643934},{"_id":"public/img/butterfly-icon.png","hash":"f5dd732fed5c3bcd4aa76bac3441bac8485fb432","modified":1752637643934},{"_id":"public/images/link_bg.jpg","hash":"6dd3656f62eeb4033e316739a1b403ee32f2f8d6","modified":1752637643934},{"_id":"public/images/archive_bg.png","hash":"c19805508ba63ba503c79d5dd49ae37766a75f67","modified":1752637643934},{"_id":"public/images/bg.png","hash":"cef5563ef113e052ca349327c802d2ba6e3f2558","modified":1752637643934},{"_id":"public/images/conan.png","hash":"435647abe572cf3e7c5102860635a8da449a30dc","modified":1752637643934},{"_id":"public/images/1.png","hash":"6cb2b484bc71add06a7df4456a2074a019cc149c","modified":1752637643934},{"_id":"public/images/gallery_bg.png","hash":"4517f57516b56a4eb9ab123e2e106eaee26d97ed","modified":1752637643934},{"_id":"public/images/music_bg.png","hash":"1f633f5279ea4987b4e7990b5207225e08c43b03","modified":1752637643934},{"_id":"public/images/home_bg.png","hash":"8a3f6f5fbc5db031dd6fe9046490cd851ac24d71","modified":1752637643934},{"_id":"public/images/about_bg.png","hash":"d96a6b64db514828be8458213f7bf82adbb59baf","modified":1752637643934}],"Category":[],"Data":[{"_id":"link","data":[{"class_name":"1.技术支持","class_desc":"本网站的搭建由以下开源作者提供技术支持","link_list":[{"name":"Hexo","link":"https://hexo.io/zh-cn/","avatar":"https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg","descr":"快速、简单且强大的网志框架","siteshot":"https://source.fomal.cc/siteshot/hexo.io.jpg"}]}]}],"Page":[{"title":"关于","date":"2024-12-04T15:10:00.000Z","top_img":"/images/about_bg.png","_content":"\n","source":"about/index.md","raw":"---\ntitle: 关于\ndate: 2024-12-04 23:10:00\ntop_img: /images/about_bg.png\n---\n\n","updated":"2024-12-26T02:56:24.797Z","path":"about/index.html","comments":1,"layout":"page","_id":"cmd5f7cqc0000iku45aj57ta5","content":"","cover":false,"excerpt":"","more":""},{"_content":"/* 侧边栏个人信息卡片动态渐变色 */\n#aside-content > .card-widget.card-info {\n  background: linear-gradient(\n    -45deg,\n    #e8d8b9,\n    #eccec5,\n    #a3e9eb,\n    #bdbdf0,\n    #eec1ea\n  );\n  box-shadow: 0 0 5px rgb(66, 68, 68);\n  position: relative;\n  background-size: 400% 400%;\n  -webkit-animation: Gradient 10s ease infinite;\n  -moz-animation: Gradient 10s ease infinite;\n  animation: Gradient 10s ease infinite !important;\n}\n@-webkit-keyframes Gradient {\n  0% {\n    background-position: 0% 50%;\n  }\n  50% {\n    background-position: 100% 50%;\n  }\n  100% {\n    background-position: 0% 50%;\n  }\n}\n@-moz-keyframes Gradient {\n  0% {\n    background-position: 0% 50%;\n  }\n  50% {\n    background-position: 100% 50%;\n  }\n  100% {\n    background-position: 0% 50%;\n  }\n}\n@keyframes Gradient {\n  0% {\n    background-position: 0% 50%;\n  }\n  50% {\n    background-position: 100% 50%;\n  }\n  100% {\n    background-position: 0% 50%;\n  }\n}\n\n/* 黑夜模式适配 */\n[data-theme=\"dark\"] #aside-content > .card-widget.card-info {\n  background: #191919ee;\n}\n\n/* 个人信息Follow me按钮 */\n#aside-content > .card-widget.card-info > #card-info-btn {\n  background-color: #3eb8be;\n  border-radius: 8px;\n}","source":"css/info_card.css","raw":"/* 侧边栏个人信息卡片动态渐变色 */\n#aside-content > .card-widget.card-info {\n  background: linear-gradient(\n    -45deg,\n    #e8d8b9,\n    #eccec5,\n    #a3e9eb,\n    #bdbdf0,\n    #eec1ea\n  );\n  box-shadow: 0 0 5px rgb(66, 68, 68);\n  position: relative;\n  background-size: 400% 400%;\n  -webkit-animation: Gradient 10s ease infinite;\n  -moz-animation: Gradient 10s ease infinite;\n  animation: Gradient 10s ease infinite !important;\n}\n@-webkit-keyframes Gradient {\n  0% {\n    background-position: 0% 50%;\n  }\n  50% {\n    background-position: 100% 50%;\n  }\n  100% {\n    background-position: 0% 50%;\n  }\n}\n@-moz-keyframes Gradient {\n  0% {\n    background-position: 0% 50%;\n  }\n  50% {\n    background-position: 100% 50%;\n  }\n  100% {\n    background-position: 0% 50%;\n  }\n}\n@keyframes Gradient {\n  0% {\n    background-position: 0% 50%;\n  }\n  50% {\n    background-position: 100% 50%;\n  }\n  100% {\n    background-position: 0% 50%;\n  }\n}\n\n/* 黑夜模式适配 */\n[data-theme=\"dark\"] #aside-content > .card-widget.card-info {\n  background: #191919ee;\n}\n\n/* 个人信息Follow me按钮 */\n#aside-content > .card-widget.card-info > #card-info-btn {\n  background-color: #3eb8be;\n  border-radius: 8px;\n}","date":"2024-12-08T14:19:34.254Z","updated":"2024-12-08T14:19:34.254Z","path":"css/info_card.css","layout":"false","title":"","comments":1,"_id":"cmd5f7cqh0002iku482rjazyc","content":"/* 侧边栏个人信息卡片动态渐变色 */\n#aside-content > .card-widget.card-info {\n  background: linear-gradient(\n    -45deg,\n    #e8d8b9,\n    #eccec5,\n    #a3e9eb,\n    #bdbdf0,\n    #eec1ea\n  );\n  box-shadow: 0 0 5px rgb(66, 68, 68);\n  position: relative;\n  background-size: 400% 400%;\n  -webkit-animation: Gradient 10s ease infinite;\n  -moz-animation: Gradient 10s ease infinite;\n  animation: Gradient 10s ease infinite !important;\n}\n@-webkit-keyframes Gradient {\n  0% {\n    background-position: 0% 50%;\n  }\n  50% {\n    background-position: 100% 50%;\n  }\n  100% {\n    background-position: 0% 50%;\n  }\n}\n@-moz-keyframes Gradient {\n  0% {\n    background-position: 0% 50%;\n  }\n  50% {\n    background-position: 100% 50%;\n  }\n  100% {\n    background-position: 0% 50%;\n  }\n}\n@keyframes Gradient {\n  0% {\n    background-position: 0% 50%;\n  }\n  50% {\n    background-position: 100% 50%;\n  }\n  100% {\n    background-position: 0% 50%;\n  }\n}\n\n/* 黑夜模式适配 */\n[data-theme=\"dark\"] #aside-content > .card-widget.card-info {\n  background: #191919ee;\n}\n\n/* 个人信息Follow me按钮 */\n#aside-content > .card-widget.card-info > #card-info-btn {\n  background-color: #3eb8be;\n  border-radius: 8px;\n}"},{"_content":"/* 一级菜单居中 */\n#nav .menus_items {\n  position: absolute !important;\n  width: fit-content !important;\n  left: 50% !important;\n  transform: translateX(-50%) !important;\n}\n/* 子菜单横向展示 */\n#nav .menus_items .menus_item:hover .menus_item_child {\n  display: flex !important;\n}\n/* 这里的2是代表导航栏的第2个元素，即有子菜单的元素，可以按自己需求修改 */\n.menus_items .menus_item:nth-child(5) .menus_item_child {\n  left: -38px;\n}","source":"css/nav.css","raw":"/* 一级菜单居中 */\n#nav .menus_items {\n  position: absolute !important;\n  width: fit-content !important;\n  left: 50% !important;\n  transform: translateX(-50%) !important;\n}\n/* 子菜单横向展示 */\n#nav .menus_items .menus_item:hover .menus_item_child {\n  display: flex !important;\n}\n/* 这里的2是代表导航栏的第2个元素，即有子菜单的元素，可以按自己需求修改 */\n.menus_items .menus_item:nth-child(5) .menus_item_child {\n  left: -38px;\n}","date":"2024-12-08T13:48:56.021Z","updated":"2024-12-08T13:48:56.021Z","path":"css/nav.css","layout":"false","title":"","comments":1,"_id":"cmd5f7cqk0005iku47och4niw","content":"/* 一级菜单居中 */\n#nav .menus_items {\n  position: absolute !important;\n  width: fit-content !important;\n  left: 50% !important;\n  transform: translateX(-50%) !important;\n}\n/* 子菜单横向展示 */\n#nav .menus_items .menus_item:hover .menus_item_child {\n  display: flex !important;\n}\n/* 这里的2是代表导航栏的第2个元素，即有子菜单的元素，可以按自己需求修改 */\n.menus_items .menus_item:nth-child(5) .menus_item_child {\n  left: -38px;\n}"},{"_content":"/* 帧率检测 */\n#fps {\n  position: fixed;\n  /* 指定位置 */\n  left: 10px;\n  bottom: 10px;\n  z-index: 1919810;\n}\n[data-theme=\"light\"] #fps {\n  background-color: rgba(255, 255, 255, 0.85);\n  backdrop-filter: var(--backdrop-filter);\n  padding: 4px;\n  border-radius: 4px;\n}\n[data-theme=\"dark\"] #fps {\n  background-color: rgba(0, 0, 0, 0.72);\n  backdrop-filter: var(--backdrop-filter);\n  padding: 4px;\n  border-radius: 4px;\n}\n\n","source":"css/custom.css","raw":"/* 帧率检测 */\n#fps {\n  position: fixed;\n  /* 指定位置 */\n  left: 10px;\n  bottom: 10px;\n  z-index: 1919810;\n}\n[data-theme=\"light\"] #fps {\n  background-color: rgba(255, 255, 255, 0.85);\n  backdrop-filter: var(--backdrop-filter);\n  padding: 4px;\n  border-radius: 4px;\n}\n[data-theme=\"dark\"] #fps {\n  background-color: rgba(0, 0, 0, 0.72);\n  backdrop-filter: var(--backdrop-filter);\n  padding: 4px;\n  border-radius: 4px;\n}\n\n","date":"2024-12-10T12:53:16.174Z","updated":"2024-12-10T12:53:16.174Z","path":"css/custom.css","layout":"false","title":"","comments":1,"_id":"cmd5f7cql0007iku4bbk80r84","content":"/* 帧率检测 */\n#fps {\n  position: fixed;\n  /* 指定位置 */\n  left: 10px;\n  bottom: 10px;\n  z-index: 1919810;\n}\n[data-theme=\"light\"] #fps {\n  background-color: rgba(255, 255, 255, 0.85);\n  backdrop-filter: var(--backdrop-filter);\n  padding: 4px;\n  border-radius: 4px;\n}\n[data-theme=\"dark\"] #fps {\n  background-color: rgba(0, 0, 0, 0.72);\n  backdrop-filter: var(--backdrop-filter);\n  padding: 4px;\n  border-radius: 4px;\n}\n\n"},{"_content":"/* 背景宇宙星光  */\n#universe{\n  display: block;\n  position: fixed;\n  margin: 0;\n  padding: 0;\n  border: 0;\n  outline: 0;\n  left: 0;\n  top: 0;\n  width: 100%;\n  height: 100%;\n  pointer-events: none;\n  /* 这个是调置顶的优先级的，-1在文章页下面，背景上面，个人推荐这种 */\n  z-index: -1;\n}","source":"css/universe.css","raw":"/* 背景宇宙星光  */\n#universe{\n  display: block;\n  position: fixed;\n  margin: 0;\n  padding: 0;\n  border: 0;\n  outline: 0;\n  left: 0;\n  top: 0;\n  width: 100%;\n  height: 100%;\n  pointer-events: none;\n  /* 这个是调置顶的优先级的，-1在文章页下面，背景上面，个人推荐这种 */\n  z-index: -1;\n}","date":"2024-12-08T14:03:47.463Z","updated":"2024-12-08T14:03:47.463Z","path":"css/universe.css","layout":"false","title":"","comments":1,"_id":"cmd5f7cqm0009iku4e64t8okq","content":"/* 背景宇宙星光  */\n#universe{\n  display: block;\n  position: fixed;\n  margin: 0;\n  padding: 0;\n  border: 0;\n  outline: 0;\n  left: 0;\n  top: 0;\n  width: 100%;\n  height: 100%;\n  pointer-events: none;\n  /* 这个是调置顶的优先级的，-1在文章页下面，背景上面，个人推荐这种 */\n  z-index: -1;\n}"},{"_content":"/* 日间模式不生效 */\n[data-theme=\"light\"] #site-name,\n[data-theme=\"light\"] #site-title,\n[data-theme=\"light\"] #site-subtitle,\n[data-theme=\"light\"] #post-info {\n  animation: none;\n}\n/* 夜间模式生效 */\n[data-theme=\"dark\"] #site-name,\n[data-theme=\"dark\"] #site-title {\n  animation: light_15px 10s linear infinite;\n}\n[data-theme=\"dark\"] #site-subtitle {\n  animation: light_10px 10s linear infinite;\n}\n[data-theme=\"dark\"] #post-info {\n  animation: light_5px 10s linear infinite;\n}\n/* 关键帧描述 */\n@keyframes light_15px {\n  0% {\n    text-shadow: #5636ed 0 0 15px;\n  }\n  12.5% {\n    text-shadow: #11ee5e 0 0 15px;\n  }\n  25% {\n    text-shadow: #f14747 0 0 15px;\n  }\n  37.5% {\n    text-shadow: #f1a247 0 0 15px;\n  }\n  50% {\n    text-shadow: #f1ee47 0 0 15px;\n  }\n  50% {\n    text-shadow: #b347f1 0 0 15px;\n  }\n  62.5% {\n    text-shadow: #002afa 0 0 15px;\n  }\n  75% {\n    text-shadow: #ed709b 0 0 15px;\n  }\n  87.5% {\n    text-shadow: #39c5bb 0 0 15px;\n  }\n  100% {\n    text-shadow: #5636ed 0 0 15px;\n  }\n}\n\n@keyframes light_10px {\n  0% {\n    text-shadow: #5636ed 0 0 10px;\n  }\n  12.5% {\n    text-shadow: #11ee5e 0 0 10px;\n  }\n  25% {\n    text-shadow: #f14747 0 0 10px;\n  }\n  37.5% {\n    text-shadow: #f1a247 0 0 10px;\n  }\n  50% {\n    text-shadow: #f1ee47 0 0 10px;\n  }\n  50% {\n    text-shadow: #b347f1 0 0 10px;\n  }\n  62.5% {\n    text-shadow: #002afa 0 0 10px;\n  }\n  75% {\n    text-shadow: #ed709b 0 0 10px;\n  }\n  87.5% {\n    text-shadow: #39c5bb 0 0 10px;\n  }\n  100% {\n    text-shadow: #5636ed 0 0 10px;\n  }\n}\n\n@keyframes light_5px {\n  0% {\n    text-shadow: #5636ed 0 0 5px;\n  }\n  12.5% {\n    text-shadow: #11ee5e 0 0 5px;\n  }\n  25% {\n    text-shadow: #f14747 0 0 5px;\n  }\n  37.5% {\n    text-shadow: #f1a247 0 0 15px;\n  }\n  50% {\n    text-shadow: #f1ee47 0 0 5px;\n  }\n  50% {\n    text-shadow: #b347f1 0 0 5px;\n  }\n  62.5% {\n    text-shadow: #002afa 0 0 5px;\n  }\n  75% {\n    text-shadow: #ed709b 0 0 5px;\n  }\n  87.5% {\n    text-shadow: #39c5bb 0 0 5px;\n  }\n  100% {\n    text-shadow: #5636ed 0 0 5px;\n  }\n}","source":"css/neon_light.css","raw":"/* 日间模式不生效 */\n[data-theme=\"light\"] #site-name,\n[data-theme=\"light\"] #site-title,\n[data-theme=\"light\"] #site-subtitle,\n[data-theme=\"light\"] #post-info {\n  animation: none;\n}\n/* 夜间模式生效 */\n[data-theme=\"dark\"] #site-name,\n[data-theme=\"dark\"] #site-title {\n  animation: light_15px 10s linear infinite;\n}\n[data-theme=\"dark\"] #site-subtitle {\n  animation: light_10px 10s linear infinite;\n}\n[data-theme=\"dark\"] #post-info {\n  animation: light_5px 10s linear infinite;\n}\n/* 关键帧描述 */\n@keyframes light_15px {\n  0% {\n    text-shadow: #5636ed 0 0 15px;\n  }\n  12.5% {\n    text-shadow: #11ee5e 0 0 15px;\n  }\n  25% {\n    text-shadow: #f14747 0 0 15px;\n  }\n  37.5% {\n    text-shadow: #f1a247 0 0 15px;\n  }\n  50% {\n    text-shadow: #f1ee47 0 0 15px;\n  }\n  50% {\n    text-shadow: #b347f1 0 0 15px;\n  }\n  62.5% {\n    text-shadow: #002afa 0 0 15px;\n  }\n  75% {\n    text-shadow: #ed709b 0 0 15px;\n  }\n  87.5% {\n    text-shadow: #39c5bb 0 0 15px;\n  }\n  100% {\n    text-shadow: #5636ed 0 0 15px;\n  }\n}\n\n@keyframes light_10px {\n  0% {\n    text-shadow: #5636ed 0 0 10px;\n  }\n  12.5% {\n    text-shadow: #11ee5e 0 0 10px;\n  }\n  25% {\n    text-shadow: #f14747 0 0 10px;\n  }\n  37.5% {\n    text-shadow: #f1a247 0 0 10px;\n  }\n  50% {\n    text-shadow: #f1ee47 0 0 10px;\n  }\n  50% {\n    text-shadow: #b347f1 0 0 10px;\n  }\n  62.5% {\n    text-shadow: #002afa 0 0 10px;\n  }\n  75% {\n    text-shadow: #ed709b 0 0 10px;\n  }\n  87.5% {\n    text-shadow: #39c5bb 0 0 10px;\n  }\n  100% {\n    text-shadow: #5636ed 0 0 10px;\n  }\n}\n\n@keyframes light_5px {\n  0% {\n    text-shadow: #5636ed 0 0 5px;\n  }\n  12.5% {\n    text-shadow: #11ee5e 0 0 5px;\n  }\n  25% {\n    text-shadow: #f14747 0 0 5px;\n  }\n  37.5% {\n    text-shadow: #f1a247 0 0 15px;\n  }\n  50% {\n    text-shadow: #f1ee47 0 0 5px;\n  }\n  50% {\n    text-shadow: #b347f1 0 0 5px;\n  }\n  62.5% {\n    text-shadow: #002afa 0 0 5px;\n  }\n  75% {\n    text-shadow: #ed709b 0 0 5px;\n  }\n  87.5% {\n    text-shadow: #39c5bb 0 0 5px;\n  }\n  100% {\n    text-shadow: #5636ed 0 0 5px;\n  }\n}","date":"2024-12-08T13:58:14.254Z","updated":"2024-12-08T13:58:14.254Z","path":"css/neon_light.css","layout":"false","title":"","comments":1,"_id":"cmd5f7cqo000ciku49nr5fatm","content":"/* 日间模式不生效 */\n[data-theme=\"light\"] #site-name,\n[data-theme=\"light\"] #site-title,\n[data-theme=\"light\"] #site-subtitle,\n[data-theme=\"light\"] #post-info {\n  animation: none;\n}\n/* 夜间模式生效 */\n[data-theme=\"dark\"] #site-name,\n[data-theme=\"dark\"] #site-title {\n  animation: light_15px 10s linear infinite;\n}\n[data-theme=\"dark\"] #site-subtitle {\n  animation: light_10px 10s linear infinite;\n}\n[data-theme=\"dark\"] #post-info {\n  animation: light_5px 10s linear infinite;\n}\n/* 关键帧描述 */\n@keyframes light_15px {\n  0% {\n    text-shadow: #5636ed 0 0 15px;\n  }\n  12.5% {\n    text-shadow: #11ee5e 0 0 15px;\n  }\n  25% {\n    text-shadow: #f14747 0 0 15px;\n  }\n  37.5% {\n    text-shadow: #f1a247 0 0 15px;\n  }\n  50% {\n    text-shadow: #f1ee47 0 0 15px;\n  }\n  50% {\n    text-shadow: #b347f1 0 0 15px;\n  }\n  62.5% {\n    text-shadow: #002afa 0 0 15px;\n  }\n  75% {\n    text-shadow: #ed709b 0 0 15px;\n  }\n  87.5% {\n    text-shadow: #39c5bb 0 0 15px;\n  }\n  100% {\n    text-shadow: #5636ed 0 0 15px;\n  }\n}\n\n@keyframes light_10px {\n  0% {\n    text-shadow: #5636ed 0 0 10px;\n  }\n  12.5% {\n    text-shadow: #11ee5e 0 0 10px;\n  }\n  25% {\n    text-shadow: #f14747 0 0 10px;\n  }\n  37.5% {\n    text-shadow: #f1a247 0 0 10px;\n  }\n  50% {\n    text-shadow: #f1ee47 0 0 10px;\n  }\n  50% {\n    text-shadow: #b347f1 0 0 10px;\n  }\n  62.5% {\n    text-shadow: #002afa 0 0 10px;\n  }\n  75% {\n    text-shadow: #ed709b 0 0 10px;\n  }\n  87.5% {\n    text-shadow: #39c5bb 0 0 10px;\n  }\n  100% {\n    text-shadow: #5636ed 0 0 10px;\n  }\n}\n\n@keyframes light_5px {\n  0% {\n    text-shadow: #5636ed 0 0 5px;\n  }\n  12.5% {\n    text-shadow: #11ee5e 0 0 5px;\n  }\n  25% {\n    text-shadow: #f14747 0 0 5px;\n  }\n  37.5% {\n    text-shadow: #f1a247 0 0 15px;\n  }\n  50% {\n    text-shadow: #f1ee47 0 0 5px;\n  }\n  50% {\n    text-shadow: #b347f1 0 0 5px;\n  }\n  62.5% {\n    text-shadow: #002afa 0 0 5px;\n  }\n  75% {\n    text-shadow: #ed709b 0 0 5px;\n  }\n  87.5% {\n    text-shadow: #39c5bb 0 0 5px;\n  }\n  100% {\n    text-shadow: #5636ed 0 0 5px;\n  }\n}"},{"_content":"if (window.localStorage.getItem(\"fpson\") == undefined || window.localStorage.getItem(\"fpson\") == \"1\") {\n    var rAF = function () {\n        return (\n            window.requestAnimationFrame ||\n            window.webkitRequestAnimationFrame ||\n            function (callback) {\n                window.setTimeout(callback, 1000 / 60);\n            }\n        );\n    }();\n    var frame = 0;\n    var allFrameCount = 0;\n    var lastTime = Date.now();\n    var lastFameTime = Date.now();\n    var loop = function () {\n        var now = Date.now();\n        var fs = (now - lastFameTime);\n        var fps = Math.round(1000 / fs);\n\n        lastFameTime = now;\n        // 不置 0，在动画的开头及结尾记录此值的差值算出 FPS\n        allFrameCount++;\n        frame++;\n\n        if (now > 1000 + lastTime) {\n            var fps = Math.round((frame * 1000) / (now - lastTime));\n            if (fps <= 5) {\n                var kd = `<span style=\"color:#bd0000\">卡成ppt🤢</span>`\n            } else if (fps <= 15) {\n                var kd = `<span style=\"color:red\">电竞级帧率😖</span>`\n            } else if (fps <= 25) {\n                var kd = `<span style=\"color:orange\">有点难受😨</span>`\n            } else if (fps < 35) {\n                var kd = `<span style=\"color:#9338e6\">不太流畅🙄</span>`\n            } else if (fps <= 45) {\n                var kd = `<span style=\"color:#08b7e4\">还不错哦😁</span>`\n            } else {\n                var kd = `<span style=\"color:#39c5bb\">十分流畅🤣</span>`\n            }\n            document.getElementById(\"fps\").innerHTML = `FPS:${fps} ${kd}`;\n            frame = 0;\n            lastTime = now;\n        };\n\n        rAF(loop);\n    }\n\n    loop();\n} else {\n    document.getElementById(\"fps\").style = \"display:none!important\"\n}","source":"js/fps.js","raw":"if (window.localStorage.getItem(\"fpson\") == undefined || window.localStorage.getItem(\"fpson\") == \"1\") {\n    var rAF = function () {\n        return (\n            window.requestAnimationFrame ||\n            window.webkitRequestAnimationFrame ||\n            function (callback) {\n                window.setTimeout(callback, 1000 / 60);\n            }\n        );\n    }();\n    var frame = 0;\n    var allFrameCount = 0;\n    var lastTime = Date.now();\n    var lastFameTime = Date.now();\n    var loop = function () {\n        var now = Date.now();\n        var fs = (now - lastFameTime);\n        var fps = Math.round(1000 / fs);\n\n        lastFameTime = now;\n        // 不置 0，在动画的开头及结尾记录此值的差值算出 FPS\n        allFrameCount++;\n        frame++;\n\n        if (now > 1000 + lastTime) {\n            var fps = Math.round((frame * 1000) / (now - lastTime));\n            if (fps <= 5) {\n                var kd = `<span style=\"color:#bd0000\">卡成ppt🤢</span>`\n            } else if (fps <= 15) {\n                var kd = `<span style=\"color:red\">电竞级帧率😖</span>`\n            } else if (fps <= 25) {\n                var kd = `<span style=\"color:orange\">有点难受😨</span>`\n            } else if (fps < 35) {\n                var kd = `<span style=\"color:#9338e6\">不太流畅🙄</span>`\n            } else if (fps <= 45) {\n                var kd = `<span style=\"color:#08b7e4\">还不错哦😁</span>`\n            } else {\n                var kd = `<span style=\"color:#39c5bb\">十分流畅🤣</span>`\n            }\n            document.getElementById(\"fps\").innerHTML = `FPS:${fps} ${kd}`;\n            frame = 0;\n            lastTime = now;\n        };\n\n        rAF(loop);\n    }\n\n    loop();\n} else {\n    document.getElementById(\"fps\").style = \"display:none!important\"\n}","date":"2024-12-10T12:15:59.459Z","updated":"2024-12-10T12:15:59.459Z","path":"js/fps.js","layout":"false","title":"","comments":1,"_id":"cmd5f7cqq000eiku42s4g63ha","content":"if (window.localStorage.getItem(\"fpson\") == undefined || window.localStorage.getItem(\"fpson\") == \"1\") {\n    var rAF = function () {\n        return (\n            window.requestAnimationFrame ||\n            window.webkitRequestAnimationFrame ||\n            function (callback) {\n                window.setTimeout(callback, 1000 / 60);\n            }\n        );\n    }();\n    var frame = 0;\n    var allFrameCount = 0;\n    var lastTime = Date.now();\n    var lastFameTime = Date.now();\n    var loop = function () {\n        var now = Date.now();\n        var fs = (now - lastFameTime);\n        var fps = Math.round(1000 / fs);\n\n        lastFameTime = now;\n        // 不置 0，在动画的开头及结尾记录此值的差值算出 FPS\n        allFrameCount++;\n        frame++;\n\n        if (now > 1000 + lastTime) {\n            var fps = Math.round((frame * 1000) / (now - lastTime));\n            if (fps <= 5) {\n                var kd = `<span style=\"color:#bd0000\">卡成ppt🤢</span>`\n            } else if (fps <= 15) {\n                var kd = `<span style=\"color:red\">电竞级帧率😖</span>`\n            } else if (fps <= 25) {\n                var kd = `<span style=\"color:orange\">有点难受😨</span>`\n            } else if (fps < 35) {\n                var kd = `<span style=\"color:#9338e6\">不太流畅🙄</span>`\n            } else if (fps <= 45) {\n                var kd = `<span style=\"color:#08b7e4\">还不错哦😁</span>`\n            } else {\n                var kd = `<span style=\"color:#39c5bb\">十分流畅🤣</span>`\n            }\n            document.getElementById(\"fps\").innerHTML = `FPS:${fps} ${kd}`;\n            frame = 0;\n            lastTime = now;\n        };\n\n        rAF(loop);\n    }\n\n    loop();\n} else {\n    document.getElementById(\"fps\").style = \"display:none!important\"\n}"},{"_content":"//动态标题\nvar OriginTitile = document.title;\nvar titleTime;\ndocument.addEventListener('visibilitychange', function () {\n  if (document.hidden) {\n    //离开当前页面时标签显示内容\n    document.title = '👀你干甚去了,额真想锤死腻~';\n    clearTimeout(titleTime);\n  } else {\n    //返回当前页面时标签显示内容\n    document.title = '🐖哟~逮到你咯小老板～';\n    //两秒后变回正常标题\n    titleTime = setTimeout(function () {\n      document.title = OriginTitile;\n    }, 2000);\n  }\n});","source":"js/title.js","raw":"//动态标题\nvar OriginTitile = document.title;\nvar titleTime;\ndocument.addEventListener('visibilitychange', function () {\n  if (document.hidden) {\n    //离开当前页面时标签显示内容\n    document.title = '👀你干甚去了,额真想锤死腻~';\n    clearTimeout(titleTime);\n  } else {\n    //返回当前页面时标签显示内容\n    document.title = '🐖哟~逮到你咯小老板～';\n    //两秒后变回正常标题\n    titleTime = setTimeout(function () {\n      document.title = OriginTitile;\n    }, 2000);\n  }\n});","date":"2024-12-26T02:52:23.402Z","updated":"2024-12-26T02:52:23.402Z","path":"js/title.js","layout":"false","title":"","comments":1,"_id":"cmd5f7cqt000iiku4huvfarfp","content":"//动态标题\nvar OriginTitile = document.title;\nvar titleTime;\ndocument.addEventListener('visibilitychange', function () {\n  if (document.hidden) {\n    //离开当前页面时标签显示内容\n    document.title = '👀你干甚去了,额真想锤死腻~';\n    clearTimeout(titleTime);\n  } else {\n    //返回当前页面时标签显示内容\n    document.title = '🐖哟~逮到你咯小老板～';\n    //两秒后变回正常标题\n    titleTime = setTimeout(function () {\n      document.title = OriginTitile;\n    }, 2000);\n  }\n});"},{"_content":"function dark() {window.requestAnimationFrame=window.requestAnimationFrame||window.mozRequestAnimationFrame||window.webkitRequestAnimationFrame||window.msRequestAnimationFrame;var n,e,i,h,t=.05,s=document.getElementById(\"universe\"),o=!0,a=\"180,184,240\",r=\"226,225,142\",d=\"226,225,224\",c=[];function f(){n=window.innerWidth,e=window.innerHeight,i=.216*n,s.setAttribute(\"width\",n),s.setAttribute(\"height\",e)}function u(){h.clearRect(0,0,n,e);for(var t=c.length,i=0;i<t;i++){var s=c[i];s.move(),s.fadeIn(),s.fadeOut(),s.draw()}}function y(){this.reset=function(){this.giant=m(3),this.comet=!this.giant&&!o&&m(10),this.x=l(0,n-10),this.y=l(0,e),this.r=l(1.1,2.6),this.dx=l(t,6*t)+(this.comet+1-1)*t*l(50,120)+2*t,this.dy=-l(t,6*t)-(this.comet+1-1)*t*l(50,120),this.fadingOut=null,this.fadingIn=!0,this.opacity=0,this.opacityTresh=l(.2,1-.4*(this.comet+1-1)),this.do=l(5e-4,.002)+.001*(this.comet+1-1)},this.fadeIn=function(){this.fadingIn&&(this.fadingIn=!(this.opacity>this.opacityTresh),this.opacity+=this.do)},this.fadeOut=function(){this.fadingOut&&(this.fadingOut=!(this.opacity<0),this.opacity-=this.do/2,(this.x>n||this.y<0)&&(this.fadingOut=!1,this.reset()))},this.draw=function(){if(h.beginPath(),this.giant)h.fillStyle=\"rgba(\"+a+\",\"+this.opacity+\")\",h.arc(this.x,this.y,2,0,2*Math.PI,!1);else if(this.comet){h.fillStyle=\"rgba(\"+d+\",\"+this.opacity+\")\",h.arc(this.x,this.y,1.5,0,2*Math.PI,!1);for(var t=0;t<30;t++)h.fillStyle=\"rgba(\"+d+\",\"+(this.opacity-this.opacity/20*t)+\")\",h.rect(this.x-this.dx/4*t,this.y-this.dy/4*t-2,2,2),h.fill()}else h.fillStyle=\"rgba(\"+r+\",\"+this.opacity+\")\",h.rect(this.x,this.y,this.r,this.r);h.closePath(),h.fill()},this.move=function(){this.x+=this.dx,this.y+=this.dy,!1===this.fadingOut&&this.reset(),(this.x>n-n/4||this.y<0)&&(this.fadingOut=!0)},setTimeout(function(){o=!1},50)}function m(t){return Math.floor(1e3*Math.random())+1<10*t}function l(t,i){return Math.random()*(i-t)+t}f(),window.addEventListener(\"resize\",f,!1),function(){h=s.getContext(\"2d\");for(var t=0;t<i;t++)c[t]=new y,c[t].reset();u()}(),function t(){document.getElementsByTagName('html')[0].getAttribute('data-theme')=='dark'&&u(),window.requestAnimationFrame(t)}()};\ndark()","source":"js/universe.js","raw":"function dark() {window.requestAnimationFrame=window.requestAnimationFrame||window.mozRequestAnimationFrame||window.webkitRequestAnimationFrame||window.msRequestAnimationFrame;var n,e,i,h,t=.05,s=document.getElementById(\"universe\"),o=!0,a=\"180,184,240\",r=\"226,225,142\",d=\"226,225,224\",c=[];function f(){n=window.innerWidth,e=window.innerHeight,i=.216*n,s.setAttribute(\"width\",n),s.setAttribute(\"height\",e)}function u(){h.clearRect(0,0,n,e);for(var t=c.length,i=0;i<t;i++){var s=c[i];s.move(),s.fadeIn(),s.fadeOut(),s.draw()}}function y(){this.reset=function(){this.giant=m(3),this.comet=!this.giant&&!o&&m(10),this.x=l(0,n-10),this.y=l(0,e),this.r=l(1.1,2.6),this.dx=l(t,6*t)+(this.comet+1-1)*t*l(50,120)+2*t,this.dy=-l(t,6*t)-(this.comet+1-1)*t*l(50,120),this.fadingOut=null,this.fadingIn=!0,this.opacity=0,this.opacityTresh=l(.2,1-.4*(this.comet+1-1)),this.do=l(5e-4,.002)+.001*(this.comet+1-1)},this.fadeIn=function(){this.fadingIn&&(this.fadingIn=!(this.opacity>this.opacityTresh),this.opacity+=this.do)},this.fadeOut=function(){this.fadingOut&&(this.fadingOut=!(this.opacity<0),this.opacity-=this.do/2,(this.x>n||this.y<0)&&(this.fadingOut=!1,this.reset()))},this.draw=function(){if(h.beginPath(),this.giant)h.fillStyle=\"rgba(\"+a+\",\"+this.opacity+\")\",h.arc(this.x,this.y,2,0,2*Math.PI,!1);else if(this.comet){h.fillStyle=\"rgba(\"+d+\",\"+this.opacity+\")\",h.arc(this.x,this.y,1.5,0,2*Math.PI,!1);for(var t=0;t<30;t++)h.fillStyle=\"rgba(\"+d+\",\"+(this.opacity-this.opacity/20*t)+\")\",h.rect(this.x-this.dx/4*t,this.y-this.dy/4*t-2,2,2),h.fill()}else h.fillStyle=\"rgba(\"+r+\",\"+this.opacity+\")\",h.rect(this.x,this.y,this.r,this.r);h.closePath(),h.fill()},this.move=function(){this.x+=this.dx,this.y+=this.dy,!1===this.fadingOut&&this.reset(),(this.x>n-n/4||this.y<0)&&(this.fadingOut=!0)},setTimeout(function(){o=!1},50)}function m(t){return Math.floor(1e3*Math.random())+1<10*t}function l(t,i){return Math.random()*(i-t)+t}f(),window.addEventListener(\"resize\",f,!1),function(){h=s.getContext(\"2d\");for(var t=0;t<i;t++)c[t]=new y,c[t].reset();u()}(),function t(){document.getElementsByTagName('html')[0].getAttribute('data-theme')=='dark'&&u(),window.requestAnimationFrame(t)}()};\ndark()","date":"2024-12-08T14:02:35.813Z","updated":"2024-12-08T14:02:35.813Z","path":"js/universe.js","layout":"false","title":"","comments":1,"_id":"cmd5f7cqu000liku4drvk38fk","content":"function dark() {window.requestAnimationFrame=window.requestAnimationFrame||window.mozRequestAnimationFrame||window.webkitRequestAnimationFrame||window.msRequestAnimationFrame;var n,e,i,h,t=.05,s=document.getElementById(\"universe\"),o=!0,a=\"180,184,240\",r=\"226,225,142\",d=\"226,225,224\",c=[];function f(){n=window.innerWidth,e=window.innerHeight,i=.216*n,s.setAttribute(\"width\",n),s.setAttribute(\"height\",e)}function u(){h.clearRect(0,0,n,e);for(var t=c.length,i=0;i<t;i++){var s=c[i];s.move(),s.fadeIn(),s.fadeOut(),s.draw()}}function y(){this.reset=function(){this.giant=m(3),this.comet=!this.giant&&!o&&m(10),this.x=l(0,n-10),this.y=l(0,e),this.r=l(1.1,2.6),this.dx=l(t,6*t)+(this.comet+1-1)*t*l(50,120)+2*t,this.dy=-l(t,6*t)-(this.comet+1-1)*t*l(50,120),this.fadingOut=null,this.fadingIn=!0,this.opacity=0,this.opacityTresh=l(.2,1-.4*(this.comet+1-1)),this.do=l(5e-4,.002)+.001*(this.comet+1-1)},this.fadeIn=function(){this.fadingIn&&(this.fadingIn=!(this.opacity>this.opacityTresh),this.opacity+=this.do)},this.fadeOut=function(){this.fadingOut&&(this.fadingOut=!(this.opacity<0),this.opacity-=this.do/2,(this.x>n||this.y<0)&&(this.fadingOut=!1,this.reset()))},this.draw=function(){if(h.beginPath(),this.giant)h.fillStyle=\"rgba(\"+a+\",\"+this.opacity+\")\",h.arc(this.x,this.y,2,0,2*Math.PI,!1);else if(this.comet){h.fillStyle=\"rgba(\"+d+\",\"+this.opacity+\")\",h.arc(this.x,this.y,1.5,0,2*Math.PI,!1);for(var t=0;t<30;t++)h.fillStyle=\"rgba(\"+d+\",\"+(this.opacity-this.opacity/20*t)+\")\",h.rect(this.x-this.dx/4*t,this.y-this.dy/4*t-2,2,2),h.fill()}else h.fillStyle=\"rgba(\"+r+\",\"+this.opacity+\")\",h.rect(this.x,this.y,this.r,this.r);h.closePath(),h.fill()},this.move=function(){this.x+=this.dx,this.y+=this.dy,!1===this.fadingOut&&this.reset(),(this.x>n-n/4||this.y<0)&&(this.fadingOut=!0)},setTimeout(function(){o=!1},50)}function m(t){return Math.floor(1e3*Math.random())+1<10*t}function l(t,i){return Math.random()*(i-t)+t}f(),window.addEventListener(\"resize\",f,!1),function(){h=s.getContext(\"2d\");for(var t=0;t<i;t++)c[t]=new y,c[t].reset();u()}(),function t(){document.getElementsByTagName('html')[0].getAttribute('data-theme')=='dark'&&u(),window.requestAnimationFrame(t)}()};\ndark()"},{"title":"音乐","date":"2024-12-04T15:10:00.000Z","top_img":"/images/music_bg.png","_content":"","source":"music/index.md","raw":"---\ntitle: 音乐\ndate: 2024-12-04 23:10:00\ntop_img: /images/music_bg.png\n---","updated":"2024-12-06T06:03:13.701Z","path":"music/index.html","comments":1,"layout":"page","_id":"cmd5f7cqw000piku4caq7fz3e","content":"","cover":false,"excerpt":"","more":""},{"title":"标签","date":"2024-12-04T14:51:20.000Z","type":"tags","top_img":"/images/tag_bg.jpg","_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ndate: 2024-12-04 22:51:20\ntype: \"tags\"\ntop_img: /images/tag_bg.jpg\n---\n","updated":"2024-12-05T16:33:31.350Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cmd5f7cqx000siku4ckza9nu7","content":"","cover":false,"excerpt":"","more":""},{"title":"照片","date":"2024-12-04T15:10:00.000Z","layout":"gallery","top_img":"/images/gallery_bg.png","_content":"\n# 时不时的偷偷扔几张照片到这里来！\n\n## 2025了，入坑这半年多，选一些还算凑活的照片放出来吧\n\n## 梦的开始...\n\n### 2024.5.20那天，在观望了两三个月后，终于出手了，人生的第一台相机---ZVE10\n\n{% gallery %}\n![zve10](http://picbed.yanzu.tech/img/camera/other/1.jpg)\n![zve10没取景器](http://picbed.yanzu.tech/img/camera/other/2.jpg)\n![zve10狗屎菜单](http://picbed.yanzu.tech/img/camera/other/3.jpg)\n{% endgallery %}\n\n### 一开始还是蛮新鲜的，用了一段时间后，索尼的通病就开始发作了。。。。屏幕色彩，狗屎的操作界面，虽说可以设置快捷键，但真的不方便啊喂\n\n### 拿到相机之后也没怎么出去拍，定焦能拍的东西太有限了，\n\n{% gallery %}\n![风筝蛇，第一张调色的照片！稍微远点的，50mm等效75真的拍不到欸](http://picbed.yanzu.tech/img/camera/1/7.jpg)\n\n![阿拉斯加？真肥啊，第一次拿着相机去扫街，那天独自去一个什么公园溜达来着](http://picbed.yanzu.tech/img/camera/1/8.jpg)\n\n![什么阴间调色啊，第一次扫街遇到的喵桑，没东西给他吃他已经不耐烦了](http://picbed.yanzu.tech/img/camera/1/9.jpg)\n\n![cool，可能快门速度还是慢了不够清晰，不过但从构图来说还是能看的](http://picbed.yanzu.tech/img/camera/1/10.jpg)\n\n![少见多怪，东北特有？倒着骑三轮，大一刚到沈阳就看到了，当时还多惊讶的](http://picbed.yanzu.tech/img/camera/1/11.jpg)\n\n![蓝调时刻不是很蓝，大烟囱，杀死那个石家庄人？](http://picbed.yanzu.tech/img/camera/1/12.jpg)\n{% endgallery %}\n\n\n## 大连呆的那两天，，，\n{% gallery %}\n![欧了，星海广场海边的海鸥，咋一看以为中间这只海鸥叼着什么，实际上是另外一只海鸥，索尼的对焦确实还不错](http://picbed.yanzu.tech/img/camera/1/1.jpg)\n\n![饱和度战士！也是星海广场(初代饱和度战士)](http://picbed.yanzu.tech/img/camera/1/2.jpg)\n\n![我挺喜欢这张的，不管是构图还是调色，偷拍钓鱼佬，虽然这张调色也有饱和度战士的嫌疑，不过我还是比较喜欢这个色](http://picbed.yanzu.tech/img/camera/1/3.jpg)\n\n![饱和度战士！](http://picbed.yanzu.tech/img/camera/1/4.jpg)\n\n![饱！](http://picbed.yanzu.tech/img/camera/1/5.jpg)\n\n![歪瑞古德，这个大叔的右手臂内侧有刺青，是为“效忠父母”，可惜没拍到](http://picbed.yanzu.tech/img/camera/1/6.jpg)\n{% endgallery %}\n\n## Nikon佬正式上线，\n### 24年8月9号，心心念念的尼康终于拿到了，记得观望的那两三个月，天天听战歌，一位风光佬即将崛起\n\n{% gallery %}\n![一开始当然是经典的套机镜头1650(穷鬼三剑客之一)。不过，尼康无狗头，这颗1650吊打某尼的1650](http://picbed.yanzu.tech/img/camera/2/1.jpg)\n![16-50](http://picbed.yanzu.tech/img/camera/2/2.jpg)\n![Z50，有取景器，有模式转盘，有触屏，操作很便捷，总结起来就一个字：6](http://picbed.yanzu.tech/img/camera/2/3.jpg)\n![拿到手了自然是拍拍拍，第一位受害人---喵桑](http://picbed.yanzu.tech/img/camera/2/4.jpg)\n![调色稍微正常了点](http://picbed.yanzu.tech/img/camera/2/5.jpg)\n![冷暖的对比还不错欸](http://picbed.yanzu.tech/img/camera/2/6.jpg)\n![天黑不回家](http://picbed.yanzu.tech/img/camera/2/7.jpg)\n![要是有个人往上走就好了](http://picbed.yanzu.tech/img/camera/2/8.jpg)\n![笑死我了](http://picbed.yanzu.tech/img/camera/2/9.jpg)\n![emm](http://picbed.yanzu.tech/img/camera/2/10.jpg)\n![比较喜欢的一张，绝了](http://picbed.yanzu.tech/img/camera/2/11.jpg)\n![蓝调时刻+1](http://picbed.yanzu.tech/img/camera/2/12.jpg)\n![摄影的尽头，nikon佬的本职工作](http://picbed.yanzu.tech/img/camera/2/13.jpg)\n![可以](http://picbed.yanzu.tech/img/camera/2/14.jpg)\n![笑狗常开](http://picbed.yanzu.tech/img/camera/2/15.jpg)\n![鹅鹅鹅鹅鹅鹅！](http://picbed.yanzu.tech/img/camera/2/16.jpg)\n![拿到尼康后，第一次去拍日出(没三脚架)](http://picbed.yanzu.tech/img/camera/2/17.jpg)\n![回想起我的中学生活了，羡慕啊](http://picbed.yanzu.tech/img/camera/2/18.jpg)\n![还行](http://picbed.yanzu.tech/img/camera/2/19.jpg)\n![到新环境后，第一次出去溜达拍照](http://picbed.yanzu.tech/img/camera/2/20.jpg)\n![柳枝间有一弯月亮就好了](http://picbed.yanzu.tech/img/camera/2/21.jpg)\n![并不好看，仅作为铺垫，为后文埋下伏笔](http://picbed.yanzu.tech/img/camera/2/22.jpg)\n{% endgallery %}\n\n### 穷玩三剑客之二，50-250到手了\n{% gallery %}\n![一家人整整齐齐的](http://picbed.yanzu.tech/img/camera/2/23.jpg)\n![还是挺不错的，等效375可打鸟可打月亮](http://picbed.yanzu.tech/img/camera/2/24.jpg)\n{% endgallery %}\n\n### 中秋来了，打月亮\n{% gallery %}\n![小试牛刀](http://picbed.yanzu.tech/img/camera/2/25.jpg)\n![冷月](http://picbed.yanzu.tech/img/camera/2/26.jpg)\n![颜色没调对，白平衡的锅？](http://picbed.yanzu.tech/img/camera/2/27.jpg)\n![可以](http://picbed.yanzu.tech/img/camera/2/28.jpg)\n![她也在拍那个太阳下山！](http://picbed.yanzu.tech/img/camera/2/29.jpg)\n![他也在拍太阳落山！](http://picbed.yanzu.tech/img/camera/2/30.jpg)\n![中秋的月亮](http://picbed.yanzu.tech/img/camera/2/31.jpg)\n![咸鸭蛋](http://picbed.yanzu.tech/img/camera/2/32.jpg)\n![还行](http://picbed.yanzu.tech/img/camera/2/33.jpg)\n![有点糊，不晓得是不是UV镜的锅](http://picbed.yanzu.tech/img/camera/2/34.jpg)\n![中秋节第二天晚上的月亮](http://picbed.yanzu.tech/img/camera/2/35.jpg)\n![不是咸鸭蛋了](http://picbed.yanzu.tech/img/camera/2/36.jpg)\n{% endgallery %}\n\n### 新阶段~\n{% gallery %}\n![目中无人，Nikon](http://picbed.yanzu.tech/img/camera/2/37.jpg)\n![太阳爆炸了？](http://picbed.yanzu.tech/img/camera/2/38.jpg)\n![这个色调好看](http://picbed.yanzu.tech/img/camera/2/39.jpg)\n![像一幅水墨画](http://picbed.yanzu.tech/img/camera/2/40.jpg)\n![山顶老喵，智慧的眼神](http://picbed.yanzu.tech/img/camera/2/41.jpg)\n![狭路相逢！什么，一开始我根本没注意到草丛里还有一只奶牛猫，我以为大橘无端发怒](http://picbed.yanzu.tech/img/camera/2/42.jpg)\n![奶牛猫滚出了草丛](http://picbed.yanzu.tech/img/camera/2/43.jpg)\n![嫉妒，使喵面目全非](http://picbed.yanzu.tech/img/camera/2/44.jpg)\n![第一张黑白色调](http://picbed.yanzu.tech/img/camera/2/45.jpg)\n![第二张](http://picbed.yanzu.tech/img/camera/2/46.jpg)\n![色彩很鲜明](http://picbed.yanzu.tech/img/camera/2/47.jpg)\n![捕获人类幼崽](http://picbed.yanzu.tech/img/camera/2/48.jpg)\n![年轻就是好](http://picbed.yanzu.tech/img/camera/2/49.jpg)\n![你很凶哦](http://picbed.yanzu.tech/img/camera/2/50.jpg)\n![不清晰但好看](http://picbed.yanzu.tech/img/camera/2/51.jpg)\n{% endgallery %}\n\n### 入秋了\n{% gallery %}\n![胶片感](http://picbed.yanzu.tech/img/camera/2/52.jpg)\n![cool](http://picbed.yanzu.tech/img/camera/2/53.jpg)\n![起飞](http://picbed.yanzu.tech/img/camera/2/54.jpg)\n![偷感很重的哈基咪](http://picbed.yanzu.tech/img/camera/2/55.jpg)\n![偷感加重的哈基咪](http://picbed.yanzu.tech/img/camera/2/56.jpg)\n{% endgallery %}","source":"gallery/index.md","raw":"---\ntitle: 照片\ndate: 2024-12-04 23:10:00\nlayout: gallery  # 确保此处使用正确的 layout\ntop_img: /images/gallery_bg.png\n---\n\n# 时不时的偷偷扔几张照片到这里来！\n\n## 2025了，入坑这半年多，选一些还算凑活的照片放出来吧\n\n## 梦的开始...\n\n### 2024.5.20那天，在观望了两三个月后，终于出手了，人生的第一台相机---ZVE10\n\n{% gallery %}\n![zve10](http://picbed.yanzu.tech/img/camera/other/1.jpg)\n![zve10没取景器](http://picbed.yanzu.tech/img/camera/other/2.jpg)\n![zve10狗屎菜单](http://picbed.yanzu.tech/img/camera/other/3.jpg)\n{% endgallery %}\n\n### 一开始还是蛮新鲜的，用了一段时间后，索尼的通病就开始发作了。。。。屏幕色彩，狗屎的操作界面，虽说可以设置快捷键，但真的不方便啊喂\n\n### 拿到相机之后也没怎么出去拍，定焦能拍的东西太有限了，\n\n{% gallery %}\n![风筝蛇，第一张调色的照片！稍微远点的，50mm等效75真的拍不到欸](http://picbed.yanzu.tech/img/camera/1/7.jpg)\n\n![阿拉斯加？真肥啊，第一次拿着相机去扫街，那天独自去一个什么公园溜达来着](http://picbed.yanzu.tech/img/camera/1/8.jpg)\n\n![什么阴间调色啊，第一次扫街遇到的喵桑，没东西给他吃他已经不耐烦了](http://picbed.yanzu.tech/img/camera/1/9.jpg)\n\n![cool，可能快门速度还是慢了不够清晰，不过但从构图来说还是能看的](http://picbed.yanzu.tech/img/camera/1/10.jpg)\n\n![少见多怪，东北特有？倒着骑三轮，大一刚到沈阳就看到了，当时还多惊讶的](http://picbed.yanzu.tech/img/camera/1/11.jpg)\n\n![蓝调时刻不是很蓝，大烟囱，杀死那个石家庄人？](http://picbed.yanzu.tech/img/camera/1/12.jpg)\n{% endgallery %}\n\n\n## 大连呆的那两天，，，\n{% gallery %}\n![欧了，星海广场海边的海鸥，咋一看以为中间这只海鸥叼着什么，实际上是另外一只海鸥，索尼的对焦确实还不错](http://picbed.yanzu.tech/img/camera/1/1.jpg)\n\n![饱和度战士！也是星海广场(初代饱和度战士)](http://picbed.yanzu.tech/img/camera/1/2.jpg)\n\n![我挺喜欢这张的，不管是构图还是调色，偷拍钓鱼佬，虽然这张调色也有饱和度战士的嫌疑，不过我还是比较喜欢这个色](http://picbed.yanzu.tech/img/camera/1/3.jpg)\n\n![饱和度战士！](http://picbed.yanzu.tech/img/camera/1/4.jpg)\n\n![饱！](http://picbed.yanzu.tech/img/camera/1/5.jpg)\n\n![歪瑞古德，这个大叔的右手臂内侧有刺青，是为“效忠父母”，可惜没拍到](http://picbed.yanzu.tech/img/camera/1/6.jpg)\n{% endgallery %}\n\n## Nikon佬正式上线，\n### 24年8月9号，心心念念的尼康终于拿到了，记得观望的那两三个月，天天听战歌，一位风光佬即将崛起\n\n{% gallery %}\n![一开始当然是经典的套机镜头1650(穷鬼三剑客之一)。不过，尼康无狗头，这颗1650吊打某尼的1650](http://picbed.yanzu.tech/img/camera/2/1.jpg)\n![16-50](http://picbed.yanzu.tech/img/camera/2/2.jpg)\n![Z50，有取景器，有模式转盘，有触屏，操作很便捷，总结起来就一个字：6](http://picbed.yanzu.tech/img/camera/2/3.jpg)\n![拿到手了自然是拍拍拍，第一位受害人---喵桑](http://picbed.yanzu.tech/img/camera/2/4.jpg)\n![调色稍微正常了点](http://picbed.yanzu.tech/img/camera/2/5.jpg)\n![冷暖的对比还不错欸](http://picbed.yanzu.tech/img/camera/2/6.jpg)\n![天黑不回家](http://picbed.yanzu.tech/img/camera/2/7.jpg)\n![要是有个人往上走就好了](http://picbed.yanzu.tech/img/camera/2/8.jpg)\n![笑死我了](http://picbed.yanzu.tech/img/camera/2/9.jpg)\n![emm](http://picbed.yanzu.tech/img/camera/2/10.jpg)\n![比较喜欢的一张，绝了](http://picbed.yanzu.tech/img/camera/2/11.jpg)\n![蓝调时刻+1](http://picbed.yanzu.tech/img/camera/2/12.jpg)\n![摄影的尽头，nikon佬的本职工作](http://picbed.yanzu.tech/img/camera/2/13.jpg)\n![可以](http://picbed.yanzu.tech/img/camera/2/14.jpg)\n![笑狗常开](http://picbed.yanzu.tech/img/camera/2/15.jpg)\n![鹅鹅鹅鹅鹅鹅！](http://picbed.yanzu.tech/img/camera/2/16.jpg)\n![拿到尼康后，第一次去拍日出(没三脚架)](http://picbed.yanzu.tech/img/camera/2/17.jpg)\n![回想起我的中学生活了，羡慕啊](http://picbed.yanzu.tech/img/camera/2/18.jpg)\n![还行](http://picbed.yanzu.tech/img/camera/2/19.jpg)\n![到新环境后，第一次出去溜达拍照](http://picbed.yanzu.tech/img/camera/2/20.jpg)\n![柳枝间有一弯月亮就好了](http://picbed.yanzu.tech/img/camera/2/21.jpg)\n![并不好看，仅作为铺垫，为后文埋下伏笔](http://picbed.yanzu.tech/img/camera/2/22.jpg)\n{% endgallery %}\n\n### 穷玩三剑客之二，50-250到手了\n{% gallery %}\n![一家人整整齐齐的](http://picbed.yanzu.tech/img/camera/2/23.jpg)\n![还是挺不错的，等效375可打鸟可打月亮](http://picbed.yanzu.tech/img/camera/2/24.jpg)\n{% endgallery %}\n\n### 中秋来了，打月亮\n{% gallery %}\n![小试牛刀](http://picbed.yanzu.tech/img/camera/2/25.jpg)\n![冷月](http://picbed.yanzu.tech/img/camera/2/26.jpg)\n![颜色没调对，白平衡的锅？](http://picbed.yanzu.tech/img/camera/2/27.jpg)\n![可以](http://picbed.yanzu.tech/img/camera/2/28.jpg)\n![她也在拍那个太阳下山！](http://picbed.yanzu.tech/img/camera/2/29.jpg)\n![他也在拍太阳落山！](http://picbed.yanzu.tech/img/camera/2/30.jpg)\n![中秋的月亮](http://picbed.yanzu.tech/img/camera/2/31.jpg)\n![咸鸭蛋](http://picbed.yanzu.tech/img/camera/2/32.jpg)\n![还行](http://picbed.yanzu.tech/img/camera/2/33.jpg)\n![有点糊，不晓得是不是UV镜的锅](http://picbed.yanzu.tech/img/camera/2/34.jpg)\n![中秋节第二天晚上的月亮](http://picbed.yanzu.tech/img/camera/2/35.jpg)\n![不是咸鸭蛋了](http://picbed.yanzu.tech/img/camera/2/36.jpg)\n{% endgallery %}\n\n### 新阶段~\n{% gallery %}\n![目中无人，Nikon](http://picbed.yanzu.tech/img/camera/2/37.jpg)\n![太阳爆炸了？](http://picbed.yanzu.tech/img/camera/2/38.jpg)\n![这个色调好看](http://picbed.yanzu.tech/img/camera/2/39.jpg)\n![像一幅水墨画](http://picbed.yanzu.tech/img/camera/2/40.jpg)\n![山顶老喵，智慧的眼神](http://picbed.yanzu.tech/img/camera/2/41.jpg)\n![狭路相逢！什么，一开始我根本没注意到草丛里还有一只奶牛猫，我以为大橘无端发怒](http://picbed.yanzu.tech/img/camera/2/42.jpg)\n![奶牛猫滚出了草丛](http://picbed.yanzu.tech/img/camera/2/43.jpg)\n![嫉妒，使喵面目全非](http://picbed.yanzu.tech/img/camera/2/44.jpg)\n![第一张黑白色调](http://picbed.yanzu.tech/img/camera/2/45.jpg)\n![第二张](http://picbed.yanzu.tech/img/camera/2/46.jpg)\n![色彩很鲜明](http://picbed.yanzu.tech/img/camera/2/47.jpg)\n![捕获人类幼崽](http://picbed.yanzu.tech/img/camera/2/48.jpg)\n![年轻就是好](http://picbed.yanzu.tech/img/camera/2/49.jpg)\n![你很凶哦](http://picbed.yanzu.tech/img/camera/2/50.jpg)\n![不清晰但好看](http://picbed.yanzu.tech/img/camera/2/51.jpg)\n{% endgallery %}\n\n### 入秋了\n{% gallery %}\n![胶片感](http://picbed.yanzu.tech/img/camera/2/52.jpg)\n![cool](http://picbed.yanzu.tech/img/camera/2/53.jpg)\n![起飞](http://picbed.yanzu.tech/img/camera/2/54.jpg)\n![偷感很重的哈基咪](http://picbed.yanzu.tech/img/camera/2/55.jpg)\n![偷感加重的哈基咪](http://picbed.yanzu.tech/img/camera/2/56.jpg)\n{% endgallery %}","updated":"2025-01-01T13:50:57.412Z","path":"gallery/index.html","comments":1,"_id":"cmd5f7cqz000wiku43bg31up6","content":"<h1 id=\"时不时的偷偷扔几张照片到这里来！\"><a href=\"#时不时的偷偷扔几张照片到这里来！\" class=\"headerlink\" title=\"时不时的偷偷扔几张照片到这里来！\"></a>时不时的偷偷扔几张照片到这里来！</h1><h2 id=\"2025了，入坑这半年多，选一些还算凑活的照片放出来吧\"><a href=\"#2025了，入坑这半年多，选一些还算凑活的照片放出来吧\" class=\"headerlink\" title=\"2025了，入坑这半年多，选一些还算凑活的照片放出来吧\"></a>2025了，入坑这半年多，选一些还算凑活的照片放出来吧</h2><h2 id=\"梦的开始…\"><a href=\"#梦的开始…\" class=\"headerlink\" title=\"梦的开始…\"></a>梦的开始…</h2><h3 id=\"2024-5-20那天，在观望了两三个月后，终于出手了，人生的第一台相机—ZVE10\"><a href=\"#2024-5-20那天，在观望了两三个月后，终于出手了，人生的第一台相机—ZVE10\" class=\"headerlink\" title=\"2024.5.20那天，在观望了两三个月后，终于出手了，人生的第一台相机—ZVE10\"></a>2024.5.20那天，在观望了两三个月后，终于出手了，人生的第一台相机—ZVE10</h3><div class=\"gallery-container\" data-type=\"data\" data-button=\"\">\n      <div class=\"gallery-items\">[{\"url\":\"http://picbed.yanzu.tech/img/camera/other/1.jpg\",\"alt\":\"zve10\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/other/2.jpg\",\"alt\":\"zve10没取景器\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/other/3.jpg\",\"alt\":\"zve10狗屎菜单\"}]</div>\n    </div>\n\n<h3 id=\"一开始还是蛮新鲜的，用了一段时间后，索尼的通病就开始发作了。。。。屏幕色彩，狗屎的操作界面，虽说可以设置快捷键，但真的不方便啊喂\"><a href=\"#一开始还是蛮新鲜的，用了一段时间后，索尼的通病就开始发作了。。。。屏幕色彩，狗屎的操作界面，虽说可以设置快捷键，但真的不方便啊喂\" class=\"headerlink\" title=\"一开始还是蛮新鲜的，用了一段时间后，索尼的通病就开始发作了。。。。屏幕色彩，狗屎的操作界面，虽说可以设置快捷键，但真的不方便啊喂\"></a>一开始还是蛮新鲜的，用了一段时间后，索尼的通病就开始发作了。。。。屏幕色彩，狗屎的操作界面，虽说可以设置快捷键，但真的不方便啊喂</h3><h3 id=\"拿到相机之后也没怎么出去拍，定焦能拍的东西太有限了，\"><a href=\"#拿到相机之后也没怎么出去拍，定焦能拍的东西太有限了，\" class=\"headerlink\" title=\"拿到相机之后也没怎么出去拍，定焦能拍的东西太有限了，\"></a>拿到相机之后也没怎么出去拍，定焦能拍的东西太有限了，</h3><div class=\"gallery-container\" data-type=\"data\" data-button=\"\">\n      <div class=\"gallery-items\">[{\"url\":\"http://picbed.yanzu.tech/img/camera/1/7.jpg\",\"alt\":\"风筝蛇，第一张调色的照片！稍微远点的，50mm等效75真的拍不到欸\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/8.jpg\",\"alt\":\"阿拉斯加？真肥啊，第一次拿着相机去扫街，那天独自去一个什么公园溜达来着\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/9.jpg\",\"alt\":\"什么阴间调色啊，第一次扫街遇到的喵桑，没东西给他吃他已经不耐烦了\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/10.jpg\",\"alt\":\"cool，可能快门速度还是慢了不够清晰，不过但从构图来说还是能看的\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/11.jpg\",\"alt\":\"少见多怪，东北特有？倒着骑三轮，大一刚到沈阳就看到了，当时还多惊讶的\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/12.jpg\",\"alt\":\"蓝调时刻不是很蓝，大烟囱，杀死那个石家庄人？\"}]</div>\n    </div>\n\n\n<h2 id=\"大连呆的那两天，，，\"><a href=\"#大连呆的那两天，，，\" class=\"headerlink\" title=\"大连呆的那两天，，，\"></a>大连呆的那两天，，，</h2><div class=\"gallery-container\" data-type=\"data\" data-button=\"\">\n      <div class=\"gallery-items\">[{\"url\":\"http://picbed.yanzu.tech/img/camera/1/1.jpg\",\"alt\":\"欧了，星海广场海边的海鸥，咋一看以为中间这只海鸥叼着什么，实际上是另外一只海鸥，索尼的对焦确实还不错\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/2.jpg\",\"alt\":\"饱和度战士！也是星海广场(初代饱和度战士)\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/3.jpg\",\"alt\":\"我挺喜欢这张的，不管是构图还是调色，偷拍钓鱼佬，虽然这张调色也有饱和度战士的嫌疑，不过我还是比较喜欢这个色\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/4.jpg\",\"alt\":\"饱和度战士！\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/5.jpg\",\"alt\":\"饱！\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/6.jpg\",\"alt\":\"歪瑞古德，这个大叔的右手臂内侧有刺青，是为“效忠父母”，可惜没拍到\"}]</div>\n    </div>\n\n<h2 id=\"Nikon佬正式上线，\"><a href=\"#Nikon佬正式上线，\" class=\"headerlink\" title=\"Nikon佬正式上线，\"></a>Nikon佬正式上线，</h2><h3 id=\"24年8月9号，心心念念的尼康终于拿到了，记得观望的那两三个月，天天听战歌，一位风光佬即将崛起\"><a href=\"#24年8月9号，心心念念的尼康终于拿到了，记得观望的那两三个月，天天听战歌，一位风光佬即将崛起\" class=\"headerlink\" title=\"24年8月9号，心心念念的尼康终于拿到了，记得观望的那两三个月，天天听战歌，一位风光佬即将崛起\"></a>24年8月9号，心心念念的尼康终于拿到了，记得观望的那两三个月，天天听战歌，一位风光佬即将崛起</h3><div class=\"gallery-container\" data-type=\"data\" data-button=\"\">\n      <div class=\"gallery-items\">[{\"url\":\"http://picbed.yanzu.tech/img/camera/2/1.jpg\",\"alt\":\"一开始当然是经典的套机镜头1650(穷鬼三剑客之一)。不过，尼康无狗头，这颗1650吊打某尼的1650\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/2.jpg\",\"alt\":\"16-50\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/3.jpg\",\"alt\":\"Z50，有取景器，有模式转盘，有触屏，操作很便捷，总结起来就一个字：6\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/4.jpg\",\"alt\":\"拿到手了自然是拍拍拍，第一位受害人---喵桑\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/5.jpg\",\"alt\":\"调色稍微正常了点\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/6.jpg\",\"alt\":\"冷暖的对比还不错欸\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/7.jpg\",\"alt\":\"天黑不回家\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/8.jpg\",\"alt\":\"要是有个人往上走就好了\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/9.jpg\",\"alt\":\"笑死我了\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/10.jpg\",\"alt\":\"emm\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/11.jpg\",\"alt\":\"比较喜欢的一张，绝了\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/12.jpg\",\"alt\":\"蓝调时刻+1\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/13.jpg\",\"alt\":\"摄影的尽头，nikon佬的本职工作\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/14.jpg\",\"alt\":\"可以\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/15.jpg\",\"alt\":\"笑狗常开\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/16.jpg\",\"alt\":\"鹅鹅鹅鹅鹅鹅！\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/17.jpg\",\"alt\":\"拿到尼康后，第一次去拍日出(没三脚架)\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/18.jpg\",\"alt\":\"回想起我的中学生活了，羡慕啊\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/19.jpg\",\"alt\":\"还行\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/20.jpg\",\"alt\":\"到新环境后，第一次出去溜达拍照\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/21.jpg\",\"alt\":\"柳枝间有一弯月亮就好了\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/22.jpg\",\"alt\":\"并不好看，仅作为铺垫，为后文埋下伏笔\"}]</div>\n    </div>\n\n<h3 id=\"穷玩三剑客之二，50-250到手了\"><a href=\"#穷玩三剑客之二，50-250到手了\" class=\"headerlink\" title=\"穷玩三剑客之二，50-250到手了\"></a>穷玩三剑客之二，50-250到手了</h3><div class=\"gallery-container\" data-type=\"data\" data-button=\"\">\n      <div class=\"gallery-items\">[{\"url\":\"http://picbed.yanzu.tech/img/camera/2/23.jpg\",\"alt\":\"一家人整整齐齐的\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/24.jpg\",\"alt\":\"还是挺不错的，等效375可打鸟可打月亮\"}]</div>\n    </div>\n\n<h3 id=\"中秋来了，打月亮\"><a href=\"#中秋来了，打月亮\" class=\"headerlink\" title=\"中秋来了，打月亮\"></a>中秋来了，打月亮</h3><div class=\"gallery-container\" data-type=\"data\" data-button=\"\">\n      <div class=\"gallery-items\">[{\"url\":\"http://picbed.yanzu.tech/img/camera/2/25.jpg\",\"alt\":\"小试牛刀\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/26.jpg\",\"alt\":\"冷月\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/27.jpg\",\"alt\":\"颜色没调对，白平衡的锅？\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/28.jpg\",\"alt\":\"可以\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/29.jpg\",\"alt\":\"她也在拍那个太阳下山！\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/30.jpg\",\"alt\":\"他也在拍太阳落山！\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/31.jpg\",\"alt\":\"中秋的月亮\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/32.jpg\",\"alt\":\"咸鸭蛋\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/33.jpg\",\"alt\":\"还行\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/34.jpg\",\"alt\":\"有点糊，不晓得是不是UV镜的锅\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/35.jpg\",\"alt\":\"中秋节第二天晚上的月亮\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/36.jpg\",\"alt\":\"不是咸鸭蛋了\"}]</div>\n    </div>\n\n<h3 id=\"新阶段\"><a href=\"#新阶段\" class=\"headerlink\" title=\"新阶段~\"></a>新阶段~</h3><div class=\"gallery-container\" data-type=\"data\" data-button=\"\">\n      <div class=\"gallery-items\">[{\"url\":\"http://picbed.yanzu.tech/img/camera/2/37.jpg\",\"alt\":\"目中无人，Nikon\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/38.jpg\",\"alt\":\"太阳爆炸了？\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/39.jpg\",\"alt\":\"这个色调好看\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/40.jpg\",\"alt\":\"像一幅水墨画\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/41.jpg\",\"alt\":\"山顶老喵，智慧的眼神\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/42.jpg\",\"alt\":\"狭路相逢！什么，一开始我根本没注意到草丛里还有一只奶牛猫，我以为大橘无端发怒\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/43.jpg\",\"alt\":\"奶牛猫滚出了草丛\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/44.jpg\",\"alt\":\"嫉妒，使喵面目全非\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/45.jpg\",\"alt\":\"第一张黑白色调\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/46.jpg\",\"alt\":\"第二张\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/47.jpg\",\"alt\":\"色彩很鲜明\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/48.jpg\",\"alt\":\"捕获人类幼崽\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/49.jpg\",\"alt\":\"年轻就是好\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/50.jpg\",\"alt\":\"你很凶哦\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/51.jpg\",\"alt\":\"不清晰但好看\"}]</div>\n    </div>\n\n<h3 id=\"入秋了\"><a href=\"#入秋了\" class=\"headerlink\" title=\"入秋了\"></a>入秋了</h3><div class=\"gallery-container\" data-type=\"data\" data-button=\"\">\n      <div class=\"gallery-items\">[{\"url\":\"http://picbed.yanzu.tech/img/camera/2/52.jpg\",\"alt\":\"胶片感\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/53.jpg\",\"alt\":\"cool\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/54.jpg\",\"alt\":\"起飞\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/55.jpg\",\"alt\":\"偷感很重的哈基咪\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/56.jpg\",\"alt\":\"偷感加重的哈基咪\"}]</div>\n    </div>","cover":false,"excerpt":"","more":"<h1 id=\"时不时的偷偷扔几张照片到这里来！\"><a href=\"#时不时的偷偷扔几张照片到这里来！\" class=\"headerlink\" title=\"时不时的偷偷扔几张照片到这里来！\"></a>时不时的偷偷扔几张照片到这里来！</h1><h2 id=\"2025了，入坑这半年多，选一些还算凑活的照片放出来吧\"><a href=\"#2025了，入坑这半年多，选一些还算凑活的照片放出来吧\" class=\"headerlink\" title=\"2025了，入坑这半年多，选一些还算凑活的照片放出来吧\"></a>2025了，入坑这半年多，选一些还算凑活的照片放出来吧</h2><h2 id=\"梦的开始…\"><a href=\"#梦的开始…\" class=\"headerlink\" title=\"梦的开始…\"></a>梦的开始…</h2><h3 id=\"2024-5-20那天，在观望了两三个月后，终于出手了，人生的第一台相机—ZVE10\"><a href=\"#2024-5-20那天，在观望了两三个月后，终于出手了，人生的第一台相机—ZVE10\" class=\"headerlink\" title=\"2024.5.20那天，在观望了两三个月后，终于出手了，人生的第一台相机—ZVE10\"></a>2024.5.20那天，在观望了两三个月后，终于出手了，人生的第一台相机—ZVE10</h3><div class=\"gallery-container\" data-type=\"data\" data-button=\"\">\n      <div class=\"gallery-items\">[{\"url\":\"http://picbed.yanzu.tech/img/camera/other/1.jpg\",\"alt\":\"zve10\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/other/2.jpg\",\"alt\":\"zve10没取景器\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/other/3.jpg\",\"alt\":\"zve10狗屎菜单\"}]</div>\n    </div>\n\n<h3 id=\"一开始还是蛮新鲜的，用了一段时间后，索尼的通病就开始发作了。。。。屏幕色彩，狗屎的操作界面，虽说可以设置快捷键，但真的不方便啊喂\"><a href=\"#一开始还是蛮新鲜的，用了一段时间后，索尼的通病就开始发作了。。。。屏幕色彩，狗屎的操作界面，虽说可以设置快捷键，但真的不方便啊喂\" class=\"headerlink\" title=\"一开始还是蛮新鲜的，用了一段时间后，索尼的通病就开始发作了。。。。屏幕色彩，狗屎的操作界面，虽说可以设置快捷键，但真的不方便啊喂\"></a>一开始还是蛮新鲜的，用了一段时间后，索尼的通病就开始发作了。。。。屏幕色彩，狗屎的操作界面，虽说可以设置快捷键，但真的不方便啊喂</h3><h3 id=\"拿到相机之后也没怎么出去拍，定焦能拍的东西太有限了，\"><a href=\"#拿到相机之后也没怎么出去拍，定焦能拍的东西太有限了，\" class=\"headerlink\" title=\"拿到相机之后也没怎么出去拍，定焦能拍的东西太有限了，\"></a>拿到相机之后也没怎么出去拍，定焦能拍的东西太有限了，</h3><div class=\"gallery-container\" data-type=\"data\" data-button=\"\">\n      <div class=\"gallery-items\">[{\"url\":\"http://picbed.yanzu.tech/img/camera/1/7.jpg\",\"alt\":\"风筝蛇，第一张调色的照片！稍微远点的，50mm等效75真的拍不到欸\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/8.jpg\",\"alt\":\"阿拉斯加？真肥啊，第一次拿着相机去扫街，那天独自去一个什么公园溜达来着\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/9.jpg\",\"alt\":\"什么阴间调色啊，第一次扫街遇到的喵桑，没东西给他吃他已经不耐烦了\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/10.jpg\",\"alt\":\"cool，可能快门速度还是慢了不够清晰，不过但从构图来说还是能看的\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/11.jpg\",\"alt\":\"少见多怪，东北特有？倒着骑三轮，大一刚到沈阳就看到了，当时还多惊讶的\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/12.jpg\",\"alt\":\"蓝调时刻不是很蓝，大烟囱，杀死那个石家庄人？\"}]</div>\n    </div>\n\n\n<h2 id=\"大连呆的那两天，，，\"><a href=\"#大连呆的那两天，，，\" class=\"headerlink\" title=\"大连呆的那两天，，，\"></a>大连呆的那两天，，，</h2><div class=\"gallery-container\" data-type=\"data\" data-button=\"\">\n      <div class=\"gallery-items\">[{\"url\":\"http://picbed.yanzu.tech/img/camera/1/1.jpg\",\"alt\":\"欧了，星海广场海边的海鸥，咋一看以为中间这只海鸥叼着什么，实际上是另外一只海鸥，索尼的对焦确实还不错\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/2.jpg\",\"alt\":\"饱和度战士！也是星海广场(初代饱和度战士)\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/3.jpg\",\"alt\":\"我挺喜欢这张的，不管是构图还是调色，偷拍钓鱼佬，虽然这张调色也有饱和度战士的嫌疑，不过我还是比较喜欢这个色\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/4.jpg\",\"alt\":\"饱和度战士！\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/5.jpg\",\"alt\":\"饱！\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/1/6.jpg\",\"alt\":\"歪瑞古德，这个大叔的右手臂内侧有刺青，是为“效忠父母”，可惜没拍到\"}]</div>\n    </div>\n\n<h2 id=\"Nikon佬正式上线，\"><a href=\"#Nikon佬正式上线，\" class=\"headerlink\" title=\"Nikon佬正式上线，\"></a>Nikon佬正式上线，</h2><h3 id=\"24年8月9号，心心念念的尼康终于拿到了，记得观望的那两三个月，天天听战歌，一位风光佬即将崛起\"><a href=\"#24年8月9号，心心念念的尼康终于拿到了，记得观望的那两三个月，天天听战歌，一位风光佬即将崛起\" class=\"headerlink\" title=\"24年8月9号，心心念念的尼康终于拿到了，记得观望的那两三个月，天天听战歌，一位风光佬即将崛起\"></a>24年8月9号，心心念念的尼康终于拿到了，记得观望的那两三个月，天天听战歌，一位风光佬即将崛起</h3><div class=\"gallery-container\" data-type=\"data\" data-button=\"\">\n      <div class=\"gallery-items\">[{\"url\":\"http://picbed.yanzu.tech/img/camera/2/1.jpg\",\"alt\":\"一开始当然是经典的套机镜头1650(穷鬼三剑客之一)。不过，尼康无狗头，这颗1650吊打某尼的1650\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/2.jpg\",\"alt\":\"16-50\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/3.jpg\",\"alt\":\"Z50，有取景器，有模式转盘，有触屏，操作很便捷，总结起来就一个字：6\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/4.jpg\",\"alt\":\"拿到手了自然是拍拍拍，第一位受害人---喵桑\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/5.jpg\",\"alt\":\"调色稍微正常了点\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/6.jpg\",\"alt\":\"冷暖的对比还不错欸\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/7.jpg\",\"alt\":\"天黑不回家\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/8.jpg\",\"alt\":\"要是有个人往上走就好了\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/9.jpg\",\"alt\":\"笑死我了\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/10.jpg\",\"alt\":\"emm\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/11.jpg\",\"alt\":\"比较喜欢的一张，绝了\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/12.jpg\",\"alt\":\"蓝调时刻+1\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/13.jpg\",\"alt\":\"摄影的尽头，nikon佬的本职工作\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/14.jpg\",\"alt\":\"可以\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/15.jpg\",\"alt\":\"笑狗常开\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/16.jpg\",\"alt\":\"鹅鹅鹅鹅鹅鹅！\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/17.jpg\",\"alt\":\"拿到尼康后，第一次去拍日出(没三脚架)\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/18.jpg\",\"alt\":\"回想起我的中学生活了，羡慕啊\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/19.jpg\",\"alt\":\"还行\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/20.jpg\",\"alt\":\"到新环境后，第一次出去溜达拍照\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/21.jpg\",\"alt\":\"柳枝间有一弯月亮就好了\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/22.jpg\",\"alt\":\"并不好看，仅作为铺垫，为后文埋下伏笔\"}]</div>\n    </div>\n\n<h3 id=\"穷玩三剑客之二，50-250到手了\"><a href=\"#穷玩三剑客之二，50-250到手了\" class=\"headerlink\" title=\"穷玩三剑客之二，50-250到手了\"></a>穷玩三剑客之二，50-250到手了</h3><div class=\"gallery-container\" data-type=\"data\" data-button=\"\">\n      <div class=\"gallery-items\">[{\"url\":\"http://picbed.yanzu.tech/img/camera/2/23.jpg\",\"alt\":\"一家人整整齐齐的\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/24.jpg\",\"alt\":\"还是挺不错的，等效375可打鸟可打月亮\"}]</div>\n    </div>\n\n<h3 id=\"中秋来了，打月亮\"><a href=\"#中秋来了，打月亮\" class=\"headerlink\" title=\"中秋来了，打月亮\"></a>中秋来了，打月亮</h3><div class=\"gallery-container\" data-type=\"data\" data-button=\"\">\n      <div class=\"gallery-items\">[{\"url\":\"http://picbed.yanzu.tech/img/camera/2/25.jpg\",\"alt\":\"小试牛刀\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/26.jpg\",\"alt\":\"冷月\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/27.jpg\",\"alt\":\"颜色没调对，白平衡的锅？\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/28.jpg\",\"alt\":\"可以\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/29.jpg\",\"alt\":\"她也在拍那个太阳下山！\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/30.jpg\",\"alt\":\"他也在拍太阳落山！\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/31.jpg\",\"alt\":\"中秋的月亮\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/32.jpg\",\"alt\":\"咸鸭蛋\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/33.jpg\",\"alt\":\"还行\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/34.jpg\",\"alt\":\"有点糊，不晓得是不是UV镜的锅\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/35.jpg\",\"alt\":\"中秋节第二天晚上的月亮\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/36.jpg\",\"alt\":\"不是咸鸭蛋了\"}]</div>\n    </div>\n\n<h3 id=\"新阶段\"><a href=\"#新阶段\" class=\"headerlink\" title=\"新阶段~\"></a>新阶段~</h3><div class=\"gallery-container\" data-type=\"data\" data-button=\"\">\n      <div class=\"gallery-items\">[{\"url\":\"http://picbed.yanzu.tech/img/camera/2/37.jpg\",\"alt\":\"目中无人，Nikon\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/38.jpg\",\"alt\":\"太阳爆炸了？\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/39.jpg\",\"alt\":\"这个色调好看\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/40.jpg\",\"alt\":\"像一幅水墨画\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/41.jpg\",\"alt\":\"山顶老喵，智慧的眼神\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/42.jpg\",\"alt\":\"狭路相逢！什么，一开始我根本没注意到草丛里还有一只奶牛猫，我以为大橘无端发怒\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/43.jpg\",\"alt\":\"奶牛猫滚出了草丛\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/44.jpg\",\"alt\":\"嫉妒，使喵面目全非\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/45.jpg\",\"alt\":\"第一张黑白色调\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/46.jpg\",\"alt\":\"第二张\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/47.jpg\",\"alt\":\"色彩很鲜明\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/48.jpg\",\"alt\":\"捕获人类幼崽\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/49.jpg\",\"alt\":\"年轻就是好\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/50.jpg\",\"alt\":\"你很凶哦\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/51.jpg\",\"alt\":\"不清晰但好看\"}]</div>\n    </div>\n\n<h3 id=\"入秋了\"><a href=\"#入秋了\" class=\"headerlink\" title=\"入秋了\"></a>入秋了</h3><div class=\"gallery-container\" data-type=\"data\" data-button=\"\">\n      <div class=\"gallery-items\">[{\"url\":\"http://picbed.yanzu.tech/img/camera/2/52.jpg\",\"alt\":\"胶片感\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/53.jpg\",\"alt\":\"cool\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/54.jpg\",\"alt\":\"起飞\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/55.jpg\",\"alt\":\"偷感很重的哈基咪\"},{\"url\":\"http://picbed.yanzu.tech/img/camera/2/56.jpg\",\"alt\":\"偷感加重的哈基咪\"}]</div>\n    </div>"}],"Post":[{"title":"第一弹","data":"2024-12-31T17:16:00.000Z","updated":"2024-12-31T17:16:00.000Z","type":"ros2_learning","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p1.png","_content":"# ROS2 的安装\n\n### 鱼香ROS安装命令\n\n> ### $ wget http://fishros.com/install -O fishros && bash fishros\n\n### 安装成功后可使用以下命令查看\n\n> ### $ ros2\n\n### 测试安装是否成功，运行小海龟\n\n#### 先运行一个小海龟节点\n\n> ### $ ros2 run turtlesim turtlesim_node\n\n#### 新建一个命令框运行另外一个节点，与前一个节点通信，从而通过键盘来控制小海龟移动\n\n> ### $ ros2 run turtlesim turtle_teleop_key\n\n![pic](http://picbed.yanzu.tech/img/learn_ros2/pic_1.png)\n\n#### 此时可以再新建一个命令框，执行\n\n> ### $ rqt\n\n### 点击 plugins -> introspection -> node graph，就可以看到当前两个节点之间的通信情况了\n\n### teleop_turtle 通过 /turtle1/cmd_vel 对 turtlesim 进行控制\n\n![pic](http://picbed.yanzu.tech/img/learn_ros2/pic_2.png)\n\n### ros2 run turtlesim turtlesim_node 这条命令是什么意思，又是如何执行的？\n\n> ### 首先， ros2 run 会找到一个环境变量 AMENT_PREFIX_PATH，可能有多个值，多值就循环遍历\n>\n> ### 接着，在这个环境变量对应的路径下去找 lib 文件，找到后再在其目录下找 turtlesim 包\n>\n> ### 找到对应的包之后，在其目录下找到可执行文件 turtlesim_node，然后执行\n>\n> ### 那么可以直接使用以下命令代替执行对应的可执行文件\n>\n> ### (但只能是 AMENT_PREFIX_PATH=/opt/ros/humble 的情况下才能这么执行)\n>\n> ### $ \\$AMENT_PREFIX_PATH/lib/turtlesim/turtlesim_node\n\n### 修改环境变量， export 环境变量名=path\n\n> ### export AMENT_PREFIX_PATH=/opt/ros\n\n### 在 linux 中，以 点(如 .a.txt) 开头的文件或文件夹是隐藏文件\n\n> ### 查看当前目录下的所有文件，包括隐藏文件\n>\n> ### $ ls -a\n\n### 只要一打开终端，就会默认启动一个脚本 .bashrc，他会设置一系列的环境变量\n\n","source":"_posts/01.md","raw":"---\ntitle: 第一弹\ndata: 2025-01-01 01:16:00\nupdated: 2025-01-01 01:16:00\ntype: ros2_learning\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p1.png\ntags:\n  - ROS2\n  - Learning\n---\n# ROS2 的安装\n\n### 鱼香ROS安装命令\n\n> ### $ wget http://fishros.com/install -O fishros && bash fishros\n\n### 安装成功后可使用以下命令查看\n\n> ### $ ros2\n\n### 测试安装是否成功，运行小海龟\n\n#### 先运行一个小海龟节点\n\n> ### $ ros2 run turtlesim turtlesim_node\n\n#### 新建一个命令框运行另外一个节点，与前一个节点通信，从而通过键盘来控制小海龟移动\n\n> ### $ ros2 run turtlesim turtle_teleop_key\n\n![pic](http://picbed.yanzu.tech/img/learn_ros2/pic_1.png)\n\n#### 此时可以再新建一个命令框，执行\n\n> ### $ rqt\n\n### 点击 plugins -> introspection -> node graph，就可以看到当前两个节点之间的通信情况了\n\n### teleop_turtle 通过 /turtle1/cmd_vel 对 turtlesim 进行控制\n\n![pic](http://picbed.yanzu.tech/img/learn_ros2/pic_2.png)\n\n### ros2 run turtlesim turtlesim_node 这条命令是什么意思，又是如何执行的？\n\n> ### 首先， ros2 run 会找到一个环境变量 AMENT_PREFIX_PATH，可能有多个值，多值就循环遍历\n>\n> ### 接着，在这个环境变量对应的路径下去找 lib 文件，找到后再在其目录下找 turtlesim 包\n>\n> ### 找到对应的包之后，在其目录下找到可执行文件 turtlesim_node，然后执行\n>\n> ### 那么可以直接使用以下命令代替执行对应的可执行文件\n>\n> ### (但只能是 AMENT_PREFIX_PATH=/opt/ros/humble 的情况下才能这么执行)\n>\n> ### $ \\$AMENT_PREFIX_PATH/lib/turtlesim/turtlesim_node\n\n### 修改环境变量， export 环境变量名=path\n\n> ### export AMENT_PREFIX_PATH=/opt/ros\n\n### 在 linux 中，以 点(如 .a.txt) 开头的文件或文件夹是隐藏文件\n\n> ### 查看当前目录下的所有文件，包括隐藏文件\n>\n> ### $ ls -a\n\n### 只要一打开终端，就会默认启动一个脚本 .bashrc，他会设置一系列的环境变量\n\n","slug":"01","published":1,"date":"2024-12-04T14:46:11.572Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cqf0001iku43zzhftob","content":"<h1 id=\"ROS2-的安装\"><a href=\"#ROS2-的安装\" class=\"headerlink\" title=\"ROS2 的安装\"></a>ROS2 的安装</h1><h3 id=\"鱼香ROS安装命令\"><a href=\"#鱼香ROS安装命令\" class=\"headerlink\" title=\"鱼香ROS安装命令\"></a>鱼香ROS安装命令</h3><blockquote>\n<h3 id=\"wget-http-fishros-com-install-O-fishros-bash-fishros\"><a href=\"#wget-http-fishros-com-install-O-fishros-bash-fishros\" class=\"headerlink\" title=\"$ wget http://fishros.com/install -O fishros &amp;&amp; bash fishros\"></a>$ wget <a href=\"http://fishros.com/install\">http://fishros.com/install</a> -O fishros &amp;&amp; bash fishros</h3></blockquote>\n<h3 id=\"安装成功后可使用以下命令查看\"><a href=\"#安装成功后可使用以下命令查看\" class=\"headerlink\" title=\"安装成功后可使用以下命令查看\"></a>安装成功后可使用以下命令查看</h3><blockquote>\n<h3 id=\"ros2\"><a href=\"#ros2\" class=\"headerlink\" title=\"$ ros2\"></a>$ ros2</h3></blockquote>\n<h3 id=\"测试安装是否成功，运行小海龟\"><a href=\"#测试安装是否成功，运行小海龟\" class=\"headerlink\" title=\"测试安装是否成功，运行小海龟\"></a>测试安装是否成功，运行小海龟</h3><h4 id=\"先运行一个小海龟节点\"><a href=\"#先运行一个小海龟节点\" class=\"headerlink\" title=\"先运行一个小海龟节点\"></a>先运行一个小海龟节点</h4><blockquote>\n<h3 id=\"ros2-run-turtlesim-turtlesim-node\"><a href=\"#ros2-run-turtlesim-turtlesim-node\" class=\"headerlink\" title=\"$ ros2 run turtlesim turtlesim_node\"></a>$ ros2 run turtlesim turtlesim_node</h3></blockquote>\n<h4 id=\"新建一个命令框运行另外一个节点，与前一个节点通信，从而通过键盘来控制小海龟移动\"><a href=\"#新建一个命令框运行另外一个节点，与前一个节点通信，从而通过键盘来控制小海龟移动\" class=\"headerlink\" title=\"新建一个命令框运行另外一个节点，与前一个节点通信，从而通过键盘来控制小海龟移动\"></a>新建一个命令框运行另外一个节点，与前一个节点通信，从而通过键盘来控制小海龟移动</h4><blockquote>\n<h3 id=\"ros2-run-turtlesim-turtle-teleop-key\"><a href=\"#ros2-run-turtlesim-turtle-teleop-key\" class=\"headerlink\" title=\"$ ros2 run turtlesim turtle_teleop_key\"></a>$ ros2 run turtlesim turtle_teleop_key</h3></blockquote>\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_1.png\" alt=\"pic\"></p>\n<h4 id=\"此时可以再新建一个命令框，执行\"><a href=\"#此时可以再新建一个命令框，执行\" class=\"headerlink\" title=\"此时可以再新建一个命令框，执行\"></a>此时可以再新建一个命令框，执行</h4><blockquote>\n<h3 id=\"rqt\"><a href=\"#rqt\" class=\"headerlink\" title=\"$ rqt\"></a>$ rqt</h3></blockquote>\n<h3 id=\"点击-plugins-introspection-node-graph，就可以看到当前两个节点之间的通信情况了\"><a href=\"#点击-plugins-introspection-node-graph，就可以看到当前两个节点之间的通信情况了\" class=\"headerlink\" title=\"点击 plugins -&gt; introspection -&gt; node graph，就可以看到当前两个节点之间的通信情况了\"></a>点击 plugins -&gt; introspection -&gt; node graph，就可以看到当前两个节点之间的通信情况了</h3><h3 id=\"teleop-turtle-通过-turtle1-cmd-vel-对-turtlesim-进行控制\"><a href=\"#teleop-turtle-通过-turtle1-cmd-vel-对-turtlesim-进行控制\" class=\"headerlink\" title=\"teleop_turtle 通过 &#x2F;turtle1&#x2F;cmd_vel 对 turtlesim 进行控制\"></a>teleop_turtle 通过 &#x2F;turtle1&#x2F;cmd_vel 对 turtlesim 进行控制</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_2.png\" alt=\"pic\"></p>\n<h3 id=\"ros2-run-turtlesim-turtlesim-node-这条命令是什么意思，又是如何执行的？\"><a href=\"#ros2-run-turtlesim-turtlesim-node-这条命令是什么意思，又是如何执行的？\" class=\"headerlink\" title=\"ros2 run turtlesim turtlesim_node 这条命令是什么意思，又是如何执行的？\"></a>ros2 run turtlesim turtlesim_node 这条命令是什么意思，又是如何执行的？</h3><blockquote>\n<h3 id=\"首先，-ros2-run-会找到一个环境变量-AMENT-PREFIX-PATH，可能有多个值，多值就循环遍历\"><a href=\"#首先，-ros2-run-会找到一个环境变量-AMENT-PREFIX-PATH，可能有多个值，多值就循环遍历\" class=\"headerlink\" title=\"首先， ros2 run 会找到一个环境变量 AMENT_PREFIX_PATH，可能有多个值，多值就循环遍历\"></a>首先， ros2 run 会找到一个环境变量 AMENT_PREFIX_PATH，可能有多个值，多值就循环遍历</h3><h3 id=\"接着，在这个环境变量对应的路径下去找-lib-文件，找到后再在其目录下找-turtlesim-包\"><a href=\"#接着，在这个环境变量对应的路径下去找-lib-文件，找到后再在其目录下找-turtlesim-包\" class=\"headerlink\" title=\"接着，在这个环境变量对应的路径下去找 lib 文件，找到后再在其目录下找 turtlesim 包\"></a>接着，在这个环境变量对应的路径下去找 lib 文件，找到后再在其目录下找 turtlesim 包</h3><h3 id=\"找到对应的包之后，在其目录下找到可执行文件-turtlesim-node，然后执行\"><a href=\"#找到对应的包之后，在其目录下找到可执行文件-turtlesim-node，然后执行\" class=\"headerlink\" title=\"找到对应的包之后，在其目录下找到可执行文件 turtlesim_node，然后执行\"></a>找到对应的包之后，在其目录下找到可执行文件 turtlesim_node，然后执行</h3><h3 id=\"那么可以直接使用以下命令代替执行对应的可执行文件\"><a href=\"#那么可以直接使用以下命令代替执行对应的可执行文件\" class=\"headerlink\" title=\"那么可以直接使用以下命令代替执行对应的可执行文件\"></a>那么可以直接使用以下命令代替执行对应的可执行文件</h3><h3 id=\"但只能是-AMENT-PREFIX-PATH-opt-ros-humble-的情况下才能这么执行\"><a href=\"#但只能是-AMENT-PREFIX-PATH-opt-ros-humble-的情况下才能这么执行\" class=\"headerlink\" title=\"(但只能是 AMENT_PREFIX_PATH&#x3D;&#x2F;opt&#x2F;ros&#x2F;humble 的情况下才能这么执行)\"></a>(但只能是 AMENT_PREFIX_PATH&#x3D;&#x2F;opt&#x2F;ros&#x2F;humble 的情况下才能这么执行)</h3><h3 id=\"AMENT-PREFIX-PATH-lib-turtlesim-turtlesim-node\"><a href=\"#AMENT-PREFIX-PATH-lib-turtlesim-turtlesim-node\" class=\"headerlink\" title=\"$ $AMENT_PREFIX_PATH&#x2F;lib&#x2F;turtlesim&#x2F;turtlesim_node\"></a>$ $AMENT_PREFIX_PATH&#x2F;lib&#x2F;turtlesim&#x2F;turtlesim_node</h3></blockquote>\n<h3 id=\"修改环境变量，-export-环境变量名-path\"><a href=\"#修改环境变量，-export-环境变量名-path\" class=\"headerlink\" title=\"修改环境变量， export 环境变量名&#x3D;path\"></a>修改环境变量， export 环境变量名&#x3D;path</h3><blockquote>\n<h3 id=\"export-AMENT-PREFIX-PATH-opt-ros\"><a href=\"#export-AMENT-PREFIX-PATH-opt-ros\" class=\"headerlink\" title=\"export AMENT_PREFIX_PATH&#x3D;&#x2F;opt&#x2F;ros\"></a>export AMENT_PREFIX_PATH&#x3D;&#x2F;opt&#x2F;ros</h3></blockquote>\n<h3 id=\"在-linux-中，以-点-如-a-txt-开头的文件或文件夹是隐藏文件\"><a href=\"#在-linux-中，以-点-如-a-txt-开头的文件或文件夹是隐藏文件\" class=\"headerlink\" title=\"在 linux 中，以 点(如 .a.txt) 开头的文件或文件夹是隐藏文件\"></a>在 linux 中，以 点(如 .a.txt) 开头的文件或文件夹是隐藏文件</h3><blockquote>\n<h3 id=\"查看当前目录下的所有文件，包括隐藏文件\"><a href=\"#查看当前目录下的所有文件，包括隐藏文件\" class=\"headerlink\" title=\"查看当前目录下的所有文件，包括隐藏文件\"></a>查看当前目录下的所有文件，包括隐藏文件</h3><h3 id=\"ls-a\"><a href=\"#ls-a\" class=\"headerlink\" title=\"$ ls -a\"></a>$ ls -a</h3></blockquote>\n<h3 id=\"只要一打开终端，就会默认启动一个脚本-bashrc，他会设置一系列的环境变量\"><a href=\"#只要一打开终端，就会默认启动一个脚本-bashrc，他会设置一系列的环境变量\" class=\"headerlink\" title=\"只要一打开终端，就会默认启动一个脚本 .bashrc，他会设置一系列的环境变量\"></a>只要一打开终端，就会默认启动一个脚本 .bashrc，他会设置一系列的环境变量</h3>","cover_type":"img","excerpt":"","more":"<h1 id=\"ROS2-的安装\"><a href=\"#ROS2-的安装\" class=\"headerlink\" title=\"ROS2 的安装\"></a>ROS2 的安装</h1><h3 id=\"鱼香ROS安装命令\"><a href=\"#鱼香ROS安装命令\" class=\"headerlink\" title=\"鱼香ROS安装命令\"></a>鱼香ROS安装命令</h3><blockquote>\n<h3 id=\"wget-http-fishros-com-install-O-fishros-bash-fishros\"><a href=\"#wget-http-fishros-com-install-O-fishros-bash-fishros\" class=\"headerlink\" title=\"$ wget http://fishros.com/install -O fishros &amp;&amp; bash fishros\"></a>$ wget <a href=\"http://fishros.com/install\">http://fishros.com/install</a> -O fishros &amp;&amp; bash fishros</h3></blockquote>\n<h3 id=\"安装成功后可使用以下命令查看\"><a href=\"#安装成功后可使用以下命令查看\" class=\"headerlink\" title=\"安装成功后可使用以下命令查看\"></a>安装成功后可使用以下命令查看</h3><blockquote>\n<h3 id=\"ros2\"><a href=\"#ros2\" class=\"headerlink\" title=\"$ ros2\"></a>$ ros2</h3></blockquote>\n<h3 id=\"测试安装是否成功，运行小海龟\"><a href=\"#测试安装是否成功，运行小海龟\" class=\"headerlink\" title=\"测试安装是否成功，运行小海龟\"></a>测试安装是否成功，运行小海龟</h3><h4 id=\"先运行一个小海龟节点\"><a href=\"#先运行一个小海龟节点\" class=\"headerlink\" title=\"先运行一个小海龟节点\"></a>先运行一个小海龟节点</h4><blockquote>\n<h3 id=\"ros2-run-turtlesim-turtlesim-node\"><a href=\"#ros2-run-turtlesim-turtlesim-node\" class=\"headerlink\" title=\"$ ros2 run turtlesim turtlesim_node\"></a>$ ros2 run turtlesim turtlesim_node</h3></blockquote>\n<h4 id=\"新建一个命令框运行另外一个节点，与前一个节点通信，从而通过键盘来控制小海龟移动\"><a href=\"#新建一个命令框运行另外一个节点，与前一个节点通信，从而通过键盘来控制小海龟移动\" class=\"headerlink\" title=\"新建一个命令框运行另外一个节点，与前一个节点通信，从而通过键盘来控制小海龟移动\"></a>新建一个命令框运行另外一个节点，与前一个节点通信，从而通过键盘来控制小海龟移动</h4><blockquote>\n<h3 id=\"ros2-run-turtlesim-turtle-teleop-key\"><a href=\"#ros2-run-turtlesim-turtle-teleop-key\" class=\"headerlink\" title=\"$ ros2 run turtlesim turtle_teleop_key\"></a>$ ros2 run turtlesim turtle_teleop_key</h3></blockquote>\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_1.png\" alt=\"pic\"></p>\n<h4 id=\"此时可以再新建一个命令框，执行\"><a href=\"#此时可以再新建一个命令框，执行\" class=\"headerlink\" title=\"此时可以再新建一个命令框，执行\"></a>此时可以再新建一个命令框，执行</h4><blockquote>\n<h3 id=\"rqt\"><a href=\"#rqt\" class=\"headerlink\" title=\"$ rqt\"></a>$ rqt</h3></blockquote>\n<h3 id=\"点击-plugins-introspection-node-graph，就可以看到当前两个节点之间的通信情况了\"><a href=\"#点击-plugins-introspection-node-graph，就可以看到当前两个节点之间的通信情况了\" class=\"headerlink\" title=\"点击 plugins -&gt; introspection -&gt; node graph，就可以看到当前两个节点之间的通信情况了\"></a>点击 plugins -&gt; introspection -&gt; node graph，就可以看到当前两个节点之间的通信情况了</h3><h3 id=\"teleop-turtle-通过-turtle1-cmd-vel-对-turtlesim-进行控制\"><a href=\"#teleop-turtle-通过-turtle1-cmd-vel-对-turtlesim-进行控制\" class=\"headerlink\" title=\"teleop_turtle 通过 &#x2F;turtle1&#x2F;cmd_vel 对 turtlesim 进行控制\"></a>teleop_turtle 通过 &#x2F;turtle1&#x2F;cmd_vel 对 turtlesim 进行控制</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_2.png\" alt=\"pic\"></p>\n<h3 id=\"ros2-run-turtlesim-turtlesim-node-这条命令是什么意思，又是如何执行的？\"><a href=\"#ros2-run-turtlesim-turtlesim-node-这条命令是什么意思，又是如何执行的？\" class=\"headerlink\" title=\"ros2 run turtlesim turtlesim_node 这条命令是什么意思，又是如何执行的？\"></a>ros2 run turtlesim turtlesim_node 这条命令是什么意思，又是如何执行的？</h3><blockquote>\n<h3 id=\"首先，-ros2-run-会找到一个环境变量-AMENT-PREFIX-PATH，可能有多个值，多值就循环遍历\"><a href=\"#首先，-ros2-run-会找到一个环境变量-AMENT-PREFIX-PATH，可能有多个值，多值就循环遍历\" class=\"headerlink\" title=\"首先， ros2 run 会找到一个环境变量 AMENT_PREFIX_PATH，可能有多个值，多值就循环遍历\"></a>首先， ros2 run 会找到一个环境变量 AMENT_PREFIX_PATH，可能有多个值，多值就循环遍历</h3><h3 id=\"接着，在这个环境变量对应的路径下去找-lib-文件，找到后再在其目录下找-turtlesim-包\"><a href=\"#接着，在这个环境变量对应的路径下去找-lib-文件，找到后再在其目录下找-turtlesim-包\" class=\"headerlink\" title=\"接着，在这个环境变量对应的路径下去找 lib 文件，找到后再在其目录下找 turtlesim 包\"></a>接着，在这个环境变量对应的路径下去找 lib 文件，找到后再在其目录下找 turtlesim 包</h3><h3 id=\"找到对应的包之后，在其目录下找到可执行文件-turtlesim-node，然后执行\"><a href=\"#找到对应的包之后，在其目录下找到可执行文件-turtlesim-node，然后执行\" class=\"headerlink\" title=\"找到对应的包之后，在其目录下找到可执行文件 turtlesim_node，然后执行\"></a>找到对应的包之后，在其目录下找到可执行文件 turtlesim_node，然后执行</h3><h3 id=\"那么可以直接使用以下命令代替执行对应的可执行文件\"><a href=\"#那么可以直接使用以下命令代替执行对应的可执行文件\" class=\"headerlink\" title=\"那么可以直接使用以下命令代替执行对应的可执行文件\"></a>那么可以直接使用以下命令代替执行对应的可执行文件</h3><h3 id=\"但只能是-AMENT-PREFIX-PATH-opt-ros-humble-的情况下才能这么执行\"><a href=\"#但只能是-AMENT-PREFIX-PATH-opt-ros-humble-的情况下才能这么执行\" class=\"headerlink\" title=\"(但只能是 AMENT_PREFIX_PATH&#x3D;&#x2F;opt&#x2F;ros&#x2F;humble 的情况下才能这么执行)\"></a>(但只能是 AMENT_PREFIX_PATH&#x3D;&#x2F;opt&#x2F;ros&#x2F;humble 的情况下才能这么执行)</h3><h3 id=\"AMENT-PREFIX-PATH-lib-turtlesim-turtlesim-node\"><a href=\"#AMENT-PREFIX-PATH-lib-turtlesim-turtlesim-node\" class=\"headerlink\" title=\"$ $AMENT_PREFIX_PATH&#x2F;lib&#x2F;turtlesim&#x2F;turtlesim_node\"></a>$ $AMENT_PREFIX_PATH&#x2F;lib&#x2F;turtlesim&#x2F;turtlesim_node</h3></blockquote>\n<h3 id=\"修改环境变量，-export-环境变量名-path\"><a href=\"#修改环境变量，-export-环境变量名-path\" class=\"headerlink\" title=\"修改环境变量， export 环境变量名&#x3D;path\"></a>修改环境变量， export 环境变量名&#x3D;path</h3><blockquote>\n<h3 id=\"export-AMENT-PREFIX-PATH-opt-ros\"><a href=\"#export-AMENT-PREFIX-PATH-opt-ros\" class=\"headerlink\" title=\"export AMENT_PREFIX_PATH&#x3D;&#x2F;opt&#x2F;ros\"></a>export AMENT_PREFIX_PATH&#x3D;&#x2F;opt&#x2F;ros</h3></blockquote>\n<h3 id=\"在-linux-中，以-点-如-a-txt-开头的文件或文件夹是隐藏文件\"><a href=\"#在-linux-中，以-点-如-a-txt-开头的文件或文件夹是隐藏文件\" class=\"headerlink\" title=\"在 linux 中，以 点(如 .a.txt) 开头的文件或文件夹是隐藏文件\"></a>在 linux 中，以 点(如 .a.txt) 开头的文件或文件夹是隐藏文件</h3><blockquote>\n<h3 id=\"查看当前目录下的所有文件，包括隐藏文件\"><a href=\"#查看当前目录下的所有文件，包括隐藏文件\" class=\"headerlink\" title=\"查看当前目录下的所有文件，包括隐藏文件\"></a>查看当前目录下的所有文件，包括隐藏文件</h3><h3 id=\"ls-a\"><a href=\"#ls-a\" class=\"headerlink\" title=\"$ ls -a\"></a>$ ls -a</h3></blockquote>\n<h3 id=\"只要一打开终端，就会默认启动一个脚本-bashrc，他会设置一系列的环境变量\"><a href=\"#只要一打开终端，就会默认启动一个脚本-bashrc，他会设置一系列的环境变量\" class=\"headerlink\" title=\"只要一打开终端，就会默认启动一个脚本 .bashrc，他会设置一系列的环境变量\"></a>只要一打开终端，就会默认启动一个脚本 .bashrc，他会设置一系列的环境变量</h3>"},{"title":"第二弹","data":"2024-12-31T17:16:00.000Z","updated":"2024-12-31T17:16:00.000Z","type":"ros2_learning","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p2.png","_content":"# 常用的 Linux 操作命令\n\n### 当前终端所在目录\n\n> ### $ pwd\n\n### 查看文件内容\n\n> ### $ cat name.txt\n\n### 卸载某个包\n\n> ### $ sudo apt remove pkg_name\n\n### 安装 .deb\n\n> ### $ sudo dpkg -i file_name.deb\n\n### 解压 .tar.gz/.tgz\n\n> ### $ tar -xzvf file.tar.gz -C /path\n\n### 解压 .zip\n\n> ### $ unzip file.zip -d /path\n\n### 解压 .rar\n\n> ### $ unrar x file.rar\n\n### 查找某个文件\n\n> ### $ whereis file_name\n\n### 使用 nano 编辑文本\n\n> ### $ nano file_name\n\n### 查看执行过的命令\n\n> ### $ history\n\n### 终端中的复制粘贴快捷键\n\n> ### ctrl + shift + C / ctrl + shift + V\n\n### 终端中执行 .py\n\n> ### $ python3 file_name.py\n\n### 给文件添加权限，文件或者目录的权限分为三类，权限也分为三类\n\n>#### 1.用户(user,u)：文件拥有者\n>\n>#### 2.组(group,g)：与user在同一组的用户\n>\n>#### 3.其他人(others,o)：所有其他用户\n>\n>#### 读，r，值为 4 ；写，w，值为 2 ；执行，x，值为 1\n\n### 赋予所有用户执行权，添加权限就是 + ，移除权限就是 -\n\n> ### $ chmod a+x file\n\n### 查看文件/目录权限\n\n> ### $ ls -l file\n\n![pic](http://picbed.yanzu.tech/img/learn_ros2/pic_3.png)\n\n### 要在命令框中直接执行 .py 文件，不使用 python3 file.py，要指定编译器，在 .py 文件顶部添加\n\n> ### #!usr/bin/python3\n>\n> ### 然后就可以使用\n>\n> ### $ ./file.py\n\n### echo 指令的使用，echo 本身的功能就是打印其后的字符\n\n> ### $ 可以找到跟在其后的字符的环境变量，echo 就能打印该环境变量\n>\n> ### $ echo \\$ROS_VERSION\n>\n> ### # 查看 ros 的发行版本\n>\n> ### $ echo \\$ROS_DISTRO\n\n### 查看所有环境变量列表\n\n> ### $ printenv\n\n","source":"_posts/02.md","raw":"---\ntitle: 第二弹\ndata: 2025-01-01 01:16:00\nupdated: 2025-01-01 01:16:00\ntype: ros2_learning\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p2.png\ntags:\n  - ROS2\n  - Learning\n---\n# 常用的 Linux 操作命令\n\n### 当前终端所在目录\n\n> ### $ pwd\n\n### 查看文件内容\n\n> ### $ cat name.txt\n\n### 卸载某个包\n\n> ### $ sudo apt remove pkg_name\n\n### 安装 .deb\n\n> ### $ sudo dpkg -i file_name.deb\n\n### 解压 .tar.gz/.tgz\n\n> ### $ tar -xzvf file.tar.gz -C /path\n\n### 解压 .zip\n\n> ### $ unzip file.zip -d /path\n\n### 解压 .rar\n\n> ### $ unrar x file.rar\n\n### 查找某个文件\n\n> ### $ whereis file_name\n\n### 使用 nano 编辑文本\n\n> ### $ nano file_name\n\n### 查看执行过的命令\n\n> ### $ history\n\n### 终端中的复制粘贴快捷键\n\n> ### ctrl + shift + C / ctrl + shift + V\n\n### 终端中执行 .py\n\n> ### $ python3 file_name.py\n\n### 给文件添加权限，文件或者目录的权限分为三类，权限也分为三类\n\n>#### 1.用户(user,u)：文件拥有者\n>\n>#### 2.组(group,g)：与user在同一组的用户\n>\n>#### 3.其他人(others,o)：所有其他用户\n>\n>#### 读，r，值为 4 ；写，w，值为 2 ；执行，x，值为 1\n\n### 赋予所有用户执行权，添加权限就是 + ，移除权限就是 -\n\n> ### $ chmod a+x file\n\n### 查看文件/目录权限\n\n> ### $ ls -l file\n\n![pic](http://picbed.yanzu.tech/img/learn_ros2/pic_3.png)\n\n### 要在命令框中直接执行 .py 文件，不使用 python3 file.py，要指定编译器，在 .py 文件顶部添加\n\n> ### #!usr/bin/python3\n>\n> ### 然后就可以使用\n>\n> ### $ ./file.py\n\n### echo 指令的使用，echo 本身的功能就是打印其后的字符\n\n> ### $ 可以找到跟在其后的字符的环境变量，echo 就能打印该环境变量\n>\n> ### $ echo \\$ROS_VERSION\n>\n> ### # 查看 ros 的发行版本\n>\n> ### $ echo \\$ROS_DISTRO\n\n### 查看所有环境变量列表\n\n> ### $ printenv\n\n","slug":"02","published":1,"date":"2024-12-31T17:23:10.325Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cqh0003iku42jy90mwy","content":"<h1 id=\"常用的-Linux-操作命令\"><a href=\"#常用的-Linux-操作命令\" class=\"headerlink\" title=\"常用的 Linux 操作命令\"></a>常用的 Linux 操作命令</h1><h3 id=\"当前终端所在目录\"><a href=\"#当前终端所在目录\" class=\"headerlink\" title=\"当前终端所在目录\"></a>当前终端所在目录</h3><blockquote>\n<h3 id=\"pwd\"><a href=\"#pwd\" class=\"headerlink\" title=\"$ pwd\"></a>$ pwd</h3></blockquote>\n<h3 id=\"查看文件内容\"><a href=\"#查看文件内容\" class=\"headerlink\" title=\"查看文件内容\"></a>查看文件内容</h3><blockquote>\n<h3 id=\"cat-name-txt\"><a href=\"#cat-name-txt\" class=\"headerlink\" title=\"$ cat name.txt\"></a>$ cat name.txt</h3></blockquote>\n<h3 id=\"卸载某个包\"><a href=\"#卸载某个包\" class=\"headerlink\" title=\"卸载某个包\"></a>卸载某个包</h3><blockquote>\n<h3 id=\"sudo-apt-remove-pkg-name\"><a href=\"#sudo-apt-remove-pkg-name\" class=\"headerlink\" title=\"$ sudo apt remove pkg_name\"></a>$ sudo apt remove pkg_name</h3></blockquote>\n<h3 id=\"安装-deb\"><a href=\"#安装-deb\" class=\"headerlink\" title=\"安装 .deb\"></a>安装 .deb</h3><blockquote>\n<h3 id=\"sudo-dpkg-i-file-name-deb\"><a href=\"#sudo-dpkg-i-file-name-deb\" class=\"headerlink\" title=\"$ sudo dpkg -i file_name.deb\"></a>$ sudo dpkg -i file_name.deb</h3></blockquote>\n<h3 id=\"解压-tar-gz-tgz\"><a href=\"#解压-tar-gz-tgz\" class=\"headerlink\" title=\"解压 .tar.gz&#x2F;.tgz\"></a>解压 .tar.gz&#x2F;.tgz</h3><blockquote>\n<h3 id=\"tar-xzvf-file-tar-gz-C-path\"><a href=\"#tar-xzvf-file-tar-gz-C-path\" class=\"headerlink\" title=\"$ tar -xzvf file.tar.gz -C &#x2F;path\"></a>$ tar -xzvf file.tar.gz -C &#x2F;path</h3></blockquote>\n<h3 id=\"解压-zip\"><a href=\"#解压-zip\" class=\"headerlink\" title=\"解压 .zip\"></a>解压 .zip</h3><blockquote>\n<h3 id=\"unzip-file-zip-d-path\"><a href=\"#unzip-file-zip-d-path\" class=\"headerlink\" title=\"$ unzip file.zip -d &#x2F;path\"></a>$ unzip file.zip -d &#x2F;path</h3></blockquote>\n<h3 id=\"解压-rar\"><a href=\"#解压-rar\" class=\"headerlink\" title=\"解压 .rar\"></a>解压 .rar</h3><blockquote>\n<h3 id=\"unrar-x-file-rar\"><a href=\"#unrar-x-file-rar\" class=\"headerlink\" title=\"$ unrar x file.rar\"></a>$ unrar x file.rar</h3></blockquote>\n<h3 id=\"查找某个文件\"><a href=\"#查找某个文件\" class=\"headerlink\" title=\"查找某个文件\"></a>查找某个文件</h3><blockquote>\n<h3 id=\"whereis-file-name\"><a href=\"#whereis-file-name\" class=\"headerlink\" title=\"$ whereis file_name\"></a>$ whereis file_name</h3></blockquote>\n<h3 id=\"使用-nano-编辑文本\"><a href=\"#使用-nano-编辑文本\" class=\"headerlink\" title=\"使用 nano 编辑文本\"></a>使用 nano 编辑文本</h3><blockquote>\n<h3 id=\"nano-file-name\"><a href=\"#nano-file-name\" class=\"headerlink\" title=\"$ nano file_name\"></a>$ nano file_name</h3></blockquote>\n<h3 id=\"查看执行过的命令\"><a href=\"#查看执行过的命令\" class=\"headerlink\" title=\"查看执行过的命令\"></a>查看执行过的命令</h3><blockquote>\n<h3 id=\"history\"><a href=\"#history\" class=\"headerlink\" title=\"$ history\"></a>$ history</h3></blockquote>\n<h3 id=\"终端中的复制粘贴快捷键\"><a href=\"#终端中的复制粘贴快捷键\" class=\"headerlink\" title=\"终端中的复制粘贴快捷键\"></a>终端中的复制粘贴快捷键</h3><blockquote>\n<h3 id=\"ctrl-shift-C-ctrl-shift-V\"><a href=\"#ctrl-shift-C-ctrl-shift-V\" class=\"headerlink\" title=\"ctrl + shift + C &#x2F; ctrl + shift + V\"></a>ctrl + shift + C &#x2F; ctrl + shift + V</h3></blockquote>\n<h3 id=\"终端中执行-py\"><a href=\"#终端中执行-py\" class=\"headerlink\" title=\"终端中执行 .py\"></a>终端中执行 .py</h3><blockquote>\n<h3 id=\"python3-file-name-py\"><a href=\"#python3-file-name-py\" class=\"headerlink\" title=\"$ python3 file_name.py\"></a>$ python3 file_name.py</h3></blockquote>\n<h3 id=\"给文件添加权限，文件或者目录的权限分为三类，权限也分为三类\"><a href=\"#给文件添加权限，文件或者目录的权限分为三类，权限也分为三类\" class=\"headerlink\" title=\"给文件添加权限，文件或者目录的权限分为三类，权限也分为三类\"></a>给文件添加权限，文件或者目录的权限分为三类，权限也分为三类</h3><blockquote>\n<h4 id=\"1-用户-user-u-：文件拥有者\"><a href=\"#1-用户-user-u-：文件拥有者\" class=\"headerlink\" title=\"1.用户(user,u)：文件拥有者\"></a>1.用户(user,u)：文件拥有者</h4><h4 id=\"2-组-group-g-：与user在同一组的用户\"><a href=\"#2-组-group-g-：与user在同一组的用户\" class=\"headerlink\" title=\"2.组(group,g)：与user在同一组的用户\"></a>2.组(group,g)：与user在同一组的用户</h4><h4 id=\"3-其他人-others-o-：所有其他用户\"><a href=\"#3-其他人-others-o-：所有其他用户\" class=\"headerlink\" title=\"3.其他人(others,o)：所有其他用户\"></a>3.其他人(others,o)：所有其他用户</h4><h4 id=\"读，r，值为-4-；写，w，值为-2-；执行，x，值为-1\"><a href=\"#读，r，值为-4-；写，w，值为-2-；执行，x，值为-1\" class=\"headerlink\" title=\"读，r，值为 4 ；写，w，值为 2 ；执行，x，值为 1\"></a>读，r，值为 4 ；写，w，值为 2 ；执行，x，值为 1</h4></blockquote>\n<h3 id=\"赋予所有用户执行权，添加权限就是-，移除权限就是\"><a href=\"#赋予所有用户执行权，添加权限就是-，移除权限就是\" class=\"headerlink\" title=\"赋予所有用户执行权，添加权限就是 + ，移除权限就是 -\"></a>赋予所有用户执行权，添加权限就是 + ，移除权限就是 -</h3><blockquote>\n<h3 id=\"chmod-a-x-file\"><a href=\"#chmod-a-x-file\" class=\"headerlink\" title=\"$ chmod a+x file\"></a>$ chmod a+x file</h3></blockquote>\n<h3 id=\"查看文件-目录权限\"><a href=\"#查看文件-目录权限\" class=\"headerlink\" title=\"查看文件&#x2F;目录权限\"></a>查看文件&#x2F;目录权限</h3><blockquote>\n<h3 id=\"ls-l-file\"><a href=\"#ls-l-file\" class=\"headerlink\" title=\"$ ls -l file\"></a>$ ls -l file</h3></blockquote>\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_3.png\" alt=\"pic\"></p>\n<h3 id=\"要在命令框中直接执行-py-文件，不使用-python3-file-py，要指定编译器，在-py-文件顶部添加\"><a href=\"#要在命令框中直接执行-py-文件，不使用-python3-file-py，要指定编译器，在-py-文件顶部添加\" class=\"headerlink\" title=\"要在命令框中直接执行 .py 文件，不使用 python3 file.py，要指定编译器，在 .py 文件顶部添加\"></a>要在命令框中直接执行 .py 文件，不使用 python3 file.py，要指定编译器，在 .py 文件顶部添加</h3><blockquote>\n<h3 id=\"usr-bin-python3\"><a href=\"#usr-bin-python3\" class=\"headerlink\" title=\"#!usr&#x2F;bin&#x2F;python3\"></a>#!usr&#x2F;bin&#x2F;python3</h3><h3 id=\"然后就可以使用\"><a href=\"#然后就可以使用\" class=\"headerlink\" title=\"然后就可以使用\"></a>然后就可以使用</h3><h3 id=\"file-py\"><a href=\"#file-py\" class=\"headerlink\" title=\"$ .&#x2F;file.py\"></a>$ .&#x2F;file.py</h3></blockquote>\n<h3 id=\"echo-指令的使用，echo-本身的功能就是打印其后的字符\"><a href=\"#echo-指令的使用，echo-本身的功能就是打印其后的字符\" class=\"headerlink\" title=\"echo 指令的使用，echo 本身的功能就是打印其后的字符\"></a>echo 指令的使用，echo 本身的功能就是打印其后的字符</h3><blockquote>\n<h3 id=\"可以找到跟在其后的字符的环境变量，echo-就能打印该环境变量\"><a href=\"#可以找到跟在其后的字符的环境变量，echo-就能打印该环境变量\" class=\"headerlink\" title=\"$ 可以找到跟在其后的字符的环境变量，echo 就能打印该环境变量\"></a>$ 可以找到跟在其后的字符的环境变量，echo 就能打印该环境变量</h3><h3 id=\"echo-ROS-VERSION\"><a href=\"#echo-ROS-VERSION\" class=\"headerlink\" title=\"$ echo $ROS_VERSION\"></a>$ echo $ROS_VERSION</h3><h3 id=\"查看-ros-的发行版本\"><a href=\"#查看-ros-的发行版本\" class=\"headerlink\" title=\"# 查看 ros 的发行版本\"></a># 查看 ros 的发行版本</h3><h3 id=\"echo-ROS-DISTRO\"><a href=\"#echo-ROS-DISTRO\" class=\"headerlink\" title=\"$ echo $ROS_DISTRO\"></a>$ echo $ROS_DISTRO</h3></blockquote>\n<h3 id=\"查看所有环境变量列表\"><a href=\"#查看所有环境变量列表\" class=\"headerlink\" title=\"查看所有环境变量列表\"></a>查看所有环境变量列表</h3><blockquote>\n<h3 id=\"printenv\"><a href=\"#printenv\" class=\"headerlink\" title=\"$ printenv\"></a>$ printenv</h3></blockquote>\n","cover_type":"img","excerpt":"","more":"<h1 id=\"常用的-Linux-操作命令\"><a href=\"#常用的-Linux-操作命令\" class=\"headerlink\" title=\"常用的 Linux 操作命令\"></a>常用的 Linux 操作命令</h1><h3 id=\"当前终端所在目录\"><a href=\"#当前终端所在目录\" class=\"headerlink\" title=\"当前终端所在目录\"></a>当前终端所在目录</h3><blockquote>\n<h3 id=\"pwd\"><a href=\"#pwd\" class=\"headerlink\" title=\"$ pwd\"></a>$ pwd</h3></blockquote>\n<h3 id=\"查看文件内容\"><a href=\"#查看文件内容\" class=\"headerlink\" title=\"查看文件内容\"></a>查看文件内容</h3><blockquote>\n<h3 id=\"cat-name-txt\"><a href=\"#cat-name-txt\" class=\"headerlink\" title=\"$ cat name.txt\"></a>$ cat name.txt</h3></blockquote>\n<h3 id=\"卸载某个包\"><a href=\"#卸载某个包\" class=\"headerlink\" title=\"卸载某个包\"></a>卸载某个包</h3><blockquote>\n<h3 id=\"sudo-apt-remove-pkg-name\"><a href=\"#sudo-apt-remove-pkg-name\" class=\"headerlink\" title=\"$ sudo apt remove pkg_name\"></a>$ sudo apt remove pkg_name</h3></blockquote>\n<h3 id=\"安装-deb\"><a href=\"#安装-deb\" class=\"headerlink\" title=\"安装 .deb\"></a>安装 .deb</h3><blockquote>\n<h3 id=\"sudo-dpkg-i-file-name-deb\"><a href=\"#sudo-dpkg-i-file-name-deb\" class=\"headerlink\" title=\"$ sudo dpkg -i file_name.deb\"></a>$ sudo dpkg -i file_name.deb</h3></blockquote>\n<h3 id=\"解压-tar-gz-tgz\"><a href=\"#解压-tar-gz-tgz\" class=\"headerlink\" title=\"解压 .tar.gz&#x2F;.tgz\"></a>解压 .tar.gz&#x2F;.tgz</h3><blockquote>\n<h3 id=\"tar-xzvf-file-tar-gz-C-path\"><a href=\"#tar-xzvf-file-tar-gz-C-path\" class=\"headerlink\" title=\"$ tar -xzvf file.tar.gz -C &#x2F;path\"></a>$ tar -xzvf file.tar.gz -C &#x2F;path</h3></blockquote>\n<h3 id=\"解压-zip\"><a href=\"#解压-zip\" class=\"headerlink\" title=\"解压 .zip\"></a>解压 .zip</h3><blockquote>\n<h3 id=\"unzip-file-zip-d-path\"><a href=\"#unzip-file-zip-d-path\" class=\"headerlink\" title=\"$ unzip file.zip -d &#x2F;path\"></a>$ unzip file.zip -d &#x2F;path</h3></blockquote>\n<h3 id=\"解压-rar\"><a href=\"#解压-rar\" class=\"headerlink\" title=\"解压 .rar\"></a>解压 .rar</h3><blockquote>\n<h3 id=\"unrar-x-file-rar\"><a href=\"#unrar-x-file-rar\" class=\"headerlink\" title=\"$ unrar x file.rar\"></a>$ unrar x file.rar</h3></blockquote>\n<h3 id=\"查找某个文件\"><a href=\"#查找某个文件\" class=\"headerlink\" title=\"查找某个文件\"></a>查找某个文件</h3><blockquote>\n<h3 id=\"whereis-file-name\"><a href=\"#whereis-file-name\" class=\"headerlink\" title=\"$ whereis file_name\"></a>$ whereis file_name</h3></blockquote>\n<h3 id=\"使用-nano-编辑文本\"><a href=\"#使用-nano-编辑文本\" class=\"headerlink\" title=\"使用 nano 编辑文本\"></a>使用 nano 编辑文本</h3><blockquote>\n<h3 id=\"nano-file-name\"><a href=\"#nano-file-name\" class=\"headerlink\" title=\"$ nano file_name\"></a>$ nano file_name</h3></blockquote>\n<h3 id=\"查看执行过的命令\"><a href=\"#查看执行过的命令\" class=\"headerlink\" title=\"查看执行过的命令\"></a>查看执行过的命令</h3><blockquote>\n<h3 id=\"history\"><a href=\"#history\" class=\"headerlink\" title=\"$ history\"></a>$ history</h3></blockquote>\n<h3 id=\"终端中的复制粘贴快捷键\"><a href=\"#终端中的复制粘贴快捷键\" class=\"headerlink\" title=\"终端中的复制粘贴快捷键\"></a>终端中的复制粘贴快捷键</h3><blockquote>\n<h3 id=\"ctrl-shift-C-ctrl-shift-V\"><a href=\"#ctrl-shift-C-ctrl-shift-V\" class=\"headerlink\" title=\"ctrl + shift + C &#x2F; ctrl + shift + V\"></a>ctrl + shift + C &#x2F; ctrl + shift + V</h3></blockquote>\n<h3 id=\"终端中执行-py\"><a href=\"#终端中执行-py\" class=\"headerlink\" title=\"终端中执行 .py\"></a>终端中执行 .py</h3><blockquote>\n<h3 id=\"python3-file-name-py\"><a href=\"#python3-file-name-py\" class=\"headerlink\" title=\"$ python3 file_name.py\"></a>$ python3 file_name.py</h3></blockquote>\n<h3 id=\"给文件添加权限，文件或者目录的权限分为三类，权限也分为三类\"><a href=\"#给文件添加权限，文件或者目录的权限分为三类，权限也分为三类\" class=\"headerlink\" title=\"给文件添加权限，文件或者目录的权限分为三类，权限也分为三类\"></a>给文件添加权限，文件或者目录的权限分为三类，权限也分为三类</h3><blockquote>\n<h4 id=\"1-用户-user-u-：文件拥有者\"><a href=\"#1-用户-user-u-：文件拥有者\" class=\"headerlink\" title=\"1.用户(user,u)：文件拥有者\"></a>1.用户(user,u)：文件拥有者</h4><h4 id=\"2-组-group-g-：与user在同一组的用户\"><a href=\"#2-组-group-g-：与user在同一组的用户\" class=\"headerlink\" title=\"2.组(group,g)：与user在同一组的用户\"></a>2.组(group,g)：与user在同一组的用户</h4><h4 id=\"3-其他人-others-o-：所有其他用户\"><a href=\"#3-其他人-others-o-：所有其他用户\" class=\"headerlink\" title=\"3.其他人(others,o)：所有其他用户\"></a>3.其他人(others,o)：所有其他用户</h4><h4 id=\"读，r，值为-4-；写，w，值为-2-；执行，x，值为-1\"><a href=\"#读，r，值为-4-；写，w，值为-2-；执行，x，值为-1\" class=\"headerlink\" title=\"读，r，值为 4 ；写，w，值为 2 ；执行，x，值为 1\"></a>读，r，值为 4 ；写，w，值为 2 ；执行，x，值为 1</h4></blockquote>\n<h3 id=\"赋予所有用户执行权，添加权限就是-，移除权限就是\"><a href=\"#赋予所有用户执行权，添加权限就是-，移除权限就是\" class=\"headerlink\" title=\"赋予所有用户执行权，添加权限就是 + ，移除权限就是 -\"></a>赋予所有用户执行权，添加权限就是 + ，移除权限就是 -</h3><blockquote>\n<h3 id=\"chmod-a-x-file\"><a href=\"#chmod-a-x-file\" class=\"headerlink\" title=\"$ chmod a+x file\"></a>$ chmod a+x file</h3></blockquote>\n<h3 id=\"查看文件-目录权限\"><a href=\"#查看文件-目录权限\" class=\"headerlink\" title=\"查看文件&#x2F;目录权限\"></a>查看文件&#x2F;目录权限</h3><blockquote>\n<h3 id=\"ls-l-file\"><a href=\"#ls-l-file\" class=\"headerlink\" title=\"$ ls -l file\"></a>$ ls -l file</h3></blockquote>\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_3.png\" alt=\"pic\"></p>\n<h3 id=\"要在命令框中直接执行-py-文件，不使用-python3-file-py，要指定编译器，在-py-文件顶部添加\"><a href=\"#要在命令框中直接执行-py-文件，不使用-python3-file-py，要指定编译器，在-py-文件顶部添加\" class=\"headerlink\" title=\"要在命令框中直接执行 .py 文件，不使用 python3 file.py，要指定编译器，在 .py 文件顶部添加\"></a>要在命令框中直接执行 .py 文件，不使用 python3 file.py，要指定编译器，在 .py 文件顶部添加</h3><blockquote>\n<h3 id=\"usr-bin-python3\"><a href=\"#usr-bin-python3\" class=\"headerlink\" title=\"#!usr&#x2F;bin&#x2F;python3\"></a>#!usr&#x2F;bin&#x2F;python3</h3><h3 id=\"然后就可以使用\"><a href=\"#然后就可以使用\" class=\"headerlink\" title=\"然后就可以使用\"></a>然后就可以使用</h3><h3 id=\"file-py\"><a href=\"#file-py\" class=\"headerlink\" title=\"$ .&#x2F;file.py\"></a>$ .&#x2F;file.py</h3></blockquote>\n<h3 id=\"echo-指令的使用，echo-本身的功能就是打印其后的字符\"><a href=\"#echo-指令的使用，echo-本身的功能就是打印其后的字符\" class=\"headerlink\" title=\"echo 指令的使用，echo 本身的功能就是打印其后的字符\"></a>echo 指令的使用，echo 本身的功能就是打印其后的字符</h3><blockquote>\n<h3 id=\"可以找到跟在其后的字符的环境变量，echo-就能打印该环境变量\"><a href=\"#可以找到跟在其后的字符的环境变量，echo-就能打印该环境变量\" class=\"headerlink\" title=\"$ 可以找到跟在其后的字符的环境变量，echo 就能打印该环境变量\"></a>$ 可以找到跟在其后的字符的环境变量，echo 就能打印该环境变量</h3><h3 id=\"echo-ROS-VERSION\"><a href=\"#echo-ROS-VERSION\" class=\"headerlink\" title=\"$ echo $ROS_VERSION\"></a>$ echo $ROS_VERSION</h3><h3 id=\"查看-ros-的发行版本\"><a href=\"#查看-ros-的发行版本\" class=\"headerlink\" title=\"# 查看 ros 的发行版本\"></a># 查看 ros 的发行版本</h3><h3 id=\"echo-ROS-DISTRO\"><a href=\"#echo-ROS-DISTRO\" class=\"headerlink\" title=\"$ echo $ROS_DISTRO\"></a>$ echo $ROS_DISTRO</h3></blockquote>\n<h3 id=\"查看所有环境变量列表\"><a href=\"#查看所有环境变量列表\" class=\"headerlink\" title=\"查看所有环境变量列表\"></a>查看所有环境变量列表</h3><blockquote>\n<h3 id=\"printenv\"><a href=\"#printenv\" class=\"headerlink\" title=\"$ printenv\"></a>$ printenv</h3></blockquote>\n"},{"title":"第三弹","data":"2024-12-31T17:16:00.000Z","updated":"2024-12-31T17:16:00.000Z","type":"ros2_learning","cover":"http://picbed.yanzu.tech/img/post_cover/p3.png","top_img":null,"_content":"# 使用 CMakeLists 管理 cpp\n\n### 在当前目录下创建一个 cpp 文件，如 hello.cpp，内容就是输出 hello world\n\n### 再创建一个 CMakeLists.txt 的文件，内容如下:\n\n> ### # 指定 cmake 的最小版本\n>\n> ### cmake_minimum_required(VERSION 3.8)\n>\n> ### # 项目名称\n>\n> ### project(hello)\n>\n> ### # 添加可执行文件，(可执行文件名，源文件名)\n>\n> ### add_executable(learn_cmake hello.cpp)\n\n### 然后执行命令(当前路径下打开命令框)，将 cmake 转换为 makefile\n\n> ### # cmake 后面跟的是目录\n>\n> ### $ cmake .\n\n### 再执行命令，编译生成可执行文件\n\n> ### $ make\n\n### 此时就生成了可执行文件 learn_cmake 了，可以直接运行\n\n> ### $ ./learn_cmake\n\n","source":"_posts/03.md","raw":"---\ntitle: 第三弹\ndata: 2025-01-01 01:16:00\nupdated: 2025-01-01 01:16:00\ntype: ros2_learning\ncover: http://picbed.yanzu.tech/img/post_cover/p3.png\ntop_img:\ntags:\n  - ROS2\n  - Learning\n---\n# 使用 CMakeLists 管理 cpp\n\n### 在当前目录下创建一个 cpp 文件，如 hello.cpp，内容就是输出 hello world\n\n### 再创建一个 CMakeLists.txt 的文件，内容如下:\n\n> ### # 指定 cmake 的最小版本\n>\n> ### cmake_minimum_required(VERSION 3.8)\n>\n> ### # 项目名称\n>\n> ### project(hello)\n>\n> ### # 添加可执行文件，(可执行文件名，源文件名)\n>\n> ### add_executable(learn_cmake hello.cpp)\n\n### 然后执行命令(当前路径下打开命令框)，将 cmake 转换为 makefile\n\n> ### # cmake 后面跟的是目录\n>\n> ### $ cmake .\n\n### 再执行命令，编译生成可执行文件\n\n> ### $ make\n\n### 此时就生成了可执行文件 learn_cmake 了，可以直接运行\n\n> ### $ ./learn_cmake\n\n","slug":"03","published":1,"date":"2024-12-31T17:24:39.501Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cqk0006iku4bysj6yuz","content":"<h1 id=\"使用-CMakeLists-管理-cpp\"><a href=\"#使用-CMakeLists-管理-cpp\" class=\"headerlink\" title=\"使用 CMakeLists 管理 cpp\"></a>使用 CMakeLists 管理 cpp</h1><h3 id=\"在当前目录下创建一个-cpp-文件，如-hello-cpp，内容就是输出-hello-world\"><a href=\"#在当前目录下创建一个-cpp-文件，如-hello-cpp，内容就是输出-hello-world\" class=\"headerlink\" title=\"在当前目录下创建一个 cpp 文件，如 hello.cpp，内容就是输出 hello world\"></a>在当前目录下创建一个 cpp 文件，如 hello.cpp，内容就是输出 hello world</h3><h3 id=\"再创建一个-CMakeLists-txt-的文件，内容如下\"><a href=\"#再创建一个-CMakeLists-txt-的文件，内容如下\" class=\"headerlink\" title=\"再创建一个 CMakeLists.txt 的文件，内容如下:\"></a>再创建一个 CMakeLists.txt 的文件，内容如下:</h3><blockquote>\n<h3 id=\"指定-cmake-的最小版本\"><a href=\"#指定-cmake-的最小版本\" class=\"headerlink\" title=\"# 指定 cmake 的最小版本\"></a># 指定 cmake 的最小版本</h3><h3 id=\"cmake-minimum-required-VERSION-3-8\"><a href=\"#cmake-minimum-required-VERSION-3-8\" class=\"headerlink\" title=\"cmake_minimum_required(VERSION 3.8)\"></a>cmake_minimum_required(VERSION 3.8)</h3><h3 id=\"项目名称\"><a href=\"#项目名称\" class=\"headerlink\" title=\"# 项目名称\"></a># 项目名称</h3><h3 id=\"project-hello\"><a href=\"#project-hello\" class=\"headerlink\" title=\"project(hello)\"></a>project(hello)</h3><h3 id=\"添加可执行文件，-可执行文件名，源文件名\"><a href=\"#添加可执行文件，-可执行文件名，源文件名\" class=\"headerlink\" title=\"# 添加可执行文件，(可执行文件名，源文件名)\"></a># 添加可执行文件，(可执行文件名，源文件名)</h3><h3 id=\"add-executable-learn-cmake-hello-cpp\"><a href=\"#add-executable-learn-cmake-hello-cpp\" class=\"headerlink\" title=\"add_executable(learn_cmake hello.cpp)\"></a>add_executable(learn_cmake hello.cpp)</h3></blockquote>\n<h3 id=\"然后执行命令-当前路径下打开命令框-，将-cmake-转换为-makefile\"><a href=\"#然后执行命令-当前路径下打开命令框-，将-cmake-转换为-makefile\" class=\"headerlink\" title=\"然后执行命令(当前路径下打开命令框)，将 cmake 转换为 makefile\"></a>然后执行命令(当前路径下打开命令框)，将 cmake 转换为 makefile</h3><blockquote>\n<h3 id=\"cmake-后面跟的是目录\"><a href=\"#cmake-后面跟的是目录\" class=\"headerlink\" title=\"# cmake 后面跟的是目录\"></a># cmake 后面跟的是目录</h3><h3 id=\"cmake\"><a href=\"#cmake\" class=\"headerlink\" title=\"$ cmake .\"></a>$ cmake .</h3></blockquote>\n<h3 id=\"再执行命令，编译生成可执行文件\"><a href=\"#再执行命令，编译生成可执行文件\" class=\"headerlink\" title=\"再执行命令，编译生成可执行文件\"></a>再执行命令，编译生成可执行文件</h3><blockquote>\n<h3 id=\"make\"><a href=\"#make\" class=\"headerlink\" title=\"$ make\"></a>$ make</h3></blockquote>\n<h3 id=\"此时就生成了可执行文件-learn-cmake-了，可以直接运行\"><a href=\"#此时就生成了可执行文件-learn-cmake-了，可以直接运行\" class=\"headerlink\" title=\"此时就生成了可执行文件 learn_cmake 了，可以直接运行\"></a>此时就生成了可执行文件 learn_cmake 了，可以直接运行</h3><blockquote>\n<h3 id=\"learn-cmake\"><a href=\"#learn-cmake\" class=\"headerlink\" title=\"$ .&#x2F;learn_cmake\"></a>$ .&#x2F;learn_cmake</h3></blockquote>\n","cover_type":"img","excerpt":"","more":"<h1 id=\"使用-CMakeLists-管理-cpp\"><a href=\"#使用-CMakeLists-管理-cpp\" class=\"headerlink\" title=\"使用 CMakeLists 管理 cpp\"></a>使用 CMakeLists 管理 cpp</h1><h3 id=\"在当前目录下创建一个-cpp-文件，如-hello-cpp，内容就是输出-hello-world\"><a href=\"#在当前目录下创建一个-cpp-文件，如-hello-cpp，内容就是输出-hello-world\" class=\"headerlink\" title=\"在当前目录下创建一个 cpp 文件，如 hello.cpp，内容就是输出 hello world\"></a>在当前目录下创建一个 cpp 文件，如 hello.cpp，内容就是输出 hello world</h3><h3 id=\"再创建一个-CMakeLists-txt-的文件，内容如下\"><a href=\"#再创建一个-CMakeLists-txt-的文件，内容如下\" class=\"headerlink\" title=\"再创建一个 CMakeLists.txt 的文件，内容如下:\"></a>再创建一个 CMakeLists.txt 的文件，内容如下:</h3><blockquote>\n<h3 id=\"指定-cmake-的最小版本\"><a href=\"#指定-cmake-的最小版本\" class=\"headerlink\" title=\"# 指定 cmake 的最小版本\"></a># 指定 cmake 的最小版本</h3><h3 id=\"cmake-minimum-required-VERSION-3-8\"><a href=\"#cmake-minimum-required-VERSION-3-8\" class=\"headerlink\" title=\"cmake_minimum_required(VERSION 3.8)\"></a>cmake_minimum_required(VERSION 3.8)</h3><h3 id=\"项目名称\"><a href=\"#项目名称\" class=\"headerlink\" title=\"# 项目名称\"></a># 项目名称</h3><h3 id=\"project-hello\"><a href=\"#project-hello\" class=\"headerlink\" title=\"project(hello)\"></a>project(hello)</h3><h3 id=\"添加可执行文件，-可执行文件名，源文件名\"><a href=\"#添加可执行文件，-可执行文件名，源文件名\" class=\"headerlink\" title=\"# 添加可执行文件，(可执行文件名，源文件名)\"></a># 添加可执行文件，(可执行文件名，源文件名)</h3><h3 id=\"add-executable-learn-cmake-hello-cpp\"><a href=\"#add-executable-learn-cmake-hello-cpp\" class=\"headerlink\" title=\"add_executable(learn_cmake hello.cpp)\"></a>add_executable(learn_cmake hello.cpp)</h3></blockquote>\n<h3 id=\"然后执行命令-当前路径下打开命令框-，将-cmake-转换为-makefile\"><a href=\"#然后执行命令-当前路径下打开命令框-，将-cmake-转换为-makefile\" class=\"headerlink\" title=\"然后执行命令(当前路径下打开命令框)，将 cmake 转换为 makefile\"></a>然后执行命令(当前路径下打开命令框)，将 cmake 转换为 makefile</h3><blockquote>\n<h3 id=\"cmake-后面跟的是目录\"><a href=\"#cmake-后面跟的是目录\" class=\"headerlink\" title=\"# cmake 后面跟的是目录\"></a># cmake 后面跟的是目录</h3><h3 id=\"cmake\"><a href=\"#cmake\" class=\"headerlink\" title=\"$ cmake .\"></a>$ cmake .</h3></blockquote>\n<h3 id=\"再执行命令，编译生成可执行文件\"><a href=\"#再执行命令，编译生成可执行文件\" class=\"headerlink\" title=\"再执行命令，编译生成可执行文件\"></a>再执行命令，编译生成可执行文件</h3><blockquote>\n<h3 id=\"make\"><a href=\"#make\" class=\"headerlink\" title=\"$ make\"></a>$ make</h3></blockquote>\n<h3 id=\"此时就生成了可执行文件-learn-cmake-了，可以直接运行\"><a href=\"#此时就生成了可执行文件-learn-cmake-了，可以直接运行\" class=\"headerlink\" title=\"此时就生成了可执行文件 learn_cmake 了，可以直接运行\"></a>此时就生成了可执行文件 learn_cmake 了，可以直接运行</h3><blockquote>\n<h3 id=\"learn-cmake\"><a href=\"#learn-cmake\" class=\"headerlink\" title=\"$ .&#x2F;learn_cmake\"></a>$ .&#x2F;learn_cmake</h3></blockquote>\n"},{"title":"Ubuntu下安装与激活Typora","data":"2025-01-02T10:10:00.000Z","updated":"2025-01-02T10:10:00.000Z","type":"安装教程","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p4.png","_content":"\n# Ubuntu下安装与激活Typora\n\n## 1.下载\n\n### 直接在官网下载最新版 https://typora.io/releases/all\n\n### 下载好 deb 之后，安装\n\n> ### sudo dpkg -i xxx.deb\n\n\n\n## 2.激活\n\n### 在home下打开终端，依次执行以下命令\n\n> ### git clone https://github.com/hazukieq/Yporaject.git\n>\n> ### sudo apt install cargo\n>\n> ### cd Yporaject/\n>\n> ### # 这一步最好打开梯子 \n>\n> ### cargo build\n>\n> ### 最后显示 finished 就说明成功了\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_4.png)\n>\n> ### # ls一下，看看有没有出现可执行文件 node_inject\n>\n> ### ls target/debug\n>\n> ### cargo run\n>\n> ### sudo cp target/debug/node_inject /usr/share/typora\n>\n> ### 上面的终端不要关闭\n\n### 在 home 下打开新的终端，依次执行一下命令，\n\n> ### cd /usr/share/typora/\n>\n> ### # 权限拉闷\n>\n> ### sudo chmod 777 node_inject\n>\n> ### sudo ./node_inject\n>\n> ### # 终端输出了以下信息就说明可以了\n>\n> ### extracting node_modules.asar\n> ### adding hook.js\n> ### applying patch\n> ### packing node_modules.asar\n> ### done!\n>\n> ### 可以关掉这个终端了\n\n### 返回之前的终端，依次执行以下命令，\n\n> ### cd license-gen/\n>\n> ### # 这一步同样打开梯子进行\n>\n> ### cargo build\n>\n> ### cargo run\n>\n> ### 然后就得到了激活码了\n\n\n\n### 这个激活教程真的太6了，当然不是我自己原创，在“垃圾堆”里面找到的，原作者也不知道是不是他，贴一手地址，\n\n### https://blog.csdn.net/weixin_65657501/article/details/142788747\n\n","source":"_posts/04.md","raw":"---\ntitle: Ubuntu下安装与激活Typora\ndata: 2025-01-02 18:10:00\nupdated: 2025-01-02 18:10:00\ntype: 安装教程\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p4.png\ntags:\n  - Ubuntu\n  - 安装教程\n---\n\n# Ubuntu下安装与激活Typora\n\n## 1.下载\n\n### 直接在官网下载最新版 https://typora.io/releases/all\n\n### 下载好 deb 之后，安装\n\n> ### sudo dpkg -i xxx.deb\n\n\n\n## 2.激活\n\n### 在home下打开终端，依次执行以下命令\n\n> ### git clone https://github.com/hazukieq/Yporaject.git\n>\n> ### sudo apt install cargo\n>\n> ### cd Yporaject/\n>\n> ### # 这一步最好打开梯子 \n>\n> ### cargo build\n>\n> ### 最后显示 finished 就说明成功了\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_4.png)\n>\n> ### # ls一下，看看有没有出现可执行文件 node_inject\n>\n> ### ls target/debug\n>\n> ### cargo run\n>\n> ### sudo cp target/debug/node_inject /usr/share/typora\n>\n> ### 上面的终端不要关闭\n\n### 在 home 下打开新的终端，依次执行一下命令，\n\n> ### cd /usr/share/typora/\n>\n> ### # 权限拉闷\n>\n> ### sudo chmod 777 node_inject\n>\n> ### sudo ./node_inject\n>\n> ### # 终端输出了以下信息就说明可以了\n>\n> ### extracting node_modules.asar\n> ### adding hook.js\n> ### applying patch\n> ### packing node_modules.asar\n> ### done!\n>\n> ### 可以关掉这个终端了\n\n### 返回之前的终端，依次执行以下命令，\n\n> ### cd license-gen/\n>\n> ### # 这一步同样打开梯子进行\n>\n> ### cargo build\n>\n> ### cargo run\n>\n> ### 然后就得到了激活码了\n\n\n\n### 这个激活教程真的太6了，当然不是我自己原创，在“垃圾堆”里面找到的，原作者也不知道是不是他，贴一手地址，\n\n### https://blog.csdn.net/weixin_65657501/article/details/142788747\n\n","slug":"04","published":1,"date":"2025-01-02T10:08:24.375Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cql0008iku4amx06iqv","content":"<h1 id=\"Ubuntu下安装与激活Typora\"><a href=\"#Ubuntu下安装与激活Typora\" class=\"headerlink\" title=\"Ubuntu下安装与激活Typora\"></a>Ubuntu下安装与激活Typora</h1><h2 id=\"1-下载\"><a href=\"#1-下载\" class=\"headerlink\" title=\"1.下载\"></a>1.下载</h2><h3 id=\"直接在官网下载最新版-https-typora-io-releases-all\"><a href=\"#直接在官网下载最新版-https-typora-io-releases-all\" class=\"headerlink\" title=\"直接在官网下载最新版 https://typora.io/releases/all\"></a>直接在官网下载最新版 <a href=\"https://typora.io/releases/all\">https://typora.io/releases/all</a></h3><h3 id=\"下载好-deb-之后，安装\"><a href=\"#下载好-deb-之后，安装\" class=\"headerlink\" title=\"下载好 deb 之后，安装\"></a>下载好 deb 之后，安装</h3><blockquote>\n<h3 id=\"sudo-dpkg-i-xxx-deb\"><a href=\"#sudo-dpkg-i-xxx-deb\" class=\"headerlink\" title=\"sudo dpkg -i xxx.deb\"></a>sudo dpkg -i xxx.deb</h3></blockquote>\n<h2 id=\"2-激活\"><a href=\"#2-激活\" class=\"headerlink\" title=\"2.激活\"></a>2.激活</h2><h3 id=\"在home下打开终端，依次执行以下命令\"><a href=\"#在home下打开终端，依次执行以下命令\" class=\"headerlink\" title=\"在home下打开终端，依次执行以下命令\"></a>在home下打开终端，依次执行以下命令</h3><blockquote>\n<h3 id=\"git-clone-https-github-com-hazukieq-Yporaject-git\"><a href=\"#git-clone-https-github-com-hazukieq-Yporaject-git\" class=\"headerlink\" title=\"git clone https://github.com/hazukieq/Yporaject.git\"></a>git clone <a href=\"https://github.com/hazukieq/Yporaject.git\">https://github.com/hazukieq/Yporaject.git</a></h3><h3 id=\"sudo-apt-install-cargo\"><a href=\"#sudo-apt-install-cargo\" class=\"headerlink\" title=\"sudo apt install cargo\"></a>sudo apt install cargo</h3><h3 id=\"cd-Yporaject\"><a href=\"#cd-Yporaject\" class=\"headerlink\" title=\"cd Yporaject&#x2F;\"></a>cd Yporaject&#x2F;</h3><h3 id=\"这一步最好打开梯子\"><a href=\"#这一步最好打开梯子\" class=\"headerlink\" title=\"# 这一步最好打开梯子\"></a># 这一步最好打开梯子</h3><h3 id=\"cargo-build\"><a href=\"#cargo-build\" class=\"headerlink\" title=\"cargo build\"></a>cargo build</h3><h3 id=\"最后显示-finished-就说明成功了\"><a href=\"#最后显示-finished-就说明成功了\" class=\"headerlink\" title=\"最后显示 finished 就说明成功了\"></a>最后显示 finished 就说明成功了</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_4.png\"></p>\n<h3 id=\"ls一下，看看有没有出现可执行文件-node-inject\"><a href=\"#ls一下，看看有没有出现可执行文件-node-inject\" class=\"headerlink\" title=\"# ls一下，看看有没有出现可执行文件 node_inject\"></a># ls一下，看看有没有出现可执行文件 node_inject</h3><h3 id=\"ls-target-debug\"><a href=\"#ls-target-debug\" class=\"headerlink\" title=\"ls target&#x2F;debug\"></a>ls target&#x2F;debug</h3><h3 id=\"cargo-run\"><a href=\"#cargo-run\" class=\"headerlink\" title=\"cargo run\"></a>cargo run</h3><h3 id=\"sudo-cp-target-debug-node-inject-usr-share-typora\"><a href=\"#sudo-cp-target-debug-node-inject-usr-share-typora\" class=\"headerlink\" title=\"sudo cp target&#x2F;debug&#x2F;node_inject &#x2F;usr&#x2F;share&#x2F;typora\"></a>sudo cp target&#x2F;debug&#x2F;node_inject &#x2F;usr&#x2F;share&#x2F;typora</h3><h3 id=\"上面的终端不要关闭\"><a href=\"#上面的终端不要关闭\" class=\"headerlink\" title=\"上面的终端不要关闭\"></a>上面的终端不要关闭</h3></blockquote>\n<h3 id=\"在-home-下打开新的终端，依次执行一下命令，\"><a href=\"#在-home-下打开新的终端，依次执行一下命令，\" class=\"headerlink\" title=\"在 home 下打开新的终端，依次执行一下命令，\"></a>在 home 下打开新的终端，依次执行一下命令，</h3><blockquote>\n<h3 id=\"cd-usr-share-typora\"><a href=\"#cd-usr-share-typora\" class=\"headerlink\" title=\"cd &#x2F;usr&#x2F;share&#x2F;typora&#x2F;\"></a>cd &#x2F;usr&#x2F;share&#x2F;typora&#x2F;</h3><h3 id=\"权限拉闷\"><a href=\"#权限拉闷\" class=\"headerlink\" title=\"# 权限拉闷\"></a># 权限拉闷</h3><h3 id=\"sudo-chmod-777-node-inject\"><a href=\"#sudo-chmod-777-node-inject\" class=\"headerlink\" title=\"sudo chmod 777 node_inject\"></a>sudo chmod 777 node_inject</h3><h3 id=\"sudo-node-inject\"><a href=\"#sudo-node-inject\" class=\"headerlink\" title=\"sudo .&#x2F;node_inject\"></a>sudo .&#x2F;node_inject</h3><h3 id=\"终端输出了以下信息就说明可以了\"><a href=\"#终端输出了以下信息就说明可以了\" class=\"headerlink\" title=\"# 终端输出了以下信息就说明可以了\"></a># 终端输出了以下信息就说明可以了</h3><h3 id=\"extracting-node-modules-asar\"><a href=\"#extracting-node-modules-asar\" class=\"headerlink\" title=\"extracting node_modules.asar\"></a>extracting node_modules.asar</h3><h3 id=\"adding-hook-js\"><a href=\"#adding-hook-js\" class=\"headerlink\" title=\"adding hook.js\"></a>adding hook.js</h3><h3 id=\"applying-patch\"><a href=\"#applying-patch\" class=\"headerlink\" title=\"applying patch\"></a>applying patch</h3><h3 id=\"packing-node-modules-asar\"><a href=\"#packing-node-modules-asar\" class=\"headerlink\" title=\"packing node_modules.asar\"></a>packing node_modules.asar</h3><h3 id=\"done\"><a href=\"#done\" class=\"headerlink\" title=\"done!\"></a>done!</h3><h3 id=\"可以关掉这个终端了\"><a href=\"#可以关掉这个终端了\" class=\"headerlink\" title=\"可以关掉这个终端了\"></a>可以关掉这个终端了</h3></blockquote>\n<h3 id=\"返回之前的终端，依次执行以下命令，\"><a href=\"#返回之前的终端，依次执行以下命令，\" class=\"headerlink\" title=\"返回之前的终端，依次执行以下命令，\"></a>返回之前的终端，依次执行以下命令，</h3><blockquote>\n<h3 id=\"cd-license-gen\"><a href=\"#cd-license-gen\" class=\"headerlink\" title=\"cd license-gen&#x2F;\"></a>cd license-gen&#x2F;</h3><h3 id=\"这一步同样打开梯子进行\"><a href=\"#这一步同样打开梯子进行\" class=\"headerlink\" title=\"# 这一步同样打开梯子进行\"></a># 这一步同样打开梯子进行</h3><h3 id=\"cargo-build-1\"><a href=\"#cargo-build-1\" class=\"headerlink\" title=\"cargo build\"></a>cargo build</h3><h3 id=\"cargo-run-1\"><a href=\"#cargo-run-1\" class=\"headerlink\" title=\"cargo run\"></a>cargo run</h3><h3 id=\"然后就得到了激活码了\"><a href=\"#然后就得到了激活码了\" class=\"headerlink\" title=\"然后就得到了激活码了\"></a>然后就得到了激活码了</h3></blockquote>\n<h3 id=\"这个激活教程真的太6了，当然不是我自己原创，在“垃圾堆”里面找到的，原作者也不知道是不是他，贴一手地址，\"><a href=\"#这个激活教程真的太6了，当然不是我自己原创，在“垃圾堆”里面找到的，原作者也不知道是不是他，贴一手地址，\" class=\"headerlink\" title=\"这个激活教程真的太6了，当然不是我自己原创，在“垃圾堆”里面找到的，原作者也不知道是不是他，贴一手地址，\"></a>这个激活教程真的太6了，当然不是我自己原创，在“垃圾堆”里面找到的，原作者也不知道是不是他，贴一手地址，</h3><h3 id=\"https-blog-csdn-net-weixin-65657501-article-details-142788747\"><a href=\"#https-blog-csdn-net-weixin-65657501-article-details-142788747\" class=\"headerlink\" title=\"https://blog.csdn.net/weixin_65657501/article/details/142788747\"></a><a href=\"https://blog.csdn.net/weixin_65657501/article/details/142788747\">https://blog.csdn.net/weixin_65657501/article/details/142788747</a></h3>","cover_type":"img","excerpt":"","more":"<h1 id=\"Ubuntu下安装与激活Typora\"><a href=\"#Ubuntu下安装与激活Typora\" class=\"headerlink\" title=\"Ubuntu下安装与激活Typora\"></a>Ubuntu下安装与激活Typora</h1><h2 id=\"1-下载\"><a href=\"#1-下载\" class=\"headerlink\" title=\"1.下载\"></a>1.下载</h2><h3 id=\"直接在官网下载最新版-https-typora-io-releases-all\"><a href=\"#直接在官网下载最新版-https-typora-io-releases-all\" class=\"headerlink\" title=\"直接在官网下载最新版 https://typora.io/releases/all\"></a>直接在官网下载最新版 <a href=\"https://typora.io/releases/all\">https://typora.io/releases/all</a></h3><h3 id=\"下载好-deb-之后，安装\"><a href=\"#下载好-deb-之后，安装\" class=\"headerlink\" title=\"下载好 deb 之后，安装\"></a>下载好 deb 之后，安装</h3><blockquote>\n<h3 id=\"sudo-dpkg-i-xxx-deb\"><a href=\"#sudo-dpkg-i-xxx-deb\" class=\"headerlink\" title=\"sudo dpkg -i xxx.deb\"></a>sudo dpkg -i xxx.deb</h3></blockquote>\n<h2 id=\"2-激活\"><a href=\"#2-激活\" class=\"headerlink\" title=\"2.激活\"></a>2.激活</h2><h3 id=\"在home下打开终端，依次执行以下命令\"><a href=\"#在home下打开终端，依次执行以下命令\" class=\"headerlink\" title=\"在home下打开终端，依次执行以下命令\"></a>在home下打开终端，依次执行以下命令</h3><blockquote>\n<h3 id=\"git-clone-https-github-com-hazukieq-Yporaject-git\"><a href=\"#git-clone-https-github-com-hazukieq-Yporaject-git\" class=\"headerlink\" title=\"git clone https://github.com/hazukieq/Yporaject.git\"></a>git clone <a href=\"https://github.com/hazukieq/Yporaject.git\">https://github.com/hazukieq/Yporaject.git</a></h3><h3 id=\"sudo-apt-install-cargo\"><a href=\"#sudo-apt-install-cargo\" class=\"headerlink\" title=\"sudo apt install cargo\"></a>sudo apt install cargo</h3><h3 id=\"cd-Yporaject\"><a href=\"#cd-Yporaject\" class=\"headerlink\" title=\"cd Yporaject&#x2F;\"></a>cd Yporaject&#x2F;</h3><h3 id=\"这一步最好打开梯子\"><a href=\"#这一步最好打开梯子\" class=\"headerlink\" title=\"# 这一步最好打开梯子\"></a># 这一步最好打开梯子</h3><h3 id=\"cargo-build\"><a href=\"#cargo-build\" class=\"headerlink\" title=\"cargo build\"></a>cargo build</h3><h3 id=\"最后显示-finished-就说明成功了\"><a href=\"#最后显示-finished-就说明成功了\" class=\"headerlink\" title=\"最后显示 finished 就说明成功了\"></a>最后显示 finished 就说明成功了</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_4.png\"></p>\n<h3 id=\"ls一下，看看有没有出现可执行文件-node-inject\"><a href=\"#ls一下，看看有没有出现可执行文件-node-inject\" class=\"headerlink\" title=\"# ls一下，看看有没有出现可执行文件 node_inject\"></a># ls一下，看看有没有出现可执行文件 node_inject</h3><h3 id=\"ls-target-debug\"><a href=\"#ls-target-debug\" class=\"headerlink\" title=\"ls target&#x2F;debug\"></a>ls target&#x2F;debug</h3><h3 id=\"cargo-run\"><a href=\"#cargo-run\" class=\"headerlink\" title=\"cargo run\"></a>cargo run</h3><h3 id=\"sudo-cp-target-debug-node-inject-usr-share-typora\"><a href=\"#sudo-cp-target-debug-node-inject-usr-share-typora\" class=\"headerlink\" title=\"sudo cp target&#x2F;debug&#x2F;node_inject &#x2F;usr&#x2F;share&#x2F;typora\"></a>sudo cp target&#x2F;debug&#x2F;node_inject &#x2F;usr&#x2F;share&#x2F;typora</h3><h3 id=\"上面的终端不要关闭\"><a href=\"#上面的终端不要关闭\" class=\"headerlink\" title=\"上面的终端不要关闭\"></a>上面的终端不要关闭</h3></blockquote>\n<h3 id=\"在-home-下打开新的终端，依次执行一下命令，\"><a href=\"#在-home-下打开新的终端，依次执行一下命令，\" class=\"headerlink\" title=\"在 home 下打开新的终端，依次执行一下命令，\"></a>在 home 下打开新的终端，依次执行一下命令，</h3><blockquote>\n<h3 id=\"cd-usr-share-typora\"><a href=\"#cd-usr-share-typora\" class=\"headerlink\" title=\"cd &#x2F;usr&#x2F;share&#x2F;typora&#x2F;\"></a>cd &#x2F;usr&#x2F;share&#x2F;typora&#x2F;</h3><h3 id=\"权限拉闷\"><a href=\"#权限拉闷\" class=\"headerlink\" title=\"# 权限拉闷\"></a># 权限拉闷</h3><h3 id=\"sudo-chmod-777-node-inject\"><a href=\"#sudo-chmod-777-node-inject\" class=\"headerlink\" title=\"sudo chmod 777 node_inject\"></a>sudo chmod 777 node_inject</h3><h3 id=\"sudo-node-inject\"><a href=\"#sudo-node-inject\" class=\"headerlink\" title=\"sudo .&#x2F;node_inject\"></a>sudo .&#x2F;node_inject</h3><h3 id=\"终端输出了以下信息就说明可以了\"><a href=\"#终端输出了以下信息就说明可以了\" class=\"headerlink\" title=\"# 终端输出了以下信息就说明可以了\"></a># 终端输出了以下信息就说明可以了</h3><h3 id=\"extracting-node-modules-asar\"><a href=\"#extracting-node-modules-asar\" class=\"headerlink\" title=\"extracting node_modules.asar\"></a>extracting node_modules.asar</h3><h3 id=\"adding-hook-js\"><a href=\"#adding-hook-js\" class=\"headerlink\" title=\"adding hook.js\"></a>adding hook.js</h3><h3 id=\"applying-patch\"><a href=\"#applying-patch\" class=\"headerlink\" title=\"applying patch\"></a>applying patch</h3><h3 id=\"packing-node-modules-asar\"><a href=\"#packing-node-modules-asar\" class=\"headerlink\" title=\"packing node_modules.asar\"></a>packing node_modules.asar</h3><h3 id=\"done\"><a href=\"#done\" class=\"headerlink\" title=\"done!\"></a>done!</h3><h3 id=\"可以关掉这个终端了\"><a href=\"#可以关掉这个终端了\" class=\"headerlink\" title=\"可以关掉这个终端了\"></a>可以关掉这个终端了</h3></blockquote>\n<h3 id=\"返回之前的终端，依次执行以下命令，\"><a href=\"#返回之前的终端，依次执行以下命令，\" class=\"headerlink\" title=\"返回之前的终端，依次执行以下命令，\"></a>返回之前的终端，依次执行以下命令，</h3><blockquote>\n<h3 id=\"cd-license-gen\"><a href=\"#cd-license-gen\" class=\"headerlink\" title=\"cd license-gen&#x2F;\"></a>cd license-gen&#x2F;</h3><h3 id=\"这一步同样打开梯子进行\"><a href=\"#这一步同样打开梯子进行\" class=\"headerlink\" title=\"# 这一步同样打开梯子进行\"></a># 这一步同样打开梯子进行</h3><h3 id=\"cargo-build-1\"><a href=\"#cargo-build-1\" class=\"headerlink\" title=\"cargo build\"></a>cargo build</h3><h3 id=\"cargo-run-1\"><a href=\"#cargo-run-1\" class=\"headerlink\" title=\"cargo run\"></a>cargo run</h3><h3 id=\"然后就得到了激活码了\"><a href=\"#然后就得到了激活码了\" class=\"headerlink\" title=\"然后就得到了激活码了\"></a>然后就得到了激活码了</h3></blockquote>\n<h3 id=\"这个激活教程真的太6了，当然不是我自己原创，在“垃圾堆”里面找到的，原作者也不知道是不是他，贴一手地址，\"><a href=\"#这个激活教程真的太6了，当然不是我自己原创，在“垃圾堆”里面找到的，原作者也不知道是不是他，贴一手地址，\" class=\"headerlink\" title=\"这个激活教程真的太6了，当然不是我自己原创，在“垃圾堆”里面找到的，原作者也不知道是不是他，贴一手地址，\"></a>这个激活教程真的太6了，当然不是我自己原创，在“垃圾堆”里面找到的，原作者也不知道是不是他，贴一手地址，</h3><h3 id=\"https-blog-csdn-net-weixin-65657501-article-details-142788747\"><a href=\"#https-blog-csdn-net-weixin-65657501-article-details-142788747\" class=\"headerlink\" title=\"https://blog.csdn.net/weixin_65657501/article/details/142788747\"></a><a href=\"https://blog.csdn.net/weixin_65657501/article/details/142788747\">https://blog.csdn.net/weixin_65657501/article/details/142788747</a></h3>"},{"title":"第四弹","data":"2025-01-03T12:50:00.000Z","updated":"2025-01-03T12:50:00.000Z","type":"ROS2","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p5.png","_content":"\n# 使用功能包组织cpp节点\n\n> ### 创建一个功能包，这里所在的目录是  ~/learn_ros2/chapter_2/\n>\n> ```shell\n> ros2 pkg create demo_one_pkg --build-type ament_cmake --license Apache-2.0\n> ```\n>\n> ### 创建好之后显示以下信息，\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_5.png)\n>\n> ### 目录结构如下，include/demo_one_pkg 文件用于存放编写的头文件/库文件\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_6.png)\n>\n> ### 创建好包之后，进入 demo_one_pkg/src/ 目录下，创建一个 .cpp 文件，这里叫 demo_one.cpp，内容如下，\n>\n> ```cpp\n> #include \"rclcpp/rclcpp.hpp\"\n> \n> int main(int argc, char **argv){\n>     // 初始化参数\n>     rclcpp::init(argc, argv);\n>     // 创建节点\n>     auto node = std::make_shared<rclcpp::Node>(\"demo_one_node\");\n>     // 打印日志\n>     RCLCPP_INFO(node->get_logger(), \"hello\");\n>     // 运行节点\n>     rclcpp::spin(node);\n>     // 关闭节点\n>     rclcpp::shutdown();\n>     return 0;\n> }\n> ```\n>\n> ### 返回上一级目录，即 demo_one_pkg/，编辑 CMakeLists.txt\n>\n> ```shell\n> cmake_minimum_required(VERSION 3.8)\n> project(demo_one_pkg)\n> \n> if(CMAKE_COMPILER_IS_GNUCXX OR CMAKE_CXX_COMPILER_ID MATCHES \"Clang\")\n>   add_compile_options(-Wall -Wextra -Wpedantic)\n> endif()\n> \n> # 找到依赖\n> find_package(ament_cmake REQUIRED)\n> \n> # 1.查找 rclcpp 的头文件和库文件\n> find_package(rclcpp REQUIRED)\n> \n> # uncomment the following section in order to fill in\n> # further dependencies manually.\n> # find_package(<dependency> REQUIRED)\n> \n> # 2.添加可执行文件，(节点名，源文件地址)\n> add_executable(demo_one_node src/demo_one.cpp)\n> \n> # 头文件包含\n> # target_inlude_directories(demo_one_node PUBLIC ${rclcpp_INCLUDE_DIRS})\n> # 库文件链接\n> # target_link_libraries(demo_one_node ${rclcpp_LIBRARIES})\n> \n> # 上面两句用这一句代替,他是依赖于 find_package(ament_cmake REQUIRED) 的\n> ament_target_dependencies(demo_one_node rclcpp)\n> \n> # 将 demo_one_node 拷贝到 install目录下\n> install(TARGETS demo_one_node\t# 这里执行后目录就已经是 install/demo_one_pkg/\n> # 拷贝到 install/pkg_name/lib/pkg_name/ 下\n> DESTINATION lib/${PROJECT_NAME}\t# 这里执行后最后目录就是 install/demo_one_pkg/lib/demo_one_pkg/\n> )\n> \n> if(BUILD_TESTING)\n>   find_package(ament_lint_auto REQUIRED)\n>   # the following line skips the linter which checks for copyrights\n>   # comment the line when a copyright and license is added to all source files\n>   set(ament_cmake_copyright_FOUND TRUE)\n>   # the following line skips cpplint (only works in a git repo)\n>   # comment the line when this package is in a git repo and when\n>   # a copyright and license is added to all source files\n>   set(ament_cmake_cpplint_FOUND TRUE)\n>   ament_lint_auto_find_test_dependencies()\n> endif()\n> \n> ament_package()\n> ```\n>\n> ### 在与 CMakeLists.txt 同级目录下还有一个 package.xml 文件，在该文件中也要对引用的头文件进行声明\n>\n> ```xml\n> <?xml version=\"1.0\"?>\n> <?xml-model href=\"http://download.ros.org/schema/package_format3.xsd\" schematypens=\"http://www.w3.org/2001/XMLSchema\"?>\n> <package format=\"3\">\n>   <name>demo_one_pkg</name>\n>   <version>0.0.0</version>\n>   <description>TODO: Package description</description>\n>   <maintainer email=\"yanzu@todo.todo\">yanzu</maintainer>\n>   <license>Apache-2.0</license>\n>   \n>   <!-- 在这里添加头文件的声明 -->\n>   <depend>rclcpp</depend>\n>   <buildtool_depend>ament_cmake</buildtool_depend>\n> \n>   <test_depend>ament_lint_auto</test_depend>\n>   <test_depend>ament_lint_common</test_depend>\n> \n>   <export>\n>     <build_type>ament_cmake</build_type>\n>   </export>\n> </package>\n> \n> ```\n>\n> \n>\n> ### 然后就可以对包进行编译了，编译时必须要在包的上一路径打开终端进行编译\n>\n> ### 在正式编译之前，可以在包目录下创建一个 build 文件先测试一下编译能否通过，创建这个build的目的是为了存放编译生成的文件\n>\n> ```shell\n> mkdir build\n> cd build/\n> cmake ../\n> make\n> ```\n>\n> ### 因为 CMakeLists.txt 在上一级目录中，要使其转换为 makefile ，所以是 cmake ../\n>\n> ### 这里在 demo_one_pkg 的上一路径下进行编译，也即 chapter_2/ 路径下进行编译\n>\n> ```shell\n> cd ..\n> colcon build\n> ```\n>\n> ### 编译成功后，显示如下信息\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_7.png)\n>\n> ### 此时，chapter_2/ 路径下分别有这些文件，build、install、log、demo_one_pkg\n>\n> ### 生成的可执行文件 demo_one_node 在 build/pkg_name/ 下，在该路径下，执行以下命令，可以查看当前节点依赖了哪些库，是否链接上\n>\n> ```shell\n> ldd demo_one_node\n> ```\n>\n> ### 正式运行节点之前要添加环境变量，可以使用以下命令查看当前包的环境变量是否配置了，未配置直接运行节点会显示找不到当前的包(以下命令都是在 chapter_2/ 路径下执行)\n>\n> ```shell\n> printenv | grep AMENT\n> ```\n>\n> ### 使用以下命令进行环境变量配置\n>\n> ```shell\n> source install/setup.bash\n> ```\n>\n> ### 再使用上面查看环境变量的命令看看是否配置成功，最后运行节点（ros2 run pkg_name node_name）\n>\n> ```shell\n> ros2 run demo_one_pkg demo_one_node\n> ```\n>\n> \n","source":"_posts/05.md","raw":"---\ntitle: 第四弹\ndata: 2025-01-03 20:50:00\nupdated: 2025-01-03 20:50:00\ntype: ROS2\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p5.png\ntags:\n  - ROS2\n  - Learning\n---\n\n# 使用功能包组织cpp节点\n\n> ### 创建一个功能包，这里所在的目录是  ~/learn_ros2/chapter_2/\n>\n> ```shell\n> ros2 pkg create demo_one_pkg --build-type ament_cmake --license Apache-2.0\n> ```\n>\n> ### 创建好之后显示以下信息，\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_5.png)\n>\n> ### 目录结构如下，include/demo_one_pkg 文件用于存放编写的头文件/库文件\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_6.png)\n>\n> ### 创建好包之后，进入 demo_one_pkg/src/ 目录下，创建一个 .cpp 文件，这里叫 demo_one.cpp，内容如下，\n>\n> ```cpp\n> #include \"rclcpp/rclcpp.hpp\"\n> \n> int main(int argc, char **argv){\n>     // 初始化参数\n>     rclcpp::init(argc, argv);\n>     // 创建节点\n>     auto node = std::make_shared<rclcpp::Node>(\"demo_one_node\");\n>     // 打印日志\n>     RCLCPP_INFO(node->get_logger(), \"hello\");\n>     // 运行节点\n>     rclcpp::spin(node);\n>     // 关闭节点\n>     rclcpp::shutdown();\n>     return 0;\n> }\n> ```\n>\n> ### 返回上一级目录，即 demo_one_pkg/，编辑 CMakeLists.txt\n>\n> ```shell\n> cmake_minimum_required(VERSION 3.8)\n> project(demo_one_pkg)\n> \n> if(CMAKE_COMPILER_IS_GNUCXX OR CMAKE_CXX_COMPILER_ID MATCHES \"Clang\")\n>   add_compile_options(-Wall -Wextra -Wpedantic)\n> endif()\n> \n> # 找到依赖\n> find_package(ament_cmake REQUIRED)\n> \n> # 1.查找 rclcpp 的头文件和库文件\n> find_package(rclcpp REQUIRED)\n> \n> # uncomment the following section in order to fill in\n> # further dependencies manually.\n> # find_package(<dependency> REQUIRED)\n> \n> # 2.添加可执行文件，(节点名，源文件地址)\n> add_executable(demo_one_node src/demo_one.cpp)\n> \n> # 头文件包含\n> # target_inlude_directories(demo_one_node PUBLIC ${rclcpp_INCLUDE_DIRS})\n> # 库文件链接\n> # target_link_libraries(demo_one_node ${rclcpp_LIBRARIES})\n> \n> # 上面两句用这一句代替,他是依赖于 find_package(ament_cmake REQUIRED) 的\n> ament_target_dependencies(demo_one_node rclcpp)\n> \n> # 将 demo_one_node 拷贝到 install目录下\n> install(TARGETS demo_one_node\t# 这里执行后目录就已经是 install/demo_one_pkg/\n> # 拷贝到 install/pkg_name/lib/pkg_name/ 下\n> DESTINATION lib/${PROJECT_NAME}\t# 这里执行后最后目录就是 install/demo_one_pkg/lib/demo_one_pkg/\n> )\n> \n> if(BUILD_TESTING)\n>   find_package(ament_lint_auto REQUIRED)\n>   # the following line skips the linter which checks for copyrights\n>   # comment the line when a copyright and license is added to all source files\n>   set(ament_cmake_copyright_FOUND TRUE)\n>   # the following line skips cpplint (only works in a git repo)\n>   # comment the line when this package is in a git repo and when\n>   # a copyright and license is added to all source files\n>   set(ament_cmake_cpplint_FOUND TRUE)\n>   ament_lint_auto_find_test_dependencies()\n> endif()\n> \n> ament_package()\n> ```\n>\n> ### 在与 CMakeLists.txt 同级目录下还有一个 package.xml 文件，在该文件中也要对引用的头文件进行声明\n>\n> ```xml\n> <?xml version=\"1.0\"?>\n> <?xml-model href=\"http://download.ros.org/schema/package_format3.xsd\" schematypens=\"http://www.w3.org/2001/XMLSchema\"?>\n> <package format=\"3\">\n>   <name>demo_one_pkg</name>\n>   <version>0.0.0</version>\n>   <description>TODO: Package description</description>\n>   <maintainer email=\"yanzu@todo.todo\">yanzu</maintainer>\n>   <license>Apache-2.0</license>\n>   \n>   <!-- 在这里添加头文件的声明 -->\n>   <depend>rclcpp</depend>\n>   <buildtool_depend>ament_cmake</buildtool_depend>\n> \n>   <test_depend>ament_lint_auto</test_depend>\n>   <test_depend>ament_lint_common</test_depend>\n> \n>   <export>\n>     <build_type>ament_cmake</build_type>\n>   </export>\n> </package>\n> \n> ```\n>\n> \n>\n> ### 然后就可以对包进行编译了，编译时必须要在包的上一路径打开终端进行编译\n>\n> ### 在正式编译之前，可以在包目录下创建一个 build 文件先测试一下编译能否通过，创建这个build的目的是为了存放编译生成的文件\n>\n> ```shell\n> mkdir build\n> cd build/\n> cmake ../\n> make\n> ```\n>\n> ### 因为 CMakeLists.txt 在上一级目录中，要使其转换为 makefile ，所以是 cmake ../\n>\n> ### 这里在 demo_one_pkg 的上一路径下进行编译，也即 chapter_2/ 路径下进行编译\n>\n> ```shell\n> cd ..\n> colcon build\n> ```\n>\n> ### 编译成功后，显示如下信息\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_7.png)\n>\n> ### 此时，chapter_2/ 路径下分别有这些文件，build、install、log、demo_one_pkg\n>\n> ### 生成的可执行文件 demo_one_node 在 build/pkg_name/ 下，在该路径下，执行以下命令，可以查看当前节点依赖了哪些库，是否链接上\n>\n> ```shell\n> ldd demo_one_node\n> ```\n>\n> ### 正式运行节点之前要添加环境变量，可以使用以下命令查看当前包的环境变量是否配置了，未配置直接运行节点会显示找不到当前的包(以下命令都是在 chapter_2/ 路径下执行)\n>\n> ```shell\n> printenv | grep AMENT\n> ```\n>\n> ### 使用以下命令进行环境变量配置\n>\n> ```shell\n> source install/setup.bash\n> ```\n>\n> ### 再使用上面查看环境变量的命令看看是否配置成功，最后运行节点（ros2 run pkg_name node_name）\n>\n> ```shell\n> ros2 run demo_one_pkg demo_one_node\n> ```\n>\n> \n","slug":"05","published":1,"date":"2025-01-03T04:49:32.511Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cqm000aiku4e74o56a5","content":"<h1 id=\"使用功能包组织cpp节点\"><a href=\"#使用功能包组织cpp节点\" class=\"headerlink\" title=\"使用功能包组织cpp节点\"></a>使用功能包组织cpp节点</h1><blockquote>\n<h3 id=\"创建一个功能包，这里所在的目录是-learn-ros2-chapter-2\"><a href=\"#创建一个功能包，这里所在的目录是-learn-ros2-chapter-2\" class=\"headerlink\" title=\"创建一个功能包，这里所在的目录是  ~&#x2F;learn_ros2&#x2F;chapter_2&#x2F;\"></a>创建一个功能包，这里所在的目录是  ~&#x2F;learn_ros2&#x2F;chapter_2&#x2F;</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 pkg create demo_one_pkg --build-type ament_cmake --license Apache-2.0</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"创建好之后显示以下信息，\"><a href=\"#创建好之后显示以下信息，\" class=\"headerlink\" title=\"创建好之后显示以下信息，\"></a>创建好之后显示以下信息，</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_5.png\"></p>\n<h3 id=\"目录结构如下，include-demo-one-pkg-文件用于存放编写的头文件-库文件\"><a href=\"#目录结构如下，include-demo-one-pkg-文件用于存放编写的头文件-库文件\" class=\"headerlink\" title=\"目录结构如下，include&#x2F;demo_one_pkg 文件用于存放编写的头文件&#x2F;库文件\"></a>目录结构如下，include&#x2F;demo_one_pkg 文件用于存放编写的头文件&#x2F;库文件</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_6.png\"></p>\n<h3 id=\"创建好包之后，进入-demo-one-pkg-src-目录下，创建一个-cpp-文件，这里叫-demo-one-cpp，内容如下，\"><a href=\"#创建好包之后，进入-demo-one-pkg-src-目录下，创建一个-cpp-文件，这里叫-demo-one-cpp，内容如下，\" class=\"headerlink\" title=\"创建好包之后，进入 demo_one_pkg&#x2F;src&#x2F; 目录下，创建一个 .cpp 文件，这里叫 demo_one.cpp，内容如下，\"></a>创建好包之后，进入 demo_one_pkg&#x2F;src&#x2F; 目录下，创建一个 .cpp 文件，这里叫 demo_one.cpp，内容如下，</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;rclcpp/rclcpp.hpp&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span> **argv)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 初始化参数</span></span><br><span class=\"line\">    rclcpp::<span class=\"built_in\">init</span>(argc, argv);</span><br><span class=\"line\">    <span class=\"comment\">// 创建节点</span></span><br><span class=\"line\">    <span class=\"keyword\">auto</span> node = std::<span class=\"built_in\">make_shared</span>&lt;rclcpp::Node&gt;(<span class=\"string\">&quot;demo_one_node&quot;</span>);</span><br><span class=\"line\">    <span class=\"comment\">// 打印日志</span></span><br><span class=\"line\">    <span class=\"built_in\">RCLCPP_INFO</span>(node-&gt;<span class=\"built_in\">get_logger</span>(), <span class=\"string\">&quot;hello&quot;</span>);</span><br><span class=\"line\">    <span class=\"comment\">// 运行节点</span></span><br><span class=\"line\">    rclcpp::<span class=\"built_in\">spin</span>(node);</span><br><span class=\"line\">    <span class=\"comment\">// 关闭节点</span></span><br><span class=\"line\">    rclcpp::<span class=\"built_in\">shutdown</span>();</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"返回上一级目录，即-demo-one-pkg-，编辑-CMakeLists-txt\"><a href=\"#返回上一级目录，即-demo-one-pkg-，编辑-CMakeLists-txt\" class=\"headerlink\" title=\"返回上一级目录，即 demo_one_pkg&#x2F;，编辑 CMakeLists.txt\"></a>返回上一级目录，即 demo_one_pkg&#x2F;，编辑 CMakeLists.txt</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cmake_minimum_required(VERSION 3.8)</span><br><span class=\"line\">project(demo_one_pkg)</span><br><span class=\"line\"></span><br><span class=\"line\">if(CMAKE_COMPILER_IS_GNUCXX OR CMAKE_CXX_COMPILER_ID MATCHES &quot;Clang&quot;)</span><br><span class=\"line\">  add_compile_options(-Wall -Wextra -Wpedantic)</span><br><span class=\"line\">endif()</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">找到依赖</span></span><br><span class=\"line\">find_package(ament_cmake REQUIRED)</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">1.查找 rclcpp 的头文件和库文件</span></span><br><span class=\"line\">find_package(rclcpp REQUIRED)</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">uncomment the following section <span class=\"keyword\">in</span> order to fill <span class=\"keyword\">in</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">further dependencies manually.</span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">find_package(&lt;dependency&gt; REQUIRED)</span></span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">2.添加可执行文件，(节点名，源文件地址)</span></span><br><span class=\"line\">add_executable(demo_one_node src/demo_one.cpp)</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">头文件包含</span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">target_inlude_directories(demo_one_node PUBLIC <span class=\"variable\">$&#123;rclcpp_INCLUDE_DIRS&#125;</span>)</span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">库文件链接</span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">target_link_libraries(demo_one_node <span class=\"variable\">$&#123;rclcpp_LIBRARIES&#125;</span>)</span></span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">上面两句用这一句代替,他是依赖于 find_package(ament_cmake REQUIRED) 的</span></span><br><span class=\"line\">ament_target_dependencies(demo_one_node rclcpp)</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">将 demo_one_node 拷贝到 install目录下</span></span><br><span class=\"line\">install(TARGETS demo_one_node\t# 这里执行后目录就已经是 install/demo_one_pkg/</span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">拷贝到 install/pkg_name/lib/pkg_name/ 下</span></span><br><span class=\"line\">DESTINATION lib/$&#123;PROJECT_NAME&#125;\t# 这里执行后最后目录就是 install/demo_one_pkg/lib/demo_one_pkg/</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">if(BUILD_TESTING)</span><br><span class=\"line\">  find_package(ament_lint_auto REQUIRED)</span><br><span class=\"line\"><span class=\"meta prompt_\">  # </span><span class=\"language-bash\">the following line skips the linter <span class=\"built_in\">which</span> checks <span class=\"keyword\">for</span> copyrights</span></span><br><span class=\"line\"><span class=\"meta prompt_\">  # </span><span class=\"language-bash\">comment the line when a copyright and license is added to all <span class=\"built_in\">source</span> files</span></span><br><span class=\"line\">  set(ament_cmake_copyright_FOUND TRUE)</span><br><span class=\"line\"><span class=\"meta prompt_\">  # </span><span class=\"language-bash\">the following line skips cpplint (only works <span class=\"keyword\">in</span> a git repo)</span></span><br><span class=\"line\"><span class=\"meta prompt_\">  # </span><span class=\"language-bash\">comment the line when this package is <span class=\"keyword\">in</span> a git repo and when</span></span><br><span class=\"line\"><span class=\"meta prompt_\">  # </span><span class=\"language-bash\">a copyright and license is added to all <span class=\"built_in\">source</span> files</span></span><br><span class=\"line\">  set(ament_cmake_cpplint_FOUND TRUE)</span><br><span class=\"line\">  ament_lint_auto_find_test_dependencies()</span><br><span class=\"line\">endif()</span><br><span class=\"line\"></span><br><span class=\"line\">ament_package()</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在与-CMakeLists-txt-同级目录下还有一个-package-xml-文件，在该文件中也要对引用的头文件进行声明\"><a href=\"#在与-CMakeLists-txt-同级目录下还有一个-package-xml-文件，在该文件中也要对引用的头文件进行声明\" class=\"headerlink\" title=\"在与 CMakeLists.txt 同级目录下还有一个 package.xml 文件，在该文件中也要对引用的头文件进行声明\"></a>在与 CMakeLists.txt 同级目录下还有一个 package.xml 文件，在该文件中也要对引用的头文件进行声明</h3><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;?xml version=<span class=\"string\">&quot;1.0&quot;</span>?&gt;</span></span><br><span class=\"line\"><span class=\"meta\">&lt;?xml-model href=<span class=\"string\">&quot;http://download.ros.org/schema/package_format3.xsd&quot;</span> schematypens=<span class=\"string\">&quot;http://www.w3.org/2001/XMLSchema&quot;</span>?&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">package</span> <span class=\"attr\">format</span>=<span class=\"string\">&quot;3&quot;</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>demo_one_pkg<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>0.0.0<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>TODO: Package description<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">maintainer</span> <span class=\"attr\">email</span>=<span class=\"string\">&quot;yanzu@todo.todo&quot;</span>&gt;</span>yanzu<span class=\"tag\">&lt;/<span class=\"name\">maintainer</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">license</span>&gt;</span>Apache-2.0<span class=\"tag\">&lt;/<span class=\"name\">license</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"comment\">&lt;!-- 在这里添加头文件的声明 --&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">depend</span>&gt;</span>rclcpp<span class=\"tag\">&lt;/<span class=\"name\">depend</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">buildtool_depend</span>&gt;</span>ament_cmake<span class=\"tag\">&lt;/<span class=\"name\">buildtool_depend</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">test_depend</span>&gt;</span>ament_lint_auto<span class=\"tag\">&lt;/<span class=\"name\">test_depend</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">test_depend</span>&gt;</span>ament_lint_common<span class=\"tag\">&lt;/<span class=\"name\">test_depend</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">export</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">build_type</span>&gt;</span>ament_cmake<span class=\"tag\">&lt;/<span class=\"name\">build_type</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">export</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">package</span>&gt;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"然后就可以对包进行编译了，编译时必须要在包的上一路径打开终端进行编译\"><a href=\"#然后就可以对包进行编译了，编译时必须要在包的上一路径打开终端进行编译\" class=\"headerlink\" title=\"然后就可以对包进行编译了，编译时必须要在包的上一路径打开终端进行编译\"></a>然后就可以对包进行编译了，编译时必须要在包的上一路径打开终端进行编译</h3><h3 id=\"在正式编译之前，可以在包目录下创建一个-build-文件先测试一下编译能否通过，创建这个build的目的是为了存放编译生成的文件\"><a href=\"#在正式编译之前，可以在包目录下创建一个-build-文件先测试一下编译能否通过，创建这个build的目的是为了存放编译生成的文件\" class=\"headerlink\" title=\"在正式编译之前，可以在包目录下创建一个 build 文件先测试一下编译能否通过，创建这个build的目的是为了存放编译生成的文件\"></a>在正式编译之前，可以在包目录下创建一个 build 文件先测试一下编译能否通过，创建这个build的目的是为了存放编译生成的文件</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir build</span><br><span class=\"line\">cd build/</span><br><span class=\"line\">cmake ../</span><br><span class=\"line\">make</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"因为-CMakeLists-txt-在上一级目录中，要使其转换为-makefile-，所以是-cmake\"><a href=\"#因为-CMakeLists-txt-在上一级目录中，要使其转换为-makefile-，所以是-cmake\" class=\"headerlink\" title=\"因为 CMakeLists.txt 在上一级目录中，要使其转换为 makefile ，所以是 cmake ..&#x2F;\"></a>因为 CMakeLists.txt 在上一级目录中，要使其转换为 makefile ，所以是 cmake ..&#x2F;</h3><h3 id=\"这里在-demo-one-pkg-的上一路径下进行编译，也即-chapter-2-路径下进行编译\"><a href=\"#这里在-demo-one-pkg-的上一路径下进行编译，也即-chapter-2-路径下进行编译\" class=\"headerlink\" title=\"这里在 demo_one_pkg 的上一路径下进行编译，也即 chapter_2&#x2F; 路径下进行编译\"></a>这里在 demo_one_pkg 的上一路径下进行编译，也即 chapter_2&#x2F; 路径下进行编译</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ..</span><br><span class=\"line\">colcon build</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编译成功后，显示如下信息\"><a href=\"#编译成功后，显示如下信息\" class=\"headerlink\" title=\"编译成功后，显示如下信息\"></a>编译成功后，显示如下信息</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_7.png\"></p>\n<h3 id=\"此时，chapter-2-路径下分别有这些文件，build、install、log、demo-one-pkg\"><a href=\"#此时，chapter-2-路径下分别有这些文件，build、install、log、demo-one-pkg\" class=\"headerlink\" title=\"此时，chapter_2&#x2F; 路径下分别有这些文件，build、install、log、demo_one_pkg\"></a>此时，chapter_2&#x2F; 路径下分别有这些文件，build、install、log、demo_one_pkg</h3><h3 id=\"生成的可执行文件-demo-one-node-在-build-pkg-name-下，在该路径下，执行以下命令，可以查看当前节点依赖了哪些库，是否链接上\"><a href=\"#生成的可执行文件-demo-one-node-在-build-pkg-name-下，在该路径下，执行以下命令，可以查看当前节点依赖了哪些库，是否链接上\" class=\"headerlink\" title=\"生成的可执行文件 demo_one_node 在 build&#x2F;pkg_name&#x2F; 下，在该路径下，执行以下命令，可以查看当前节点依赖了哪些库，是否链接上\"></a>生成的可执行文件 demo_one_node 在 build&#x2F;pkg_name&#x2F; 下，在该路径下，执行以下命令，可以查看当前节点依赖了哪些库，是否链接上</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ldd demo_one_node</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"正式运行节点之前要添加环境变量，可以使用以下命令查看当前包的环境变量是否配置了，未配置直接运行节点会显示找不到当前的包-以下命令都是在-chapter-2-路径下执行\"><a href=\"#正式运行节点之前要添加环境变量，可以使用以下命令查看当前包的环境变量是否配置了，未配置直接运行节点会显示找不到当前的包-以下命令都是在-chapter-2-路径下执行\" class=\"headerlink\" title=\"正式运行节点之前要添加环境变量，可以使用以下命令查看当前包的环境变量是否配置了，未配置直接运行节点会显示找不到当前的包(以下命令都是在 chapter_2&#x2F; 路径下执行)\"></a>正式运行节点之前要添加环境变量，可以使用以下命令查看当前包的环境变量是否配置了，未配置直接运行节点会显示找不到当前的包(以下命令都是在 chapter_2&#x2F; 路径下执行)</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">printenv | grep AMENT</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"使用以下命令进行环境变量配置\"><a href=\"#使用以下命令进行环境变量配置\" class=\"headerlink\" title=\"使用以下命令进行环境变量配置\"></a>使用以下命令进行环境变量配置</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source install/setup.bash</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"再使用上面查看环境变量的命令看看是否配置成功，最后运行节点（ros2-run-pkg-name-node-name）\"><a href=\"#再使用上面查看环境变量的命令看看是否配置成功，最后运行节点（ros2-run-pkg-name-node-name）\" class=\"headerlink\" title=\"再使用上面查看环境变量的命令看看是否配置成功，最后运行节点（ros2 run pkg_name node_name）\"></a>再使用上面查看环境变量的命令看看是否配置成功，最后运行节点（ros2 run pkg_name node_name）</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 run demo_one_pkg demo_one_node</span><br></pre></td></tr></table></figure>\n\n\n</blockquote>\n","cover_type":"img","excerpt":"","more":"<h1 id=\"使用功能包组织cpp节点\"><a href=\"#使用功能包组织cpp节点\" class=\"headerlink\" title=\"使用功能包组织cpp节点\"></a>使用功能包组织cpp节点</h1><blockquote>\n<h3 id=\"创建一个功能包，这里所在的目录是-learn-ros2-chapter-2\"><a href=\"#创建一个功能包，这里所在的目录是-learn-ros2-chapter-2\" class=\"headerlink\" title=\"创建一个功能包，这里所在的目录是  ~&#x2F;learn_ros2&#x2F;chapter_2&#x2F;\"></a>创建一个功能包，这里所在的目录是  ~&#x2F;learn_ros2&#x2F;chapter_2&#x2F;</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 pkg create demo_one_pkg --build-type ament_cmake --license Apache-2.0</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"创建好之后显示以下信息，\"><a href=\"#创建好之后显示以下信息，\" class=\"headerlink\" title=\"创建好之后显示以下信息，\"></a>创建好之后显示以下信息，</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_5.png\"></p>\n<h3 id=\"目录结构如下，include-demo-one-pkg-文件用于存放编写的头文件-库文件\"><a href=\"#目录结构如下，include-demo-one-pkg-文件用于存放编写的头文件-库文件\" class=\"headerlink\" title=\"目录结构如下，include&#x2F;demo_one_pkg 文件用于存放编写的头文件&#x2F;库文件\"></a>目录结构如下，include&#x2F;demo_one_pkg 文件用于存放编写的头文件&#x2F;库文件</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_6.png\"></p>\n<h3 id=\"创建好包之后，进入-demo-one-pkg-src-目录下，创建一个-cpp-文件，这里叫-demo-one-cpp，内容如下，\"><a href=\"#创建好包之后，进入-demo-one-pkg-src-目录下，创建一个-cpp-文件，这里叫-demo-one-cpp，内容如下，\" class=\"headerlink\" title=\"创建好包之后，进入 demo_one_pkg&#x2F;src&#x2F; 目录下，创建一个 .cpp 文件，这里叫 demo_one.cpp，内容如下，\"></a>创建好包之后，进入 demo_one_pkg&#x2F;src&#x2F; 目录下，创建一个 .cpp 文件，这里叫 demo_one.cpp，内容如下，</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;rclcpp/rclcpp.hpp&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span> **argv)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 初始化参数</span></span><br><span class=\"line\">    rclcpp::<span class=\"built_in\">init</span>(argc, argv);</span><br><span class=\"line\">    <span class=\"comment\">// 创建节点</span></span><br><span class=\"line\">    <span class=\"keyword\">auto</span> node = std::<span class=\"built_in\">make_shared</span>&lt;rclcpp::Node&gt;(<span class=\"string\">&quot;demo_one_node&quot;</span>);</span><br><span class=\"line\">    <span class=\"comment\">// 打印日志</span></span><br><span class=\"line\">    <span class=\"built_in\">RCLCPP_INFO</span>(node-&gt;<span class=\"built_in\">get_logger</span>(), <span class=\"string\">&quot;hello&quot;</span>);</span><br><span class=\"line\">    <span class=\"comment\">// 运行节点</span></span><br><span class=\"line\">    rclcpp::<span class=\"built_in\">spin</span>(node);</span><br><span class=\"line\">    <span class=\"comment\">// 关闭节点</span></span><br><span class=\"line\">    rclcpp::<span class=\"built_in\">shutdown</span>();</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"返回上一级目录，即-demo-one-pkg-，编辑-CMakeLists-txt\"><a href=\"#返回上一级目录，即-demo-one-pkg-，编辑-CMakeLists-txt\" class=\"headerlink\" title=\"返回上一级目录，即 demo_one_pkg&#x2F;，编辑 CMakeLists.txt\"></a>返回上一级目录，即 demo_one_pkg&#x2F;，编辑 CMakeLists.txt</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cmake_minimum_required(VERSION 3.8)</span><br><span class=\"line\">project(demo_one_pkg)</span><br><span class=\"line\"></span><br><span class=\"line\">if(CMAKE_COMPILER_IS_GNUCXX OR CMAKE_CXX_COMPILER_ID MATCHES &quot;Clang&quot;)</span><br><span class=\"line\">  add_compile_options(-Wall -Wextra -Wpedantic)</span><br><span class=\"line\">endif()</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">找到依赖</span></span><br><span class=\"line\">find_package(ament_cmake REQUIRED)</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">1.查找 rclcpp 的头文件和库文件</span></span><br><span class=\"line\">find_package(rclcpp REQUIRED)</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">uncomment the following section <span class=\"keyword\">in</span> order to fill <span class=\"keyword\">in</span></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">further dependencies manually.</span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">find_package(&lt;dependency&gt; REQUIRED)</span></span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">2.添加可执行文件，(节点名，源文件地址)</span></span><br><span class=\"line\">add_executable(demo_one_node src/demo_one.cpp)</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">头文件包含</span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">target_inlude_directories(demo_one_node PUBLIC <span class=\"variable\">$&#123;rclcpp_INCLUDE_DIRS&#125;</span>)</span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">库文件链接</span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">target_link_libraries(demo_one_node <span class=\"variable\">$&#123;rclcpp_LIBRARIES&#125;</span>)</span></span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">上面两句用这一句代替,他是依赖于 find_package(ament_cmake REQUIRED) 的</span></span><br><span class=\"line\">ament_target_dependencies(demo_one_node rclcpp)</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">将 demo_one_node 拷贝到 install目录下</span></span><br><span class=\"line\">install(TARGETS demo_one_node\t# 这里执行后目录就已经是 install/demo_one_pkg/</span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">拷贝到 install/pkg_name/lib/pkg_name/ 下</span></span><br><span class=\"line\">DESTINATION lib/$&#123;PROJECT_NAME&#125;\t# 这里执行后最后目录就是 install/demo_one_pkg/lib/demo_one_pkg/</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">if(BUILD_TESTING)</span><br><span class=\"line\">  find_package(ament_lint_auto REQUIRED)</span><br><span class=\"line\"><span class=\"meta prompt_\">  # </span><span class=\"language-bash\">the following line skips the linter <span class=\"built_in\">which</span> checks <span class=\"keyword\">for</span> copyrights</span></span><br><span class=\"line\"><span class=\"meta prompt_\">  # </span><span class=\"language-bash\">comment the line when a copyright and license is added to all <span class=\"built_in\">source</span> files</span></span><br><span class=\"line\">  set(ament_cmake_copyright_FOUND TRUE)</span><br><span class=\"line\"><span class=\"meta prompt_\">  # </span><span class=\"language-bash\">the following line skips cpplint (only works <span class=\"keyword\">in</span> a git repo)</span></span><br><span class=\"line\"><span class=\"meta prompt_\">  # </span><span class=\"language-bash\">comment the line when this package is <span class=\"keyword\">in</span> a git repo and when</span></span><br><span class=\"line\"><span class=\"meta prompt_\">  # </span><span class=\"language-bash\">a copyright and license is added to all <span class=\"built_in\">source</span> files</span></span><br><span class=\"line\">  set(ament_cmake_cpplint_FOUND TRUE)</span><br><span class=\"line\">  ament_lint_auto_find_test_dependencies()</span><br><span class=\"line\">endif()</span><br><span class=\"line\"></span><br><span class=\"line\">ament_package()</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在与-CMakeLists-txt-同级目录下还有一个-package-xml-文件，在该文件中也要对引用的头文件进行声明\"><a href=\"#在与-CMakeLists-txt-同级目录下还有一个-package-xml-文件，在该文件中也要对引用的头文件进行声明\" class=\"headerlink\" title=\"在与 CMakeLists.txt 同级目录下还有一个 package.xml 文件，在该文件中也要对引用的头文件进行声明\"></a>在与 CMakeLists.txt 同级目录下还有一个 package.xml 文件，在该文件中也要对引用的头文件进行声明</h3><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;?xml version=<span class=\"string\">&quot;1.0&quot;</span>?&gt;</span></span><br><span class=\"line\"><span class=\"meta\">&lt;?xml-model href=<span class=\"string\">&quot;http://download.ros.org/schema/package_format3.xsd&quot;</span> schematypens=<span class=\"string\">&quot;http://www.w3.org/2001/XMLSchema&quot;</span>?&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">package</span> <span class=\"attr\">format</span>=<span class=\"string\">&quot;3&quot;</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>demo_one_pkg<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">version</span>&gt;</span>0.0.0<span class=\"tag\">&lt;/<span class=\"name\">version</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>TODO: Package description<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">maintainer</span> <span class=\"attr\">email</span>=<span class=\"string\">&quot;yanzu@todo.todo&quot;</span>&gt;</span>yanzu<span class=\"tag\">&lt;/<span class=\"name\">maintainer</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">license</span>&gt;</span>Apache-2.0<span class=\"tag\">&lt;/<span class=\"name\">license</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"comment\">&lt;!-- 在这里添加头文件的声明 --&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">depend</span>&gt;</span>rclcpp<span class=\"tag\">&lt;/<span class=\"name\">depend</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">buildtool_depend</span>&gt;</span>ament_cmake<span class=\"tag\">&lt;/<span class=\"name\">buildtool_depend</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">test_depend</span>&gt;</span>ament_lint_auto<span class=\"tag\">&lt;/<span class=\"name\">test_depend</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">test_depend</span>&gt;</span>ament_lint_common<span class=\"tag\">&lt;/<span class=\"name\">test_depend</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">export</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">build_type</span>&gt;</span>ament_cmake<span class=\"tag\">&lt;/<span class=\"name\">build_type</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">export</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">package</span>&gt;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"然后就可以对包进行编译了，编译时必须要在包的上一路径打开终端进行编译\"><a href=\"#然后就可以对包进行编译了，编译时必须要在包的上一路径打开终端进行编译\" class=\"headerlink\" title=\"然后就可以对包进行编译了，编译时必须要在包的上一路径打开终端进行编译\"></a>然后就可以对包进行编译了，编译时必须要在包的上一路径打开终端进行编译</h3><h3 id=\"在正式编译之前，可以在包目录下创建一个-build-文件先测试一下编译能否通过，创建这个build的目的是为了存放编译生成的文件\"><a href=\"#在正式编译之前，可以在包目录下创建一个-build-文件先测试一下编译能否通过，创建这个build的目的是为了存放编译生成的文件\" class=\"headerlink\" title=\"在正式编译之前，可以在包目录下创建一个 build 文件先测试一下编译能否通过，创建这个build的目的是为了存放编译生成的文件\"></a>在正式编译之前，可以在包目录下创建一个 build 文件先测试一下编译能否通过，创建这个build的目的是为了存放编译生成的文件</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir build</span><br><span class=\"line\">cd build/</span><br><span class=\"line\">cmake ../</span><br><span class=\"line\">make</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"因为-CMakeLists-txt-在上一级目录中，要使其转换为-makefile-，所以是-cmake\"><a href=\"#因为-CMakeLists-txt-在上一级目录中，要使其转换为-makefile-，所以是-cmake\" class=\"headerlink\" title=\"因为 CMakeLists.txt 在上一级目录中，要使其转换为 makefile ，所以是 cmake ..&#x2F;\"></a>因为 CMakeLists.txt 在上一级目录中，要使其转换为 makefile ，所以是 cmake ..&#x2F;</h3><h3 id=\"这里在-demo-one-pkg-的上一路径下进行编译，也即-chapter-2-路径下进行编译\"><a href=\"#这里在-demo-one-pkg-的上一路径下进行编译，也即-chapter-2-路径下进行编译\" class=\"headerlink\" title=\"这里在 demo_one_pkg 的上一路径下进行编译，也即 chapter_2&#x2F; 路径下进行编译\"></a>这里在 demo_one_pkg 的上一路径下进行编译，也即 chapter_2&#x2F; 路径下进行编译</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ..</span><br><span class=\"line\">colcon build</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编译成功后，显示如下信息\"><a href=\"#编译成功后，显示如下信息\" class=\"headerlink\" title=\"编译成功后，显示如下信息\"></a>编译成功后，显示如下信息</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_7.png\"></p>\n<h3 id=\"此时，chapter-2-路径下分别有这些文件，build、install、log、demo-one-pkg\"><a href=\"#此时，chapter-2-路径下分别有这些文件，build、install、log、demo-one-pkg\" class=\"headerlink\" title=\"此时，chapter_2&#x2F; 路径下分别有这些文件，build、install、log、demo_one_pkg\"></a>此时，chapter_2&#x2F; 路径下分别有这些文件，build、install、log、demo_one_pkg</h3><h3 id=\"生成的可执行文件-demo-one-node-在-build-pkg-name-下，在该路径下，执行以下命令，可以查看当前节点依赖了哪些库，是否链接上\"><a href=\"#生成的可执行文件-demo-one-node-在-build-pkg-name-下，在该路径下，执行以下命令，可以查看当前节点依赖了哪些库，是否链接上\" class=\"headerlink\" title=\"生成的可执行文件 demo_one_node 在 build&#x2F;pkg_name&#x2F; 下，在该路径下，执行以下命令，可以查看当前节点依赖了哪些库，是否链接上\"></a>生成的可执行文件 demo_one_node 在 build&#x2F;pkg_name&#x2F; 下，在该路径下，执行以下命令，可以查看当前节点依赖了哪些库，是否链接上</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ldd demo_one_node</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"正式运行节点之前要添加环境变量，可以使用以下命令查看当前包的环境变量是否配置了，未配置直接运行节点会显示找不到当前的包-以下命令都是在-chapter-2-路径下执行\"><a href=\"#正式运行节点之前要添加环境变量，可以使用以下命令查看当前包的环境变量是否配置了，未配置直接运行节点会显示找不到当前的包-以下命令都是在-chapter-2-路径下执行\" class=\"headerlink\" title=\"正式运行节点之前要添加环境变量，可以使用以下命令查看当前包的环境变量是否配置了，未配置直接运行节点会显示找不到当前的包(以下命令都是在 chapter_2&#x2F; 路径下执行)\"></a>正式运行节点之前要添加环境变量，可以使用以下命令查看当前包的环境变量是否配置了，未配置直接运行节点会显示找不到当前的包(以下命令都是在 chapter_2&#x2F; 路径下执行)</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">printenv | grep AMENT</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"使用以下命令进行环境变量配置\"><a href=\"#使用以下命令进行环境变量配置\" class=\"headerlink\" title=\"使用以下命令进行环境变量配置\"></a>使用以下命令进行环境变量配置</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source install/setup.bash</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"再使用上面查看环境变量的命令看看是否配置成功，最后运行节点（ros2-run-pkg-name-node-name）\"><a href=\"#再使用上面查看环境变量的命令看看是否配置成功，最后运行节点（ros2-run-pkg-name-node-name）\" class=\"headerlink\" title=\"再使用上面查看环境变量的命令看看是否配置成功，最后运行节点（ros2 run pkg_name node_name）\"></a>再使用上面查看环境变量的命令看看是否配置成功，最后运行节点（ros2 run pkg_name node_name）</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 run demo_one_pkg demo_one_node</span><br></pre></td></tr></table></figure>\n\n\n</blockquote>\n"},{"title":"第五弹","data":"2025-02-17T12:40:00.000Z","updated":"2025-02-17T12:40:00.000Z","type":"ROS2","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p6.png","_content":"\n# ROS2基础：cpp面向对象编程\n\n### 在 chapter2_ws/sec/demo_one_pkg/src 下创建一个 person_node.cpp\n\n> ```shell\n> cd chapter2_ws/src/demo_one_pkg/src\n> touch person_node.cpp\n> gedit person_node.cpp\n> ```\n\n### 然后，编辑 person_node.cpp 的内容\n\n> ```cpp\n> #include \"rclcpp/rclcpp.hpp\"\n> \n> // 定义一个类，使其继承 Node\n> class PersonNode : public rclcpp::Node{\n> private:\n> \tstd::string name_;\n> \tint age_;\n> public:\n> \t// 构造函数,调用父类的构造函数,属性初始化\n> \tPersonNode(const std::string &node_name, const std::string &name, const int &age):Node(node_name){\n> \t\tthis->name_ = name;\n> \t\tthis->age_ = age;\n> \t};\n> \t\n> \tvoid eat(const std::string &food_name){\n>         // 这里用的是 C 的格式化输出，故需要将 name_ 转换为 C 风格的字符串\n> \t\tRCLCPP_INFO(this->get_logger(), \"I'm %s, %d years old and love eatting %s\", this->name_.c_str(), this->age_, food_name.c_str());\t\n> \t};\n> };\n> \n> \n> int main(int argc, char** argv){\n> \trclcpp::init(argc, argv);\n> \t// 这里的 make_shared 需要修改为 PersonNode 类型,传入的参数个数和形式必须与上面的PersonNode的一致\n> \tauto node = std::make_shared<PersonNode>(\"person_node\",\"Peter\",18);\n> \tRCLCPP_INFO(node->get_logger(),\"hello\");\n>     // 如果上面的 make_shared 不修改为 PersonNode 类型，那么这里无法调用其公共函数\n> \tnode->eat(\"apple\");\n> \trclcpp::spin(node);\n> \trclcpp::shutdown();\n> \treturn 0;\n> }\n> ```\n\n### 修改 demo_one_pkg 的 CmakeLists 文件\n\n> ```shell\n> # 添加可执行文件\n> add_executable(demo_one_node src/demo_one.cpp)\n> add_executable(person_node src/person_node.cpp)\t# 添加这句\n> \n> # 添加依赖\n> ament_target_dependencies(demo_one_node rclcpp)\n> ament_target_dependencies(person_node rclcpp)\t# 添加这句\n> \n> # 拷贝节点到 install\n> install(TARGETS demo_one_node person_node\n> DESTINATION lib/${PROJECT_NAME}\n> )\n> ```\n\n### 重新编译并执行\n\n> ```shell\n> cd ./learn_ros2/chapter_2/chapter2_ws/\n> colcon build\n> ros2 run demo_one_pkg person_node\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_8.png)","source":"_posts/06.md","raw":"---\ntitle: 第五弹\ndata: 2025-02-17 20:40:00\nupdated: 2025-02-17 20:40:00\ntype: ROS2\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p6.png\ntags:\n  - ROS2\n  - Learning\n---\n\n# ROS2基础：cpp面向对象编程\n\n### 在 chapter2_ws/sec/demo_one_pkg/src 下创建一个 person_node.cpp\n\n> ```shell\n> cd chapter2_ws/src/demo_one_pkg/src\n> touch person_node.cpp\n> gedit person_node.cpp\n> ```\n\n### 然后，编辑 person_node.cpp 的内容\n\n> ```cpp\n> #include \"rclcpp/rclcpp.hpp\"\n> \n> // 定义一个类，使其继承 Node\n> class PersonNode : public rclcpp::Node{\n> private:\n> \tstd::string name_;\n> \tint age_;\n> public:\n> \t// 构造函数,调用父类的构造函数,属性初始化\n> \tPersonNode(const std::string &node_name, const std::string &name, const int &age):Node(node_name){\n> \t\tthis->name_ = name;\n> \t\tthis->age_ = age;\n> \t};\n> \t\n> \tvoid eat(const std::string &food_name){\n>         // 这里用的是 C 的格式化输出，故需要将 name_ 转换为 C 风格的字符串\n> \t\tRCLCPP_INFO(this->get_logger(), \"I'm %s, %d years old and love eatting %s\", this->name_.c_str(), this->age_, food_name.c_str());\t\n> \t};\n> };\n> \n> \n> int main(int argc, char** argv){\n> \trclcpp::init(argc, argv);\n> \t// 这里的 make_shared 需要修改为 PersonNode 类型,传入的参数个数和形式必须与上面的PersonNode的一致\n> \tauto node = std::make_shared<PersonNode>(\"person_node\",\"Peter\",18);\n> \tRCLCPP_INFO(node->get_logger(),\"hello\");\n>     // 如果上面的 make_shared 不修改为 PersonNode 类型，那么这里无法调用其公共函数\n> \tnode->eat(\"apple\");\n> \trclcpp::spin(node);\n> \trclcpp::shutdown();\n> \treturn 0;\n> }\n> ```\n\n### 修改 demo_one_pkg 的 CmakeLists 文件\n\n> ```shell\n> # 添加可执行文件\n> add_executable(demo_one_node src/demo_one.cpp)\n> add_executable(person_node src/person_node.cpp)\t# 添加这句\n> \n> # 添加依赖\n> ament_target_dependencies(demo_one_node rclcpp)\n> ament_target_dependencies(person_node rclcpp)\t# 添加这句\n> \n> # 拷贝节点到 install\n> install(TARGETS demo_one_node person_node\n> DESTINATION lib/${PROJECT_NAME}\n> )\n> ```\n\n### 重新编译并执行\n\n> ```shell\n> cd ./learn_ros2/chapter_2/chapter2_ws/\n> colcon build\n> ros2 run demo_one_pkg person_node\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_8.png)","slug":"06","published":1,"date":"2025-02-17T12:40:21.048Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cqp000diku4f8mk9wyx","content":"<h1 id=\"ROS2基础：cpp面向对象编程\"><a href=\"#ROS2基础：cpp面向对象编程\" class=\"headerlink\" title=\"ROS2基础：cpp面向对象编程\"></a>ROS2基础：cpp面向对象编程</h1><h3 id=\"在-chapter2-ws-sec-demo-one-pkg-src-下创建一个-person-node-cpp\"><a href=\"#在-chapter2-ws-sec-demo-one-pkg-src-下创建一个-person-node-cpp\" class=\"headerlink\" title=\"在 chapter2_ws&#x2F;sec&#x2F;demo_one_pkg&#x2F;src 下创建一个 person_node.cpp\"></a>在 chapter2_ws&#x2F;sec&#x2F;demo_one_pkg&#x2F;src 下创建一个 person_node.cpp</h3><blockquote>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd chapter2_ws/src/demo_one_pkg/src</span><br><span class=\"line\">touch person_node.cpp</span><br><span class=\"line\">gedit person_node.cpp</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"然后，编辑-person-node-cpp-的内容\"><a href=\"#然后，编辑-person-node-cpp-的内容\" class=\"headerlink\" title=\"然后，编辑 person_node.cpp 的内容\"></a>然后，编辑 person_node.cpp 的内容</h3><blockquote>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;rclcpp/rclcpp.hpp&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 定义一个类，使其继承 Node</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">PersonNode</span> : <span class=\"keyword\">public</span> rclcpp::Node&#123;</span><br><span class=\"line\"><span class=\"keyword\">private</span>:</span><br><span class=\"line\">\tstd::string name_;</span><br><span class=\"line\">\t<span class=\"type\">int</span> age_;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"comment\">// 构造函数,调用父类的构造函数,属性初始化</span></span><br><span class=\"line\">\t<span class=\"built_in\">PersonNode</span>(<span class=\"type\">const</span> std::string &amp;node_name, <span class=\"type\">const</span> std::string &amp;name, <span class=\"type\">const</span> <span class=\"type\">int</span> &amp;age):<span class=\"built_in\">Node</span>(node_name)&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">this</span>-&gt;name_ = name;</span><br><span class=\"line\">\t\t<span class=\"keyword\">this</span>-&gt;age_ = age;</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">eat</span><span class=\"params\">(<span class=\"type\">const</span> std::string &amp;food_name)</span></span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// 这里用的是 C 的格式化输出，故需要将 name_ 转换为 C 风格的字符串</span></span><br><span class=\"line\">\t\t<span class=\"built_in\">RCLCPP_INFO</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">get_logger</span>(), <span class=\"string\">&quot;I&#x27;m %s, %d years old and love eatting %s&quot;</span>, <span class=\"keyword\">this</span>-&gt;name_.<span class=\"built_in\">c_str</span>(), <span class=\"keyword\">this</span>-&gt;age_, food_name.<span class=\"built_in\">c_str</span>());\t</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span>** argv)</span></span>&#123;</span><br><span class=\"line\">\trclcpp::<span class=\"built_in\">init</span>(argc, argv);</span><br><span class=\"line\">\t<span class=\"comment\">// 这里的 make_shared 需要修改为 PersonNode 类型,传入的参数个数和形式必须与上面的PersonNode的一致</span></span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> node = std::<span class=\"built_in\">make_shared</span>&lt;PersonNode&gt;(<span class=\"string\">&quot;person_node&quot;</span>,<span class=\"string\">&quot;Peter&quot;</span>,<span class=\"number\">18</span>);</span><br><span class=\"line\">\t<span class=\"built_in\">RCLCPP_INFO</span>(node-&gt;<span class=\"built_in\">get_logger</span>(),<span class=\"string\">&quot;hello&quot;</span>);</span><br><span class=\"line\">    <span class=\"comment\">// 如果上面的 make_shared 不修改为 PersonNode 类型，那么这里无法调用其公共函数</span></span><br><span class=\"line\">\tnode-&gt;<span class=\"built_in\">eat</span>(<span class=\"string\">&quot;apple&quot;</span>);</span><br><span class=\"line\">\trclcpp::<span class=\"built_in\">spin</span>(node);</span><br><span class=\"line\">\trclcpp::<span class=\"built_in\">shutdown</span>();</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"修改-demo-one-pkg-的-CmakeLists-文件\"><a href=\"#修改-demo-one-pkg-的-CmakeLists-文件\" class=\"headerlink\" title=\"修改 demo_one_pkg 的 CmakeLists 文件\"></a>修改 demo_one_pkg 的 CmakeLists 文件</h3><blockquote>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">添加可执行文件</span></span><br><span class=\"line\">add_executable(demo_one_node src/demo_one.cpp)</span><br><span class=\"line\">add_executable(person_node src/person_node.cpp)\t# 添加这句</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">添加依赖</span></span><br><span class=\"line\">ament_target_dependencies(demo_one_node rclcpp)</span><br><span class=\"line\">ament_target_dependencies(person_node rclcpp)\t# 添加这句</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">拷贝节点到 install</span></span><br><span class=\"line\">install(TARGETS demo_one_node person_node</span><br><span class=\"line\">DESTINATION lib/$&#123;PROJECT_NAME&#125;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"重新编译并执行\"><a href=\"#重新编译并执行\" class=\"headerlink\" title=\"重新编译并执行\"></a>重新编译并执行</h3><blockquote>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ./learn_ros2/chapter_2/chapter2_ws/</span><br><span class=\"line\">colcon build</span><br><span class=\"line\">ros2 run demo_one_pkg person_node</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_8.png\"></p>\n</blockquote>\n","cover_type":"img","excerpt":"","more":"<h1 id=\"ROS2基础：cpp面向对象编程\"><a href=\"#ROS2基础：cpp面向对象编程\" class=\"headerlink\" title=\"ROS2基础：cpp面向对象编程\"></a>ROS2基础：cpp面向对象编程</h1><h3 id=\"在-chapter2-ws-sec-demo-one-pkg-src-下创建一个-person-node-cpp\"><a href=\"#在-chapter2-ws-sec-demo-one-pkg-src-下创建一个-person-node-cpp\" class=\"headerlink\" title=\"在 chapter2_ws&#x2F;sec&#x2F;demo_one_pkg&#x2F;src 下创建一个 person_node.cpp\"></a>在 chapter2_ws&#x2F;sec&#x2F;demo_one_pkg&#x2F;src 下创建一个 person_node.cpp</h3><blockquote>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd chapter2_ws/src/demo_one_pkg/src</span><br><span class=\"line\">touch person_node.cpp</span><br><span class=\"line\">gedit person_node.cpp</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"然后，编辑-person-node-cpp-的内容\"><a href=\"#然后，编辑-person-node-cpp-的内容\" class=\"headerlink\" title=\"然后，编辑 person_node.cpp 的内容\"></a>然后，编辑 person_node.cpp 的内容</h3><blockquote>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;rclcpp/rclcpp.hpp&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 定义一个类，使其继承 Node</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">PersonNode</span> : <span class=\"keyword\">public</span> rclcpp::Node&#123;</span><br><span class=\"line\"><span class=\"keyword\">private</span>:</span><br><span class=\"line\">\tstd::string name_;</span><br><span class=\"line\">\t<span class=\"type\">int</span> age_;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"comment\">// 构造函数,调用父类的构造函数,属性初始化</span></span><br><span class=\"line\">\t<span class=\"built_in\">PersonNode</span>(<span class=\"type\">const</span> std::string &amp;node_name, <span class=\"type\">const</span> std::string &amp;name, <span class=\"type\">const</span> <span class=\"type\">int</span> &amp;age):<span class=\"built_in\">Node</span>(node_name)&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">this</span>-&gt;name_ = name;</span><br><span class=\"line\">\t\t<span class=\"keyword\">this</span>-&gt;age_ = age;</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">eat</span><span class=\"params\">(<span class=\"type\">const</span> std::string &amp;food_name)</span></span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// 这里用的是 C 的格式化输出，故需要将 name_ 转换为 C 风格的字符串</span></span><br><span class=\"line\">\t\t<span class=\"built_in\">RCLCPP_INFO</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">get_logger</span>(), <span class=\"string\">&quot;I&#x27;m %s, %d years old and love eatting %s&quot;</span>, <span class=\"keyword\">this</span>-&gt;name_.<span class=\"built_in\">c_str</span>(), <span class=\"keyword\">this</span>-&gt;age_, food_name.<span class=\"built_in\">c_str</span>());\t</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span>** argv)</span></span>&#123;</span><br><span class=\"line\">\trclcpp::<span class=\"built_in\">init</span>(argc, argv);</span><br><span class=\"line\">\t<span class=\"comment\">// 这里的 make_shared 需要修改为 PersonNode 类型,传入的参数个数和形式必须与上面的PersonNode的一致</span></span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> node = std::<span class=\"built_in\">make_shared</span>&lt;PersonNode&gt;(<span class=\"string\">&quot;person_node&quot;</span>,<span class=\"string\">&quot;Peter&quot;</span>,<span class=\"number\">18</span>);</span><br><span class=\"line\">\t<span class=\"built_in\">RCLCPP_INFO</span>(node-&gt;<span class=\"built_in\">get_logger</span>(),<span class=\"string\">&quot;hello&quot;</span>);</span><br><span class=\"line\">    <span class=\"comment\">// 如果上面的 make_shared 不修改为 PersonNode 类型，那么这里无法调用其公共函数</span></span><br><span class=\"line\">\tnode-&gt;<span class=\"built_in\">eat</span>(<span class=\"string\">&quot;apple&quot;</span>);</span><br><span class=\"line\">\trclcpp::<span class=\"built_in\">spin</span>(node);</span><br><span class=\"line\">\trclcpp::<span class=\"built_in\">shutdown</span>();</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"修改-demo-one-pkg-的-CmakeLists-文件\"><a href=\"#修改-demo-one-pkg-的-CmakeLists-文件\" class=\"headerlink\" title=\"修改 demo_one_pkg 的 CmakeLists 文件\"></a>修改 demo_one_pkg 的 CmakeLists 文件</h3><blockquote>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">添加可执行文件</span></span><br><span class=\"line\">add_executable(demo_one_node src/demo_one.cpp)</span><br><span class=\"line\">add_executable(person_node src/person_node.cpp)\t# 添加这句</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">添加依赖</span></span><br><span class=\"line\">ament_target_dependencies(demo_one_node rclcpp)</span><br><span class=\"line\">ament_target_dependencies(person_node rclcpp)\t# 添加这句</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">拷贝节点到 install</span></span><br><span class=\"line\">install(TARGETS demo_one_node person_node</span><br><span class=\"line\">DESTINATION lib/$&#123;PROJECT_NAME&#125;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"重新编译并执行\"><a href=\"#重新编译并执行\" class=\"headerlink\" title=\"重新编译并执行\"></a>重新编译并执行</h3><blockquote>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ./learn_ros2/chapter_2/chapter2_ws/</span><br><span class=\"line\">colcon build</span><br><span class=\"line\">ros2 run demo_one_pkg person_node</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_8.png\"></p>\n</blockquote>\n"},{"title":"第七弹","data":"2025-02-18T08:20:00.000Z","updated":"2025-02-18T08:20:00.000Z","type":"ROS2","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p8.png","_content":"\n# 多线程与回调函数\n\n### 需要下载一个 cpp-httplib ，下载到 workspace/src/demo_one_pkg/include/\n\n> ```shell\n> git clone https://gitee.com/ohhuo/cpp-httplib.git\n> ```\n>\n> ### 下载后得到如下文件\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_12.png)\n\n### 下载好之后，修改 demo_one_pkg 的 CMakeLists 文件\n\n> ```cmake\n> # 包含 cpp-httplib 的 include\n> include_directories(include)\n> ```\n\n### 接着，在 demo_one_pkg/src 下创建 learn_thread.cpp，内容如下\n\n> ```cpp\n> #include <iostream>\n> #include <thread>\t// 多线程\n> #include <chrono>\t// 时间相关\n> #include <functional>\t// 函数包装器\n> #include \"cpp-httplib/httplib.h\" \n> #include <future>\t// 用于 std::async 处理异步任务\n> \n> class Download{\n> \n> private:\n> \n> \n> public:\n> \t// 下载函数,负责下载文件并调用回调函数\n> \t// host 主机， path 路径，回调函数\n> \tvoid download(const std::string& host, const std::string& path, const std::function<void(const std::string &, const std::string &)> &callback_word_count){\n> \t\t// 输出当前线程ID，便于调试\n> \t\tstd::cout << \"线程编号： \" << std::this_thread::get_id() << std::endl;\n> \t\t// 创建客户端对象，指定主机\n> \t\thttplib::Client client(host);\n> \t\t// 发送 GET 请求，获取文件内容\n> \t\tauto response = client.Get(path);\n> \t\t// 如果请求成功且状态码为200\n> \t\tif(response && response->status == 200){\n> \t\t\t// 调用回调函数并传递文件内容\n> \t\t\tcallback_word_count(path, response->body);\n> \t\t}\n> \t};\n> \t// 启动下载函数,使用 std::async 异步执行下载任务\n> \tvoid start_download(const std::string& host, const std::string& path, const std::function<void(const std::string &, const std::string &)> &callback_word_count){\n> \t\t// 函数包装器进行包装\n> \t\t// auto download_fun = std::bind(&Download::start_download, this, std::placeholders::_1, std::placeholders::_2, std::placeholders::_3);\n> \t\t// 创建线程\n> \t\t// std::thread thread(download_fun, host, path, callback_word_count);\n> \t\t// 等待线程结束\n> \t\t// thread.join();\n>         \n> \t\t// 直接通过 std::async 可以异步执行任务，同时它会返回一个 std::future 对象，保证任务的异步执行，而主线程会等待任务完成\n> \t\t// std::launch::async 会指示 std::async 强制使用新的线程执行任务\n> \t\tauto future = std::async(std::launch::async, [=](){\n> \t\t\tdownload(host, path, callback_word_count);\n> \t\t});\n> \t};\n> \n> };\n> \n> \n> int main(){\n> \t\n> \tauto d = Download();\n> \t// 创建回调函数, path 地址， result 结果\n> \tauto word_count = [](const std::string &path, const std::string &result) -> void{\n> \t\t// 输出文件路径，文件内容的长度，以及文件内容的前五个字符\n> \t\tstd::cout << \"下载完成 \" << path << \": \" << result.length() << \" -> \" << result.substr(0, 5) << std::endl;\n> \t};\n> \t\n> \t// 启动多个下载任务，分别下载 novel1.txt、novel2.txt 和 novel3.txt\n> \td.start_download(\"http://0.0.0.0:8080\", \"/novel1.txt\", word_count);\n> \td.start_download(\"http://0.0.0.0:8080\", \"/novel2.txt\", word_count);\n> \td.start_download(\"http://0.0.0.0:8080\", \"/novel3.txt\", word_count);\n> \t\n> \t// 主线程休眠10秒，确保所有异步线程有足够时间完成下载任务\n> \tstd::this_thread::sleep_for(std::chrono::milliseconds(1000*10));\n> \t\n> \treturn 0;\n> }\n> ```\n>\n> ### 这里需要注意的是 std::async 的用法\n>\n> > ### 它是 C++11 引入的一个标准库函数，用于启动异步任务。它接受一个可调用对象（比如函数、函数指针、Lambda 表达式等）和一些参数，并返回一个 std::future 对象，表示异步操作的结果。\n> >\n> > ### 语法如下\n> >\n> > ```cpp\n> > std::future<T> async(std::launch policy, F&& f, Args&&... args);\n> > ```\n> >\n> > ### std::launch 指定任务的执行策略，如 std::launch::async 或 std::launch::deferred。前者是任务会异步启动并在独立的线程中执行后者是任务会被推迟执行，直到调用 future.get() 时，才会在当前线程中执行。\n> >\n> > ### Lambda 表达式\n> >\n> > ```cpp\n> > [捕获列表](参数列表) -> 返回类型 { 函数体 }\n> > ```\n> >\n> > ### [=] ：按值捕获外部变量\n> >\n> > ### [&] : 按引用捕获外部变量\n\n### 在 CMakeLists 文件中添加相应语句\n\n> ```cmake\n> # 添加可执行文件\n> add_executable(learn_thread src/learn_thread.cpp)\n> # 将节点拷贝到install\n> install(TARGETS demo_one_node person_node learn_shared_ptr learn_lambda learn_functional learn_thread\n> DESTINATION lib/${PROJECT_NAME}\n> )\n> ```\n\n### 在 chapter2_ws 目录下创建三个 txt 文件，并写入简单的内容，然后在该路径下运行本地服务器，并保持打开状态\n\n> ```shell\n> echo \"第一章 hello word\" > novel1.txt\n> echo \"第二章 hello guys\" > novel2.txt\n> echo \"第三章 hello girls\" > novel3.txt\n> ```\n>\n> ```shell\n> python3 -m http.server\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_13.png)\n\n### 在同一路径下打开另外一个终端，编译并执行 learn_thread 节点\n\n> ```shell\n> colcon build\n> source install/setup.bash\n> ros2 run demo_one_pkg learn_thread\n> ```\n>\n> ### 执行后会得到如下输出\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_14.png)\n\n","source":"_posts/08.md","raw":"---\ntitle: 第七弹\ndata: 2025-02-18 16:20:00\nupdated: 2025-02-18 16:20:00\ntype: ROS2\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p8.png\ntags:\n  - ROS2\n  - Learning\n---\n\n# 多线程与回调函数\n\n### 需要下载一个 cpp-httplib ，下载到 workspace/src/demo_one_pkg/include/\n\n> ```shell\n> git clone https://gitee.com/ohhuo/cpp-httplib.git\n> ```\n>\n> ### 下载后得到如下文件\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_12.png)\n\n### 下载好之后，修改 demo_one_pkg 的 CMakeLists 文件\n\n> ```cmake\n> # 包含 cpp-httplib 的 include\n> include_directories(include)\n> ```\n\n### 接着，在 demo_one_pkg/src 下创建 learn_thread.cpp，内容如下\n\n> ```cpp\n> #include <iostream>\n> #include <thread>\t// 多线程\n> #include <chrono>\t// 时间相关\n> #include <functional>\t// 函数包装器\n> #include \"cpp-httplib/httplib.h\" \n> #include <future>\t// 用于 std::async 处理异步任务\n> \n> class Download{\n> \n> private:\n> \n> \n> public:\n> \t// 下载函数,负责下载文件并调用回调函数\n> \t// host 主机， path 路径，回调函数\n> \tvoid download(const std::string& host, const std::string& path, const std::function<void(const std::string &, const std::string &)> &callback_word_count){\n> \t\t// 输出当前线程ID，便于调试\n> \t\tstd::cout << \"线程编号： \" << std::this_thread::get_id() << std::endl;\n> \t\t// 创建客户端对象，指定主机\n> \t\thttplib::Client client(host);\n> \t\t// 发送 GET 请求，获取文件内容\n> \t\tauto response = client.Get(path);\n> \t\t// 如果请求成功且状态码为200\n> \t\tif(response && response->status == 200){\n> \t\t\t// 调用回调函数并传递文件内容\n> \t\t\tcallback_word_count(path, response->body);\n> \t\t}\n> \t};\n> \t// 启动下载函数,使用 std::async 异步执行下载任务\n> \tvoid start_download(const std::string& host, const std::string& path, const std::function<void(const std::string &, const std::string &)> &callback_word_count){\n> \t\t// 函数包装器进行包装\n> \t\t// auto download_fun = std::bind(&Download::start_download, this, std::placeholders::_1, std::placeholders::_2, std::placeholders::_3);\n> \t\t// 创建线程\n> \t\t// std::thread thread(download_fun, host, path, callback_word_count);\n> \t\t// 等待线程结束\n> \t\t// thread.join();\n>         \n> \t\t// 直接通过 std::async 可以异步执行任务，同时它会返回一个 std::future 对象，保证任务的异步执行，而主线程会等待任务完成\n> \t\t// std::launch::async 会指示 std::async 强制使用新的线程执行任务\n> \t\tauto future = std::async(std::launch::async, [=](){\n> \t\t\tdownload(host, path, callback_word_count);\n> \t\t});\n> \t};\n> \n> };\n> \n> \n> int main(){\n> \t\n> \tauto d = Download();\n> \t// 创建回调函数, path 地址， result 结果\n> \tauto word_count = [](const std::string &path, const std::string &result) -> void{\n> \t\t// 输出文件路径，文件内容的长度，以及文件内容的前五个字符\n> \t\tstd::cout << \"下载完成 \" << path << \": \" << result.length() << \" -> \" << result.substr(0, 5) << std::endl;\n> \t};\n> \t\n> \t// 启动多个下载任务，分别下载 novel1.txt、novel2.txt 和 novel3.txt\n> \td.start_download(\"http://0.0.0.0:8080\", \"/novel1.txt\", word_count);\n> \td.start_download(\"http://0.0.0.0:8080\", \"/novel2.txt\", word_count);\n> \td.start_download(\"http://0.0.0.0:8080\", \"/novel3.txt\", word_count);\n> \t\n> \t// 主线程休眠10秒，确保所有异步线程有足够时间完成下载任务\n> \tstd::this_thread::sleep_for(std::chrono::milliseconds(1000*10));\n> \t\n> \treturn 0;\n> }\n> ```\n>\n> ### 这里需要注意的是 std::async 的用法\n>\n> > ### 它是 C++11 引入的一个标准库函数，用于启动异步任务。它接受一个可调用对象（比如函数、函数指针、Lambda 表达式等）和一些参数，并返回一个 std::future 对象，表示异步操作的结果。\n> >\n> > ### 语法如下\n> >\n> > ```cpp\n> > std::future<T> async(std::launch policy, F&& f, Args&&... args);\n> > ```\n> >\n> > ### std::launch 指定任务的执行策略，如 std::launch::async 或 std::launch::deferred。前者是任务会异步启动并在独立的线程中执行后者是任务会被推迟执行，直到调用 future.get() 时，才会在当前线程中执行。\n> >\n> > ### Lambda 表达式\n> >\n> > ```cpp\n> > [捕获列表](参数列表) -> 返回类型 { 函数体 }\n> > ```\n> >\n> > ### [=] ：按值捕获外部变量\n> >\n> > ### [&] : 按引用捕获外部变量\n\n### 在 CMakeLists 文件中添加相应语句\n\n> ```cmake\n> # 添加可执行文件\n> add_executable(learn_thread src/learn_thread.cpp)\n> # 将节点拷贝到install\n> install(TARGETS demo_one_node person_node learn_shared_ptr learn_lambda learn_functional learn_thread\n> DESTINATION lib/${PROJECT_NAME}\n> )\n> ```\n\n### 在 chapter2_ws 目录下创建三个 txt 文件，并写入简单的内容，然后在该路径下运行本地服务器，并保持打开状态\n\n> ```shell\n> echo \"第一章 hello word\" > novel1.txt\n> echo \"第二章 hello guys\" > novel2.txt\n> echo \"第三章 hello girls\" > novel3.txt\n> ```\n>\n> ```shell\n> python3 -m http.server\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_13.png)\n\n### 在同一路径下打开另外一个终端，编译并执行 learn_thread 节点\n\n> ```shell\n> colcon build\n> source install/setup.bash\n> ros2 run demo_one_pkg learn_thread\n> ```\n>\n> ### 执行后会得到如下输出\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_14.png)\n\n","slug":"08","published":1,"date":"2025-02-18T08:16:55.396Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cqq000fiku46zl1fl8x","content":"<h1 id=\"多线程与回调函数\"><a href=\"#多线程与回调函数\" class=\"headerlink\" title=\"多线程与回调函数\"></a>多线程与回调函数</h1><h3 id=\"需要下载一个-cpp-httplib-，下载到-workspace-src-demo-one-pkg-include\"><a href=\"#需要下载一个-cpp-httplib-，下载到-workspace-src-demo-one-pkg-include\" class=\"headerlink\" title=\"需要下载一个 cpp-httplib ，下载到 workspace&#x2F;src&#x2F;demo_one_pkg&#x2F;include&#x2F;\"></a>需要下载一个 cpp-httplib ，下载到 workspace&#x2F;src&#x2F;demo_one_pkg&#x2F;include&#x2F;</h3><blockquote>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https://gitee.com/ohhuo/cpp-httplib.git</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"下载后得到如下文件\"><a href=\"#下载后得到如下文件\" class=\"headerlink\" title=\"下载后得到如下文件\"></a>下载后得到如下文件</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_12.png\"></p>\n</blockquote>\n<h3 id=\"下载好之后，修改-demo-one-pkg-的-CMakeLists-文件\"><a href=\"#下载好之后，修改-demo-one-pkg-的-CMakeLists-文件\" class=\"headerlink\" title=\"下载好之后，修改 demo_one_pkg 的 CMakeLists 文件\"></a>下载好之后，修改 demo_one_pkg 的 CMakeLists 文件</h3><blockquote>\n<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 包含 cpp-httplib 的 include</span></span><br><span class=\"line\"><span class=\"keyword\">include_directories</span>(<span class=\"keyword\">include</span>)</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"接着，在-demo-one-pkg-src-下创建-learn-thread-cpp，内容如下\"><a href=\"#接着，在-demo-one-pkg-src-下创建-learn-thread-cpp，内容如下\" class=\"headerlink\" title=\"接着，在 demo_one_pkg&#x2F;src 下创建 learn_thread.cpp，内容如下\"></a>接着，在 demo_one_pkg&#x2F;src 下创建 learn_thread.cpp，内容如下</h3><blockquote>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thread&gt;</span>\t<span class=\"comment\">// 多线程</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;chrono&gt;</span>\t<span class=\"comment\">// 时间相关</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;functional&gt;</span>\t<span class=\"comment\">// 函数包装器</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cpp-httplib/httplib.h&quot;</span> </span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;future&gt;</span>\t<span class=\"comment\">// 用于 std::async 处理异步任务</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Download</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span>:</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"comment\">// 下载函数,负责下载文件并调用回调函数</span></span><br><span class=\"line\">\t<span class=\"comment\">// host 主机， path 路径，回调函数</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">download</span><span class=\"params\">(<span class=\"type\">const</span> std::string&amp; host, <span class=\"type\">const</span> std::string&amp; path, <span class=\"type\">const</span> std::function&lt;<span class=\"type\">void</span>(<span class=\"type\">const</span> std::string &amp;, <span class=\"type\">const</span> std::string &amp;)&gt; &amp;callback_word_count)</span></span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 输出当前线程ID，便于调试</span></span><br><span class=\"line\">\t\tstd::cout &lt;&lt; <span class=\"string\">&quot;线程编号： &quot;</span> &lt;&lt; std::this_thread::<span class=\"built_in\">get_id</span>() &lt;&lt; std::endl;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 创建客户端对象，指定主机</span></span><br><span class=\"line\">\t\t<span class=\"function\">httplib::Client <span class=\"title\">client</span><span class=\"params\">(host)</span></span>;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 发送 GET 请求，获取文件内容</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">auto</span> response = client.<span class=\"built_in\">Get</span>(path);</span><br><span class=\"line\">\t\t<span class=\"comment\">// 如果请求成功且状态码为200</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span>(response &amp;&amp; response-&gt;status == <span class=\"number\">200</span>)&#123;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 调用回调函数并传递文件内容</span></span><br><span class=\"line\">\t\t\t<span class=\"built_in\">callback_word_count</span>(path, response-&gt;body);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">\t<span class=\"comment\">// 启动下载函数,使用 std::async 异步执行下载任务</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">start_download</span><span class=\"params\">(<span class=\"type\">const</span> std::string&amp; host, <span class=\"type\">const</span> std::string&amp; path, <span class=\"type\">const</span> std::function&lt;<span class=\"type\">void</span>(<span class=\"type\">const</span> std::string &amp;, <span class=\"type\">const</span> std::string &amp;)&gt; &amp;callback_word_count)</span></span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 函数包装器进行包装</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// auto download_fun = std::bind(&amp;Download::start_download, this, std::placeholders::_1, std::placeholders::_2, std::placeholders::_3);</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// 创建线程</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// std::thread thread(download_fun, host, path, callback_word_count);</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// 等待线程结束</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// thread.join();</span></span><br><span class=\"line\">        </span><br><span class=\"line\">\t\t<span class=\"comment\">// 直接通过 std::async 可以异步执行任务，同时它会返回一个 std::future 对象，保证任务的异步执行，而主线程会等待任务完成</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// std::launch::async 会指示 std::async 强制使用新的线程执行任务</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">auto</span> future = std::<span class=\"built_in\">async</span>(std::launch::async, [=]()&#123;</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">download</span>(host, path, callback_word_count);</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> d = <span class=\"built_in\">Download</span>();</span><br><span class=\"line\">\t<span class=\"comment\">// 创建回调函数, path 地址， result 结果</span></span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> word_count = [](<span class=\"type\">const</span> std::string &amp;path, <span class=\"type\">const</span> std::string &amp;result) -&gt; <span class=\"type\">void</span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 输出文件路径，文件内容的长度，以及文件内容的前五个字符</span></span><br><span class=\"line\">\t\tstd::cout &lt;&lt; <span class=\"string\">&quot;下载完成 &quot;</span> &lt;&lt; path &lt;&lt; <span class=\"string\">&quot;: &quot;</span> &lt;&lt; result.<span class=\"built_in\">length</span>() &lt;&lt; <span class=\"string\">&quot; -&gt; &quot;</span> &lt;&lt; result.<span class=\"built_in\">substr</span>(<span class=\"number\">0</span>, <span class=\"number\">5</span>) &lt;&lt; std::endl;</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// 启动多个下载任务，分别下载 novel1.txt、novel2.txt 和 novel3.txt</span></span><br><span class=\"line\">\td.<span class=\"built_in\">start_download</span>(<span class=\"string\">&quot;http://0.0.0.0:8080&quot;</span>, <span class=\"string\">&quot;/novel1.txt&quot;</span>, word_count);</span><br><span class=\"line\">\td.<span class=\"built_in\">start_download</span>(<span class=\"string\">&quot;http://0.0.0.0:8080&quot;</span>, <span class=\"string\">&quot;/novel2.txt&quot;</span>, word_count);</span><br><span class=\"line\">\td.<span class=\"built_in\">start_download</span>(<span class=\"string\">&quot;http://0.0.0.0:8080&quot;</span>, <span class=\"string\">&quot;/novel3.txt&quot;</span>, word_count);</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// 主线程休眠10秒，确保所有异步线程有足够时间完成下载任务</span></span><br><span class=\"line\">\tstd::this_thread::<span class=\"built_in\">sleep_for</span>(std::chrono::<span class=\"built_in\">milliseconds</span>(<span class=\"number\">1000</span>*<span class=\"number\">10</span>));</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"这里需要注意的是-std-async-的用法\"><a href=\"#这里需要注意的是-std-async-的用法\" class=\"headerlink\" title=\"这里需要注意的是 std::async 的用法\"></a>这里需要注意的是 std::async 的用法</h3><blockquote>\n<h3 id=\"它是-C-11-引入的一个标准库函数，用于启动异步任务。它接受一个可调用对象（比如函数、函数指针、Lambda-表达式等）和一些参数，并返回一个-std-future-对象，表示异步操作的结果。\"><a href=\"#它是-C-11-引入的一个标准库函数，用于启动异步任务。它接受一个可调用对象（比如函数、函数指针、Lambda-表达式等）和一些参数，并返回一个-std-future-对象，表示异步操作的结果。\" class=\"headerlink\" title=\"它是 C++11 引入的一个标准库函数，用于启动异步任务。它接受一个可调用对象（比如函数、函数指针、Lambda 表达式等）和一些参数，并返回一个 std::future 对象，表示异步操作的结果。\"></a>它是 C++11 引入的一个标准库函数，用于启动异步任务。它接受一个可调用对象（比如函数、函数指针、Lambda 表达式等）和一些参数，并返回一个 std::future 对象，表示异步操作的结果。</h3><h3 id=\"语法如下\"><a href=\"#语法如下\" class=\"headerlink\" title=\"语法如下\"></a>语法如下</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">std::future&lt;T&gt; <span class=\"title\">async</span><span class=\"params\">(std::launch policy, F&amp;&amp; f, Args&amp;&amp;... args)</span></span>;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"std-launch-指定任务的执行策略，如-std-launch-async-或-std-launch-deferred。前者是任务会异步启动并在独立的线程中执行后者是任务会被推迟执行，直到调用-future-get-时，才会在当前线程中执行。\"><a href=\"#std-launch-指定任务的执行策略，如-std-launch-async-或-std-launch-deferred。前者是任务会异步启动并在独立的线程中执行后者是任务会被推迟执行，直到调用-future-get-时，才会在当前线程中执行。\" class=\"headerlink\" title=\"std::launch 指定任务的执行策略，如 std::launch::async 或 std::launch::deferred。前者是任务会异步启动并在独立的线程中执行后者是任务会被推迟执行，直到调用 future.get() 时，才会在当前线程中执行。\"></a>std::launch 指定任务的执行策略，如 std::launch::async 或 std::launch::deferred。前者是任务会异步启动并在独立的线程中执行后者是任务会被推迟执行，直到调用 future.get() 时，才会在当前线程中执行。</h3><h3 id=\"Lambda-表达式\"><a href=\"#Lambda-表达式\" class=\"headerlink\" title=\"Lambda 表达式\"></a>Lambda 表达式</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[捕获列表](参数列表) -&gt; 返回类型 &#123; 函数体 &#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"：按值捕获外部变量\"><a href=\"#：按值捕获外部变量\" class=\"headerlink\" title=\"[&#x3D;] ：按值捕获外部变量\"></a>[&#x3D;] ：按值捕获外部变量</h3><h3 id=\"按引用捕获外部变量\"><a href=\"#按引用捕获外部变量\" class=\"headerlink\" title=\"[&amp;] : 按引用捕获外部变量\"></a>[&amp;] : 按引用捕获外部变量</h3></blockquote>\n</blockquote>\n<h3 id=\"在-CMakeLists-文件中添加相应语句\"><a href=\"#在-CMakeLists-文件中添加相应语句\" class=\"headerlink\" title=\"在 CMakeLists 文件中添加相应语句\"></a>在 CMakeLists 文件中添加相应语句</h3><blockquote>\n<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 添加可执行文件</span></span><br><span class=\"line\"><span class=\"keyword\">add_executable</span>(learn_thread src/learn_thread.cpp)</span><br><span class=\"line\"><span class=\"comment\"># 将节点拷贝到install</span></span><br><span class=\"line\"><span class=\"keyword\">install</span>(TARGETS demo_one_node person_node learn_shared_ptr learn_lambda learn_functional learn_thread</span><br><span class=\"line\">DESTINATION lib/<span class=\"variable\">$&#123;PROJECT_NAME&#125;</span></span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"在-chapter2-ws-目录下创建三个-txt-文件，并写入简单的内容，然后在该路径下运行本地服务器，并保持打开状态\"><a href=\"#在-chapter2-ws-目录下创建三个-txt-文件，并写入简单的内容，然后在该路径下运行本地服务器，并保持打开状态\" class=\"headerlink\" title=\"在 chapter2_ws 目录下创建三个 txt 文件，并写入简单的内容，然后在该路径下运行本地服务器，并保持打开状态\"></a>在 chapter2_ws 目录下创建三个 txt 文件，并写入简单的内容，然后在该路径下运行本地服务器，并保持打开状态</h3><blockquote>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo &quot;第一章 hello word&quot; &gt; novel1.txt</span><br><span class=\"line\">echo &quot;第二章 hello guys&quot; &gt; novel2.txt</span><br><span class=\"line\">echo &quot;第三章 hello girls&quot; &gt; novel3.txt</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m http.server</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_13.png\"></p>\n</blockquote>\n<h3 id=\"在同一路径下打开另外一个终端，编译并执行-learn-thread-节点\"><a href=\"#在同一路径下打开另外一个终端，编译并执行-learn-thread-节点\" class=\"headerlink\" title=\"在同一路径下打开另外一个终端，编译并执行 learn_thread 节点\"></a>在同一路径下打开另外一个终端，编译并执行 learn_thread 节点</h3><blockquote>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 run demo_one_pkg learn_thread</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"执行后会得到如下输出\"><a href=\"#执行后会得到如下输出\" class=\"headerlink\" title=\"执行后会得到如下输出\"></a>执行后会得到如下输出</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_14.png\"></p>\n</blockquote>\n","cover_type":"img","excerpt":"","more":"<h1 id=\"多线程与回调函数\"><a href=\"#多线程与回调函数\" class=\"headerlink\" title=\"多线程与回调函数\"></a>多线程与回调函数</h1><h3 id=\"需要下载一个-cpp-httplib-，下载到-workspace-src-demo-one-pkg-include\"><a href=\"#需要下载一个-cpp-httplib-，下载到-workspace-src-demo-one-pkg-include\" class=\"headerlink\" title=\"需要下载一个 cpp-httplib ，下载到 workspace&#x2F;src&#x2F;demo_one_pkg&#x2F;include&#x2F;\"></a>需要下载一个 cpp-httplib ，下载到 workspace&#x2F;src&#x2F;demo_one_pkg&#x2F;include&#x2F;</h3><blockquote>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https://gitee.com/ohhuo/cpp-httplib.git</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"下载后得到如下文件\"><a href=\"#下载后得到如下文件\" class=\"headerlink\" title=\"下载后得到如下文件\"></a>下载后得到如下文件</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_12.png\"></p>\n</blockquote>\n<h3 id=\"下载好之后，修改-demo-one-pkg-的-CMakeLists-文件\"><a href=\"#下载好之后，修改-demo-one-pkg-的-CMakeLists-文件\" class=\"headerlink\" title=\"下载好之后，修改 demo_one_pkg 的 CMakeLists 文件\"></a>下载好之后，修改 demo_one_pkg 的 CMakeLists 文件</h3><blockquote>\n<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 包含 cpp-httplib 的 include</span></span><br><span class=\"line\"><span class=\"keyword\">include_directories</span>(<span class=\"keyword\">include</span>)</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"接着，在-demo-one-pkg-src-下创建-learn-thread-cpp，内容如下\"><a href=\"#接着，在-demo-one-pkg-src-下创建-learn-thread-cpp，内容如下\" class=\"headerlink\" title=\"接着，在 demo_one_pkg&#x2F;src 下创建 learn_thread.cpp，内容如下\"></a>接着，在 demo_one_pkg&#x2F;src 下创建 learn_thread.cpp，内容如下</h3><blockquote>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;thread&gt;</span>\t<span class=\"comment\">// 多线程</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;chrono&gt;</span>\t<span class=\"comment\">// 时间相关</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;functional&gt;</span>\t<span class=\"comment\">// 函数包装器</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;cpp-httplib/httplib.h&quot;</span> </span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;future&gt;</span>\t<span class=\"comment\">// 用于 std::async 处理异步任务</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Download</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span>:</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"comment\">// 下载函数,负责下载文件并调用回调函数</span></span><br><span class=\"line\">\t<span class=\"comment\">// host 主机， path 路径，回调函数</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">download</span><span class=\"params\">(<span class=\"type\">const</span> std::string&amp; host, <span class=\"type\">const</span> std::string&amp; path, <span class=\"type\">const</span> std::function&lt;<span class=\"type\">void</span>(<span class=\"type\">const</span> std::string &amp;, <span class=\"type\">const</span> std::string &amp;)&gt; &amp;callback_word_count)</span></span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 输出当前线程ID，便于调试</span></span><br><span class=\"line\">\t\tstd::cout &lt;&lt; <span class=\"string\">&quot;线程编号： &quot;</span> &lt;&lt; std::this_thread::<span class=\"built_in\">get_id</span>() &lt;&lt; std::endl;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 创建客户端对象，指定主机</span></span><br><span class=\"line\">\t\t<span class=\"function\">httplib::Client <span class=\"title\">client</span><span class=\"params\">(host)</span></span>;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 发送 GET 请求，获取文件内容</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">auto</span> response = client.<span class=\"built_in\">Get</span>(path);</span><br><span class=\"line\">\t\t<span class=\"comment\">// 如果请求成功且状态码为200</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span>(response &amp;&amp; response-&gt;status == <span class=\"number\">200</span>)&#123;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">// 调用回调函数并传递文件内容</span></span><br><span class=\"line\">\t\t\t<span class=\"built_in\">callback_word_count</span>(path, response-&gt;body);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">\t<span class=\"comment\">// 启动下载函数,使用 std::async 异步执行下载任务</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">start_download</span><span class=\"params\">(<span class=\"type\">const</span> std::string&amp; host, <span class=\"type\">const</span> std::string&amp; path, <span class=\"type\">const</span> std::function&lt;<span class=\"type\">void</span>(<span class=\"type\">const</span> std::string &amp;, <span class=\"type\">const</span> std::string &amp;)&gt; &amp;callback_word_count)</span></span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 函数包装器进行包装</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// auto download_fun = std::bind(&amp;Download::start_download, this, std::placeholders::_1, std::placeholders::_2, std::placeholders::_3);</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// 创建线程</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// std::thread thread(download_fun, host, path, callback_word_count);</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// 等待线程结束</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// thread.join();</span></span><br><span class=\"line\">        </span><br><span class=\"line\">\t\t<span class=\"comment\">// 直接通过 std::async 可以异步执行任务，同时它会返回一个 std::future 对象，保证任务的异步执行，而主线程会等待任务完成</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// std::launch::async 会指示 std::async 强制使用新的线程执行任务</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">auto</span> future = std::<span class=\"built_in\">async</span>(std::launch::async, [=]()&#123;</span><br><span class=\"line\">\t\t\t<span class=\"built_in\">download</span>(host, path, callback_word_count);</span><br><span class=\"line\">\t\t&#125;);</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> d = <span class=\"built_in\">Download</span>();</span><br><span class=\"line\">\t<span class=\"comment\">// 创建回调函数, path 地址， result 结果</span></span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> word_count = [](<span class=\"type\">const</span> std::string &amp;path, <span class=\"type\">const</span> std::string &amp;result) -&gt; <span class=\"type\">void</span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 输出文件路径，文件内容的长度，以及文件内容的前五个字符</span></span><br><span class=\"line\">\t\tstd::cout &lt;&lt; <span class=\"string\">&quot;下载完成 &quot;</span> &lt;&lt; path &lt;&lt; <span class=\"string\">&quot;: &quot;</span> &lt;&lt; result.<span class=\"built_in\">length</span>() &lt;&lt; <span class=\"string\">&quot; -&gt; &quot;</span> &lt;&lt; result.<span class=\"built_in\">substr</span>(<span class=\"number\">0</span>, <span class=\"number\">5</span>) &lt;&lt; std::endl;</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// 启动多个下载任务，分别下载 novel1.txt、novel2.txt 和 novel3.txt</span></span><br><span class=\"line\">\td.<span class=\"built_in\">start_download</span>(<span class=\"string\">&quot;http://0.0.0.0:8080&quot;</span>, <span class=\"string\">&quot;/novel1.txt&quot;</span>, word_count);</span><br><span class=\"line\">\td.<span class=\"built_in\">start_download</span>(<span class=\"string\">&quot;http://0.0.0.0:8080&quot;</span>, <span class=\"string\">&quot;/novel2.txt&quot;</span>, word_count);</span><br><span class=\"line\">\td.<span class=\"built_in\">start_download</span>(<span class=\"string\">&quot;http://0.0.0.0:8080&quot;</span>, <span class=\"string\">&quot;/novel3.txt&quot;</span>, word_count);</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// 主线程休眠10秒，确保所有异步线程有足够时间完成下载任务</span></span><br><span class=\"line\">\tstd::this_thread::<span class=\"built_in\">sleep_for</span>(std::chrono::<span class=\"built_in\">milliseconds</span>(<span class=\"number\">1000</span>*<span class=\"number\">10</span>));</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"这里需要注意的是-std-async-的用法\"><a href=\"#这里需要注意的是-std-async-的用法\" class=\"headerlink\" title=\"这里需要注意的是 std::async 的用法\"></a>这里需要注意的是 std::async 的用法</h3><blockquote>\n<h3 id=\"它是-C-11-引入的一个标准库函数，用于启动异步任务。它接受一个可调用对象（比如函数、函数指针、Lambda-表达式等）和一些参数，并返回一个-std-future-对象，表示异步操作的结果。\"><a href=\"#它是-C-11-引入的一个标准库函数，用于启动异步任务。它接受一个可调用对象（比如函数、函数指针、Lambda-表达式等）和一些参数，并返回一个-std-future-对象，表示异步操作的结果。\" class=\"headerlink\" title=\"它是 C++11 引入的一个标准库函数，用于启动异步任务。它接受一个可调用对象（比如函数、函数指针、Lambda 表达式等）和一些参数，并返回一个 std::future 对象，表示异步操作的结果。\"></a>它是 C++11 引入的一个标准库函数，用于启动异步任务。它接受一个可调用对象（比如函数、函数指针、Lambda 表达式等）和一些参数，并返回一个 std::future 对象，表示异步操作的结果。</h3><h3 id=\"语法如下\"><a href=\"#语法如下\" class=\"headerlink\" title=\"语法如下\"></a>语法如下</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">std::future&lt;T&gt; <span class=\"title\">async</span><span class=\"params\">(std::launch policy, F&amp;&amp; f, Args&amp;&amp;... args)</span></span>;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"std-launch-指定任务的执行策略，如-std-launch-async-或-std-launch-deferred。前者是任务会异步启动并在独立的线程中执行后者是任务会被推迟执行，直到调用-future-get-时，才会在当前线程中执行。\"><a href=\"#std-launch-指定任务的执行策略，如-std-launch-async-或-std-launch-deferred。前者是任务会异步启动并在独立的线程中执行后者是任务会被推迟执行，直到调用-future-get-时，才会在当前线程中执行。\" class=\"headerlink\" title=\"std::launch 指定任务的执行策略，如 std::launch::async 或 std::launch::deferred。前者是任务会异步启动并在独立的线程中执行后者是任务会被推迟执行，直到调用 future.get() 时，才会在当前线程中执行。\"></a>std::launch 指定任务的执行策略，如 std::launch::async 或 std::launch::deferred。前者是任务会异步启动并在独立的线程中执行后者是任务会被推迟执行，直到调用 future.get() 时，才会在当前线程中执行。</h3><h3 id=\"Lambda-表达式\"><a href=\"#Lambda-表达式\" class=\"headerlink\" title=\"Lambda 表达式\"></a>Lambda 表达式</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[捕获列表](参数列表) -&gt; 返回类型 &#123; 函数体 &#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"：按值捕获外部变量\"><a href=\"#：按值捕获外部变量\" class=\"headerlink\" title=\"[&#x3D;] ：按值捕获外部变量\"></a>[&#x3D;] ：按值捕获外部变量</h3><h3 id=\"按引用捕获外部变量\"><a href=\"#按引用捕获外部变量\" class=\"headerlink\" title=\"[&amp;] : 按引用捕获外部变量\"></a>[&amp;] : 按引用捕获外部变量</h3></blockquote>\n</blockquote>\n<h3 id=\"在-CMakeLists-文件中添加相应语句\"><a href=\"#在-CMakeLists-文件中添加相应语句\" class=\"headerlink\" title=\"在 CMakeLists 文件中添加相应语句\"></a>在 CMakeLists 文件中添加相应语句</h3><blockquote>\n<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 添加可执行文件</span></span><br><span class=\"line\"><span class=\"keyword\">add_executable</span>(learn_thread src/learn_thread.cpp)</span><br><span class=\"line\"><span class=\"comment\"># 将节点拷贝到install</span></span><br><span class=\"line\"><span class=\"keyword\">install</span>(TARGETS demo_one_node person_node learn_shared_ptr learn_lambda learn_functional learn_thread</span><br><span class=\"line\">DESTINATION lib/<span class=\"variable\">$&#123;PROJECT_NAME&#125;</span></span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"在-chapter2-ws-目录下创建三个-txt-文件，并写入简单的内容，然后在该路径下运行本地服务器，并保持打开状态\"><a href=\"#在-chapter2-ws-目录下创建三个-txt-文件，并写入简单的内容，然后在该路径下运行本地服务器，并保持打开状态\" class=\"headerlink\" title=\"在 chapter2_ws 目录下创建三个 txt 文件，并写入简单的内容，然后在该路径下运行本地服务器，并保持打开状态\"></a>在 chapter2_ws 目录下创建三个 txt 文件，并写入简单的内容，然后在该路径下运行本地服务器，并保持打开状态</h3><blockquote>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo &quot;第一章 hello word&quot; &gt; novel1.txt</span><br><span class=\"line\">echo &quot;第二章 hello guys&quot; &gt; novel2.txt</span><br><span class=\"line\">echo &quot;第三章 hello girls&quot; &gt; novel3.txt</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m http.server</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_13.png\"></p>\n</blockquote>\n<h3 id=\"在同一路径下打开另外一个终端，编译并执行-learn-thread-节点\"><a href=\"#在同一路径下打开另外一个终端，编译并执行-learn-thread-节点\" class=\"headerlink\" title=\"在同一路径下打开另外一个终端，编译并执行 learn_thread 节点\"></a>在同一路径下打开另外一个终端，编译并执行 learn_thread 节点</h3><blockquote>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 run demo_one_pkg learn_thread</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"执行后会得到如下输出\"><a href=\"#执行后会得到如下输出\" class=\"headerlink\" title=\"执行后会得到如下输出\"></a>执行后会得到如下输出</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_14.png\"></p>\n</blockquote>\n"},{"title":"Ubuntu使用","data":"2025-02-19T15:12:00.000Z","updated":"2025-02-19T15:12:00.000Z","type":"Ubuntu","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p9.png","_content":"\n# Ubuntu 换源\n\n### 在使用 sudo apt-get update 和 sudo apt-get upgrade 进行更新时，可能下载会很慢，这个时候使用国内的镜像资源可能会明显提升下载速度。\n\n\n\n> ### 先是要备份原来的源文件\n>\n> ```shell\n> sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak\n> ```\n>\n> ### 再编辑 sources.list 文件\n>\n> ```shell\n> sudo nano /etc/apt/sources.list\n> ```\n>\n> ### 这里使用阿里云的镜像\n>\n> ```txt\n> http://mirrors.aliyun.com/ubuntu/\n> ```\n>\n> ### 替换后，再进行更新\n>\n> ```shell\n> sudo apt-get update\n> sudo apt-get upgrade\n> ```","source":"_posts/09.md","raw":"---\ntitle: Ubuntu使用\ndata: 2025-02-19 23:12:00\nupdated: 2025-02-19 23:12:00\ntype: Ubuntu\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p9.png\ntags:\n  - Ubuntu\n  - Learning\n---\n\n# Ubuntu 换源\n\n### 在使用 sudo apt-get update 和 sudo apt-get upgrade 进行更新时，可能下载会很慢，这个时候使用国内的镜像资源可能会明显提升下载速度。\n\n\n\n> ### 先是要备份原来的源文件\n>\n> ```shell\n> sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak\n> ```\n>\n> ### 再编辑 sources.list 文件\n>\n> ```shell\n> sudo nano /etc/apt/sources.list\n> ```\n>\n> ### 这里使用阿里云的镜像\n>\n> ```txt\n> http://mirrors.aliyun.com/ubuntu/\n> ```\n>\n> ### 替换后，再进行更新\n>\n> ```shell\n> sudo apt-get update\n> sudo apt-get upgrade\n> ```","slug":"09","published":1,"date":"2025-02-19T15:11:54.631Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cqu000jiku4g9c52rex","content":"<h1 id=\"Ubuntu-换源\"><a href=\"#Ubuntu-换源\" class=\"headerlink\" title=\"Ubuntu 换源\"></a>Ubuntu 换源</h1><h3 id=\"在使用-sudo-apt-get-update-和-sudo-apt-get-upgrade-进行更新时，可能下载会很慢，这个时候使用国内的镜像资源可能会明显提升下载速度。\"><a href=\"#在使用-sudo-apt-get-update-和-sudo-apt-get-upgrade-进行更新时，可能下载会很慢，这个时候使用国内的镜像资源可能会明显提升下载速度。\" class=\"headerlink\" title=\"在使用 sudo apt-get update 和 sudo apt-get upgrade 进行更新时，可能下载会很慢，这个时候使用国内的镜像资源可能会明显提升下载速度。\"></a>在使用 sudo apt-get update 和 sudo apt-get upgrade 进行更新时，可能下载会很慢，这个时候使用国内的镜像资源可能会明显提升下载速度。</h3><blockquote>\n<h3 id=\"先是要备份原来的源文件\"><a href=\"#先是要备份原来的源文件\" class=\"headerlink\" title=\"先是要备份原来的源文件\"></a>先是要备份原来的源文件</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"再编辑-sources-list-文件\"><a href=\"#再编辑-sources-list-文件\" class=\"headerlink\" title=\"再编辑 sources.list 文件\"></a>再编辑 sources.list 文件</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo nano /etc/apt/sources.list</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"这里使用阿里云的镜像\"><a href=\"#这里使用阿里云的镜像\" class=\"headerlink\" title=\"这里使用阿里云的镜像\"></a>这里使用阿里云的镜像</h3><figure class=\"highlight txt\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://mirrors.aliyun.com/ubuntu/</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"替换后，再进行更新\"><a href=\"#替换后，再进行更新\" class=\"headerlink\" title=\"替换后，再进行更新\"></a>替换后，再进行更新</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get update</span><br><span class=\"line\">sudo apt-get upgrade</span><br></pre></td></tr></table></figure></blockquote>\n","cover_type":"img","excerpt":"","more":"<h1 id=\"Ubuntu-换源\"><a href=\"#Ubuntu-换源\" class=\"headerlink\" title=\"Ubuntu 换源\"></a>Ubuntu 换源</h1><h3 id=\"在使用-sudo-apt-get-update-和-sudo-apt-get-upgrade-进行更新时，可能下载会很慢，这个时候使用国内的镜像资源可能会明显提升下载速度。\"><a href=\"#在使用-sudo-apt-get-update-和-sudo-apt-get-upgrade-进行更新时，可能下载会很慢，这个时候使用国内的镜像资源可能会明显提升下载速度。\" class=\"headerlink\" title=\"在使用 sudo apt-get update 和 sudo apt-get upgrade 进行更新时，可能下载会很慢，这个时候使用国内的镜像资源可能会明显提升下载速度。\"></a>在使用 sudo apt-get update 和 sudo apt-get upgrade 进行更新时，可能下载会很慢，这个时候使用国内的镜像资源可能会明显提升下载速度。</h3><blockquote>\n<h3 id=\"先是要备份原来的源文件\"><a href=\"#先是要备份原来的源文件\" class=\"headerlink\" title=\"先是要备份原来的源文件\"></a>先是要备份原来的源文件</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"再编辑-sources-list-文件\"><a href=\"#再编辑-sources-list-文件\" class=\"headerlink\" title=\"再编辑 sources.list 文件\"></a>再编辑 sources.list 文件</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo nano /etc/apt/sources.list</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"这里使用阿里云的镜像\"><a href=\"#这里使用阿里云的镜像\" class=\"headerlink\" title=\"这里使用阿里云的镜像\"></a>这里使用阿里云的镜像</h3><figure class=\"highlight txt\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">http://mirrors.aliyun.com/ubuntu/</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"替换后，再进行更新\"><a href=\"#替换后，再进行更新\" class=\"headerlink\" title=\"替换后，再进行更新\"></a>替换后，再进行更新</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get update</span><br><span class=\"line\">sudo apt-get upgrade</span><br></pre></td></tr></table></figure></blockquote>\n"},{"title":"第八弹","data":"2025-02-20T11:46:00.000Z","updated":"2025-02-20T11:46:00.000Z","type":"ROS2","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p10.png","_content":"\n# 话题\n\n### 一个话题包含了发布节点、订阅节点、话题名称和话题类型四个方面\n\n> ### 小海龟示例\n>\n> ### 启动小海龟模拟器\n>\n> ```shell\n> ros2 run turtlesim turtlesim_node\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_15.png)\n>\n> ### 然后在同一路径下打开一个新的终端\n>\n> ### 查看当前运行的节点列表\n>\n> ```shell\n> ros2 node list\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_16.png)\n>\n> ### 查看当前运行节点的信息\n>\n> ```shell\n> ros2 node info /turtlesim\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_17.png)\n>\n> ### 其中，订阅者里的 /turtle1/cmd_vel 话题是负责控制小海龟的，发布者里的 /turtle1/pose 是小海龟的实时位置信息\n>\n> ### 输出小海龟位置信息\n>\n> ```shell\n> ros2 topic echo /turtle1/pose\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_18.png)\n>\n> ### 小海龟的位置信息包含五个参数，坐标(x, y)，头的朝向的角度 theta，线速度 linear_velocity，角速度 angular_velocity\n>\n> ### 获取某个话题的消息接口\n>\n> ```shell\n> ros2 topic info /turtle1/cmd_vel\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_19.png)\n>\n> \n>\n> ### 查看接口的详细定义\n>\n> ```shell\n> ros2 interface show geometry_msgs/msg/Twist\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_20.png)\n>\n> ### 在确认某个节点的消息接口之后，可以直接对话题发布数据\n>\n> ### /turtle1/cmd_vel 是话题名，geometry_msgs/msg/Twist 消息接口，后面是 yaml 格式的数据，\n>\n> ```shell\n> ros2 topic pub /turtle1/cmd_vel geometry_msgs/msg/Twist \"{linear: {x: 0.5, y: 0.0} , angular: {z: 0.0}}\"\n> ```\n>\n> ### 如果同一时刻，有多个终端对该消息接口发布数据，那么小海龟的运动就会变得奇怪起来\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_21.png)\n>\n> ","source":"_posts/10.md","raw":"---\ntitle: 第八弹\ndata: 2025-02-20 19:46:00\nupdated: 2025-02-20 19:46:00\ntype: ROS2\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p10.png\ntags:\n  - ROS2\n  - Learning\n---\n\n# 话题\n\n### 一个话题包含了发布节点、订阅节点、话题名称和话题类型四个方面\n\n> ### 小海龟示例\n>\n> ### 启动小海龟模拟器\n>\n> ```shell\n> ros2 run turtlesim turtlesim_node\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_15.png)\n>\n> ### 然后在同一路径下打开一个新的终端\n>\n> ### 查看当前运行的节点列表\n>\n> ```shell\n> ros2 node list\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_16.png)\n>\n> ### 查看当前运行节点的信息\n>\n> ```shell\n> ros2 node info /turtlesim\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_17.png)\n>\n> ### 其中，订阅者里的 /turtle1/cmd_vel 话题是负责控制小海龟的，发布者里的 /turtle1/pose 是小海龟的实时位置信息\n>\n> ### 输出小海龟位置信息\n>\n> ```shell\n> ros2 topic echo /turtle1/pose\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_18.png)\n>\n> ### 小海龟的位置信息包含五个参数，坐标(x, y)，头的朝向的角度 theta，线速度 linear_velocity，角速度 angular_velocity\n>\n> ### 获取某个话题的消息接口\n>\n> ```shell\n> ros2 topic info /turtle1/cmd_vel\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_19.png)\n>\n> \n>\n> ### 查看接口的详细定义\n>\n> ```shell\n> ros2 interface show geometry_msgs/msg/Twist\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_20.png)\n>\n> ### 在确认某个节点的消息接口之后，可以直接对话题发布数据\n>\n> ### /turtle1/cmd_vel 是话题名，geometry_msgs/msg/Twist 消息接口，后面是 yaml 格式的数据，\n>\n> ```shell\n> ros2 topic pub /turtle1/cmd_vel geometry_msgs/msg/Twist \"{linear: {x: 0.5, y: 0.0} , angular: {z: 0.0}}\"\n> ```\n>\n> ### 如果同一时刻，有多个终端对该消息接口发布数据，那么小海龟的运动就会变得奇怪起来\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_21.png)\n>\n> ","slug":"10","published":1,"date":"2025-02-20T11:37:25.679Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cqv000miku4bhzec2en","content":"<h1 id=\"话题\"><a href=\"#话题\" class=\"headerlink\" title=\"话题\"></a>话题</h1><h3 id=\"一个话题包含了发布节点、订阅节点、话题名称和话题类型四个方面\"><a href=\"#一个话题包含了发布节点、订阅节点、话题名称和话题类型四个方面\" class=\"headerlink\" title=\"一个话题包含了发布节点、订阅节点、话题名称和话题类型四个方面\"></a>一个话题包含了发布节点、订阅节点、话题名称和话题类型四个方面</h3><blockquote>\n<h3 id=\"小海龟示例\"><a href=\"#小海龟示例\" class=\"headerlink\" title=\"小海龟示例\"></a>小海龟示例</h3><h3 id=\"启动小海龟模拟器\"><a href=\"#启动小海龟模拟器\" class=\"headerlink\" title=\"启动小海龟模拟器\"></a>启动小海龟模拟器</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 run turtlesim turtlesim_node</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_15.png\"></p>\n<h3 id=\"然后在同一路径下打开一个新的终端\"><a href=\"#然后在同一路径下打开一个新的终端\" class=\"headerlink\" title=\"然后在同一路径下打开一个新的终端\"></a>然后在同一路径下打开一个新的终端</h3><h3 id=\"查看当前运行的节点列表\"><a href=\"#查看当前运行的节点列表\" class=\"headerlink\" title=\"查看当前运行的节点列表\"></a>查看当前运行的节点列表</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 node list</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_16.png\"></p>\n<h3 id=\"查看当前运行节点的信息\"><a href=\"#查看当前运行节点的信息\" class=\"headerlink\" title=\"查看当前运行节点的信息\"></a>查看当前运行节点的信息</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 node info /turtlesim</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_17.png\"></p>\n<h3 id=\"其中，订阅者里的-turtle1-cmd-vel-话题是负责控制小海龟的，发布者里的-turtle1-pose-是小海龟的实时位置信息\"><a href=\"#其中，订阅者里的-turtle1-cmd-vel-话题是负责控制小海龟的，发布者里的-turtle1-pose-是小海龟的实时位置信息\" class=\"headerlink\" title=\"其中，订阅者里的 &#x2F;turtle1&#x2F;cmd_vel 话题是负责控制小海龟的，发布者里的 &#x2F;turtle1&#x2F;pose 是小海龟的实时位置信息\"></a>其中，订阅者里的 &#x2F;turtle1&#x2F;cmd_vel 话题是负责控制小海龟的，发布者里的 &#x2F;turtle1&#x2F;pose 是小海龟的实时位置信息</h3><h3 id=\"输出小海龟位置信息\"><a href=\"#输出小海龟位置信息\" class=\"headerlink\" title=\"输出小海龟位置信息\"></a>输出小海龟位置信息</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 topic echo /turtle1/pose</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_18.png\"></p>\n<h3 id=\"小海龟的位置信息包含五个参数，坐标-x-y-，头的朝向的角度-theta，线速度-linear-velocity，角速度-angular-velocity\"><a href=\"#小海龟的位置信息包含五个参数，坐标-x-y-，头的朝向的角度-theta，线速度-linear-velocity，角速度-angular-velocity\" class=\"headerlink\" title=\"小海龟的位置信息包含五个参数，坐标(x, y)，头的朝向的角度 theta，线速度 linear_velocity，角速度 angular_velocity\"></a>小海龟的位置信息包含五个参数，坐标(x, y)，头的朝向的角度 theta，线速度 linear_velocity，角速度 angular_velocity</h3><h3 id=\"获取某个话题的消息接口\"><a href=\"#获取某个话题的消息接口\" class=\"headerlink\" title=\"获取某个话题的消息接口\"></a>获取某个话题的消息接口</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 topic info /turtle1/cmd_vel</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_19.png\"></p>\n<h3 id=\"查看接口的详细定义\"><a href=\"#查看接口的详细定义\" class=\"headerlink\" title=\"查看接口的详细定义\"></a>查看接口的详细定义</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 interface show geometry_msgs/msg/Twist</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_20.png\"></p>\n<h3 id=\"在确认某个节点的消息接口之后，可以直接对话题发布数据\"><a href=\"#在确认某个节点的消息接口之后，可以直接对话题发布数据\" class=\"headerlink\" title=\"在确认某个节点的消息接口之后，可以直接对话题发布数据\"></a>在确认某个节点的消息接口之后，可以直接对话题发布数据</h3><h3 id=\"turtle1-cmd-vel-是话题名，geometry-msgs-msg-Twist-消息接口，后面是-yaml-格式的数据，\"><a href=\"#turtle1-cmd-vel-是话题名，geometry-msgs-msg-Twist-消息接口，后面是-yaml-格式的数据，\" class=\"headerlink\" title=\"&#x2F;turtle1&#x2F;cmd_vel 是话题名，geometry_msgs&#x2F;msg&#x2F;Twist 消息接口，后面是 yaml 格式的数据，\"></a>&#x2F;turtle1&#x2F;cmd_vel 是话题名，geometry_msgs&#x2F;msg&#x2F;Twist 消息接口，后面是 yaml 格式的数据，</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 topic pub /turtle1/cmd_vel geometry_msgs/msg/Twist &quot;&#123;linear: &#123;x: 0.5, y: 0.0&#125; , angular: &#123;z: 0.0&#125;&#125;&quot;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"如果同一时刻，有多个终端对该消息接口发布数据，那么小海龟的运动就会变得奇怪起来\"><a href=\"#如果同一时刻，有多个终端对该消息接口发布数据，那么小海龟的运动就会变得奇怪起来\" class=\"headerlink\" title=\"如果同一时刻，有多个终端对该消息接口发布数据，那么小海龟的运动就会变得奇怪起来\"></a>如果同一时刻，有多个终端对该消息接口发布数据，那么小海龟的运动就会变得奇怪起来</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_21.png\"></p>\n</blockquote>\n","cover_type":"img","excerpt":"","more":"<h1 id=\"话题\"><a href=\"#话题\" class=\"headerlink\" title=\"话题\"></a>话题</h1><h3 id=\"一个话题包含了发布节点、订阅节点、话题名称和话题类型四个方面\"><a href=\"#一个话题包含了发布节点、订阅节点、话题名称和话题类型四个方面\" class=\"headerlink\" title=\"一个话题包含了发布节点、订阅节点、话题名称和话题类型四个方面\"></a>一个话题包含了发布节点、订阅节点、话题名称和话题类型四个方面</h3><blockquote>\n<h3 id=\"小海龟示例\"><a href=\"#小海龟示例\" class=\"headerlink\" title=\"小海龟示例\"></a>小海龟示例</h3><h3 id=\"启动小海龟模拟器\"><a href=\"#启动小海龟模拟器\" class=\"headerlink\" title=\"启动小海龟模拟器\"></a>启动小海龟模拟器</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 run turtlesim turtlesim_node</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_15.png\"></p>\n<h3 id=\"然后在同一路径下打开一个新的终端\"><a href=\"#然后在同一路径下打开一个新的终端\" class=\"headerlink\" title=\"然后在同一路径下打开一个新的终端\"></a>然后在同一路径下打开一个新的终端</h3><h3 id=\"查看当前运行的节点列表\"><a href=\"#查看当前运行的节点列表\" class=\"headerlink\" title=\"查看当前运行的节点列表\"></a>查看当前运行的节点列表</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 node list</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_16.png\"></p>\n<h3 id=\"查看当前运行节点的信息\"><a href=\"#查看当前运行节点的信息\" class=\"headerlink\" title=\"查看当前运行节点的信息\"></a>查看当前运行节点的信息</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 node info /turtlesim</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_17.png\"></p>\n<h3 id=\"其中，订阅者里的-turtle1-cmd-vel-话题是负责控制小海龟的，发布者里的-turtle1-pose-是小海龟的实时位置信息\"><a href=\"#其中，订阅者里的-turtle1-cmd-vel-话题是负责控制小海龟的，发布者里的-turtle1-pose-是小海龟的实时位置信息\" class=\"headerlink\" title=\"其中，订阅者里的 &#x2F;turtle1&#x2F;cmd_vel 话题是负责控制小海龟的，发布者里的 &#x2F;turtle1&#x2F;pose 是小海龟的实时位置信息\"></a>其中，订阅者里的 &#x2F;turtle1&#x2F;cmd_vel 话题是负责控制小海龟的，发布者里的 &#x2F;turtle1&#x2F;pose 是小海龟的实时位置信息</h3><h3 id=\"输出小海龟位置信息\"><a href=\"#输出小海龟位置信息\" class=\"headerlink\" title=\"输出小海龟位置信息\"></a>输出小海龟位置信息</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 topic echo /turtle1/pose</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_18.png\"></p>\n<h3 id=\"小海龟的位置信息包含五个参数，坐标-x-y-，头的朝向的角度-theta，线速度-linear-velocity，角速度-angular-velocity\"><a href=\"#小海龟的位置信息包含五个参数，坐标-x-y-，头的朝向的角度-theta，线速度-linear-velocity，角速度-angular-velocity\" class=\"headerlink\" title=\"小海龟的位置信息包含五个参数，坐标(x, y)，头的朝向的角度 theta，线速度 linear_velocity，角速度 angular_velocity\"></a>小海龟的位置信息包含五个参数，坐标(x, y)，头的朝向的角度 theta，线速度 linear_velocity，角速度 angular_velocity</h3><h3 id=\"获取某个话题的消息接口\"><a href=\"#获取某个话题的消息接口\" class=\"headerlink\" title=\"获取某个话题的消息接口\"></a>获取某个话题的消息接口</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 topic info /turtle1/cmd_vel</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_19.png\"></p>\n<h3 id=\"查看接口的详细定义\"><a href=\"#查看接口的详细定义\" class=\"headerlink\" title=\"查看接口的详细定义\"></a>查看接口的详细定义</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 interface show geometry_msgs/msg/Twist</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_20.png\"></p>\n<h3 id=\"在确认某个节点的消息接口之后，可以直接对话题发布数据\"><a href=\"#在确认某个节点的消息接口之后，可以直接对话题发布数据\" class=\"headerlink\" title=\"在确认某个节点的消息接口之后，可以直接对话题发布数据\"></a>在确认某个节点的消息接口之后，可以直接对话题发布数据</h3><h3 id=\"turtle1-cmd-vel-是话题名，geometry-msgs-msg-Twist-消息接口，后面是-yaml-格式的数据，\"><a href=\"#turtle1-cmd-vel-是话题名，geometry-msgs-msg-Twist-消息接口，后面是-yaml-格式的数据，\" class=\"headerlink\" title=\"&#x2F;turtle1&#x2F;cmd_vel 是话题名，geometry_msgs&#x2F;msg&#x2F;Twist 消息接口，后面是 yaml 格式的数据，\"></a>&#x2F;turtle1&#x2F;cmd_vel 是话题名，geometry_msgs&#x2F;msg&#x2F;Twist 消息接口，后面是 yaml 格式的数据，</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 topic pub /turtle1/cmd_vel geometry_msgs/msg/Twist &quot;&#123;linear: &#123;x: 0.5, y: 0.0&#125; , angular: &#123;z: 0.0&#125;&#125;&quot;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"如果同一时刻，有多个终端对该消息接口发布数据，那么小海龟的运动就会变得奇怪起来\"><a href=\"#如果同一时刻，有多个终端对该消息接口发布数据，那么小海龟的运动就会变得奇怪起来\" class=\"headerlink\" title=\"如果同一时刻，有多个终端对该消息接口发布数据，那么小海龟的运动就会变得奇怪起来\"></a>如果同一时刻，有多个终端对该消息接口发布数据，那么小海龟的运动就会变得奇怪起来</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_21.png\"></p>\n</blockquote>\n"},{"title":"第六弹","data":"2025-02-17T12:40:00.000Z","updated":"2025-02-17T12:40:00.000Z","type":"ROS2","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p7.png","_content":"\n# ROS2 中，cpp新特性的应用\n\n### auto\n\n> ```cpp\n> // auto 自动推导类型\n> auto a = 1; // int a = 1\n> ```\n\n### 智能指针\n\n> ### 智能指针分为三种\n>\n> - #### shared_ptr 共享的智能指针\n>\n> - #### weak_ptr 弱引用的智能指针\n>\n> - #### unique_ptr 独占的智能指针\n>\n> ### std::shared_ptr<T> 是一个类模板，它的对象的行为类似于指针，它可以记录共享它所管理的内存对象的对象个数。多个共享指针可以共享同一个对象，当最后一个共享指针被销毁时，会自动释放其所指向的对象。一个共享指针通常使用 make_shared<> 来创建，也可以通过拷贝或者赋值其他共享指针的方式创建。\n>\n> ### 代码示例，在 demo_one_pkg/src/ 下创建 learn_shared_ptr.cpp\n>\n> ```cpp\n> #include <iostream>\n> #include <memory>\n> \n> int main(){\n> \t\n> \t// 创建共享智能指针 <数据类型/类>(参数) 返回值，对应类的共享指针\n> \tauto p1 = std::make_shared<std::string>(\"This is a string.\");\n> \tstd::cout << \"p1的引用计数: \" << p1.use_count() << \", 指向内存地址: \" << p1.get() << std::endl;\n> \t\n> \tauto p2 = p1;\t\n> \tstd::cout << \"p1的引用计数: \" << p1.use_count() << \", 指向内存地址: \" << p1.get() << std::endl;\n> \tstd::cout << \"p2的引用计数: \" << p2.use_count() << \", 指向内存地址: \" << p2.get() << std::endl;\n> \t\n> \t// 释放引用\n> \tp1.reset();\n> \tstd::cout << \"p1的引用计数: \" << p1.use_count() << \", 指向内存地址: \" << p1.get() << std::endl;\n> \tstd::cout << \"p2的引用计数: \" << p2.use_count() << \", 指向内存地址: \" << p2.get() << std::endl;\n> \t\n> \t// p2->c_str() 调用成员函数\n> \tstd::cout << \"p2的指向内存地址数据: \" << p2->c_str() << std::endl;\n> \t\n> \treturn 0;\n> }\n> ```\n>\n> ### 在 CMakeLists 文件中添加\n>\n> ```txt\n> # 添加可执行文件\n> add_executable(learn_shared_ptr src/learn_shared_ptr.cpp)\n> # 拷贝节点到install\n> install(TARGETS demo_one_node person_node learn_shared_ptr\n> DESTINATION lib/${PROJECT_NAME}\n> )\n> ```\n>\n> ### 编译并执行\n>\n> ```shell\n> colcon build\n> source install/setup.bash\n> ros2 run demo_one_pkg learn_shared_ptr\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_9.png)\n>\n\n### Lambda 表达式\n\n> ### 可以利用它来编写内嵌的匿名函数，用以替换独立函数或函数对象。\n>\n> ### 代码示例，在 demo_one_pkg/src/ 下创建 learn_lambda.cpp\n>\n> ```cpp\n> #include <iostream>\n> #include <algorithm>\n> \n> int main(){\n> \t\n> \t// [] 是捕获参数列表，() 中放置参数，{} 中是函数体， -> 后面是返回类型\n> \tauto add = [](int a, int b) -> int {return a + b;};\n> \tint sum = add(200, 50);\n> \t// 这里的 [sum] 可以替换为 [&]，后者可以捕获其之前的所有参数\n> \tauto print_sum = [sum]() -> void {std::cout << sum << std::endl;};\n> \tprint_sum();\n> \t\n> \treturn 0;\n> }\n> ```\n>\n> ### 在 CMakeLists 文件中添加\n>\n> ```txt\n> # 添加可执行文件\n> add_executable(learn_lambda src/learn_lambda.cpp)\n> # 拷贝节点到install\n> install(TARGETS demo_one_node person_node learn_shared_ptr learn_lambda\n> DESTINATION lib/${PROJECT_NAME}\n> )\n> ```\n>\n> ### 编译并执行\n>\n> ```shell\n> colcon build\n> source install/setup.bash\n> ros2 run demo_one_pkg learn_lambda\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_10.png)\n\n### 函数包装器\n\n> ### cpp 中函数可大致分为三类，一类是自由函数，一类是成员函数，另外一类就是 Lambda函数。三种函数的调用方式不同，自由函数直接函数名加参数调用，成员函数需要对象来调用。函数包装器就是用于统一这三种函数的调用方式的。\n>\n> ### 代码示例，在 demo_one_pkg/src/ 下创建 learn_functional.cpp\n>\n> ```cpp\n> #include <iostream>\n> #include <functional>\t// 函数包装器头文件\n> \n> // 自由函数\n> void save_with_free_fun(const std::string & file_name){\n> \tstd::cout << \"自由函数: \" << file_name << std::endl;\n> }\n> \n> class FileSave{\n> \n> private:\n> \t\n> \n> public:\n> \tFileSave() = default;\n> \t~FileSave() = default;\n> \t\n> \t// 成员函数\n> \tvoid save_with_member_fun(const std::string & file_name){\n> \t\tstd::cout << \"成员函数: \" << file_name << std::endl;\n> \t};\n> };\n> \n> \n> int main(){\n> \n> \tFileSave file_save;\n> \t\n> \t// Lambda 函数\n> \tauto save_with_lambda_fun = [](const std::string & file_name) -> void {std::cout << \"Lambda 函数: \" << file_name << std::endl;};\n> \t\n> \t// 未统一调用\n> \tsave_with_free_fun(\"file.txt\");\n> \tfile_save.save_with_member_fun(\"file.txt\");\n> \tsave_with_lambda_fun(\"file.txt\");\n> \t\n> \t// 使用函数包装器统一调用\n> \tstd::function<void(const std::string&)>save1 = save_with_free_fun;\n> \t// 成员函数放入包装器较为复杂，涉及三个参数\n> \tstd::function<void(const std::string&)>save2 = std::bind(&FileSave::save_with_member_fun, &file_save, std::placeholders::_1);\n> \tstd::function<void(const std::string&)>save3 = save_with_lambda_fun;\n> \t\n> \t// 调用\n> \tsave1(\"file.txt\");\n> \tsave2(\"file.txt\");\n> \tsave3(\"file.txt\");\n> \t\n> \treturn 0;\n> }\n> ```\n>\n> ### 在 CMakeLists 文件中添加\n>\n> ```txt\n> # 添加可执行文件\n> add_executable(learn_functional src/learn_functional.cpp)\n> # 拷贝节点到install\n> install(TARGETS demo_one_node person_node learn_shared_ptr learn_lambda learn_functional\n> DESTINATION lib/${PROJECT_NAME}\n> )\n> ```\n>\n> ### 编译并执行\n>\n> ```shell\n> colcon build\n> source install/setup.bash\n> ros2 run demo_one_pkg learn_functional\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_11.png)","source":"_posts/07.md","raw":"---\ntitle: 第六弹\ndata: 2025-02-17 20:40:00\nupdated: 2025-02-17 20:40:00\ntype: ROS2\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p7.png\ntags:\n  - ROS2\n  - Learning\n---\n\n# ROS2 中，cpp新特性的应用\n\n### auto\n\n> ```cpp\n> // auto 自动推导类型\n> auto a = 1; // int a = 1\n> ```\n\n### 智能指针\n\n> ### 智能指针分为三种\n>\n> - #### shared_ptr 共享的智能指针\n>\n> - #### weak_ptr 弱引用的智能指针\n>\n> - #### unique_ptr 独占的智能指针\n>\n> ### std::shared_ptr<T> 是一个类模板，它的对象的行为类似于指针，它可以记录共享它所管理的内存对象的对象个数。多个共享指针可以共享同一个对象，当最后一个共享指针被销毁时，会自动释放其所指向的对象。一个共享指针通常使用 make_shared<> 来创建，也可以通过拷贝或者赋值其他共享指针的方式创建。\n>\n> ### 代码示例，在 demo_one_pkg/src/ 下创建 learn_shared_ptr.cpp\n>\n> ```cpp\n> #include <iostream>\n> #include <memory>\n> \n> int main(){\n> \t\n> \t// 创建共享智能指针 <数据类型/类>(参数) 返回值，对应类的共享指针\n> \tauto p1 = std::make_shared<std::string>(\"This is a string.\");\n> \tstd::cout << \"p1的引用计数: \" << p1.use_count() << \", 指向内存地址: \" << p1.get() << std::endl;\n> \t\n> \tauto p2 = p1;\t\n> \tstd::cout << \"p1的引用计数: \" << p1.use_count() << \", 指向内存地址: \" << p1.get() << std::endl;\n> \tstd::cout << \"p2的引用计数: \" << p2.use_count() << \", 指向内存地址: \" << p2.get() << std::endl;\n> \t\n> \t// 释放引用\n> \tp1.reset();\n> \tstd::cout << \"p1的引用计数: \" << p1.use_count() << \", 指向内存地址: \" << p1.get() << std::endl;\n> \tstd::cout << \"p2的引用计数: \" << p2.use_count() << \", 指向内存地址: \" << p2.get() << std::endl;\n> \t\n> \t// p2->c_str() 调用成员函数\n> \tstd::cout << \"p2的指向内存地址数据: \" << p2->c_str() << std::endl;\n> \t\n> \treturn 0;\n> }\n> ```\n>\n> ### 在 CMakeLists 文件中添加\n>\n> ```txt\n> # 添加可执行文件\n> add_executable(learn_shared_ptr src/learn_shared_ptr.cpp)\n> # 拷贝节点到install\n> install(TARGETS demo_one_node person_node learn_shared_ptr\n> DESTINATION lib/${PROJECT_NAME}\n> )\n> ```\n>\n> ### 编译并执行\n>\n> ```shell\n> colcon build\n> source install/setup.bash\n> ros2 run demo_one_pkg learn_shared_ptr\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_9.png)\n>\n\n### Lambda 表达式\n\n> ### 可以利用它来编写内嵌的匿名函数，用以替换独立函数或函数对象。\n>\n> ### 代码示例，在 demo_one_pkg/src/ 下创建 learn_lambda.cpp\n>\n> ```cpp\n> #include <iostream>\n> #include <algorithm>\n> \n> int main(){\n> \t\n> \t// [] 是捕获参数列表，() 中放置参数，{} 中是函数体， -> 后面是返回类型\n> \tauto add = [](int a, int b) -> int {return a + b;};\n> \tint sum = add(200, 50);\n> \t// 这里的 [sum] 可以替换为 [&]，后者可以捕获其之前的所有参数\n> \tauto print_sum = [sum]() -> void {std::cout << sum << std::endl;};\n> \tprint_sum();\n> \t\n> \treturn 0;\n> }\n> ```\n>\n> ### 在 CMakeLists 文件中添加\n>\n> ```txt\n> # 添加可执行文件\n> add_executable(learn_lambda src/learn_lambda.cpp)\n> # 拷贝节点到install\n> install(TARGETS demo_one_node person_node learn_shared_ptr learn_lambda\n> DESTINATION lib/${PROJECT_NAME}\n> )\n> ```\n>\n> ### 编译并执行\n>\n> ```shell\n> colcon build\n> source install/setup.bash\n> ros2 run demo_one_pkg learn_lambda\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_10.png)\n\n### 函数包装器\n\n> ### cpp 中函数可大致分为三类，一类是自由函数，一类是成员函数，另外一类就是 Lambda函数。三种函数的调用方式不同，自由函数直接函数名加参数调用，成员函数需要对象来调用。函数包装器就是用于统一这三种函数的调用方式的。\n>\n> ### 代码示例，在 demo_one_pkg/src/ 下创建 learn_functional.cpp\n>\n> ```cpp\n> #include <iostream>\n> #include <functional>\t// 函数包装器头文件\n> \n> // 自由函数\n> void save_with_free_fun(const std::string & file_name){\n> \tstd::cout << \"自由函数: \" << file_name << std::endl;\n> }\n> \n> class FileSave{\n> \n> private:\n> \t\n> \n> public:\n> \tFileSave() = default;\n> \t~FileSave() = default;\n> \t\n> \t// 成员函数\n> \tvoid save_with_member_fun(const std::string & file_name){\n> \t\tstd::cout << \"成员函数: \" << file_name << std::endl;\n> \t};\n> };\n> \n> \n> int main(){\n> \n> \tFileSave file_save;\n> \t\n> \t// Lambda 函数\n> \tauto save_with_lambda_fun = [](const std::string & file_name) -> void {std::cout << \"Lambda 函数: \" << file_name << std::endl;};\n> \t\n> \t// 未统一调用\n> \tsave_with_free_fun(\"file.txt\");\n> \tfile_save.save_with_member_fun(\"file.txt\");\n> \tsave_with_lambda_fun(\"file.txt\");\n> \t\n> \t// 使用函数包装器统一调用\n> \tstd::function<void(const std::string&)>save1 = save_with_free_fun;\n> \t// 成员函数放入包装器较为复杂，涉及三个参数\n> \tstd::function<void(const std::string&)>save2 = std::bind(&FileSave::save_with_member_fun, &file_save, std::placeholders::_1);\n> \tstd::function<void(const std::string&)>save3 = save_with_lambda_fun;\n> \t\n> \t// 调用\n> \tsave1(\"file.txt\");\n> \tsave2(\"file.txt\");\n> \tsave3(\"file.txt\");\n> \t\n> \treturn 0;\n> }\n> ```\n>\n> ### 在 CMakeLists 文件中添加\n>\n> ```txt\n> # 添加可执行文件\n> add_executable(learn_functional src/learn_functional.cpp)\n> # 拷贝节点到install\n> install(TARGETS demo_one_node person_node learn_shared_ptr learn_lambda learn_functional\n> DESTINATION lib/${PROJECT_NAME}\n> )\n> ```\n>\n> ### 编译并执行\n>\n> ```shell\n> colcon build\n> source install/setup.bash\n> ros2 run demo_one_pkg learn_functional\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_11.png)","slug":"07","published":1,"date":"2025-02-17T12:48:41.238Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cqw000qiku41p7ihznr","content":"<h1 id=\"ROS2-中，cpp新特性的应用\"><a href=\"#ROS2-中，cpp新特性的应用\" class=\"headerlink\" title=\"ROS2 中，cpp新特性的应用\"></a>ROS2 中，cpp新特性的应用</h1><h3 id=\"auto\"><a href=\"#auto\" class=\"headerlink\" title=\"auto\"></a>auto</h3><blockquote>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// auto 自动推导类型</span></span><br><span class=\"line\"><span class=\"keyword\">auto</span> a = <span class=\"number\">1</span>; <span class=\"comment\">// int a = 1</span></span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"智能指针\"><a href=\"#智能指针\" class=\"headerlink\" title=\"智能指针\"></a>智能指针</h3><blockquote>\n<h3 id=\"智能指针分为三种\"><a href=\"#智能指针分为三种\" class=\"headerlink\" title=\"智能指针分为三种\"></a>智能指针分为三种</h3><ul>\n<li><h4 id=\"shared-ptr-共享的智能指针\"><a href=\"#shared-ptr-共享的智能指针\" class=\"headerlink\" title=\"shared_ptr 共享的智能指针\"></a>shared_ptr 共享的智能指针</h4></li>\n<li><h4 id=\"weak-ptr-弱引用的智能指针\"><a href=\"#weak-ptr-弱引用的智能指针\" class=\"headerlink\" title=\"weak_ptr 弱引用的智能指针\"></a>weak_ptr 弱引用的智能指针</h4></li>\n<li><h4 id=\"unique-ptr-独占的智能指针\"><a href=\"#unique-ptr-独占的智能指针\" class=\"headerlink\" title=\"unique_ptr 独占的智能指针\"></a>unique_ptr 独占的智能指针</h4></li>\n</ul>\n<h3 id=\"std-shared-ptr-是一个类模板，它的对象的行为类似于指针，它可以记录共享它所管理的内存对象的对象个数。多个共享指针可以共享同一个对象，当最后一个共享指针被销毁时，会自动释放其所指向的对象。一个共享指针通常使用-make-shared-来创建，也可以通过拷贝或者赋值其他共享指针的方式创建。\"><a href=\"#std-shared-ptr-是一个类模板，它的对象的行为类似于指针，它可以记录共享它所管理的内存对象的对象个数。多个共享指针可以共享同一个对象，当最后一个共享指针被销毁时，会自动释放其所指向的对象。一个共享指针通常使用-make-shared-来创建，也可以通过拷贝或者赋值其他共享指针的方式创建。\" class=\"headerlink\" title=\"std::shared_ptr 是一个类模板，它的对象的行为类似于指针，它可以记录共享它所管理的内存对象的对象个数。多个共享指针可以共享同一个对象，当最后一个共享指针被销毁时，会自动释放其所指向的对象。一个共享指针通常使用 make_shared&lt;&gt; 来创建，也可以通过拷贝或者赋值其他共享指针的方式创建。\"></a>std::shared_ptr<T> 是一个类模板，它的对象的行为类似于指针，它可以记录共享它所管理的内存对象的对象个数。多个共享指针可以共享同一个对象，当最后一个共享指针被销毁时，会自动释放其所指向的对象。一个共享指针通常使用 make_shared&lt;&gt; 来创建，也可以通过拷贝或者赋值其他共享指针的方式创建。</h3><h3 id=\"代码示例，在-demo-one-pkg-src-下创建-learn-shared-ptr-cpp\"><a href=\"#代码示例，在-demo-one-pkg-src-下创建-learn-shared-ptr-cpp\" class=\"headerlink\" title=\"代码示例，在 demo_one_pkg&#x2F;src&#x2F; 下创建 learn_shared_ptr.cpp\"></a>代码示例，在 demo_one_pkg&#x2F;src&#x2F; 下创建 learn_shared_ptr.cpp</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;memory&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// 创建共享智能指针 &lt;数据类型/类&gt;(参数) 返回值，对应类的共享指针</span></span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> p1 = std::<span class=\"built_in\">make_shared</span>&lt;std::string&gt;(<span class=\"string\">&quot;This is a string.&quot;</span>);</span><br><span class=\"line\">\tstd::cout &lt;&lt; <span class=\"string\">&quot;p1的引用计数: &quot;</span> &lt;&lt; p<span class=\"number\">1.</span><span class=\"built_in\">use_count</span>() &lt;&lt; <span class=\"string\">&quot;, 指向内存地址: &quot;</span> &lt;&lt; p<span class=\"number\">1.</span><span class=\"built_in\">get</span>() &lt;&lt; std::endl;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> p2 = p1;\t</span><br><span class=\"line\">\tstd::cout &lt;&lt; <span class=\"string\">&quot;p1的引用计数: &quot;</span> &lt;&lt; p<span class=\"number\">1.</span><span class=\"built_in\">use_count</span>() &lt;&lt; <span class=\"string\">&quot;, 指向内存地址: &quot;</span> &lt;&lt; p<span class=\"number\">1.</span><span class=\"built_in\">get</span>() &lt;&lt; std::endl;</span><br><span class=\"line\">\tstd::cout &lt;&lt; <span class=\"string\">&quot;p2的引用计数: &quot;</span> &lt;&lt; p<span class=\"number\">2.</span><span class=\"built_in\">use_count</span>() &lt;&lt; <span class=\"string\">&quot;, 指向内存地址: &quot;</span> &lt;&lt; p<span class=\"number\">2.</span><span class=\"built_in\">get</span>() &lt;&lt; std::endl;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// 释放引用</span></span><br><span class=\"line\">\tp<span class=\"number\">1.</span><span class=\"built_in\">reset</span>();</span><br><span class=\"line\">\tstd::cout &lt;&lt; <span class=\"string\">&quot;p1的引用计数: &quot;</span> &lt;&lt; p<span class=\"number\">1.</span><span class=\"built_in\">use_count</span>() &lt;&lt; <span class=\"string\">&quot;, 指向内存地址: &quot;</span> &lt;&lt; p<span class=\"number\">1.</span><span class=\"built_in\">get</span>() &lt;&lt; std::endl;</span><br><span class=\"line\">\tstd::cout &lt;&lt; <span class=\"string\">&quot;p2的引用计数: &quot;</span> &lt;&lt; p<span class=\"number\">2.</span><span class=\"built_in\">use_count</span>() &lt;&lt; <span class=\"string\">&quot;, 指向内存地址: &quot;</span> &lt;&lt; p<span class=\"number\">2.</span><span class=\"built_in\">get</span>() &lt;&lt; std::endl;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// p2-&gt;c_str() 调用成员函数</span></span><br><span class=\"line\">\tstd::cout &lt;&lt; <span class=\"string\">&quot;p2的指向内存地址数据: &quot;</span> &lt;&lt; p2-&gt;<span class=\"built_in\">c_str</span>() &lt;&lt; std::endl;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-CMakeLists-文件中添加\"><a href=\"#在-CMakeLists-文件中添加\" class=\"headerlink\" title=\"在 CMakeLists 文件中添加\"></a>在 CMakeLists 文件中添加</h3><figure class=\"highlight txt\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 添加可执行文件</span><br><span class=\"line\">add_executable(learn_shared_ptr src/learn_shared_ptr.cpp)</span><br><span class=\"line\"># 拷贝节点到install</span><br><span class=\"line\">install(TARGETS demo_one_node person_node learn_shared_ptr</span><br><span class=\"line\">DESTINATION lib/$&#123;PROJECT_NAME&#125;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编译并执行\"><a href=\"#编译并执行\" class=\"headerlink\" title=\"编译并执行\"></a>编译并执行</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 run demo_one_pkg learn_shared_ptr</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_9.png\"></p>\n</blockquote>\n<h3 id=\"Lambda-表达式\"><a href=\"#Lambda-表达式\" class=\"headerlink\" title=\"Lambda 表达式\"></a>Lambda 表达式</h3><blockquote>\n<h3 id=\"可以利用它来编写内嵌的匿名函数，用以替换独立函数或函数对象。\"><a href=\"#可以利用它来编写内嵌的匿名函数，用以替换独立函数或函数对象。\" class=\"headerlink\" title=\"可以利用它来编写内嵌的匿名函数，用以替换独立函数或函数对象。\"></a>可以利用它来编写内嵌的匿名函数，用以替换独立函数或函数对象。</h3><h3 id=\"代码示例，在-demo-one-pkg-src-下创建-learn-lambda-cpp\"><a href=\"#代码示例，在-demo-one-pkg-src-下创建-learn-lambda-cpp\" class=\"headerlink\" title=\"代码示例，在 demo_one_pkg&#x2F;src&#x2F; 下创建 learn_lambda.cpp\"></a>代码示例，在 demo_one_pkg&#x2F;src&#x2F; 下创建 learn_lambda.cpp</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;algorithm&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// [] 是捕获参数列表，() 中放置参数，&#123;&#125; 中是函数体， -&gt; 后面是返回类型</span></span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> add = [](<span class=\"type\">int</span> a, <span class=\"type\">int</span> b) -&gt; <span class=\"type\">int</span> &#123;<span class=\"keyword\">return</span> a + b;&#125;;</span><br><span class=\"line\">\t<span class=\"type\">int</span> sum = <span class=\"built_in\">add</span>(<span class=\"number\">200</span>, <span class=\"number\">50</span>);</span><br><span class=\"line\">\t<span class=\"comment\">// 这里的 [sum] 可以替换为 [&amp;]，后者可以捕获其之前的所有参数</span></span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> print_sum = [sum]() -&gt; <span class=\"type\">void</span> &#123;std::cout &lt;&lt; sum &lt;&lt; std::endl;&#125;;</span><br><span class=\"line\">\t<span class=\"built_in\">print_sum</span>();</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-CMakeLists-文件中添加-1\"><a href=\"#在-CMakeLists-文件中添加-1\" class=\"headerlink\" title=\"在 CMakeLists 文件中添加\"></a>在 CMakeLists 文件中添加</h3><figure class=\"highlight txt\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 添加可执行文件</span><br><span class=\"line\">add_executable(learn_lambda src/learn_lambda.cpp)</span><br><span class=\"line\"># 拷贝节点到install</span><br><span class=\"line\">install(TARGETS demo_one_node person_node learn_shared_ptr learn_lambda</span><br><span class=\"line\">DESTINATION lib/$&#123;PROJECT_NAME&#125;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编译并执行-1\"><a href=\"#编译并执行-1\" class=\"headerlink\" title=\"编译并执行\"></a>编译并执行</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 run demo_one_pkg learn_lambda</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_10.png\"></p>\n</blockquote>\n<h3 id=\"函数包装器\"><a href=\"#函数包装器\" class=\"headerlink\" title=\"函数包装器\"></a>函数包装器</h3><blockquote>\n<h3 id=\"cpp-中函数可大致分为三类，一类是自由函数，一类是成员函数，另外一类就是-Lambda函数。三种函数的调用方式不同，自由函数直接函数名加参数调用，成员函数需要对象来调用。函数包装器就是用于统一这三种函数的调用方式的。\"><a href=\"#cpp-中函数可大致分为三类，一类是自由函数，一类是成员函数，另外一类就是-Lambda函数。三种函数的调用方式不同，自由函数直接函数名加参数调用，成员函数需要对象来调用。函数包装器就是用于统一这三种函数的调用方式的。\" class=\"headerlink\" title=\"cpp 中函数可大致分为三类，一类是自由函数，一类是成员函数，另外一类就是 Lambda函数。三种函数的调用方式不同，自由函数直接函数名加参数调用，成员函数需要对象来调用。函数包装器就是用于统一这三种函数的调用方式的。\"></a>cpp 中函数可大致分为三类，一类是自由函数，一类是成员函数，另外一类就是 Lambda函数。三种函数的调用方式不同，自由函数直接函数名加参数调用，成员函数需要对象来调用。函数包装器就是用于统一这三种函数的调用方式的。</h3><h3 id=\"代码示例，在-demo-one-pkg-src-下创建-learn-functional-cpp\"><a href=\"#代码示例，在-demo-one-pkg-src-下创建-learn-functional-cpp\" class=\"headerlink\" title=\"代码示例，在 demo_one_pkg&#x2F;src&#x2F; 下创建 learn_functional.cpp\"></a>代码示例，在 demo_one_pkg&#x2F;src&#x2F; 下创建 learn_functional.cpp</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;functional&gt;</span>\t<span class=\"comment\">// 函数包装器头文件</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 自由函数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">save_with_free_fun</span><span class=\"params\">(<span class=\"type\">const</span> std::string &amp; file_name)</span></span>&#123;</span><br><span class=\"line\">\tstd::cout &lt;&lt; <span class=\"string\">&quot;自由函数: &quot;</span> &lt;&lt; file_name &lt;&lt; std::endl;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">FileSave</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span>:</span><br><span class=\"line\">\t</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"built_in\">FileSave</span>() = <span class=\"keyword\">default</span>;</span><br><span class=\"line\">\t~<span class=\"built_in\">FileSave</span>() = <span class=\"keyword\">default</span>;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// 成员函数</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">save_with_member_fun</span><span class=\"params\">(<span class=\"type\">const</span> std::string &amp; file_name)</span></span>&#123;</span><br><span class=\"line\">\t\tstd::cout &lt;&lt; <span class=\"string\">&quot;成员函数: &quot;</span> &lt;&lt; file_name &lt;&lt; std::endl;</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\tFileSave file_save;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// Lambda 函数</span></span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> save_with_lambda_fun = [](<span class=\"type\">const</span> std::string &amp; file_name) -&gt; <span class=\"type\">void</span> &#123;std::cout &lt;&lt; <span class=\"string\">&quot;Lambda 函数: &quot;</span> &lt;&lt; file_name &lt;&lt; std::endl;&#125;;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// 未统一调用</span></span><br><span class=\"line\">\t<span class=\"built_in\">save_with_free_fun</span>(<span class=\"string\">&quot;file.txt&quot;</span>);</span><br><span class=\"line\">\tfile_save.<span class=\"built_in\">save_with_member_fun</span>(<span class=\"string\">&quot;file.txt&quot;</span>);</span><br><span class=\"line\">\t<span class=\"built_in\">save_with_lambda_fun</span>(<span class=\"string\">&quot;file.txt&quot;</span>);</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// 使用函数包装器统一调用</span></span><br><span class=\"line\">\tstd::function&lt;<span class=\"type\">void</span>(<span class=\"type\">const</span> std::string&amp;)&gt;save1 = save_with_free_fun;</span><br><span class=\"line\">\t<span class=\"comment\">// 成员函数放入包装器较为复杂，涉及三个参数</span></span><br><span class=\"line\">\tstd::function&lt;<span class=\"type\">void</span>(<span class=\"type\">const</span> std::string&amp;)&gt;save2 = std::<span class=\"built_in\">bind</span>(&amp;FileSave::save_with_member_fun, &amp;file_save, std::placeholders::_1);</span><br><span class=\"line\">\tstd::function&lt;<span class=\"type\">void</span>(<span class=\"type\">const</span> std::string&amp;)&gt;save3 = save_with_lambda_fun;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// 调用</span></span><br><span class=\"line\">\t<span class=\"built_in\">save1</span>(<span class=\"string\">&quot;file.txt&quot;</span>);</span><br><span class=\"line\">\t<span class=\"built_in\">save2</span>(<span class=\"string\">&quot;file.txt&quot;</span>);</span><br><span class=\"line\">\t<span class=\"built_in\">save3</span>(<span class=\"string\">&quot;file.txt&quot;</span>);</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-CMakeLists-文件中添加-2\"><a href=\"#在-CMakeLists-文件中添加-2\" class=\"headerlink\" title=\"在 CMakeLists 文件中添加\"></a>在 CMakeLists 文件中添加</h3><figure class=\"highlight txt\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 添加可执行文件</span><br><span class=\"line\">add_executable(learn_functional src/learn_functional.cpp)</span><br><span class=\"line\"># 拷贝节点到install</span><br><span class=\"line\">install(TARGETS demo_one_node person_node learn_shared_ptr learn_lambda learn_functional</span><br><span class=\"line\">DESTINATION lib/$&#123;PROJECT_NAME&#125;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编译并执行-2\"><a href=\"#编译并执行-2\" class=\"headerlink\" title=\"编译并执行\"></a>编译并执行</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 run demo_one_pkg learn_functional</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_11.png\"></p>\n</blockquote>\n","cover_type":"img","excerpt":"","more":"<h1 id=\"ROS2-中，cpp新特性的应用\"><a href=\"#ROS2-中，cpp新特性的应用\" class=\"headerlink\" title=\"ROS2 中，cpp新特性的应用\"></a>ROS2 中，cpp新特性的应用</h1><h3 id=\"auto\"><a href=\"#auto\" class=\"headerlink\" title=\"auto\"></a>auto</h3><blockquote>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// auto 自动推导类型</span></span><br><span class=\"line\"><span class=\"keyword\">auto</span> a = <span class=\"number\">1</span>; <span class=\"comment\">// int a = 1</span></span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"智能指针\"><a href=\"#智能指针\" class=\"headerlink\" title=\"智能指针\"></a>智能指针</h3><blockquote>\n<h3 id=\"智能指针分为三种\"><a href=\"#智能指针分为三种\" class=\"headerlink\" title=\"智能指针分为三种\"></a>智能指针分为三种</h3><ul>\n<li><h4 id=\"shared-ptr-共享的智能指针\"><a href=\"#shared-ptr-共享的智能指针\" class=\"headerlink\" title=\"shared_ptr 共享的智能指针\"></a>shared_ptr 共享的智能指针</h4></li>\n<li><h4 id=\"weak-ptr-弱引用的智能指针\"><a href=\"#weak-ptr-弱引用的智能指针\" class=\"headerlink\" title=\"weak_ptr 弱引用的智能指针\"></a>weak_ptr 弱引用的智能指针</h4></li>\n<li><h4 id=\"unique-ptr-独占的智能指针\"><a href=\"#unique-ptr-独占的智能指针\" class=\"headerlink\" title=\"unique_ptr 独占的智能指针\"></a>unique_ptr 独占的智能指针</h4></li>\n</ul>\n<h3 id=\"std-shared-ptr-是一个类模板，它的对象的行为类似于指针，它可以记录共享它所管理的内存对象的对象个数。多个共享指针可以共享同一个对象，当最后一个共享指针被销毁时，会自动释放其所指向的对象。一个共享指针通常使用-make-shared-来创建，也可以通过拷贝或者赋值其他共享指针的方式创建。\"><a href=\"#std-shared-ptr-是一个类模板，它的对象的行为类似于指针，它可以记录共享它所管理的内存对象的对象个数。多个共享指针可以共享同一个对象，当最后一个共享指针被销毁时，会自动释放其所指向的对象。一个共享指针通常使用-make-shared-来创建，也可以通过拷贝或者赋值其他共享指针的方式创建。\" class=\"headerlink\" title=\"std::shared_ptr 是一个类模板，它的对象的行为类似于指针，它可以记录共享它所管理的内存对象的对象个数。多个共享指针可以共享同一个对象，当最后一个共享指针被销毁时，会自动释放其所指向的对象。一个共享指针通常使用 make_shared&lt;&gt; 来创建，也可以通过拷贝或者赋值其他共享指针的方式创建。\"></a>std::shared_ptr<T> 是一个类模板，它的对象的行为类似于指针，它可以记录共享它所管理的内存对象的对象个数。多个共享指针可以共享同一个对象，当最后一个共享指针被销毁时，会自动释放其所指向的对象。一个共享指针通常使用 make_shared&lt;&gt; 来创建，也可以通过拷贝或者赋值其他共享指针的方式创建。</h3><h3 id=\"代码示例，在-demo-one-pkg-src-下创建-learn-shared-ptr-cpp\"><a href=\"#代码示例，在-demo-one-pkg-src-下创建-learn-shared-ptr-cpp\" class=\"headerlink\" title=\"代码示例，在 demo_one_pkg&#x2F;src&#x2F; 下创建 learn_shared_ptr.cpp\"></a>代码示例，在 demo_one_pkg&#x2F;src&#x2F; 下创建 learn_shared_ptr.cpp</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;memory&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// 创建共享智能指针 &lt;数据类型/类&gt;(参数) 返回值，对应类的共享指针</span></span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> p1 = std::<span class=\"built_in\">make_shared</span>&lt;std::string&gt;(<span class=\"string\">&quot;This is a string.&quot;</span>);</span><br><span class=\"line\">\tstd::cout &lt;&lt; <span class=\"string\">&quot;p1的引用计数: &quot;</span> &lt;&lt; p<span class=\"number\">1.</span><span class=\"built_in\">use_count</span>() &lt;&lt; <span class=\"string\">&quot;, 指向内存地址: &quot;</span> &lt;&lt; p<span class=\"number\">1.</span><span class=\"built_in\">get</span>() &lt;&lt; std::endl;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> p2 = p1;\t</span><br><span class=\"line\">\tstd::cout &lt;&lt; <span class=\"string\">&quot;p1的引用计数: &quot;</span> &lt;&lt; p<span class=\"number\">1.</span><span class=\"built_in\">use_count</span>() &lt;&lt; <span class=\"string\">&quot;, 指向内存地址: &quot;</span> &lt;&lt; p<span class=\"number\">1.</span><span class=\"built_in\">get</span>() &lt;&lt; std::endl;</span><br><span class=\"line\">\tstd::cout &lt;&lt; <span class=\"string\">&quot;p2的引用计数: &quot;</span> &lt;&lt; p<span class=\"number\">2.</span><span class=\"built_in\">use_count</span>() &lt;&lt; <span class=\"string\">&quot;, 指向内存地址: &quot;</span> &lt;&lt; p<span class=\"number\">2.</span><span class=\"built_in\">get</span>() &lt;&lt; std::endl;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// 释放引用</span></span><br><span class=\"line\">\tp<span class=\"number\">1.</span><span class=\"built_in\">reset</span>();</span><br><span class=\"line\">\tstd::cout &lt;&lt; <span class=\"string\">&quot;p1的引用计数: &quot;</span> &lt;&lt; p<span class=\"number\">1.</span><span class=\"built_in\">use_count</span>() &lt;&lt; <span class=\"string\">&quot;, 指向内存地址: &quot;</span> &lt;&lt; p<span class=\"number\">1.</span><span class=\"built_in\">get</span>() &lt;&lt; std::endl;</span><br><span class=\"line\">\tstd::cout &lt;&lt; <span class=\"string\">&quot;p2的引用计数: &quot;</span> &lt;&lt; p<span class=\"number\">2.</span><span class=\"built_in\">use_count</span>() &lt;&lt; <span class=\"string\">&quot;, 指向内存地址: &quot;</span> &lt;&lt; p<span class=\"number\">2.</span><span class=\"built_in\">get</span>() &lt;&lt; std::endl;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// p2-&gt;c_str() 调用成员函数</span></span><br><span class=\"line\">\tstd::cout &lt;&lt; <span class=\"string\">&quot;p2的指向内存地址数据: &quot;</span> &lt;&lt; p2-&gt;<span class=\"built_in\">c_str</span>() &lt;&lt; std::endl;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-CMakeLists-文件中添加\"><a href=\"#在-CMakeLists-文件中添加\" class=\"headerlink\" title=\"在 CMakeLists 文件中添加\"></a>在 CMakeLists 文件中添加</h3><figure class=\"highlight txt\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 添加可执行文件</span><br><span class=\"line\">add_executable(learn_shared_ptr src/learn_shared_ptr.cpp)</span><br><span class=\"line\"># 拷贝节点到install</span><br><span class=\"line\">install(TARGETS demo_one_node person_node learn_shared_ptr</span><br><span class=\"line\">DESTINATION lib/$&#123;PROJECT_NAME&#125;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编译并执行\"><a href=\"#编译并执行\" class=\"headerlink\" title=\"编译并执行\"></a>编译并执行</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 run demo_one_pkg learn_shared_ptr</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_9.png\"></p>\n</blockquote>\n<h3 id=\"Lambda-表达式\"><a href=\"#Lambda-表达式\" class=\"headerlink\" title=\"Lambda 表达式\"></a>Lambda 表达式</h3><blockquote>\n<h3 id=\"可以利用它来编写内嵌的匿名函数，用以替换独立函数或函数对象。\"><a href=\"#可以利用它来编写内嵌的匿名函数，用以替换独立函数或函数对象。\" class=\"headerlink\" title=\"可以利用它来编写内嵌的匿名函数，用以替换独立函数或函数对象。\"></a>可以利用它来编写内嵌的匿名函数，用以替换独立函数或函数对象。</h3><h3 id=\"代码示例，在-demo-one-pkg-src-下创建-learn-lambda-cpp\"><a href=\"#代码示例，在-demo-one-pkg-src-下创建-learn-lambda-cpp\" class=\"headerlink\" title=\"代码示例，在 demo_one_pkg&#x2F;src&#x2F; 下创建 learn_lambda.cpp\"></a>代码示例，在 demo_one_pkg&#x2F;src&#x2F; 下创建 learn_lambda.cpp</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;algorithm&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// [] 是捕获参数列表，() 中放置参数，&#123;&#125; 中是函数体， -&gt; 后面是返回类型</span></span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> add = [](<span class=\"type\">int</span> a, <span class=\"type\">int</span> b) -&gt; <span class=\"type\">int</span> &#123;<span class=\"keyword\">return</span> a + b;&#125;;</span><br><span class=\"line\">\t<span class=\"type\">int</span> sum = <span class=\"built_in\">add</span>(<span class=\"number\">200</span>, <span class=\"number\">50</span>);</span><br><span class=\"line\">\t<span class=\"comment\">// 这里的 [sum] 可以替换为 [&amp;]，后者可以捕获其之前的所有参数</span></span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> print_sum = [sum]() -&gt; <span class=\"type\">void</span> &#123;std::cout &lt;&lt; sum &lt;&lt; std::endl;&#125;;</span><br><span class=\"line\">\t<span class=\"built_in\">print_sum</span>();</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-CMakeLists-文件中添加-1\"><a href=\"#在-CMakeLists-文件中添加-1\" class=\"headerlink\" title=\"在 CMakeLists 文件中添加\"></a>在 CMakeLists 文件中添加</h3><figure class=\"highlight txt\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 添加可执行文件</span><br><span class=\"line\">add_executable(learn_lambda src/learn_lambda.cpp)</span><br><span class=\"line\"># 拷贝节点到install</span><br><span class=\"line\">install(TARGETS demo_one_node person_node learn_shared_ptr learn_lambda</span><br><span class=\"line\">DESTINATION lib/$&#123;PROJECT_NAME&#125;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编译并执行-1\"><a href=\"#编译并执行-1\" class=\"headerlink\" title=\"编译并执行\"></a>编译并执行</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 run demo_one_pkg learn_lambda</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_10.png\"></p>\n</blockquote>\n<h3 id=\"函数包装器\"><a href=\"#函数包装器\" class=\"headerlink\" title=\"函数包装器\"></a>函数包装器</h3><blockquote>\n<h3 id=\"cpp-中函数可大致分为三类，一类是自由函数，一类是成员函数，另外一类就是-Lambda函数。三种函数的调用方式不同，自由函数直接函数名加参数调用，成员函数需要对象来调用。函数包装器就是用于统一这三种函数的调用方式的。\"><a href=\"#cpp-中函数可大致分为三类，一类是自由函数，一类是成员函数，另外一类就是-Lambda函数。三种函数的调用方式不同，自由函数直接函数名加参数调用，成员函数需要对象来调用。函数包装器就是用于统一这三种函数的调用方式的。\" class=\"headerlink\" title=\"cpp 中函数可大致分为三类，一类是自由函数，一类是成员函数，另外一类就是 Lambda函数。三种函数的调用方式不同，自由函数直接函数名加参数调用，成员函数需要对象来调用。函数包装器就是用于统一这三种函数的调用方式的。\"></a>cpp 中函数可大致分为三类，一类是自由函数，一类是成员函数，另外一类就是 Lambda函数。三种函数的调用方式不同，自由函数直接函数名加参数调用，成员函数需要对象来调用。函数包装器就是用于统一这三种函数的调用方式的。</h3><h3 id=\"代码示例，在-demo-one-pkg-src-下创建-learn-functional-cpp\"><a href=\"#代码示例，在-demo-one-pkg-src-下创建-learn-functional-cpp\" class=\"headerlink\" title=\"代码示例，在 demo_one_pkg&#x2F;src&#x2F; 下创建 learn_functional.cpp\"></a>代码示例，在 demo_one_pkg&#x2F;src&#x2F; 下创建 learn_functional.cpp</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;functional&gt;</span>\t<span class=\"comment\">// 函数包装器头文件</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 自由函数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">save_with_free_fun</span><span class=\"params\">(<span class=\"type\">const</span> std::string &amp; file_name)</span></span>&#123;</span><br><span class=\"line\">\tstd::cout &lt;&lt; <span class=\"string\">&quot;自由函数: &quot;</span> &lt;&lt; file_name &lt;&lt; std::endl;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">FileSave</span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span>:</span><br><span class=\"line\">\t</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"built_in\">FileSave</span>() = <span class=\"keyword\">default</span>;</span><br><span class=\"line\">\t~<span class=\"built_in\">FileSave</span>() = <span class=\"keyword\">default</span>;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// 成员函数</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">save_with_member_fun</span><span class=\"params\">(<span class=\"type\">const</span> std::string &amp; file_name)</span></span>&#123;</span><br><span class=\"line\">\t\tstd::cout &lt;&lt; <span class=\"string\">&quot;成员函数: &quot;</span> &lt;&lt; file_name &lt;&lt; std::endl;</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\tFileSave file_save;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// Lambda 函数</span></span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> save_with_lambda_fun = [](<span class=\"type\">const</span> std::string &amp; file_name) -&gt; <span class=\"type\">void</span> &#123;std::cout &lt;&lt; <span class=\"string\">&quot;Lambda 函数: &quot;</span> &lt;&lt; file_name &lt;&lt; std::endl;&#125;;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// 未统一调用</span></span><br><span class=\"line\">\t<span class=\"built_in\">save_with_free_fun</span>(<span class=\"string\">&quot;file.txt&quot;</span>);</span><br><span class=\"line\">\tfile_save.<span class=\"built_in\">save_with_member_fun</span>(<span class=\"string\">&quot;file.txt&quot;</span>);</span><br><span class=\"line\">\t<span class=\"built_in\">save_with_lambda_fun</span>(<span class=\"string\">&quot;file.txt&quot;</span>);</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// 使用函数包装器统一调用</span></span><br><span class=\"line\">\tstd::function&lt;<span class=\"type\">void</span>(<span class=\"type\">const</span> std::string&amp;)&gt;save1 = save_with_free_fun;</span><br><span class=\"line\">\t<span class=\"comment\">// 成员函数放入包装器较为复杂，涉及三个参数</span></span><br><span class=\"line\">\tstd::function&lt;<span class=\"type\">void</span>(<span class=\"type\">const</span> std::string&amp;)&gt;save2 = std::<span class=\"built_in\">bind</span>(&amp;FileSave::save_with_member_fun, &amp;file_save, std::placeholders::_1);</span><br><span class=\"line\">\tstd::function&lt;<span class=\"type\">void</span>(<span class=\"type\">const</span> std::string&amp;)&gt;save3 = save_with_lambda_fun;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"comment\">// 调用</span></span><br><span class=\"line\">\t<span class=\"built_in\">save1</span>(<span class=\"string\">&quot;file.txt&quot;</span>);</span><br><span class=\"line\">\t<span class=\"built_in\">save2</span>(<span class=\"string\">&quot;file.txt&quot;</span>);</span><br><span class=\"line\">\t<span class=\"built_in\">save3</span>(<span class=\"string\">&quot;file.txt&quot;</span>);</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-CMakeLists-文件中添加-2\"><a href=\"#在-CMakeLists-文件中添加-2\" class=\"headerlink\" title=\"在 CMakeLists 文件中添加\"></a>在 CMakeLists 文件中添加</h3><figure class=\"highlight txt\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 添加可执行文件</span><br><span class=\"line\">add_executable(learn_functional src/learn_functional.cpp)</span><br><span class=\"line\"># 拷贝节点到install</span><br><span class=\"line\">install(TARGETS demo_one_node person_node learn_shared_ptr learn_lambda learn_functional</span><br><span class=\"line\">DESTINATION lib/$&#123;PROJECT_NAME&#125;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编译并执行-2\"><a href=\"#编译并执行-2\" class=\"headerlink\" title=\"编译并执行\"></a>编译并执行</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 run demo_one_pkg learn_functional</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_11.png\"></p>\n</blockquote>\n"},{"title":"第十弹","data":"2025-02-23T06:51:00.000Z","updated":"2025-02-23T06:51:00.000Z","type":"ROS2","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p12.png","_content":"\n# 话题通信实践案例\n\n## 自定义通信接口\n\n> ### 在 chapter_3 下创建一个新的文件层，接着在 src/ 下创建功能包，并进入功能包，在其目录下创建文件 msg，再进入 msg/，创建文件SystemStatus.msg\n>\n> ```shell\n> cd ~/learn_ros2/chapter_3/\n> mkdir -p topic_practice_ws/src\n> cd src/\n> ros2 pkg create status_interfaces --dependencies builtin_interfaces rosidl_default_generators --license Apache-2.0\n> cd status_interfaces/\n> mkdir msg\n> cd msg\n> touch SystemStatus.msg\n> ```\n>\n> ### 编辑 .msg 文件内容\n>\n> ```txt\n> builtin_interfaces/Time stamp #记录时间戳\n> string host_name # 主机名字\n> float32 cpu_percent # cpu使用率\n> float32 memory_percent # 内存使用率\n> float32 memory_available # 内存总大小\n> float64 net_sent # 网络发送数据总量\n> float64 net_receive # 网络数据接收总量\n> ```\n>\n> ### 在 CMakeLists 和 package.xml 文件中添加相应语句\n>\n> ```cmake\n> # cmake函数，来自与rosidl_default_generators，将msg等消息接口定义文件转换成库或者头文件类\n> # 它会根据提供的消息接口去生成对应的cpp文件\n> rosidl_generate_interfaces(${PROJECT_NAME}\n> \"msg/SystemStatus.msg\"\n> DEPENDENCIES builtin_interfaces\n> )\n> ```\n>\n> ```xml\n> <license>Apache-2.0</license>\n> # 添加这句\n> <member_of_group>rosidl_interface_packages</member_of_group>\n> ```\n>\n> ### 然后编译并修改环境变量\n>\n> ```shell\n> cd ~/learn_ros2/chapter_3/topic_practice_ws/\n> colcon build\n> source install/setup.bash\n> ```\n>\n> ### 编译好之后，在 topic_practice_ws/installl 下，有一个与功能包同名的文件，在该文件下有一个 include 文件，该文件下也有一个与包名同名的文件，里面有个 msg文件，在该文件中，就是 .msg 文件转换得来的 .hpp 和 .h 文件\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_29.png)\n>\n> ### 另外，在 detail 文件中，system_status__struct.hpp 文件里有 .msg文件中定义的属性\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_30.png)\n>\n> ### 查看生成的头文件的定义\n>\n> ```shell\n> ros2 interface show status_interfaces/msg/SystemStatus\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_31.png)\n\n## 系统信息发布与获取\n\n> ### 在 topic_practice_ws/src/ 下创建一个功能包 status_publisher，要依赖 status_interfaces，这里创建python的，因为要使用python的一个库 psutil，可以相对简单的得到系统当前的各种信息\n>\n> ```shell\n> ros2 pkg create status_publisher --build-type ament_python --dependencies rclpy status_interfaces --licence Apache-2.0\n> ```\n>\n> ### 在 status_publisher/src 下创建文件 sys_status_pub.cpp，内容如下\n>\n> ```python\n> import rclpy  # 导入ROS 2的Python客户端库\n> from status_interfaces.msg import SystemStatus  # 导入自定义消息类型 SystemStatus\n> from rclpy.node import Node  # 导入ROS2节点类\n> import psutil  # 用于获取系统的硬件状态，如CPU、内存和网络等\n> import platform  # 用于获取系统的基本信息，如主机名\n> \n> # 定义一个类 SysStatusPub，继承自 Node 类\n> class SysStatusPub(Node):\n>     def __init__(self, node_name):\n>         # 初始化节点，node_name是节点的名字\n>         super().__init__(node_name)  \n>            \n>            # 创建一个发布器，用于发布类型为 SystemStatus 的消息到 'sys_status' 话题，队列大小为 10\n>            self.status_publisher_ = self.create_publisher(SystemStatus, 'sys_status', 10)\n>            \n>            # 创建定时器，每隔1秒调用一次 timer_callback 方法\n>            self.timer_ = self.create_timer(1.0, self.timer_callback)\n> \n>        # 定时器回调函数，每隔1秒被调用一次\n>        def timer_callback(self):\n>            # 获取CPU的使用率，返回值为浮动百分比\n>            cpu_percent = psutil.cpu_percent()\n>         \n>         # 获取内存的使用信息，包括已使用、总量、可用等数据\n>            memory_info = psutil.virtual_memory()\n>            \n>            # 获取网络I/O的发送和接收信息（字节数）\n>            net_io_counters = psutil.net_io_counters()\n>    \n>            # 创建一个SystemStatus消息对象\n>            msg = SystemStatus()\n>            \n>            # 设置时间戳，获取当前时间并转换为消息格式\n>         msg.stamp = self.get_clock().now().to_msg()\n>            \n>            # 获取主机名\n>                 msg.host_name = platform.node()\n>            \n>            # 获取并设置CPU使用率\n>            msg.cpu_percent = cpu_percent\n>            \n>            # 获取并设置内存的使用百分比\n>            msg.memory_percent = memory_info.percent\n>            \n>            # 获取并设置内存的总大小，转换为float类型\n>         msg.memory_total = float(memory_info.total)\n>            \n>            # 获取并设置内存的可用大小，转换为float类型\n>            msg.memory_available = float(memory_info.available)\n>         \n>            # 获取并设置网络发送的字节数，单位转换为MB\n>            msg.net_sent = net_io_counters.bytes_sent / 1024 / 1024\n>            \n>         # 获取并设置网络接收的字节数，单位转换为MB\n>            msg.net_recv = net_io_counters.bytes_recv / 1024 / 1024\n>    \n>            # 记录日志，输出当前的SystemStatus消息\n>            self.get_logger().info(f'发布: {str(msg)}')\n>            \n>            # 发布消息\n>            self.status_publisher_.publish(msg)\n>         \n>    \n>    # main函数，程序入口\n>    def main():\n>             # 初始化rclpy客户端库\n>        rclpy.init()\n>    \n>     # 创建一个SysStatusPub节点对象，节点名称为 'sys_status_pub'\n>        node = SysStatusPub('sys_status_pub')\n>        \n>        # 使节点进入循环，保持节点运行并监听定时器事件\n>     rclpy.spin(node)\n>    \n>        # 关闭节点并关闭rclpy\n>        rclpy.shutdown()\n>    ```\n>    \n>    ### 在 setup.py 中添加相应语句\n>         \n>    ```python\n>    from setuptools import find_packages, setup\n>    \n>    package_name = 'status_publisher'\n>    \n>    setup(\n>        name=package_name,\n>        version='0.0.0',\n>     packages=find_packages(exclude=['test']),\n>        data_files=[\n>            ('share/ament_index/resource_index/packages',\n>                ['resource/' + package_name]),\n>         ('share/' + package_name, ['package.xml']),\n>        ],\n>        install_requires=['setuptools'],\n>        zip_safe=True,\n>        maintainer='yanzu',\n>        maintainer_email='yanzu@todo.todo',\n>        description='TODO: Package description',\n>             license='Apache-2.0',\n>        tests_require=['pytest'],\n>        entry_points={\n>            'console_scripts': [\n>                # 添加这句\n>            \t'sys_status_pub = status_publisher.sys_status_pub:main',\n>            ],\n>     },\n>    )\n>    ```\n>    \n> ### 然后编译运行\n>    \n>    ```shell\n>    colcon build\n>    source install/setup.bash\n>    ros2 run status_publisher sys_status_pub\n>    ```\n> \n>    ![](http://picbed.yanzu.tech/img/learn_ros2/pic_32.png)\n>    \n>    ### 打印接口信息\n>    \n>    ```shell\n>    ros2 topic echo /sys_status\n>    ```\n>    \n>    ![](http://picbed.yanzu.tech/img/learn_ros2/pic_33.png)\n\n## 在功能包中使用QT\n\n> ### 在 topic_practice_ws/src/ 下创建功能包 status_display，且要依赖 status_interfaces\n>\n> ```shell\n> ros2 pkg create status_display --dependencies rclcpp status_interfaces --license Apache-2.0\n> ```\n>\n> ### 在 status_display/src/ 下创建文件 hello_qt.cpp，内容如下\n>\n> ```cpp\n> #include <QApplication>\n> #include <QLabel>\n> #include <QString>\n> \n> \n> int main(int argc, char** argv){\n> \t\n> \tQApplication app(argc, argv);\n> \tQLabel* label = new QLabel();\n> \tQString message = QString::fromStdString(\"hello qt!\");\n> \tlabel->setText(message);\n> \tlabel->show();\n> \tapp.exec();\t// 执行应用,阻塞代码\n> \n> \treturn 0;\n> }\n> ```\n>\n> ### 在 CMakeLists 中添加相应语句\n>\n> ```shell\n> # 查找 qt5 组件\n> find_package(Qt5 REQUIRED COMPONENTS Widgets)\n> \n> # 添加可执行文件\n> add_executable(hello_qt src/hello_qt.cpp)\n> \n> # 添加依赖库\n> target_link_libraries(hello_qt Qt5::Widgets)\n> \n> # 拷贝节点到 install/lib\n> install(TARGETS hello_qt\n> DESTINATION lib/${PROJECT_NAME}\n> )\n> ```\n>\n> ### 然后编译执行\n>\n> ```shell\n> colcon build\n> source install/setup.bash\n> ros2 run status_display hello_qt\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_34.png)\n\n## 订阅数据并使用Qt显示\n\n> ### 在 status_display/src/ 下创建文件 sys_status_display.cpp，内容如下\n>\n> ```cpp\n> #include <QApplication>      // 引入Qt应用程序库，用于创建GUI界面\n> #include <QLabel>            // 引入Qt标签控件，用于显示系统状态信息\n> #include <QString>           // 引入Qt字符串类，用于处理字符串\n> #include <rclcpp/rclcpp.hpp> // 引入ROS2 C++客户端库\n> #include \"status_interfaces/msg/system_status.hpp\" // 引入自定义消息类型 SystemStatus\n> \n> // 定义 SystemStatus 类型简写\n> using SystemStatus = status_interfaces::msg::SystemStatus;\n> \n> // 定义 SysStatusDisplay 类，继承自 ROS2 的 Node 类\n> class SysStatusDisplay : public rclcpp::Node {\n> \n> private:\n>     // 定义一个共享指针的订阅者，订阅 SystemStatus 类型的消息\n>     rclcpp::Subscription<SystemStatus>::SharedPtr subscriber_;\n> \n>     // 定义 QLabel 控件，用于显示系统状态信息\n>     QLabel* label_;\n> \n> public:\n>     // SysStatusDisplay 构造函数，初始化节点名为 \"sys_status_display\"\n>     SysStatusDisplay() : Node(\"sys_status_display\") {\n>         // 初始化 QLabel 控件\n>         label_ = new QLabel();\n>         \n>         // 创建一个订阅者，订阅 \"sys_status\" 话题，消息队列大小为10\n>         subscriber_ = this->create_subscription<SystemStatus>(\n>             \"sys_status\", 10, \n>             [&](const SystemStatus::SharedPtr msg) -> void {\n>                 // 每当接收到消息时，更新标签文本\n>                 label_->setText(get_qstr_from_msg(msg));\n>             });\n> \n>         // 初始化时，先显示一个默认的空状态\n>         label_->setText(get_qstr_from_msg(std::make_shared<SystemStatus>()));\n>         \n>         // 显示 QLabel 控件\n>         label_->show();\n>     };\n> \n>     // 将接收到的 SystemStatus 消息转换为 QString 类型的文本\n>     QString get_qstr_from_msg(const SystemStatus::SharedPtr msg) {\n>         \n>         // 使用字符串流构造格式化的字符串\n>         std::stringstream show_str;\n>         show_str <<\n>         \"==========系统状态可视化工具==========\\n\" <<\n>         \"数 据 时 间：\\t\" << msg->stamp.sec << \"\\ts\\n\" <<\n>         \"主 机 名 字：\\t\" << msg->host_name << \"\\t\\n\" <<\n>         \"CPU使用率：\\t\" << msg->cpu_percent << \"\\t%\\n\" <<\n>         \"内存使用率：\\t\" << msg->memory_percent << \"\\t%\\n\" <<\n>         \"可 用 内 存：\\t\" << msg->memory_available << \"\\tMB\\n\" <<\n>         \"网络发送数据量：\\t\" << msg->net_sent << \"\\tMB\\n\" <<\n>         \"网络接收数据量：\\t\" << msg->net_recv << \"\\tMB\\n\" <<\n>         \"====================================\";\n> \n>         // 将构造的字符串转换为 QString 类型返回\n>         return QString::fromStdString(show_str.str());\n>     };\n> };\n> \n> // main 函数，程序入口\n> int main(int argc, char** argv) {\n> \n>     // 初始化 ROS2 客户端库\n>     rclcpp::init(argc, argv);\n> \n>     // 初始化 Qt 应用程序\n>     QApplication app(argc, argv);\n> \n>     // 创建 SysStatusDisplay 对象（ROS2节点和GUI控件）\n>     auto node = std::make_shared<SysStatusDisplay>();\n> \n>     // 创建并启动一个新的线程，运行 ROS2 节点的 spin 方法\n>     std::thread spin_thread([&]()->void{\n>         // 运行 ROS2 事件循环，处理订阅和回调\n>         rclcpp::spin(node);\n>     });\n>     // 分离线程，使其在后台继续执行\n>     spin_thread.detach();\n>     \n>     // 启动 Qt 应用程序的事件循环，开始界面显示并处理用户交互\n>     app.exec(); // 阻塞，直到应用程序结束\n> \n>     return 0;  // 程序正常退出\n> }\n> ```\n>\n> ### 在 CMakeLists 中添加相应语句\n>\n> ```cmake\n> # 添加可执行文件\n> add_executable(sys_status_display src/sys_status_display.cpp)\n> # 添加依赖库\n> target_link_libraries(sys_status_display Qt5::Widgets)\n> ament_target_dependencies(sys_status_display rclcpp status_interfaces)\n> # 拷贝节点到 install/lib\n> install(TARGETS hello_qt sys_status_display\n> DESTINATION lib/${PROJECT_NAME}\n> )\n> ```\n>\n> ### 然后构建运行(提前运行 sys_status_pub 节点)\n>\n> ```shell\n> colcon build\n> source install/setup.bash\n> ros2 run status_display sys_status_display\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_35.png)","source":"_posts/12.md","raw":"---\ntitle: 第十弹\ndata: 2025-02-23 14:51:00\nupdated: 2025-02-23 14:51:00\ntype: ROS2\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p12.png\ntags:\n  - ROS2\n  - Learning\n---\n\n# 话题通信实践案例\n\n## 自定义通信接口\n\n> ### 在 chapter_3 下创建一个新的文件层，接着在 src/ 下创建功能包，并进入功能包，在其目录下创建文件 msg，再进入 msg/，创建文件SystemStatus.msg\n>\n> ```shell\n> cd ~/learn_ros2/chapter_3/\n> mkdir -p topic_practice_ws/src\n> cd src/\n> ros2 pkg create status_interfaces --dependencies builtin_interfaces rosidl_default_generators --license Apache-2.0\n> cd status_interfaces/\n> mkdir msg\n> cd msg\n> touch SystemStatus.msg\n> ```\n>\n> ### 编辑 .msg 文件内容\n>\n> ```txt\n> builtin_interfaces/Time stamp #记录时间戳\n> string host_name # 主机名字\n> float32 cpu_percent # cpu使用率\n> float32 memory_percent # 内存使用率\n> float32 memory_available # 内存总大小\n> float64 net_sent # 网络发送数据总量\n> float64 net_receive # 网络数据接收总量\n> ```\n>\n> ### 在 CMakeLists 和 package.xml 文件中添加相应语句\n>\n> ```cmake\n> # cmake函数，来自与rosidl_default_generators，将msg等消息接口定义文件转换成库或者头文件类\n> # 它会根据提供的消息接口去生成对应的cpp文件\n> rosidl_generate_interfaces(${PROJECT_NAME}\n> \"msg/SystemStatus.msg\"\n> DEPENDENCIES builtin_interfaces\n> )\n> ```\n>\n> ```xml\n> <license>Apache-2.0</license>\n> # 添加这句\n> <member_of_group>rosidl_interface_packages</member_of_group>\n> ```\n>\n> ### 然后编译并修改环境变量\n>\n> ```shell\n> cd ~/learn_ros2/chapter_3/topic_practice_ws/\n> colcon build\n> source install/setup.bash\n> ```\n>\n> ### 编译好之后，在 topic_practice_ws/installl 下，有一个与功能包同名的文件，在该文件下有一个 include 文件，该文件下也有一个与包名同名的文件，里面有个 msg文件，在该文件中，就是 .msg 文件转换得来的 .hpp 和 .h 文件\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_29.png)\n>\n> ### 另外，在 detail 文件中，system_status__struct.hpp 文件里有 .msg文件中定义的属性\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_30.png)\n>\n> ### 查看生成的头文件的定义\n>\n> ```shell\n> ros2 interface show status_interfaces/msg/SystemStatus\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_31.png)\n\n## 系统信息发布与获取\n\n> ### 在 topic_practice_ws/src/ 下创建一个功能包 status_publisher，要依赖 status_interfaces，这里创建python的，因为要使用python的一个库 psutil，可以相对简单的得到系统当前的各种信息\n>\n> ```shell\n> ros2 pkg create status_publisher --build-type ament_python --dependencies rclpy status_interfaces --licence Apache-2.0\n> ```\n>\n> ### 在 status_publisher/src 下创建文件 sys_status_pub.cpp，内容如下\n>\n> ```python\n> import rclpy  # 导入ROS 2的Python客户端库\n> from status_interfaces.msg import SystemStatus  # 导入自定义消息类型 SystemStatus\n> from rclpy.node import Node  # 导入ROS2节点类\n> import psutil  # 用于获取系统的硬件状态，如CPU、内存和网络等\n> import platform  # 用于获取系统的基本信息，如主机名\n> \n> # 定义一个类 SysStatusPub，继承自 Node 类\n> class SysStatusPub(Node):\n>     def __init__(self, node_name):\n>         # 初始化节点，node_name是节点的名字\n>         super().__init__(node_name)  \n>            \n>            # 创建一个发布器，用于发布类型为 SystemStatus 的消息到 'sys_status' 话题，队列大小为 10\n>            self.status_publisher_ = self.create_publisher(SystemStatus, 'sys_status', 10)\n>            \n>            # 创建定时器，每隔1秒调用一次 timer_callback 方法\n>            self.timer_ = self.create_timer(1.0, self.timer_callback)\n> \n>        # 定时器回调函数，每隔1秒被调用一次\n>        def timer_callback(self):\n>            # 获取CPU的使用率，返回值为浮动百分比\n>            cpu_percent = psutil.cpu_percent()\n>         \n>         # 获取内存的使用信息，包括已使用、总量、可用等数据\n>            memory_info = psutil.virtual_memory()\n>            \n>            # 获取网络I/O的发送和接收信息（字节数）\n>            net_io_counters = psutil.net_io_counters()\n>    \n>            # 创建一个SystemStatus消息对象\n>            msg = SystemStatus()\n>            \n>            # 设置时间戳，获取当前时间并转换为消息格式\n>         msg.stamp = self.get_clock().now().to_msg()\n>            \n>            # 获取主机名\n>                 msg.host_name = platform.node()\n>            \n>            # 获取并设置CPU使用率\n>            msg.cpu_percent = cpu_percent\n>            \n>            # 获取并设置内存的使用百分比\n>            msg.memory_percent = memory_info.percent\n>            \n>            # 获取并设置内存的总大小，转换为float类型\n>         msg.memory_total = float(memory_info.total)\n>            \n>            # 获取并设置内存的可用大小，转换为float类型\n>            msg.memory_available = float(memory_info.available)\n>         \n>            # 获取并设置网络发送的字节数，单位转换为MB\n>            msg.net_sent = net_io_counters.bytes_sent / 1024 / 1024\n>            \n>         # 获取并设置网络接收的字节数，单位转换为MB\n>            msg.net_recv = net_io_counters.bytes_recv / 1024 / 1024\n>    \n>            # 记录日志，输出当前的SystemStatus消息\n>            self.get_logger().info(f'发布: {str(msg)}')\n>            \n>            # 发布消息\n>            self.status_publisher_.publish(msg)\n>         \n>    \n>    # main函数，程序入口\n>    def main():\n>             # 初始化rclpy客户端库\n>        rclpy.init()\n>    \n>     # 创建一个SysStatusPub节点对象，节点名称为 'sys_status_pub'\n>        node = SysStatusPub('sys_status_pub')\n>        \n>        # 使节点进入循环，保持节点运行并监听定时器事件\n>     rclpy.spin(node)\n>    \n>        # 关闭节点并关闭rclpy\n>        rclpy.shutdown()\n>    ```\n>    \n>    ### 在 setup.py 中添加相应语句\n>         \n>    ```python\n>    from setuptools import find_packages, setup\n>    \n>    package_name = 'status_publisher'\n>    \n>    setup(\n>        name=package_name,\n>        version='0.0.0',\n>     packages=find_packages(exclude=['test']),\n>        data_files=[\n>            ('share/ament_index/resource_index/packages',\n>                ['resource/' + package_name]),\n>         ('share/' + package_name, ['package.xml']),\n>        ],\n>        install_requires=['setuptools'],\n>        zip_safe=True,\n>        maintainer='yanzu',\n>        maintainer_email='yanzu@todo.todo',\n>        description='TODO: Package description',\n>             license='Apache-2.0',\n>        tests_require=['pytest'],\n>        entry_points={\n>            'console_scripts': [\n>                # 添加这句\n>            \t'sys_status_pub = status_publisher.sys_status_pub:main',\n>            ],\n>     },\n>    )\n>    ```\n>    \n> ### 然后编译运行\n>    \n>    ```shell\n>    colcon build\n>    source install/setup.bash\n>    ros2 run status_publisher sys_status_pub\n>    ```\n> \n>    ![](http://picbed.yanzu.tech/img/learn_ros2/pic_32.png)\n>    \n>    ### 打印接口信息\n>    \n>    ```shell\n>    ros2 topic echo /sys_status\n>    ```\n>    \n>    ![](http://picbed.yanzu.tech/img/learn_ros2/pic_33.png)\n\n## 在功能包中使用QT\n\n> ### 在 topic_practice_ws/src/ 下创建功能包 status_display，且要依赖 status_interfaces\n>\n> ```shell\n> ros2 pkg create status_display --dependencies rclcpp status_interfaces --license Apache-2.0\n> ```\n>\n> ### 在 status_display/src/ 下创建文件 hello_qt.cpp，内容如下\n>\n> ```cpp\n> #include <QApplication>\n> #include <QLabel>\n> #include <QString>\n> \n> \n> int main(int argc, char** argv){\n> \t\n> \tQApplication app(argc, argv);\n> \tQLabel* label = new QLabel();\n> \tQString message = QString::fromStdString(\"hello qt!\");\n> \tlabel->setText(message);\n> \tlabel->show();\n> \tapp.exec();\t// 执行应用,阻塞代码\n> \n> \treturn 0;\n> }\n> ```\n>\n> ### 在 CMakeLists 中添加相应语句\n>\n> ```shell\n> # 查找 qt5 组件\n> find_package(Qt5 REQUIRED COMPONENTS Widgets)\n> \n> # 添加可执行文件\n> add_executable(hello_qt src/hello_qt.cpp)\n> \n> # 添加依赖库\n> target_link_libraries(hello_qt Qt5::Widgets)\n> \n> # 拷贝节点到 install/lib\n> install(TARGETS hello_qt\n> DESTINATION lib/${PROJECT_NAME}\n> )\n> ```\n>\n> ### 然后编译执行\n>\n> ```shell\n> colcon build\n> source install/setup.bash\n> ros2 run status_display hello_qt\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_34.png)\n\n## 订阅数据并使用Qt显示\n\n> ### 在 status_display/src/ 下创建文件 sys_status_display.cpp，内容如下\n>\n> ```cpp\n> #include <QApplication>      // 引入Qt应用程序库，用于创建GUI界面\n> #include <QLabel>            // 引入Qt标签控件，用于显示系统状态信息\n> #include <QString>           // 引入Qt字符串类，用于处理字符串\n> #include <rclcpp/rclcpp.hpp> // 引入ROS2 C++客户端库\n> #include \"status_interfaces/msg/system_status.hpp\" // 引入自定义消息类型 SystemStatus\n> \n> // 定义 SystemStatus 类型简写\n> using SystemStatus = status_interfaces::msg::SystemStatus;\n> \n> // 定义 SysStatusDisplay 类，继承自 ROS2 的 Node 类\n> class SysStatusDisplay : public rclcpp::Node {\n> \n> private:\n>     // 定义一个共享指针的订阅者，订阅 SystemStatus 类型的消息\n>     rclcpp::Subscription<SystemStatus>::SharedPtr subscriber_;\n> \n>     // 定义 QLabel 控件，用于显示系统状态信息\n>     QLabel* label_;\n> \n> public:\n>     // SysStatusDisplay 构造函数，初始化节点名为 \"sys_status_display\"\n>     SysStatusDisplay() : Node(\"sys_status_display\") {\n>         // 初始化 QLabel 控件\n>         label_ = new QLabel();\n>         \n>         // 创建一个订阅者，订阅 \"sys_status\" 话题，消息队列大小为10\n>         subscriber_ = this->create_subscription<SystemStatus>(\n>             \"sys_status\", 10, \n>             [&](const SystemStatus::SharedPtr msg) -> void {\n>                 // 每当接收到消息时，更新标签文本\n>                 label_->setText(get_qstr_from_msg(msg));\n>             });\n> \n>         // 初始化时，先显示一个默认的空状态\n>         label_->setText(get_qstr_from_msg(std::make_shared<SystemStatus>()));\n>         \n>         // 显示 QLabel 控件\n>         label_->show();\n>     };\n> \n>     // 将接收到的 SystemStatus 消息转换为 QString 类型的文本\n>     QString get_qstr_from_msg(const SystemStatus::SharedPtr msg) {\n>         \n>         // 使用字符串流构造格式化的字符串\n>         std::stringstream show_str;\n>         show_str <<\n>         \"==========系统状态可视化工具==========\\n\" <<\n>         \"数 据 时 间：\\t\" << msg->stamp.sec << \"\\ts\\n\" <<\n>         \"主 机 名 字：\\t\" << msg->host_name << \"\\t\\n\" <<\n>         \"CPU使用率：\\t\" << msg->cpu_percent << \"\\t%\\n\" <<\n>         \"内存使用率：\\t\" << msg->memory_percent << \"\\t%\\n\" <<\n>         \"可 用 内 存：\\t\" << msg->memory_available << \"\\tMB\\n\" <<\n>         \"网络发送数据量：\\t\" << msg->net_sent << \"\\tMB\\n\" <<\n>         \"网络接收数据量：\\t\" << msg->net_recv << \"\\tMB\\n\" <<\n>         \"====================================\";\n> \n>         // 将构造的字符串转换为 QString 类型返回\n>         return QString::fromStdString(show_str.str());\n>     };\n> };\n> \n> // main 函数，程序入口\n> int main(int argc, char** argv) {\n> \n>     // 初始化 ROS2 客户端库\n>     rclcpp::init(argc, argv);\n> \n>     // 初始化 Qt 应用程序\n>     QApplication app(argc, argv);\n> \n>     // 创建 SysStatusDisplay 对象（ROS2节点和GUI控件）\n>     auto node = std::make_shared<SysStatusDisplay>();\n> \n>     // 创建并启动一个新的线程，运行 ROS2 节点的 spin 方法\n>     std::thread spin_thread([&]()->void{\n>         // 运行 ROS2 事件循环，处理订阅和回调\n>         rclcpp::spin(node);\n>     });\n>     // 分离线程，使其在后台继续执行\n>     spin_thread.detach();\n>     \n>     // 启动 Qt 应用程序的事件循环，开始界面显示并处理用户交互\n>     app.exec(); // 阻塞，直到应用程序结束\n> \n>     return 0;  // 程序正常退出\n> }\n> ```\n>\n> ### 在 CMakeLists 中添加相应语句\n>\n> ```cmake\n> # 添加可执行文件\n> add_executable(sys_status_display src/sys_status_display.cpp)\n> # 添加依赖库\n> target_link_libraries(sys_status_display Qt5::Widgets)\n> ament_target_dependencies(sys_status_display rclcpp status_interfaces)\n> # 拷贝节点到 install/lib\n> install(TARGETS hello_qt sys_status_display\n> DESTINATION lib/${PROJECT_NAME}\n> )\n> ```\n>\n> ### 然后构建运行(提前运行 sys_status_pub 节点)\n>\n> ```shell\n> colcon build\n> source install/setup.bash\n> ros2 run status_display sys_status_display\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_35.png)","slug":"12","published":1,"date":"2025-02-23T06:51:07.312Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cqx000tiku4540243f3","content":"<h1 id=\"话题通信实践案例\"><a href=\"#话题通信实践案例\" class=\"headerlink\" title=\"话题通信实践案例\"></a>话题通信实践案例</h1><h2 id=\"自定义通信接口\"><a href=\"#自定义通信接口\" class=\"headerlink\" title=\"自定义通信接口\"></a>自定义通信接口</h2><blockquote>\n<h3 id=\"在-chapter-3-下创建一个新的文件层，接着在-src-下创建功能包，并进入功能包，在其目录下创建文件-msg，再进入-msg-，创建文件SystemStatus-msg\"><a href=\"#在-chapter-3-下创建一个新的文件层，接着在-src-下创建功能包，并进入功能包，在其目录下创建文件-msg，再进入-msg-，创建文件SystemStatus-msg\" class=\"headerlink\" title=\"在 chapter_3 下创建一个新的文件层，接着在 src&#x2F; 下创建功能包，并进入功能包，在其目录下创建文件 msg，再进入 msg&#x2F;，创建文件SystemStatus.msg\"></a>在 chapter_3 下创建一个新的文件层，接着在 src&#x2F; 下创建功能包，并进入功能包，在其目录下创建文件 msg，再进入 msg&#x2F;，创建文件SystemStatus.msg</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/learn_ros2/chapter_3/</span><br><span class=\"line\">mkdir -p topic_practice_ws/src</span><br><span class=\"line\">cd src/</span><br><span class=\"line\">ros2 pkg create status_interfaces --dependencies builtin_interfaces rosidl_default_generators --license Apache-2.0</span><br><span class=\"line\">cd status_interfaces/</span><br><span class=\"line\">mkdir msg</span><br><span class=\"line\">cd msg</span><br><span class=\"line\">touch SystemStatus.msg</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编辑-msg-文件内容\"><a href=\"#编辑-msg-文件内容\" class=\"headerlink\" title=\"编辑 .msg 文件内容\"></a>编辑 .msg 文件内容</h3><figure class=\"highlight txt\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">builtin_interfaces/Time stamp #记录时间戳</span><br><span class=\"line\">string host_name # 主机名字</span><br><span class=\"line\">float32 cpu_percent # cpu使用率</span><br><span class=\"line\">float32 memory_percent # 内存使用率</span><br><span class=\"line\">float32 memory_available # 内存总大小</span><br><span class=\"line\">float64 net_sent # 网络发送数据总量</span><br><span class=\"line\">float64 net_receive # 网络数据接收总量</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-CMakeLists-和-package-xml-文件中添加相应语句\"><a href=\"#在-CMakeLists-和-package-xml-文件中添加相应语句\" class=\"headerlink\" title=\"在 CMakeLists 和 package.xml 文件中添加相应语句\"></a>在 CMakeLists 和 package.xml 文件中添加相应语句</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># cmake函数，来自与rosidl_default_generators，将msg等消息接口定义文件转换成库或者头文件类</span></span><br><span class=\"line\"><span class=\"comment\"># 它会根据提供的消息接口去生成对应的cpp文件</span></span><br><span class=\"line\">rosidl_generate_interfaces(<span class=\"variable\">$&#123;PROJECT_NAME&#125;</span></span><br><span class=\"line\"><span class=\"string\">&quot;msg/SystemStatus.msg&quot;</span></span><br><span class=\"line\">DEPENDENCIES builtin_interfaces</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">license</span>&gt;</span>Apache-2.0<span class=\"tag\">&lt;/<span class=\"name\">license</span>&gt;</span></span><br><span class=\"line\"># 添加这句</span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">member_of_group</span>&gt;</span>rosidl_interface_packages<span class=\"tag\">&lt;/<span class=\"name\">member_of_group</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"然后编译并修改环境变量\"><a href=\"#然后编译并修改环境变量\" class=\"headerlink\" title=\"然后编译并修改环境变量\"></a>然后编译并修改环境变量</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/learn_ros2/chapter_3/topic_practice_ws/</span><br><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编译好之后，在-topic-practice-ws-installl-下，有一个与功能包同名的文件，在该文件下有一个-include-文件，该文件下也有一个与包名同名的文件，里面有个-msg文件，在该文件中，就是-msg-文件转换得来的-hpp-和-h-文件\"><a href=\"#编译好之后，在-topic-practice-ws-installl-下，有一个与功能包同名的文件，在该文件下有一个-include-文件，该文件下也有一个与包名同名的文件，里面有个-msg文件，在该文件中，就是-msg-文件转换得来的-hpp-和-h-文件\" class=\"headerlink\" title=\"编译好之后，在 topic_practice_ws&#x2F;installl 下，有一个与功能包同名的文件，在该文件下有一个 include 文件，该文件下也有一个与包名同名的文件，里面有个 msg文件，在该文件中，就是 .msg 文件转换得来的 .hpp 和 .h 文件\"></a>编译好之后，在 topic_practice_ws&#x2F;installl 下，有一个与功能包同名的文件，在该文件下有一个 include 文件，该文件下也有一个与包名同名的文件，里面有个 msg文件，在该文件中，就是 .msg 文件转换得来的 .hpp 和 .h 文件</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_29.png\"></p>\n<h3 id=\"另外，在-detail-文件中，system-status-struct-hpp-文件里有-msg文件中定义的属性\"><a href=\"#另外，在-detail-文件中，system-status-struct-hpp-文件里有-msg文件中定义的属性\" class=\"headerlink\" title=\"另外，在 detail 文件中，system_status__struct.hpp 文件里有 .msg文件中定义的属性\"></a>另外，在 detail 文件中，system_status__struct.hpp 文件里有 .msg文件中定义的属性</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_30.png\"></p>\n<h3 id=\"查看生成的头文件的定义\"><a href=\"#查看生成的头文件的定义\" class=\"headerlink\" title=\"查看生成的头文件的定义\"></a>查看生成的头文件的定义</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 interface show status_interfaces/msg/SystemStatus</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_31.png\"></p>\n</blockquote>\n<h2 id=\"系统信息发布与获取\"><a href=\"#系统信息发布与获取\" class=\"headerlink\" title=\"系统信息发布与获取\"></a>系统信息发布与获取</h2><blockquote>\n<h3 id=\"在-topic-practice-ws-src-下创建一个功能包-status-publisher，要依赖-status-interfaces，这里创建python的，因为要使用python的一个库-psutil，可以相对简单的得到系统当前的各种信息\"><a href=\"#在-topic-practice-ws-src-下创建一个功能包-status-publisher，要依赖-status-interfaces，这里创建python的，因为要使用python的一个库-psutil，可以相对简单的得到系统当前的各种信息\" class=\"headerlink\" title=\"在 topic_practice_ws&#x2F;src&#x2F; 下创建一个功能包 status_publisher，要依赖 status_interfaces，这里创建python的，因为要使用python的一个库 psutil，可以相对简单的得到系统当前的各种信息\"></a>在 topic_practice_ws&#x2F;src&#x2F; 下创建一个功能包 status_publisher，要依赖 status_interfaces，这里创建python的，因为要使用python的一个库 psutil，可以相对简单的得到系统当前的各种信息</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 pkg create status_publisher --build-type ament_python --dependencies rclpy status_interfaces --licence Apache-2.0</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-status-publisher-src-下创建文件-sys-status-pub-cpp，内容如下\"><a href=\"#在-status-publisher-src-下创建文件-sys-status-pub-cpp，内容如下\" class=\"headerlink\" title=\"在 status_publisher&#x2F;src 下创建文件 sys_status_pub.cpp，内容如下\"></a>在 status_publisher&#x2F;src 下创建文件 sys_status_pub.cpp，内容如下</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> rclpy  <span class=\"comment\"># 导入ROS 2的Python客户端库</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> status_interfaces.msg <span class=\"keyword\">import</span> SystemStatus  <span class=\"comment\"># 导入自定义消息类型 SystemStatus</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> rclpy.node <span class=\"keyword\">import</span> Node  <span class=\"comment\"># 导入ROS2节点类</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> psutil  <span class=\"comment\"># 用于获取系统的硬件状态，如CPU、内存和网络等</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> platform  <span class=\"comment\"># 用于获取系统的基本信息，如主机名</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义一个类 SysStatusPub，继承自 Node 类</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SysStatusPub</span>(<span class=\"title class_ inherited__\">Node</span>):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, node_name</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 初始化节点，node_name是节点的名字</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__(node_name)  </span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 创建一个发布器，用于发布类型为 SystemStatus 的消息到 &#x27;sys_status&#x27; 话题，队列大小为 10</span></span><br><span class=\"line\">           <span class=\"variable language_\">self</span>.status_publisher_ = <span class=\"variable language_\">self</span>.create_publisher(SystemStatus, <span class=\"string\">&#x27;sys_status&#x27;</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 创建定时器，每隔1秒调用一次 timer_callback 方法</span></span><br><span class=\"line\">           <span class=\"variable language_\">self</span>.timer_ = <span class=\"variable language_\">self</span>.create_timer(<span class=\"number\">1.0</span>, <span class=\"variable language_\">self</span>.timer_callback)</span><br><span class=\"line\"></span><br><span class=\"line\">       <span class=\"comment\"># 定时器回调函数，每隔1秒被调用一次</span></span><br><span class=\"line\">       <span class=\"keyword\">def</span> <span class=\"title function_\">timer_callback</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">           <span class=\"comment\"># 获取CPU的使用率，返回值为浮动百分比</span></span><br><span class=\"line\">           cpu_percent = psutil.cpu_percent()</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 获取内存的使用信息，包括已使用、总量、可用等数据</span></span><br><span class=\"line\">           memory_info = psutil.virtual_memory()</span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 获取网络I/O的发送和接收信息（字节数）</span></span><br><span class=\"line\">           net_io_counters = psutil.net_io_counters()</span><br><span class=\"line\">   </span><br><span class=\"line\">           <span class=\"comment\"># 创建一个SystemStatus消息对象</span></span><br><span class=\"line\">           msg = SystemStatus()</span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 设置时间戳，获取当前时间并转换为消息格式</span></span><br><span class=\"line\">        msg.stamp = <span class=\"variable language_\">self</span>.get_clock().now().to_msg()</span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 获取主机名</span></span><br><span class=\"line\">                msg.host_name = platform.node()</span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 获取并设置CPU使用率</span></span><br><span class=\"line\">           msg.cpu_percent = cpu_percent</span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 获取并设置内存的使用百分比</span></span><br><span class=\"line\">           msg.memory_percent = memory_info.percent</span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 获取并设置内存的总大小，转换为float类型</span></span><br><span class=\"line\">        msg.memory_total = <span class=\"built_in\">float</span>(memory_info.total)</span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 获取并设置内存的可用大小，转换为float类型</span></span><br><span class=\"line\">           msg.memory_available = <span class=\"built_in\">float</span>(memory_info.available)</span><br><span class=\"line\">        </span><br><span class=\"line\">           <span class=\"comment\"># 获取并设置网络发送的字节数，单位转换为MB</span></span><br><span class=\"line\">           msg.net_sent = net_io_counters.bytes_sent / <span class=\"number\">1024</span> / <span class=\"number\">1024</span></span><br><span class=\"line\">           </span><br><span class=\"line\">        <span class=\"comment\"># 获取并设置网络接收的字节数，单位转换为MB</span></span><br><span class=\"line\">           msg.net_recv = net_io_counters.bytes_recv / <span class=\"number\">1024</span> / <span class=\"number\">1024</span></span><br><span class=\"line\">   </span><br><span class=\"line\">           <span class=\"comment\"># 记录日志，输出当前的SystemStatus消息</span></span><br><span class=\"line\">           <span class=\"variable language_\">self</span>.get_logger().info(<span class=\"string\">f&#x27;发布: <span class=\"subst\">&#123;<span class=\"built_in\">str</span>(msg)&#125;</span>&#x27;</span>)</span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 发布消息</span></span><br><span class=\"line\">           <span class=\"variable language_\">self</span>.status_publisher_.publish(msg)</span><br><span class=\"line\">        </span><br><span class=\"line\">   </span><br><span class=\"line\">   <span class=\"comment\"># main函数，程序入口</span></span><br><span class=\"line\">   <span class=\"keyword\">def</span> <span class=\"title function_\">main</span>():</span><br><span class=\"line\">            <span class=\"comment\"># 初始化rclpy客户端库</span></span><br><span class=\"line\">       rclpy.init()</span><br><span class=\"line\">   </span><br><span class=\"line\">    <span class=\"comment\"># 创建一个SysStatusPub节点对象，节点名称为 &#x27;sys_status_pub&#x27;</span></span><br><span class=\"line\">       node = SysStatusPub(<span class=\"string\">&#x27;sys_status_pub&#x27;</span>)</span><br><span class=\"line\">       </span><br><span class=\"line\">       <span class=\"comment\"># 使节点进入循环，保持节点运行并监听定时器事件</span></span><br><span class=\"line\">    rclpy.spin(node)</span><br><span class=\"line\">   </span><br><span class=\"line\">       <span class=\"comment\"># 关闭节点并关闭rclpy</span></span><br><span class=\"line\">       rclpy.shutdown()</span><br></pre></td></tr></table></figure>\n<h3 id=\"在-setup-py-中添加相应语句\"><a href=\"#在-setup-py-中添加相应语句\" class=\"headerlink\" title=\"在 setup.py 中添加相应语句\"></a>在 setup.py 中添加相应语句</h3>   <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> setuptools <span class=\"keyword\">import</span> find_packages, setup</span><br><span class=\"line\"></span><br><span class=\"line\">package_name = <span class=\"string\">&#x27;status_publisher&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">setup(</span><br><span class=\"line\">    name=package_name,</span><br><span class=\"line\">    version=<span class=\"string\">&#x27;0.0.0&#x27;</span>,</span><br><span class=\"line\"> packages=find_packages(exclude=[<span class=\"string\">&#x27;test&#x27;</span>]),</span><br><span class=\"line\">    data_files=[</span><br><span class=\"line\">        (<span class=\"string\">&#x27;share/ament_index/resource_index/packages&#x27;</span>,</span><br><span class=\"line\">            [<span class=\"string\">&#x27;resource/&#x27;</span> + package_name]),</span><br><span class=\"line\">     (<span class=\"string\">&#x27;share/&#x27;</span> + package_name, [<span class=\"string\">&#x27;package.xml&#x27;</span>]),</span><br><span class=\"line\">    ],</span><br><span class=\"line\">    install_requires=[<span class=\"string\">&#x27;setuptools&#x27;</span>],</span><br><span class=\"line\">    zip_safe=<span class=\"literal\">True</span>,</span><br><span class=\"line\">    maintainer=<span class=\"string\">&#x27;yanzu&#x27;</span>,</span><br><span class=\"line\">    maintainer_email=<span class=\"string\">&#x27;yanzu@todo.todo&#x27;</span>,</span><br><span class=\"line\">    description=<span class=\"string\">&#x27;TODO: Package description&#x27;</span>,</span><br><span class=\"line\">         license=<span class=\"string\">&#x27;Apache-2.0&#x27;</span>,</span><br><span class=\"line\">    tests_require=[<span class=\"string\">&#x27;pytest&#x27;</span>],</span><br><span class=\"line\">    entry_points=&#123;</span><br><span class=\"line\">        <span class=\"string\">&#x27;console_scripts&#x27;</span>: [</span><br><span class=\"line\">            <span class=\"comment\"># 添加这句</span></span><br><span class=\"line\">        \t<span class=\"string\">&#x27;sys_status_pub = status_publisher.sys_status_pub:main&#x27;</span>,</span><br><span class=\"line\">        ],</span><br><span class=\"line\"> &#125;,</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<h3 id=\"然后编译运行\"><a href=\"#然后编译运行\" class=\"headerlink\" title=\"然后编译运行\"></a>然后编译运行</h3>   <figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 run status_publisher sys_status_pub</span><br></pre></td></tr></table></figure>\n\n<p>   <img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_32.png\"></p>\n<h3 id=\"打印接口信息\"><a href=\"#打印接口信息\" class=\"headerlink\" title=\"打印接口信息\"></a>打印接口信息</h3>   <figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 topic echo /sys_status</span><br></pre></td></tr></table></figure>\n<p>   <img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_33.png\"></p>\n</blockquote>\n<h2 id=\"在功能包中使用QT\"><a href=\"#在功能包中使用QT\" class=\"headerlink\" title=\"在功能包中使用QT\"></a>在功能包中使用QT</h2><blockquote>\n<h3 id=\"在-topic-practice-ws-src-下创建功能包-status-display，且要依赖-status-interfaces\"><a href=\"#在-topic-practice-ws-src-下创建功能包-status-display，且要依赖-status-interfaces\" class=\"headerlink\" title=\"在 topic_practice_ws&#x2F;src&#x2F; 下创建功能包 status_display，且要依赖 status_interfaces\"></a>在 topic_practice_ws&#x2F;src&#x2F; 下创建功能包 status_display，且要依赖 status_interfaces</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 pkg create status_display --dependencies rclcpp status_interfaces --license Apache-2.0</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-status-display-src-下创建文件-hello-qt-cpp，内容如下\"><a href=\"#在-status-display-src-下创建文件-hello-qt-cpp，内容如下\" class=\"headerlink\" title=\"在 status_display&#x2F;src&#x2F; 下创建文件 hello_qt.cpp，内容如下\"></a>在 status_display&#x2F;src&#x2F; 下创建文件 hello_qt.cpp，内容如下</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;QApplication&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;QLabel&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;QString&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span>** argv)</span></span>&#123;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"function\">QApplication <span class=\"title\">app</span><span class=\"params\">(argc, argv)</span></span>;</span><br><span class=\"line\">\tQLabel* label = <span class=\"keyword\">new</span> <span class=\"built_in\">QLabel</span>();</span><br><span class=\"line\">\tQString message = QString::<span class=\"built_in\">fromStdString</span>(<span class=\"string\">&quot;hello qt!&quot;</span>);</span><br><span class=\"line\">\tlabel-&gt;<span class=\"built_in\">setText</span>(message);</span><br><span class=\"line\">\tlabel-&gt;<span class=\"built_in\">show</span>();</span><br><span class=\"line\">\tapp.<span class=\"built_in\">exec</span>();\t<span class=\"comment\">// 执行应用,阻塞代码</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-CMakeLists-中添加相应语句\"><a href=\"#在-CMakeLists-中添加相应语句\" class=\"headerlink\" title=\"在 CMakeLists 中添加相应语句\"></a>在 CMakeLists 中添加相应语句</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">查找 qt5 组件</span></span><br><span class=\"line\">find_package(Qt5 REQUIRED COMPONENTS Widgets)</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">添加可执行文件</span></span><br><span class=\"line\">add_executable(hello_qt src/hello_qt.cpp)</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">添加依赖库</span></span><br><span class=\"line\">target_link_libraries(hello_qt Qt5::Widgets)</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">拷贝节点到 install/lib</span></span><br><span class=\"line\">install(TARGETS hello_qt</span><br><span class=\"line\">DESTINATION lib/$&#123;PROJECT_NAME&#125;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"然后编译执行\"><a href=\"#然后编译执行\" class=\"headerlink\" title=\"然后编译执行\"></a>然后编译执行</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 run status_display hello_qt</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_34.png\"></p>\n</blockquote>\n<h2 id=\"订阅数据并使用Qt显示\"><a href=\"#订阅数据并使用Qt显示\" class=\"headerlink\" title=\"订阅数据并使用Qt显示\"></a>订阅数据并使用Qt显示</h2><blockquote>\n<h3 id=\"在-status-display-src-下创建文件-sys-status-display-cpp，内容如下\"><a href=\"#在-status-display-src-下创建文件-sys-status-display-cpp，内容如下\" class=\"headerlink\" title=\"在 status_display&#x2F;src&#x2F; 下创建文件 sys_status_display.cpp，内容如下\"></a>在 status_display&#x2F;src&#x2F; 下创建文件 sys_status_display.cpp，内容如下</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;QApplication&gt;</span>      <span class=\"comment\">// 引入Qt应用程序库，用于创建GUI界面</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;QLabel&gt;</span>            <span class=\"comment\">// 引入Qt标签控件，用于显示系统状态信息</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;QString&gt;</span>           <span class=\"comment\">// 引入Qt字符串类，用于处理字符串</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;rclcpp/rclcpp.hpp&gt;</span> <span class=\"comment\">// 引入ROS2 C++客户端库</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;status_interfaces/msg/system_status.hpp&quot;</span> <span class=\"comment\">// 引入自定义消息类型 SystemStatus</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 定义 SystemStatus 类型简写</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> SystemStatus = status_interfaces::msg::SystemStatus;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 定义 SysStatusDisplay 类，继承自 ROS2 的 Node 类</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SysStatusDisplay</span> : <span class=\"keyword\">public</span> rclcpp::Node &#123;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span>:</span><br><span class=\"line\">    <span class=\"comment\">// 定义一个共享指针的订阅者，订阅 SystemStatus 类型的消息</span></span><br><span class=\"line\">    rclcpp::Subscription&lt;SystemStatus&gt;::SharedPtr subscriber_;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 定义 QLabel 控件，用于显示系统状态信息</span></span><br><span class=\"line\">    QLabel* label_;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"comment\">// SysStatusDisplay 构造函数，初始化节点名为 &quot;sys_status_display&quot;</span></span><br><span class=\"line\">    <span class=\"built_in\">SysStatusDisplay</span>() : <span class=\"built_in\">Node</span>(<span class=\"string\">&quot;sys_status_display&quot;</span>) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 初始化 QLabel 控件</span></span><br><span class=\"line\">        label_ = <span class=\"keyword\">new</span> <span class=\"built_in\">QLabel</span>();</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">// 创建一个订阅者，订阅 &quot;sys_status&quot; 话题，消息队列大小为10</span></span><br><span class=\"line\">        subscriber_ = <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">create_subscription</span>&lt;SystemStatus&gt;(</span><br><span class=\"line\">            <span class=\"string\">&quot;sys_status&quot;</span>, <span class=\"number\">10</span>, </span><br><span class=\"line\">            [&amp;](<span class=\"type\">const</span> SystemStatus::SharedPtr msg) -&gt; <span class=\"type\">void</span> &#123;</span><br><span class=\"line\">                <span class=\"comment\">// 每当接收到消息时，更新标签文本</span></span><br><span class=\"line\">                label_-&gt;<span class=\"built_in\">setText</span>(<span class=\"built_in\">get_qstr_from_msg</span>(msg));</span><br><span class=\"line\">            &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 初始化时，先显示一个默认的空状态</span></span><br><span class=\"line\">        label_-&gt;<span class=\"built_in\">setText</span>(<span class=\"built_in\">get_qstr_from_msg</span>(std::<span class=\"built_in\">make_shared</span>&lt;SystemStatus&gt;()));</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">// 显示 QLabel 控件</span></span><br><span class=\"line\">        label_-&gt;<span class=\"built_in\">show</span>();</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 将接收到的 SystemStatus 消息转换为 QString 类型的文本</span></span><br><span class=\"line\">    <span class=\"function\">QString <span class=\"title\">get_qstr_from_msg</span><span class=\"params\">(<span class=\"type\">const</span> SystemStatus::SharedPtr msg)</span> </span>&#123;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">// 使用字符串流构造格式化的字符串</span></span><br><span class=\"line\">        std::stringstream show_str;</span><br><span class=\"line\">        show_str &lt;&lt;</span><br><span class=\"line\">        <span class=\"string\">&quot;==========系统状态可视化工具==========\\n&quot;</span> &lt;&lt;</span><br><span class=\"line\">        <span class=\"string\">&quot;数 据 时 间：\\t&quot;</span> &lt;&lt; msg-&gt;stamp.sec &lt;&lt; <span class=\"string\">&quot;\\ts\\n&quot;</span> &lt;&lt;</span><br><span class=\"line\">        <span class=\"string\">&quot;主 机 名 字：\\t&quot;</span> &lt;&lt; msg-&gt;host_name &lt;&lt; <span class=\"string\">&quot;\\t\\n&quot;</span> &lt;&lt;</span><br><span class=\"line\">        <span class=\"string\">&quot;CPU使用率：\\t&quot;</span> &lt;&lt; msg-&gt;cpu_percent &lt;&lt; <span class=\"string\">&quot;\\t%\\n&quot;</span> &lt;&lt;</span><br><span class=\"line\">        <span class=\"string\">&quot;内存使用率：\\t&quot;</span> &lt;&lt; msg-&gt;memory_percent &lt;&lt; <span class=\"string\">&quot;\\t%\\n&quot;</span> &lt;&lt;</span><br><span class=\"line\">        <span class=\"string\">&quot;可 用 内 存：\\t&quot;</span> &lt;&lt; msg-&gt;memory_available &lt;&lt; <span class=\"string\">&quot;\\tMB\\n&quot;</span> &lt;&lt;</span><br><span class=\"line\">        <span class=\"string\">&quot;网络发送数据量：\\t&quot;</span> &lt;&lt; msg-&gt;net_sent &lt;&lt; <span class=\"string\">&quot;\\tMB\\n&quot;</span> &lt;&lt;</span><br><span class=\"line\">        <span class=\"string\">&quot;网络接收数据量：\\t&quot;</span> &lt;&lt; msg-&gt;net_recv &lt;&lt; <span class=\"string\">&quot;\\tMB\\n&quot;</span> &lt;&lt;</span><br><span class=\"line\">        <span class=\"string\">&quot;====================================&quot;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 将构造的字符串转换为 QString 类型返回</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> QString::<span class=\"built_in\">fromStdString</span>(show_str.<span class=\"built_in\">str</span>());</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// main 函数，程序入口</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span>** argv)</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 初始化 ROS2 客户端库</span></span><br><span class=\"line\">    rclcpp::<span class=\"built_in\">init</span>(argc, argv);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 初始化 Qt 应用程序</span></span><br><span class=\"line\">    <span class=\"function\">QApplication <span class=\"title\">app</span><span class=\"params\">(argc, argv)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 创建 SysStatusDisplay 对象（ROS2节点和GUI控件）</span></span><br><span class=\"line\">    <span class=\"keyword\">auto</span> node = std::<span class=\"built_in\">make_shared</span>&lt;SysStatusDisplay&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 创建并启动一个新的线程，运行 ROS2 节点的 spin 方法</span></span><br><span class=\"line\">    <span class=\"function\">std::thread <span class=\"title\">spin_thread</span><span class=\"params\">([&amp;]()-&gt;<span class=\"type\">void</span>&#123;</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">        <span class=\"comment\">// 运行 ROS2 事件循环，处理订阅和回调</span></span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">        rclcpp::spin(node);</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    &#125;)</span></span>;</span><br><span class=\"line\">    <span class=\"comment\">// 分离线程，使其在后台继续执行</span></span><br><span class=\"line\">    spin_thread.<span class=\"built_in\">detach</span>();</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 启动 Qt 应用程序的事件循环，开始界面显示并处理用户交互</span></span><br><span class=\"line\">    app.<span class=\"built_in\">exec</span>(); <span class=\"comment\">// 阻塞，直到应用程序结束</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;  <span class=\"comment\">// 程序正常退出</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-CMakeLists-中添加相应语句-1\"><a href=\"#在-CMakeLists-中添加相应语句-1\" class=\"headerlink\" title=\"在 CMakeLists 中添加相应语句\"></a>在 CMakeLists 中添加相应语句</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 添加可执行文件</span></span><br><span class=\"line\"><span class=\"keyword\">add_executable</span>(sys_status_display src/sys_status_display.cpp)</span><br><span class=\"line\"><span class=\"comment\"># 添加依赖库</span></span><br><span class=\"line\"><span class=\"keyword\">target_link_libraries</span>(sys_status_display Qt5::Widgets)</span><br><span class=\"line\">ament_target_dependencies(sys_status_display rclcpp status_interfaces)</span><br><span class=\"line\"><span class=\"comment\"># 拷贝节点到 install/lib</span></span><br><span class=\"line\"><span class=\"keyword\">install</span>(TARGETS hello_qt sys_status_display</span><br><span class=\"line\">DESTINATION lib/<span class=\"variable\">$&#123;PROJECT_NAME&#125;</span></span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"然后构建运行-提前运行-sys-status-pub-节点\"><a href=\"#然后构建运行-提前运行-sys-status-pub-节点\" class=\"headerlink\" title=\"然后构建运行(提前运行 sys_status_pub 节点)\"></a>然后构建运行(提前运行 sys_status_pub 节点)</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 run status_display sys_status_display</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_35.png\"></p>\n</blockquote>\n","cover_type":"img","excerpt":"","more":"<h1 id=\"话题通信实践案例\"><a href=\"#话题通信实践案例\" class=\"headerlink\" title=\"话题通信实践案例\"></a>话题通信实践案例</h1><h2 id=\"自定义通信接口\"><a href=\"#自定义通信接口\" class=\"headerlink\" title=\"自定义通信接口\"></a>自定义通信接口</h2><blockquote>\n<h3 id=\"在-chapter-3-下创建一个新的文件层，接着在-src-下创建功能包，并进入功能包，在其目录下创建文件-msg，再进入-msg-，创建文件SystemStatus-msg\"><a href=\"#在-chapter-3-下创建一个新的文件层，接着在-src-下创建功能包，并进入功能包，在其目录下创建文件-msg，再进入-msg-，创建文件SystemStatus-msg\" class=\"headerlink\" title=\"在 chapter_3 下创建一个新的文件层，接着在 src&#x2F; 下创建功能包，并进入功能包，在其目录下创建文件 msg，再进入 msg&#x2F;，创建文件SystemStatus.msg\"></a>在 chapter_3 下创建一个新的文件层，接着在 src&#x2F; 下创建功能包，并进入功能包，在其目录下创建文件 msg，再进入 msg&#x2F;，创建文件SystemStatus.msg</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/learn_ros2/chapter_3/</span><br><span class=\"line\">mkdir -p topic_practice_ws/src</span><br><span class=\"line\">cd src/</span><br><span class=\"line\">ros2 pkg create status_interfaces --dependencies builtin_interfaces rosidl_default_generators --license Apache-2.0</span><br><span class=\"line\">cd status_interfaces/</span><br><span class=\"line\">mkdir msg</span><br><span class=\"line\">cd msg</span><br><span class=\"line\">touch SystemStatus.msg</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编辑-msg-文件内容\"><a href=\"#编辑-msg-文件内容\" class=\"headerlink\" title=\"编辑 .msg 文件内容\"></a>编辑 .msg 文件内容</h3><figure class=\"highlight txt\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">builtin_interfaces/Time stamp #记录时间戳</span><br><span class=\"line\">string host_name # 主机名字</span><br><span class=\"line\">float32 cpu_percent # cpu使用率</span><br><span class=\"line\">float32 memory_percent # 内存使用率</span><br><span class=\"line\">float32 memory_available # 内存总大小</span><br><span class=\"line\">float64 net_sent # 网络发送数据总量</span><br><span class=\"line\">float64 net_receive # 网络数据接收总量</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-CMakeLists-和-package-xml-文件中添加相应语句\"><a href=\"#在-CMakeLists-和-package-xml-文件中添加相应语句\" class=\"headerlink\" title=\"在 CMakeLists 和 package.xml 文件中添加相应语句\"></a>在 CMakeLists 和 package.xml 文件中添加相应语句</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># cmake函数，来自与rosidl_default_generators，将msg等消息接口定义文件转换成库或者头文件类</span></span><br><span class=\"line\"><span class=\"comment\"># 它会根据提供的消息接口去生成对应的cpp文件</span></span><br><span class=\"line\">rosidl_generate_interfaces(<span class=\"variable\">$&#123;PROJECT_NAME&#125;</span></span><br><span class=\"line\"><span class=\"string\">&quot;msg/SystemStatus.msg&quot;</span></span><br><span class=\"line\">DEPENDENCIES builtin_interfaces</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">license</span>&gt;</span>Apache-2.0<span class=\"tag\">&lt;/<span class=\"name\">license</span>&gt;</span></span><br><span class=\"line\"># 添加这句</span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">member_of_group</span>&gt;</span>rosidl_interface_packages<span class=\"tag\">&lt;/<span class=\"name\">member_of_group</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"然后编译并修改环境变量\"><a href=\"#然后编译并修改环境变量\" class=\"headerlink\" title=\"然后编译并修改环境变量\"></a>然后编译并修改环境变量</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/learn_ros2/chapter_3/topic_practice_ws/</span><br><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编译好之后，在-topic-practice-ws-installl-下，有一个与功能包同名的文件，在该文件下有一个-include-文件，该文件下也有一个与包名同名的文件，里面有个-msg文件，在该文件中，就是-msg-文件转换得来的-hpp-和-h-文件\"><a href=\"#编译好之后，在-topic-practice-ws-installl-下，有一个与功能包同名的文件，在该文件下有一个-include-文件，该文件下也有一个与包名同名的文件，里面有个-msg文件，在该文件中，就是-msg-文件转换得来的-hpp-和-h-文件\" class=\"headerlink\" title=\"编译好之后，在 topic_practice_ws&#x2F;installl 下，有一个与功能包同名的文件，在该文件下有一个 include 文件，该文件下也有一个与包名同名的文件，里面有个 msg文件，在该文件中，就是 .msg 文件转换得来的 .hpp 和 .h 文件\"></a>编译好之后，在 topic_practice_ws&#x2F;installl 下，有一个与功能包同名的文件，在该文件下有一个 include 文件，该文件下也有一个与包名同名的文件，里面有个 msg文件，在该文件中，就是 .msg 文件转换得来的 .hpp 和 .h 文件</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_29.png\"></p>\n<h3 id=\"另外，在-detail-文件中，system-status-struct-hpp-文件里有-msg文件中定义的属性\"><a href=\"#另外，在-detail-文件中，system-status-struct-hpp-文件里有-msg文件中定义的属性\" class=\"headerlink\" title=\"另外，在 detail 文件中，system_status__struct.hpp 文件里有 .msg文件中定义的属性\"></a>另外，在 detail 文件中，system_status__struct.hpp 文件里有 .msg文件中定义的属性</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_30.png\"></p>\n<h3 id=\"查看生成的头文件的定义\"><a href=\"#查看生成的头文件的定义\" class=\"headerlink\" title=\"查看生成的头文件的定义\"></a>查看生成的头文件的定义</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 interface show status_interfaces/msg/SystemStatus</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_31.png\"></p>\n</blockquote>\n<h2 id=\"系统信息发布与获取\"><a href=\"#系统信息发布与获取\" class=\"headerlink\" title=\"系统信息发布与获取\"></a>系统信息发布与获取</h2><blockquote>\n<h3 id=\"在-topic-practice-ws-src-下创建一个功能包-status-publisher，要依赖-status-interfaces，这里创建python的，因为要使用python的一个库-psutil，可以相对简单的得到系统当前的各种信息\"><a href=\"#在-topic-practice-ws-src-下创建一个功能包-status-publisher，要依赖-status-interfaces，这里创建python的，因为要使用python的一个库-psutil，可以相对简单的得到系统当前的各种信息\" class=\"headerlink\" title=\"在 topic_practice_ws&#x2F;src&#x2F; 下创建一个功能包 status_publisher，要依赖 status_interfaces，这里创建python的，因为要使用python的一个库 psutil，可以相对简单的得到系统当前的各种信息\"></a>在 topic_practice_ws&#x2F;src&#x2F; 下创建一个功能包 status_publisher，要依赖 status_interfaces，这里创建python的，因为要使用python的一个库 psutil，可以相对简单的得到系统当前的各种信息</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 pkg create status_publisher --build-type ament_python --dependencies rclpy status_interfaces --licence Apache-2.0</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-status-publisher-src-下创建文件-sys-status-pub-cpp，内容如下\"><a href=\"#在-status-publisher-src-下创建文件-sys-status-pub-cpp，内容如下\" class=\"headerlink\" title=\"在 status_publisher&#x2F;src 下创建文件 sys_status_pub.cpp，内容如下\"></a>在 status_publisher&#x2F;src 下创建文件 sys_status_pub.cpp，内容如下</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> rclpy  <span class=\"comment\"># 导入ROS 2的Python客户端库</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> status_interfaces.msg <span class=\"keyword\">import</span> SystemStatus  <span class=\"comment\"># 导入自定义消息类型 SystemStatus</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> rclpy.node <span class=\"keyword\">import</span> Node  <span class=\"comment\"># 导入ROS2节点类</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> psutil  <span class=\"comment\"># 用于获取系统的硬件状态，如CPU、内存和网络等</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> platform  <span class=\"comment\"># 用于获取系统的基本信息，如主机名</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义一个类 SysStatusPub，继承自 Node 类</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SysStatusPub</span>(<span class=\"title class_ inherited__\">Node</span>):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, node_name</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 初始化节点，node_name是节点的名字</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__(node_name)  </span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 创建一个发布器，用于发布类型为 SystemStatus 的消息到 &#x27;sys_status&#x27; 话题，队列大小为 10</span></span><br><span class=\"line\">           <span class=\"variable language_\">self</span>.status_publisher_ = <span class=\"variable language_\">self</span>.create_publisher(SystemStatus, <span class=\"string\">&#x27;sys_status&#x27;</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 创建定时器，每隔1秒调用一次 timer_callback 方法</span></span><br><span class=\"line\">           <span class=\"variable language_\">self</span>.timer_ = <span class=\"variable language_\">self</span>.create_timer(<span class=\"number\">1.0</span>, <span class=\"variable language_\">self</span>.timer_callback)</span><br><span class=\"line\"></span><br><span class=\"line\">       <span class=\"comment\"># 定时器回调函数，每隔1秒被调用一次</span></span><br><span class=\"line\">       <span class=\"keyword\">def</span> <span class=\"title function_\">timer_callback</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">           <span class=\"comment\"># 获取CPU的使用率，返回值为浮动百分比</span></span><br><span class=\"line\">           cpu_percent = psutil.cpu_percent()</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 获取内存的使用信息，包括已使用、总量、可用等数据</span></span><br><span class=\"line\">           memory_info = psutil.virtual_memory()</span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 获取网络I/O的发送和接收信息（字节数）</span></span><br><span class=\"line\">           net_io_counters = psutil.net_io_counters()</span><br><span class=\"line\">   </span><br><span class=\"line\">           <span class=\"comment\"># 创建一个SystemStatus消息对象</span></span><br><span class=\"line\">           msg = SystemStatus()</span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 设置时间戳，获取当前时间并转换为消息格式</span></span><br><span class=\"line\">        msg.stamp = <span class=\"variable language_\">self</span>.get_clock().now().to_msg()</span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 获取主机名</span></span><br><span class=\"line\">                msg.host_name = platform.node()</span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 获取并设置CPU使用率</span></span><br><span class=\"line\">           msg.cpu_percent = cpu_percent</span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 获取并设置内存的使用百分比</span></span><br><span class=\"line\">           msg.memory_percent = memory_info.percent</span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 获取并设置内存的总大小，转换为float类型</span></span><br><span class=\"line\">        msg.memory_total = <span class=\"built_in\">float</span>(memory_info.total)</span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 获取并设置内存的可用大小，转换为float类型</span></span><br><span class=\"line\">           msg.memory_available = <span class=\"built_in\">float</span>(memory_info.available)</span><br><span class=\"line\">        </span><br><span class=\"line\">           <span class=\"comment\"># 获取并设置网络发送的字节数，单位转换为MB</span></span><br><span class=\"line\">           msg.net_sent = net_io_counters.bytes_sent / <span class=\"number\">1024</span> / <span class=\"number\">1024</span></span><br><span class=\"line\">           </span><br><span class=\"line\">        <span class=\"comment\"># 获取并设置网络接收的字节数，单位转换为MB</span></span><br><span class=\"line\">           msg.net_recv = net_io_counters.bytes_recv / <span class=\"number\">1024</span> / <span class=\"number\">1024</span></span><br><span class=\"line\">   </span><br><span class=\"line\">           <span class=\"comment\"># 记录日志，输出当前的SystemStatus消息</span></span><br><span class=\"line\">           <span class=\"variable language_\">self</span>.get_logger().info(<span class=\"string\">f&#x27;发布: <span class=\"subst\">&#123;<span class=\"built_in\">str</span>(msg)&#125;</span>&#x27;</span>)</span><br><span class=\"line\">           </span><br><span class=\"line\">           <span class=\"comment\"># 发布消息</span></span><br><span class=\"line\">           <span class=\"variable language_\">self</span>.status_publisher_.publish(msg)</span><br><span class=\"line\">        </span><br><span class=\"line\">   </span><br><span class=\"line\">   <span class=\"comment\"># main函数，程序入口</span></span><br><span class=\"line\">   <span class=\"keyword\">def</span> <span class=\"title function_\">main</span>():</span><br><span class=\"line\">            <span class=\"comment\"># 初始化rclpy客户端库</span></span><br><span class=\"line\">       rclpy.init()</span><br><span class=\"line\">   </span><br><span class=\"line\">    <span class=\"comment\"># 创建一个SysStatusPub节点对象，节点名称为 &#x27;sys_status_pub&#x27;</span></span><br><span class=\"line\">       node = SysStatusPub(<span class=\"string\">&#x27;sys_status_pub&#x27;</span>)</span><br><span class=\"line\">       </span><br><span class=\"line\">       <span class=\"comment\"># 使节点进入循环，保持节点运行并监听定时器事件</span></span><br><span class=\"line\">    rclpy.spin(node)</span><br><span class=\"line\">   </span><br><span class=\"line\">       <span class=\"comment\"># 关闭节点并关闭rclpy</span></span><br><span class=\"line\">       rclpy.shutdown()</span><br></pre></td></tr></table></figure>\n<h3 id=\"在-setup-py-中添加相应语句\"><a href=\"#在-setup-py-中添加相应语句\" class=\"headerlink\" title=\"在 setup.py 中添加相应语句\"></a>在 setup.py 中添加相应语句</h3>   <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> setuptools <span class=\"keyword\">import</span> find_packages, setup</span><br><span class=\"line\"></span><br><span class=\"line\">package_name = <span class=\"string\">&#x27;status_publisher&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">setup(</span><br><span class=\"line\">    name=package_name,</span><br><span class=\"line\">    version=<span class=\"string\">&#x27;0.0.0&#x27;</span>,</span><br><span class=\"line\"> packages=find_packages(exclude=[<span class=\"string\">&#x27;test&#x27;</span>]),</span><br><span class=\"line\">    data_files=[</span><br><span class=\"line\">        (<span class=\"string\">&#x27;share/ament_index/resource_index/packages&#x27;</span>,</span><br><span class=\"line\">            [<span class=\"string\">&#x27;resource/&#x27;</span> + package_name]),</span><br><span class=\"line\">     (<span class=\"string\">&#x27;share/&#x27;</span> + package_name, [<span class=\"string\">&#x27;package.xml&#x27;</span>]),</span><br><span class=\"line\">    ],</span><br><span class=\"line\">    install_requires=[<span class=\"string\">&#x27;setuptools&#x27;</span>],</span><br><span class=\"line\">    zip_safe=<span class=\"literal\">True</span>,</span><br><span class=\"line\">    maintainer=<span class=\"string\">&#x27;yanzu&#x27;</span>,</span><br><span class=\"line\">    maintainer_email=<span class=\"string\">&#x27;yanzu@todo.todo&#x27;</span>,</span><br><span class=\"line\">    description=<span class=\"string\">&#x27;TODO: Package description&#x27;</span>,</span><br><span class=\"line\">         license=<span class=\"string\">&#x27;Apache-2.0&#x27;</span>,</span><br><span class=\"line\">    tests_require=[<span class=\"string\">&#x27;pytest&#x27;</span>],</span><br><span class=\"line\">    entry_points=&#123;</span><br><span class=\"line\">        <span class=\"string\">&#x27;console_scripts&#x27;</span>: [</span><br><span class=\"line\">            <span class=\"comment\"># 添加这句</span></span><br><span class=\"line\">        \t<span class=\"string\">&#x27;sys_status_pub = status_publisher.sys_status_pub:main&#x27;</span>,</span><br><span class=\"line\">        ],</span><br><span class=\"line\"> &#125;,</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<h3 id=\"然后编译运行\"><a href=\"#然后编译运行\" class=\"headerlink\" title=\"然后编译运行\"></a>然后编译运行</h3>   <figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 run status_publisher sys_status_pub</span><br></pre></td></tr></table></figure>\n\n<p>   <img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_32.png\"></p>\n<h3 id=\"打印接口信息\"><a href=\"#打印接口信息\" class=\"headerlink\" title=\"打印接口信息\"></a>打印接口信息</h3>   <figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 topic echo /sys_status</span><br></pre></td></tr></table></figure>\n<p>   <img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_33.png\"></p>\n</blockquote>\n<h2 id=\"在功能包中使用QT\"><a href=\"#在功能包中使用QT\" class=\"headerlink\" title=\"在功能包中使用QT\"></a>在功能包中使用QT</h2><blockquote>\n<h3 id=\"在-topic-practice-ws-src-下创建功能包-status-display，且要依赖-status-interfaces\"><a href=\"#在-topic-practice-ws-src-下创建功能包-status-display，且要依赖-status-interfaces\" class=\"headerlink\" title=\"在 topic_practice_ws&#x2F;src&#x2F; 下创建功能包 status_display，且要依赖 status_interfaces\"></a>在 topic_practice_ws&#x2F;src&#x2F; 下创建功能包 status_display，且要依赖 status_interfaces</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 pkg create status_display --dependencies rclcpp status_interfaces --license Apache-2.0</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-status-display-src-下创建文件-hello-qt-cpp，内容如下\"><a href=\"#在-status-display-src-下创建文件-hello-qt-cpp，内容如下\" class=\"headerlink\" title=\"在 status_display&#x2F;src&#x2F; 下创建文件 hello_qt.cpp，内容如下\"></a>在 status_display&#x2F;src&#x2F; 下创建文件 hello_qt.cpp，内容如下</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;QApplication&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;QLabel&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;QString&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span>** argv)</span></span>&#123;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"function\">QApplication <span class=\"title\">app</span><span class=\"params\">(argc, argv)</span></span>;</span><br><span class=\"line\">\tQLabel* label = <span class=\"keyword\">new</span> <span class=\"built_in\">QLabel</span>();</span><br><span class=\"line\">\tQString message = QString::<span class=\"built_in\">fromStdString</span>(<span class=\"string\">&quot;hello qt!&quot;</span>);</span><br><span class=\"line\">\tlabel-&gt;<span class=\"built_in\">setText</span>(message);</span><br><span class=\"line\">\tlabel-&gt;<span class=\"built_in\">show</span>();</span><br><span class=\"line\">\tapp.<span class=\"built_in\">exec</span>();\t<span class=\"comment\">// 执行应用,阻塞代码</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-CMakeLists-中添加相应语句\"><a href=\"#在-CMakeLists-中添加相应语句\" class=\"headerlink\" title=\"在 CMakeLists 中添加相应语句\"></a>在 CMakeLists 中添加相应语句</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">查找 qt5 组件</span></span><br><span class=\"line\">find_package(Qt5 REQUIRED COMPONENTS Widgets)</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">添加可执行文件</span></span><br><span class=\"line\">add_executable(hello_qt src/hello_qt.cpp)</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">添加依赖库</span></span><br><span class=\"line\">target_link_libraries(hello_qt Qt5::Widgets)</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">拷贝节点到 install/lib</span></span><br><span class=\"line\">install(TARGETS hello_qt</span><br><span class=\"line\">DESTINATION lib/$&#123;PROJECT_NAME&#125;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"然后编译执行\"><a href=\"#然后编译执行\" class=\"headerlink\" title=\"然后编译执行\"></a>然后编译执行</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 run status_display hello_qt</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_34.png\"></p>\n</blockquote>\n<h2 id=\"订阅数据并使用Qt显示\"><a href=\"#订阅数据并使用Qt显示\" class=\"headerlink\" title=\"订阅数据并使用Qt显示\"></a>订阅数据并使用Qt显示</h2><blockquote>\n<h3 id=\"在-status-display-src-下创建文件-sys-status-display-cpp，内容如下\"><a href=\"#在-status-display-src-下创建文件-sys-status-display-cpp，内容如下\" class=\"headerlink\" title=\"在 status_display&#x2F;src&#x2F; 下创建文件 sys_status_display.cpp，内容如下\"></a>在 status_display&#x2F;src&#x2F; 下创建文件 sys_status_display.cpp，内容如下</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;QApplication&gt;</span>      <span class=\"comment\">// 引入Qt应用程序库，用于创建GUI界面</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;QLabel&gt;</span>            <span class=\"comment\">// 引入Qt标签控件，用于显示系统状态信息</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;QString&gt;</span>           <span class=\"comment\">// 引入Qt字符串类，用于处理字符串</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;rclcpp/rclcpp.hpp&gt;</span> <span class=\"comment\">// 引入ROS2 C++客户端库</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;status_interfaces/msg/system_status.hpp&quot;</span> <span class=\"comment\">// 引入自定义消息类型 SystemStatus</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 定义 SystemStatus 类型简写</span></span><br><span class=\"line\"><span class=\"keyword\">using</span> SystemStatus = status_interfaces::msg::SystemStatus;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 定义 SysStatusDisplay 类，继承自 ROS2 的 Node 类</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SysStatusDisplay</span> : <span class=\"keyword\">public</span> rclcpp::Node &#123;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span>:</span><br><span class=\"line\">    <span class=\"comment\">// 定义一个共享指针的订阅者，订阅 SystemStatus 类型的消息</span></span><br><span class=\"line\">    rclcpp::Subscription&lt;SystemStatus&gt;::SharedPtr subscriber_;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 定义 QLabel 控件，用于显示系统状态信息</span></span><br><span class=\"line\">    QLabel* label_;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"comment\">// SysStatusDisplay 构造函数，初始化节点名为 &quot;sys_status_display&quot;</span></span><br><span class=\"line\">    <span class=\"built_in\">SysStatusDisplay</span>() : <span class=\"built_in\">Node</span>(<span class=\"string\">&quot;sys_status_display&quot;</span>) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 初始化 QLabel 控件</span></span><br><span class=\"line\">        label_ = <span class=\"keyword\">new</span> <span class=\"built_in\">QLabel</span>();</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">// 创建一个订阅者，订阅 &quot;sys_status&quot; 话题，消息队列大小为10</span></span><br><span class=\"line\">        subscriber_ = <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">create_subscription</span>&lt;SystemStatus&gt;(</span><br><span class=\"line\">            <span class=\"string\">&quot;sys_status&quot;</span>, <span class=\"number\">10</span>, </span><br><span class=\"line\">            [&amp;](<span class=\"type\">const</span> SystemStatus::SharedPtr msg) -&gt; <span class=\"type\">void</span> &#123;</span><br><span class=\"line\">                <span class=\"comment\">// 每当接收到消息时，更新标签文本</span></span><br><span class=\"line\">                label_-&gt;<span class=\"built_in\">setText</span>(<span class=\"built_in\">get_qstr_from_msg</span>(msg));</span><br><span class=\"line\">            &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 初始化时，先显示一个默认的空状态</span></span><br><span class=\"line\">        label_-&gt;<span class=\"built_in\">setText</span>(<span class=\"built_in\">get_qstr_from_msg</span>(std::<span class=\"built_in\">make_shared</span>&lt;SystemStatus&gt;()));</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">// 显示 QLabel 控件</span></span><br><span class=\"line\">        label_-&gt;<span class=\"built_in\">show</span>();</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 将接收到的 SystemStatus 消息转换为 QString 类型的文本</span></span><br><span class=\"line\">    <span class=\"function\">QString <span class=\"title\">get_qstr_from_msg</span><span class=\"params\">(<span class=\"type\">const</span> SystemStatus::SharedPtr msg)</span> </span>&#123;</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">// 使用字符串流构造格式化的字符串</span></span><br><span class=\"line\">        std::stringstream show_str;</span><br><span class=\"line\">        show_str &lt;&lt;</span><br><span class=\"line\">        <span class=\"string\">&quot;==========系统状态可视化工具==========\\n&quot;</span> &lt;&lt;</span><br><span class=\"line\">        <span class=\"string\">&quot;数 据 时 间：\\t&quot;</span> &lt;&lt; msg-&gt;stamp.sec &lt;&lt; <span class=\"string\">&quot;\\ts\\n&quot;</span> &lt;&lt;</span><br><span class=\"line\">        <span class=\"string\">&quot;主 机 名 字：\\t&quot;</span> &lt;&lt; msg-&gt;host_name &lt;&lt; <span class=\"string\">&quot;\\t\\n&quot;</span> &lt;&lt;</span><br><span class=\"line\">        <span class=\"string\">&quot;CPU使用率：\\t&quot;</span> &lt;&lt; msg-&gt;cpu_percent &lt;&lt; <span class=\"string\">&quot;\\t%\\n&quot;</span> &lt;&lt;</span><br><span class=\"line\">        <span class=\"string\">&quot;内存使用率：\\t&quot;</span> &lt;&lt; msg-&gt;memory_percent &lt;&lt; <span class=\"string\">&quot;\\t%\\n&quot;</span> &lt;&lt;</span><br><span class=\"line\">        <span class=\"string\">&quot;可 用 内 存：\\t&quot;</span> &lt;&lt; msg-&gt;memory_available &lt;&lt; <span class=\"string\">&quot;\\tMB\\n&quot;</span> &lt;&lt;</span><br><span class=\"line\">        <span class=\"string\">&quot;网络发送数据量：\\t&quot;</span> &lt;&lt; msg-&gt;net_sent &lt;&lt; <span class=\"string\">&quot;\\tMB\\n&quot;</span> &lt;&lt;</span><br><span class=\"line\">        <span class=\"string\">&quot;网络接收数据量：\\t&quot;</span> &lt;&lt; msg-&gt;net_recv &lt;&lt; <span class=\"string\">&quot;\\tMB\\n&quot;</span> &lt;&lt;</span><br><span class=\"line\">        <span class=\"string\">&quot;====================================&quot;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 将构造的字符串转换为 QString 类型返回</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> QString::<span class=\"built_in\">fromStdString</span>(show_str.<span class=\"built_in\">str</span>());</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// main 函数，程序入口</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span>** argv)</span> </span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 初始化 ROS2 客户端库</span></span><br><span class=\"line\">    rclcpp::<span class=\"built_in\">init</span>(argc, argv);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 初始化 Qt 应用程序</span></span><br><span class=\"line\">    <span class=\"function\">QApplication <span class=\"title\">app</span><span class=\"params\">(argc, argv)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 创建 SysStatusDisplay 对象（ROS2节点和GUI控件）</span></span><br><span class=\"line\">    <span class=\"keyword\">auto</span> node = std::<span class=\"built_in\">make_shared</span>&lt;SysStatusDisplay&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 创建并启动一个新的线程，运行 ROS2 节点的 spin 方法</span></span><br><span class=\"line\">    <span class=\"function\">std::thread <span class=\"title\">spin_thread</span><span class=\"params\">([&amp;]()-&gt;<span class=\"type\">void</span>&#123;</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">        <span class=\"comment\">// 运行 ROS2 事件循环，处理订阅和回调</span></span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">        rclcpp::spin(node);</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">    &#125;)</span></span>;</span><br><span class=\"line\">    <span class=\"comment\">// 分离线程，使其在后台继续执行</span></span><br><span class=\"line\">    spin_thread.<span class=\"built_in\">detach</span>();</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 启动 Qt 应用程序的事件循环，开始界面显示并处理用户交互</span></span><br><span class=\"line\">    app.<span class=\"built_in\">exec</span>(); <span class=\"comment\">// 阻塞，直到应用程序结束</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;  <span class=\"comment\">// 程序正常退出</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-CMakeLists-中添加相应语句-1\"><a href=\"#在-CMakeLists-中添加相应语句-1\" class=\"headerlink\" title=\"在 CMakeLists 中添加相应语句\"></a>在 CMakeLists 中添加相应语句</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 添加可执行文件</span></span><br><span class=\"line\"><span class=\"keyword\">add_executable</span>(sys_status_display src/sys_status_display.cpp)</span><br><span class=\"line\"><span class=\"comment\"># 添加依赖库</span></span><br><span class=\"line\"><span class=\"keyword\">target_link_libraries</span>(sys_status_display Qt5::Widgets)</span><br><span class=\"line\">ament_target_dependencies(sys_status_display rclcpp status_interfaces)</span><br><span class=\"line\"><span class=\"comment\"># 拷贝节点到 install/lib</span></span><br><span class=\"line\"><span class=\"keyword\">install</span>(TARGETS hello_qt sys_status_display</span><br><span class=\"line\">DESTINATION lib/<span class=\"variable\">$&#123;PROJECT_NAME&#125;</span></span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"然后构建运行-提前运行-sys-status-pub-节点\"><a href=\"#然后构建运行-提前运行-sys-status-pub-节点\" class=\"headerlink\" title=\"然后构建运行(提前运行 sys_status_pub 节点)\"></a>然后构建运行(提前运行 sys_status_pub 节点)</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 run status_display sys_status_display</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_35.png\"></p>\n</blockquote>\n"},{"title":"插曲一(ros1)","data":"2025-04-08T15:28:00.000Z","updated":"2025-04-08T15:28:00.000Z","type":"ROS1","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p13.png","_content":"\n# cpp节点编写(ros1)\n\n> ### ROS1中，cpp对应的接口叫---roscpp\n>\n> ### 与ROS2一样，创建一个功能包之后，在该包的src(source)文件下新建cpp文件，先在工作空间下的 src/ 下创建一个功能包\n>\n> ```shell\n> roscd ros_workspace/src/\n> catkin_create_pkg test_pkg roscpp rospy std_msgs\n> ```\n>\n> ### 进入 test_pkg/src/ ，创建一个cpp文件，\n>\n> ```shell\n> cd test_pkg/src\n> touch test.cpp\n> ```\n>\n> ### 然后开始编写第一个cpp节点\n>\n> ```cpp\n> #include \"ros/ros.h\"        // 包含ROS的核心头文件\n> #include \"std_msgs/String.h\" // 包含标准消息类型String的头文件\n> #include <sstream>           // 包含字符串流处理的头文件\n> \n> int main(int argc, char **argv){\n>     // 初始化ROS节点，命名为\"talker\"\n>     ros::init(argc, argv, \"talker\");\n> \n>     // 创建一个NodeHandle对象，用于与ROS系统通信\n>     ros::NodeHandle n;\n> \n>     // 创建一个发布者对象，发布名为\"chatter\"的消息队列长度为1000\n>     ros::Publisher chatter_pub = n.advertise<std_msgs::String>(\"chatter\", 1000);\n> \n>     // 设置循环频率为10Hz\n>     ros::Rate loop_rate(10);\n> \n>     // 计数器变量，用于在发布的消息中增加计数值\n>     int count = 0;\n>     \n>     // 进入主循环，只要ROS系统正常运行就会一直执行\n>     while(ros::ok()){\n>         // 创建一个String类型的消息对象\n>         std_msgs::String msg;\n> \n>         // 使用字符串流创建消息内容，包含\"hello world\"和当前计数值\n>         std::stringstream ss;\n>         ss << \"hello world \" << count;\n>         msg.data = ss.str();\n>         \n>         // 在控制台打印出即将发布的消息内容\n>         ROS_INFO(\"%s\", msg.data.c_str());\n> \n>         // 发布消息到\"chatter\"主题\n>         chatter_pub.publish(msg);\n> \n>         // 处理回调函数，确保其他节点可以及时收到消息\n>         ros::spinOnce();\n> \n>         // 按照设定的频率休眠一段时间\n>         loop_rate.sleep();\n> \n>         // 增加计数器值\n>         ++count;\t\t\n>     }\n> \n>     // 主程序返回0，表示成功结束\n>     return 0;\n> }\n> ```\n>\n> ### 与 ros2 的cpp节点大致是一样的，都有节点的初始化，只是使用的方法不同了\n>\n> ### 创建并编写好结点之后，修改CMakeList文件\n>\n> ```txt\n> # 指定src/下需要编译为可执行文件的源文件，前面节点名可以直接自己指定\n> add_executable(${PROJECT_NAME}_node  src/test.cpp)\n> \n> # 指定所要使用的链接库\n> target_link_libraries(${PROJECT_NAME}_node\n> ${catkin_LIBRARIES}\n> ）\n> ```\n>\n> ### 修改完CMakeList文件之后，就开始编译\n>\n> ```shell\n> roscd ros_workspace/\n> catkin_make\n> source ~/.bashrc\n> ```\n>\n> ### 执行，先打开一个命令框执行 roscore\n>\n> ```shell\n> roscore\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros1/1.png)\n>\n> ### 再打开一个窗口运行节点\n>\n> ```shell\n> rosrun test_pkg test_pkg_node\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros1/2.png)\n>\n> ### 头文件的引用，在 ros1 下，cpp的 .h 头文件要放在功能包目录下的include/pkg_name/ 路径下，\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros1/3.png)\n>\n> ### 创建好了头文件之后，需要引用头文件的cpp文件需要导入该头文件\n>\n> ```cpp\n> #include \"pkg_name/xxx.h\"\n> ```\n>\n> ### 编译功能包时，cpp文件中引用的头文件，ros会在该功能包的 include/ 文件下开始找，给出以上路径后就可以精确找到被引用的头文件\n>\n> ### 如果要引用同一工作空间其他包的头文件，那么在创建功能包时，需要依赖要引用头文件所在的包\n>\n> ```shell\n> catkin_create_pkg my_pkg roscpp rospy std_msgs test_pkg\n> ```\n>\n> ### 包创建好之后，他的CMakeList修改没有区别跟其他的包，被引用头文件的包的CMakeList文件需要修改\n>\n> ```txt\n> catkin_package(\n>   INCLUDE_DIRS include\n> #  LIBRARIES test_pkg\n> #  CATKIN_DEPENDS roscpp rospy std_msgs\n> #  DEPENDS system_lib\n> )\n> ```\n>\n> ### 这样修改之后，引用该包的头文件的包就能找到根据对应的路径找到需要引用的头文件了\n\n### launch文件的编写\n\n> ### 因为，一个完整的ros程序，需要多个节点来完成所需实现的功能，而roslaunch一次性执行好多个节点的方法显然不够合理和高效，由此引出 launch文件。\n>\n> ### launch文件就是一个xml格式的脚本文件，把需要启动的节点都写进launch文件 中，这样就可以通过roslaunch工具来调用launch文件，执行这个脚本文件就一次性启动所有的节点程序。\n>\n> ### 创建一个包 robot_bringup，在其路径下创建一个文件 launch，在 launch/ 下创建并编辑 startup.launch 文件，用于同时启动多个节点\n>\n> ```shell\n> catkin_create_pkg robot_bringup\n> cd robot_bringup\n> mkdir launch\n> cd launch/\n> touch startup.launch\n> subl startup.launch\n> ```\n>\n> ### 编辑使之能同时运行 test_pkg_node 和 my_pkg_node 两个节点\n>\n> ```xml\n> # <launch> 是根标签\n> # <node> 是节点标签,至少要有三个属性 pkg type name, type是指节点名,name给所运行的节点指定名称,他会覆盖ros::init的节点名, output 可以将单个节点的标准输出到终端\n> <launch>\n> <node pkg=\"test_pkg\" type=\"test_pkg_node\" name=\"test_pkg_node\" output=\"screen\"/>\n> <node pkg=\"my_pkg\" type=\"my_pkg_node\" name=\"my_pkg_node\" output=\"screen\"/>\n> </launch>\n> ```\n>\n> ### 编译并运行\n>\n> ```shell\n> catkin_make\n> source ~/.bashrc\n> roslaunch robot_bringup startup.launch\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros1/4.png)\n>\n> ### 使用 include 标签，可以在当前 launch 文件中调用另外一个包的 launch文件\n>\n> ```xml\n> # find 命令可以直接搜索功能包的位置，代替直接使用绝对路径\n> <include  file=\"$(find package-name)/launch-file-name\">\n> ```\n>\n> ### 创建新的功能包 third_pkg，在 src 中创建并编写 third_pkg.cpp\n>\n> ```shell\n> catkin_create_pkg third_pkg roscpp rospy std_msgs\n> cd third_pkg/src\n> touch third_pkg.cpp\n> subl third_pkg.cpp\n> ```\n>\n> ```cpp\n> \n> #include \"ros/ros.h\"\n> #include\"std_msgs/String.h\"\n> #include<sstream>\n> \n> int main(int argc,char **argv){\n> \n> \tros::init(argc,argv,\"third_pkg\");\n> \n> \tros::NodeHandle n;// 更新话题的消息格式为自定义的消息格式\n> \n> \tros::Publisher chatter_pub = n.advertise<std_msgs::String>(\"third_pkg\",1000);\n> \n> \tros::Rate loop_rate(10);\n> \n> \tint count = 0;\n> \n> \twhile(ros::ok()){\n> \n> \t\tstd_msgs::String msg;\n> \n> \t\tstd::stringstream ss;\n> \n> \t\tss<<\"third pkg:\"<<count;\n> \n> \t\tmsg.data = ss.str();\n> \n> \t\tROS_INFO(\"%s\",msg.data.c_str());\n> \n> \t\tchatter_pub.publish(msg);//将消息发布到话题中\n> \n> \t\tros::spinOnce();\n> \n> \t\tloop_rate.sleep();\n> \n> \t\t++count;\n> \t}\n> \t\n> \treturn 0;\n> }\n> ```\n>\n> ### 修改CMakeList文件\n>\n> ```txt\n> add_executable(third_pkg_node src/third_pkg.cpp)\n> \n> target_link_libraries(third_pkg_node\n>   ${catkin_LIBRARIES}\n> )\n> ```\n>\n> ### 在 third_pkg/ 下创建一个 launch 文件夹，并在该文件中创建并编译 third_pkg.launch 文件\n>\n> ```shell\n> mkdir launch\n> cd launch\n> touch third_pkg.launch\n> subl third_pkg.launch\n> ```\n>\n> ```xml\n> <launch>\n> <node pkg=\"third_pkg\" type=\"third_pkg_node\" name=\"third_pkg_node\" output=\"screen\"/>\n> </launch>\n> ```\n>\n> ### 修改 robot_bringup/launch/ 下的 launch 文件\n>\n> ```xml\n> <launch>\n> \t<node pkg=\"test_pkg\" type=\"test_pkg_node\" name=\"test_pkg_node\" respawn=\"true\" output=\"screen\"/>\n> \t<node pkg=\"my_pkg\" type=\"my_pkg_node\" name=\"my_pkg_node\" required=\"true\" output=\"screen\"/>\n> \t<include file=\"$(find third_pkg)/launch/third_pkg.launch\"/>\n> </launch>\n> ```\n>\n> ### 然后编译运行\n>\n> ```shell\n> catkin_make\n> source ~/.bashrc\n> roslaunch robot_bringup startup.launch\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros1/5.png)\n>\n> \n\n### 好久没更新了，damn!","source":"_posts/13.md","raw":"---\ntitle: 插曲一(ros1)\ndata: 2025-04-08 23:28:00\nupdated: 2025-04-08 23:28:00\ntype: ROS1\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p13.png\ntags:\n  - ROS1\n  - Learning\n---\n\n# cpp节点编写(ros1)\n\n> ### ROS1中，cpp对应的接口叫---roscpp\n>\n> ### 与ROS2一样，创建一个功能包之后，在该包的src(source)文件下新建cpp文件，先在工作空间下的 src/ 下创建一个功能包\n>\n> ```shell\n> roscd ros_workspace/src/\n> catkin_create_pkg test_pkg roscpp rospy std_msgs\n> ```\n>\n> ### 进入 test_pkg/src/ ，创建一个cpp文件，\n>\n> ```shell\n> cd test_pkg/src\n> touch test.cpp\n> ```\n>\n> ### 然后开始编写第一个cpp节点\n>\n> ```cpp\n> #include \"ros/ros.h\"        // 包含ROS的核心头文件\n> #include \"std_msgs/String.h\" // 包含标准消息类型String的头文件\n> #include <sstream>           // 包含字符串流处理的头文件\n> \n> int main(int argc, char **argv){\n>     // 初始化ROS节点，命名为\"talker\"\n>     ros::init(argc, argv, \"talker\");\n> \n>     // 创建一个NodeHandle对象，用于与ROS系统通信\n>     ros::NodeHandle n;\n> \n>     // 创建一个发布者对象，发布名为\"chatter\"的消息队列长度为1000\n>     ros::Publisher chatter_pub = n.advertise<std_msgs::String>(\"chatter\", 1000);\n> \n>     // 设置循环频率为10Hz\n>     ros::Rate loop_rate(10);\n> \n>     // 计数器变量，用于在发布的消息中增加计数值\n>     int count = 0;\n>     \n>     // 进入主循环，只要ROS系统正常运行就会一直执行\n>     while(ros::ok()){\n>         // 创建一个String类型的消息对象\n>         std_msgs::String msg;\n> \n>         // 使用字符串流创建消息内容，包含\"hello world\"和当前计数值\n>         std::stringstream ss;\n>         ss << \"hello world \" << count;\n>         msg.data = ss.str();\n>         \n>         // 在控制台打印出即将发布的消息内容\n>         ROS_INFO(\"%s\", msg.data.c_str());\n> \n>         // 发布消息到\"chatter\"主题\n>         chatter_pub.publish(msg);\n> \n>         // 处理回调函数，确保其他节点可以及时收到消息\n>         ros::spinOnce();\n> \n>         // 按照设定的频率休眠一段时间\n>         loop_rate.sleep();\n> \n>         // 增加计数器值\n>         ++count;\t\t\n>     }\n> \n>     // 主程序返回0，表示成功结束\n>     return 0;\n> }\n> ```\n>\n> ### 与 ros2 的cpp节点大致是一样的，都有节点的初始化，只是使用的方法不同了\n>\n> ### 创建并编写好结点之后，修改CMakeList文件\n>\n> ```txt\n> # 指定src/下需要编译为可执行文件的源文件，前面节点名可以直接自己指定\n> add_executable(${PROJECT_NAME}_node  src/test.cpp)\n> \n> # 指定所要使用的链接库\n> target_link_libraries(${PROJECT_NAME}_node\n> ${catkin_LIBRARIES}\n> ）\n> ```\n>\n> ### 修改完CMakeList文件之后，就开始编译\n>\n> ```shell\n> roscd ros_workspace/\n> catkin_make\n> source ~/.bashrc\n> ```\n>\n> ### 执行，先打开一个命令框执行 roscore\n>\n> ```shell\n> roscore\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros1/1.png)\n>\n> ### 再打开一个窗口运行节点\n>\n> ```shell\n> rosrun test_pkg test_pkg_node\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros1/2.png)\n>\n> ### 头文件的引用，在 ros1 下，cpp的 .h 头文件要放在功能包目录下的include/pkg_name/ 路径下，\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros1/3.png)\n>\n> ### 创建好了头文件之后，需要引用头文件的cpp文件需要导入该头文件\n>\n> ```cpp\n> #include \"pkg_name/xxx.h\"\n> ```\n>\n> ### 编译功能包时，cpp文件中引用的头文件，ros会在该功能包的 include/ 文件下开始找，给出以上路径后就可以精确找到被引用的头文件\n>\n> ### 如果要引用同一工作空间其他包的头文件，那么在创建功能包时，需要依赖要引用头文件所在的包\n>\n> ```shell\n> catkin_create_pkg my_pkg roscpp rospy std_msgs test_pkg\n> ```\n>\n> ### 包创建好之后，他的CMakeList修改没有区别跟其他的包，被引用头文件的包的CMakeList文件需要修改\n>\n> ```txt\n> catkin_package(\n>   INCLUDE_DIRS include\n> #  LIBRARIES test_pkg\n> #  CATKIN_DEPENDS roscpp rospy std_msgs\n> #  DEPENDS system_lib\n> )\n> ```\n>\n> ### 这样修改之后，引用该包的头文件的包就能找到根据对应的路径找到需要引用的头文件了\n\n### launch文件的编写\n\n> ### 因为，一个完整的ros程序，需要多个节点来完成所需实现的功能，而roslaunch一次性执行好多个节点的方法显然不够合理和高效，由此引出 launch文件。\n>\n> ### launch文件就是一个xml格式的脚本文件，把需要启动的节点都写进launch文件 中，这样就可以通过roslaunch工具来调用launch文件，执行这个脚本文件就一次性启动所有的节点程序。\n>\n> ### 创建一个包 robot_bringup，在其路径下创建一个文件 launch，在 launch/ 下创建并编辑 startup.launch 文件，用于同时启动多个节点\n>\n> ```shell\n> catkin_create_pkg robot_bringup\n> cd robot_bringup\n> mkdir launch\n> cd launch/\n> touch startup.launch\n> subl startup.launch\n> ```\n>\n> ### 编辑使之能同时运行 test_pkg_node 和 my_pkg_node 两个节点\n>\n> ```xml\n> # <launch> 是根标签\n> # <node> 是节点标签,至少要有三个属性 pkg type name, type是指节点名,name给所运行的节点指定名称,他会覆盖ros::init的节点名, output 可以将单个节点的标准输出到终端\n> <launch>\n> <node pkg=\"test_pkg\" type=\"test_pkg_node\" name=\"test_pkg_node\" output=\"screen\"/>\n> <node pkg=\"my_pkg\" type=\"my_pkg_node\" name=\"my_pkg_node\" output=\"screen\"/>\n> </launch>\n> ```\n>\n> ### 编译并运行\n>\n> ```shell\n> catkin_make\n> source ~/.bashrc\n> roslaunch robot_bringup startup.launch\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros1/4.png)\n>\n> ### 使用 include 标签，可以在当前 launch 文件中调用另外一个包的 launch文件\n>\n> ```xml\n> # find 命令可以直接搜索功能包的位置，代替直接使用绝对路径\n> <include  file=\"$(find package-name)/launch-file-name\">\n> ```\n>\n> ### 创建新的功能包 third_pkg，在 src 中创建并编写 third_pkg.cpp\n>\n> ```shell\n> catkin_create_pkg third_pkg roscpp rospy std_msgs\n> cd third_pkg/src\n> touch third_pkg.cpp\n> subl third_pkg.cpp\n> ```\n>\n> ```cpp\n> \n> #include \"ros/ros.h\"\n> #include\"std_msgs/String.h\"\n> #include<sstream>\n> \n> int main(int argc,char **argv){\n> \n> \tros::init(argc,argv,\"third_pkg\");\n> \n> \tros::NodeHandle n;// 更新话题的消息格式为自定义的消息格式\n> \n> \tros::Publisher chatter_pub = n.advertise<std_msgs::String>(\"third_pkg\",1000);\n> \n> \tros::Rate loop_rate(10);\n> \n> \tint count = 0;\n> \n> \twhile(ros::ok()){\n> \n> \t\tstd_msgs::String msg;\n> \n> \t\tstd::stringstream ss;\n> \n> \t\tss<<\"third pkg:\"<<count;\n> \n> \t\tmsg.data = ss.str();\n> \n> \t\tROS_INFO(\"%s\",msg.data.c_str());\n> \n> \t\tchatter_pub.publish(msg);//将消息发布到话题中\n> \n> \t\tros::spinOnce();\n> \n> \t\tloop_rate.sleep();\n> \n> \t\t++count;\n> \t}\n> \t\n> \treturn 0;\n> }\n> ```\n>\n> ### 修改CMakeList文件\n>\n> ```txt\n> add_executable(third_pkg_node src/third_pkg.cpp)\n> \n> target_link_libraries(third_pkg_node\n>   ${catkin_LIBRARIES}\n> )\n> ```\n>\n> ### 在 third_pkg/ 下创建一个 launch 文件夹，并在该文件中创建并编译 third_pkg.launch 文件\n>\n> ```shell\n> mkdir launch\n> cd launch\n> touch third_pkg.launch\n> subl third_pkg.launch\n> ```\n>\n> ```xml\n> <launch>\n> <node pkg=\"third_pkg\" type=\"third_pkg_node\" name=\"third_pkg_node\" output=\"screen\"/>\n> </launch>\n> ```\n>\n> ### 修改 robot_bringup/launch/ 下的 launch 文件\n>\n> ```xml\n> <launch>\n> \t<node pkg=\"test_pkg\" type=\"test_pkg_node\" name=\"test_pkg_node\" respawn=\"true\" output=\"screen\"/>\n> \t<node pkg=\"my_pkg\" type=\"my_pkg_node\" name=\"my_pkg_node\" required=\"true\" output=\"screen\"/>\n> \t<include file=\"$(find third_pkg)/launch/third_pkg.launch\"/>\n> </launch>\n> ```\n>\n> ### 然后编译运行\n>\n> ```shell\n> catkin_make\n> source ~/.bashrc\n> roslaunch robot_bringup startup.launch\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros1/5.png)\n>\n> \n\n### 好久没更新了，damn!","slug":"13","published":1,"date":"2025-04-08T15:27:16.759Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cqz000xiku49hclctwg","content":"<h1 id=\"cpp节点编写-ros1\"><a href=\"#cpp节点编写-ros1\" class=\"headerlink\" title=\"cpp节点编写(ros1)\"></a>cpp节点编写(ros1)</h1><blockquote>\n<h3 id=\"ROS1中，cpp对应的接口叫—roscpp\"><a href=\"#ROS1中，cpp对应的接口叫—roscpp\" class=\"headerlink\" title=\"ROS1中，cpp对应的接口叫—roscpp\"></a>ROS1中，cpp对应的接口叫—roscpp</h3><h3 id=\"与ROS2一样，创建一个功能包之后，在该包的src-source-文件下新建cpp文件，先在工作空间下的-src-下创建一个功能包\"><a href=\"#与ROS2一样，创建一个功能包之后，在该包的src-source-文件下新建cpp文件，先在工作空间下的-src-下创建一个功能包\" class=\"headerlink\" title=\"与ROS2一样，创建一个功能包之后，在该包的src(source)文件下新建cpp文件，先在工作空间下的 src&#x2F; 下创建一个功能包\"></a>与ROS2一样，创建一个功能包之后，在该包的src(source)文件下新建cpp文件，先在工作空间下的 src&#x2F; 下创建一个功能包</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">roscd ros_workspace/src/</span><br><span class=\"line\">catkin_create_pkg test_pkg roscpp rospy std_msgs</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"进入-test-pkg-src-，创建一个cpp文件，\"><a href=\"#进入-test-pkg-src-，创建一个cpp文件，\" class=\"headerlink\" title=\"进入 test_pkg&#x2F;src&#x2F; ，创建一个cpp文件，\"></a>进入 test_pkg&#x2F;src&#x2F; ，创建一个cpp文件，</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd test_pkg/src</span><br><span class=\"line\">touch test.cpp</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"然后开始编写第一个cpp节点\"><a href=\"#然后开始编写第一个cpp节点\" class=\"headerlink\" title=\"然后开始编写第一个cpp节点\"></a>然后开始编写第一个cpp节点</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;ros/ros.h&quot;</span>        <span class=\"comment\">// 包含ROS的核心头文件</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;std_msgs/String.h&quot;</span> <span class=\"comment\">// 包含标准消息类型String的头文件</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;sstream&gt;</span>           <span class=\"comment\">// 包含字符串流处理的头文件</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span> **argv)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 初始化ROS节点，命名为&quot;talker&quot;</span></span><br><span class=\"line\">    ros::<span class=\"built_in\">init</span>(argc, argv, <span class=\"string\">&quot;talker&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 创建一个NodeHandle对象，用于与ROS系统通信</span></span><br><span class=\"line\">    ros::NodeHandle n;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 创建一个发布者对象，发布名为&quot;chatter&quot;的消息队列长度为1000</span></span><br><span class=\"line\">    ros::Publisher chatter_pub = n.<span class=\"built_in\">advertise</span>&lt;std_msgs::String&gt;(<span class=\"string\">&quot;chatter&quot;</span>, <span class=\"number\">1000</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 设置循环频率为10Hz</span></span><br><span class=\"line\">    <span class=\"function\">ros::Rate <span class=\"title\">loop_rate</span><span class=\"params\">(<span class=\"number\">10</span>)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 计数器变量，用于在发布的消息中增加计数值</span></span><br><span class=\"line\">    <span class=\"type\">int</span> count = <span class=\"number\">0</span>;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 进入主循环，只要ROS系统正常运行就会一直执行</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span>(ros::<span class=\"built_in\">ok</span>())&#123;</span><br><span class=\"line\">        <span class=\"comment\">// 创建一个String类型的消息对象</span></span><br><span class=\"line\">        std_msgs::String msg;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 使用字符串流创建消息内容，包含&quot;hello world&quot;和当前计数值</span></span><br><span class=\"line\">        std::stringstream ss;</span><br><span class=\"line\">        ss &lt;&lt; <span class=\"string\">&quot;hello world &quot;</span> &lt;&lt; count;</span><br><span class=\"line\">        msg.data = ss.<span class=\"built_in\">str</span>();</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">// 在控制台打印出即将发布的消息内容</span></span><br><span class=\"line\">        <span class=\"built_in\">ROS_INFO</span>(<span class=\"string\">&quot;%s&quot;</span>, msg.data.<span class=\"built_in\">c_str</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 发布消息到&quot;chatter&quot;主题</span></span><br><span class=\"line\">        chatter_pub.<span class=\"built_in\">publish</span>(msg);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 处理回调函数，确保其他节点可以及时收到消息</span></span><br><span class=\"line\">        ros::<span class=\"built_in\">spinOnce</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 按照设定的频率休眠一段时间</span></span><br><span class=\"line\">        loop_rate.<span class=\"built_in\">sleep</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 增加计数器值</span></span><br><span class=\"line\">        ++count;\t\t</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 主程序返回0，表示成功结束</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"与-ros2-的cpp节点大致是一样的，都有节点的初始化，只是使用的方法不同了\"><a href=\"#与-ros2-的cpp节点大致是一样的，都有节点的初始化，只是使用的方法不同了\" class=\"headerlink\" title=\"与 ros2 的cpp节点大致是一样的，都有节点的初始化，只是使用的方法不同了\"></a>与 ros2 的cpp节点大致是一样的，都有节点的初始化，只是使用的方法不同了</h3><h3 id=\"创建并编写好结点之后，修改CMakeList文件\"><a href=\"#创建并编写好结点之后，修改CMakeList文件\" class=\"headerlink\" title=\"创建并编写好结点之后，修改CMakeList文件\"></a>创建并编写好结点之后，修改CMakeList文件</h3><figure class=\"highlight txt\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 指定src/下需要编译为可执行文件的源文件，前面节点名可以直接自己指定</span><br><span class=\"line\">add_executable($&#123;PROJECT_NAME&#125;_node  src/test.cpp)</span><br><span class=\"line\"></span><br><span class=\"line\"># 指定所要使用的链接库</span><br><span class=\"line\">target_link_libraries($&#123;PROJECT_NAME&#125;_node</span><br><span class=\"line\">$&#123;catkin_LIBRARIES&#125;</span><br><span class=\"line\">）</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"修改完CMakeList文件之后，就开始编译\"><a href=\"#修改完CMakeList文件之后，就开始编译\" class=\"headerlink\" title=\"修改完CMakeList文件之后，就开始编译\"></a>修改完CMakeList文件之后，就开始编译</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">roscd ros_workspace/</span><br><span class=\"line\">catkin_make</span><br><span class=\"line\">source ~/.bashrc</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"执行，先打开一个命令框执行-roscore\"><a href=\"#执行，先打开一个命令框执行-roscore\" class=\"headerlink\" title=\"执行，先打开一个命令框执行 roscore\"></a>执行，先打开一个命令框执行 roscore</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">roscore</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros1/1.png\"></p>\n<h3 id=\"再打开一个窗口运行节点\"><a href=\"#再打开一个窗口运行节点\" class=\"headerlink\" title=\"再打开一个窗口运行节点\"></a>再打开一个窗口运行节点</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rosrun test_pkg test_pkg_node</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros1/2.png\"></p>\n<h3 id=\"头文件的引用，在-ros1-下，cpp的-h-头文件要放在功能包目录下的include-pkg-name-路径下，\"><a href=\"#头文件的引用，在-ros1-下，cpp的-h-头文件要放在功能包目录下的include-pkg-name-路径下，\" class=\"headerlink\" title=\"头文件的引用，在 ros1 下，cpp的 .h 头文件要放在功能包目录下的include&#x2F;pkg_name&#x2F; 路径下，\"></a>头文件的引用，在 ros1 下，cpp的 .h 头文件要放在功能包目录下的include&#x2F;pkg_name&#x2F; 路径下，</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros1/3.png\"></p>\n<h3 id=\"创建好了头文件之后，需要引用头文件的cpp文件需要导入该头文件\"><a href=\"#创建好了头文件之后，需要引用头文件的cpp文件需要导入该头文件\" class=\"headerlink\" title=\"创建好了头文件之后，需要引用头文件的cpp文件需要导入该头文件\"></a>创建好了头文件之后，需要引用头文件的cpp文件需要导入该头文件</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;pkg_name/xxx.h&quot;</span></span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编译功能包时，cpp文件中引用的头文件，ros会在该功能包的-include-文件下开始找，给出以上路径后就可以精确找到被引用的头文件\"><a href=\"#编译功能包时，cpp文件中引用的头文件，ros会在该功能包的-include-文件下开始找，给出以上路径后就可以精确找到被引用的头文件\" class=\"headerlink\" title=\"编译功能包时，cpp文件中引用的头文件，ros会在该功能包的 include&#x2F; 文件下开始找，给出以上路径后就可以精确找到被引用的头文件\"></a>编译功能包时，cpp文件中引用的头文件，ros会在该功能包的 include&#x2F; 文件下开始找，给出以上路径后就可以精确找到被引用的头文件</h3><h3 id=\"如果要引用同一工作空间其他包的头文件，那么在创建功能包时，需要依赖要引用头文件所在的包\"><a href=\"#如果要引用同一工作空间其他包的头文件，那么在创建功能包时，需要依赖要引用头文件所在的包\" class=\"headerlink\" title=\"如果要引用同一工作空间其他包的头文件，那么在创建功能包时，需要依赖要引用头文件所在的包\"></a>如果要引用同一工作空间其他包的头文件，那么在创建功能包时，需要依赖要引用头文件所在的包</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">catkin_create_pkg my_pkg roscpp rospy std_msgs test_pkg</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"包创建好之后，他的CMakeList修改没有区别跟其他的包，被引用头文件的包的CMakeList文件需要修改\"><a href=\"#包创建好之后，他的CMakeList修改没有区别跟其他的包，被引用头文件的包的CMakeList文件需要修改\" class=\"headerlink\" title=\"包创建好之后，他的CMakeList修改没有区别跟其他的包，被引用头文件的包的CMakeList文件需要修改\"></a>包创建好之后，他的CMakeList修改没有区别跟其他的包，被引用头文件的包的CMakeList文件需要修改</h3><figure class=\"highlight txt\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">catkin_package(</span><br><span class=\"line\">  INCLUDE_DIRS include</span><br><span class=\"line\">#  LIBRARIES test_pkg</span><br><span class=\"line\">#  CATKIN_DEPENDS roscpp rospy std_msgs</span><br><span class=\"line\">#  DEPENDS system_lib</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"这样修改之后，引用该包的头文件的包就能找到根据对应的路径找到需要引用的头文件了\"><a href=\"#这样修改之后，引用该包的头文件的包就能找到根据对应的路径找到需要引用的头文件了\" class=\"headerlink\" title=\"这样修改之后，引用该包的头文件的包就能找到根据对应的路径找到需要引用的头文件了\"></a>这样修改之后，引用该包的头文件的包就能找到根据对应的路径找到需要引用的头文件了</h3></blockquote>\n<h3 id=\"launch文件的编写\"><a href=\"#launch文件的编写\" class=\"headerlink\" title=\"launch文件的编写\"></a>launch文件的编写</h3><blockquote>\n<h3 id=\"因为，一个完整的ros程序，需要多个节点来完成所需实现的功能，而roslaunch一次性执行好多个节点的方法显然不够合理和高效，由此引出-launch文件。\"><a href=\"#因为，一个完整的ros程序，需要多个节点来完成所需实现的功能，而roslaunch一次性执行好多个节点的方法显然不够合理和高效，由此引出-launch文件。\" class=\"headerlink\" title=\"因为，一个完整的ros程序，需要多个节点来完成所需实现的功能，而roslaunch一次性执行好多个节点的方法显然不够合理和高效，由此引出 launch文件。\"></a>因为，一个完整的ros程序，需要多个节点来完成所需实现的功能，而roslaunch一次性执行好多个节点的方法显然不够合理和高效，由此引出 launch文件。</h3><h3 id=\"launch文件就是一个xml格式的脚本文件，把需要启动的节点都写进launch文件-中，这样就可以通过roslaunch工具来调用launch文件，执行这个脚本文件就一次性启动所有的节点程序。\"><a href=\"#launch文件就是一个xml格式的脚本文件，把需要启动的节点都写进launch文件-中，这样就可以通过roslaunch工具来调用launch文件，执行这个脚本文件就一次性启动所有的节点程序。\" class=\"headerlink\" title=\"launch文件就是一个xml格式的脚本文件，把需要启动的节点都写进launch文件 中，这样就可以通过roslaunch工具来调用launch文件，执行这个脚本文件就一次性启动所有的节点程序。\"></a>launch文件就是一个xml格式的脚本文件，把需要启动的节点都写进launch文件 中，这样就可以通过roslaunch工具来调用launch文件，执行这个脚本文件就一次性启动所有的节点程序。</h3><h3 id=\"创建一个包-robot-bringup，在其路径下创建一个文件-launch，在-launch-下创建并编辑-startup-launch-文件，用于同时启动多个节点\"><a href=\"#创建一个包-robot-bringup，在其路径下创建一个文件-launch，在-launch-下创建并编辑-startup-launch-文件，用于同时启动多个节点\" class=\"headerlink\" title=\"创建一个包 robot_bringup，在其路径下创建一个文件 launch，在 launch&#x2F; 下创建并编辑 startup.launch 文件，用于同时启动多个节点\"></a>创建一个包 robot_bringup，在其路径下创建一个文件 launch，在 launch&#x2F; 下创建并编辑 startup.launch 文件，用于同时启动多个节点</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">catkin_create_pkg robot_bringup</span><br><span class=\"line\">cd robot_bringup</span><br><span class=\"line\">mkdir launch</span><br><span class=\"line\">cd launch/</span><br><span class=\"line\">touch startup.launch</span><br><span class=\"line\">subl startup.launch</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编辑使之能同时运行-test-pkg-node-和-my-pkg-node-两个节点\"><a href=\"#编辑使之能同时运行-test-pkg-node-和-my-pkg-node-两个节点\" class=\"headerlink\" title=\"编辑使之能同时运行 test_pkg_node 和 my_pkg_node 两个节点\"></a>编辑使之能同时运行 test_pkg_node 和 my_pkg_node 两个节点</h3><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># <span class=\"tag\">&lt;<span class=\"name\">launch</span>&gt;</span> 是根标签</span><br><span class=\"line\"># <span class=\"tag\">&lt;<span class=\"name\">node</span>&gt;</span> 是节点标签,至少要有三个属性 pkg type name, type是指节点名,name给所运行的节点指定名称,他会覆盖ros::init的节点名, output 可以将单个节点的标准输出到终端</span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">launch</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">node</span> <span class=\"attr\">pkg</span>=<span class=\"string\">&quot;test_pkg&quot;</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;test_pkg_node&quot;</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;test_pkg_node&quot;</span> <span class=\"attr\">output</span>=<span class=\"string\">&quot;screen&quot;</span>/&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">node</span> <span class=\"attr\">pkg</span>=<span class=\"string\">&quot;my_pkg&quot;</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;my_pkg_node&quot;</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;my_pkg_node&quot;</span> <span class=\"attr\">output</span>=<span class=\"string\">&quot;screen&quot;</span>/&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">launch</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编译并运行\"><a href=\"#编译并运行\" class=\"headerlink\" title=\"编译并运行\"></a>编译并运行</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">catkin_make</span><br><span class=\"line\">source ~/.bashrc</span><br><span class=\"line\">roslaunch robot_bringup startup.launch</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros1/4.png\"></p>\n<h3 id=\"使用-include-标签，可以在当前-launch-文件中调用另外一个包的-launch文件\"><a href=\"#使用-include-标签，可以在当前-launch-文件中调用另外一个包的-launch文件\" class=\"headerlink\" title=\"使用 include 标签，可以在当前 launch 文件中调用另外一个包的 launch文件\"></a>使用 include 标签，可以在当前 launch 文件中调用另外一个包的 launch文件</h3><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># find 命令可以直接搜索功能包的位置，代替直接使用绝对路径</span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">include</span>  <span class=\"attr\">file</span>=<span class=\"string\">&quot;$(find package-name)/launch-file-name&quot;</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"创建新的功能包-third-pkg，在-src-中创建并编写-third-pkg-cpp\"><a href=\"#创建新的功能包-third-pkg，在-src-中创建并编写-third-pkg-cpp\" class=\"headerlink\" title=\"创建新的功能包 third_pkg，在 src 中创建并编写 third_pkg.cpp\"></a>创建新的功能包 third_pkg，在 src 中创建并编写 third_pkg.cpp</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">catkin_create_pkg third_pkg roscpp rospy std_msgs</span><br><span class=\"line\">cd third_pkg/src</span><br><span class=\"line\">touch third_pkg.cpp</span><br><span class=\"line\">subl third_pkg.cpp</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;ros/ros.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&quot;std_msgs/String.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;sstream&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc,<span class=\"type\">char</span> **argv)</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\tros::<span class=\"built_in\">init</span>(argc,argv,<span class=\"string\">&quot;third_pkg&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">\tros::NodeHandle n;<span class=\"comment\">// 更新话题的消息格式为自定义的消息格式</span></span><br><span class=\"line\"></span><br><span class=\"line\">\tros::Publisher chatter_pub = n.<span class=\"built_in\">advertise</span>&lt;std_msgs::String&gt;(<span class=\"string\">&quot;third_pkg&quot;</span>,<span class=\"number\">1000</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\">ros::Rate <span class=\"title\">loop_rate</span><span class=\"params\">(<span class=\"number\">10</span>)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"type\">int</span> count = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">while</span>(ros::<span class=\"built_in\">ok</span>())&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tstd_msgs::String msg;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tstd::stringstream ss;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tss&lt;&lt;<span class=\"string\">&quot;third pkg:&quot;</span>&lt;&lt;count;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tmsg.data = ss.<span class=\"built_in\">str</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"built_in\">ROS_INFO</span>(<span class=\"string\">&quot;%s&quot;</span>,msg.data.<span class=\"built_in\">c_str</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tchatter_pub.<span class=\"built_in\">publish</span>(msg);<span class=\"comment\">//将消息发布到话题中</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\tros::<span class=\"built_in\">spinOnce</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tloop_rate.<span class=\"built_in\">sleep</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t++count;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"修改CMakeList文件\"><a href=\"#修改CMakeList文件\" class=\"headerlink\" title=\"修改CMakeList文件\"></a>修改CMakeList文件</h3><figure class=\"highlight txt\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_executable(third_pkg_node src/third_pkg.cpp)</span><br><span class=\"line\"></span><br><span class=\"line\">target_link_libraries(third_pkg_node</span><br><span class=\"line\">  $&#123;catkin_LIBRARIES&#125;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-third-pkg-下创建一个-launch-文件夹，并在该文件中创建并编译-third-pkg-launch-文件\"><a href=\"#在-third-pkg-下创建一个-launch-文件夹，并在该文件中创建并编译-third-pkg-launch-文件\" class=\"headerlink\" title=\"在 third_pkg&#x2F; 下创建一个 launch 文件夹，并在该文件中创建并编译 third_pkg.launch 文件\"></a>在 third_pkg&#x2F; 下创建一个 launch 文件夹，并在该文件中创建并编译 third_pkg.launch 文件</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir launch</span><br><span class=\"line\">cd launch</span><br><span class=\"line\">touch third_pkg.launch</span><br><span class=\"line\">subl third_pkg.launch</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">launch</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">node</span> <span class=\"attr\">pkg</span>=<span class=\"string\">&quot;third_pkg&quot;</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;third_pkg_node&quot;</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;third_pkg_node&quot;</span> <span class=\"attr\">output</span>=<span class=\"string\">&quot;screen&quot;</span>/&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">launch</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"修改-robot-bringup-launch-下的-launch-文件\"><a href=\"#修改-robot-bringup-launch-下的-launch-文件\" class=\"headerlink\" title=\"修改 robot_bringup&#x2F;launch&#x2F; 下的 launch 文件\"></a>修改 robot_bringup&#x2F;launch&#x2F; 下的 launch 文件</h3><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">launch</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">node</span> <span class=\"attr\">pkg</span>=<span class=\"string\">&quot;test_pkg&quot;</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;test_pkg_node&quot;</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;test_pkg_node&quot;</span> <span class=\"attr\">respawn</span>=<span class=\"string\">&quot;true&quot;</span> <span class=\"attr\">output</span>=<span class=\"string\">&quot;screen&quot;</span>/&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">node</span> <span class=\"attr\">pkg</span>=<span class=\"string\">&quot;my_pkg&quot;</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;my_pkg_node&quot;</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;my_pkg_node&quot;</span> <span class=\"attr\">required</span>=<span class=\"string\">&quot;true&quot;</span> <span class=\"attr\">output</span>=<span class=\"string\">&quot;screen&quot;</span>/&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">include</span> <span class=\"attr\">file</span>=<span class=\"string\">&quot;$(find third_pkg)/launch/third_pkg.launch&quot;</span>/&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">launch</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"然后编译运行\"><a href=\"#然后编译运行\" class=\"headerlink\" title=\"然后编译运行\"></a>然后编译运行</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">catkin_make</span><br><span class=\"line\">source ~/.bashrc</span><br><span class=\"line\">roslaunch robot_bringup startup.launch</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros1/5.png\"></p>\n</blockquote>\n<h3 id=\"好久没更新了，damn\"><a href=\"#好久没更新了，damn\" class=\"headerlink\" title=\"好久没更新了，damn!\"></a>好久没更新了，damn!</h3>","cover_type":"img","excerpt":"","more":"<h1 id=\"cpp节点编写-ros1\"><a href=\"#cpp节点编写-ros1\" class=\"headerlink\" title=\"cpp节点编写(ros1)\"></a>cpp节点编写(ros1)</h1><blockquote>\n<h3 id=\"ROS1中，cpp对应的接口叫—roscpp\"><a href=\"#ROS1中，cpp对应的接口叫—roscpp\" class=\"headerlink\" title=\"ROS1中，cpp对应的接口叫—roscpp\"></a>ROS1中，cpp对应的接口叫—roscpp</h3><h3 id=\"与ROS2一样，创建一个功能包之后，在该包的src-source-文件下新建cpp文件，先在工作空间下的-src-下创建一个功能包\"><a href=\"#与ROS2一样，创建一个功能包之后，在该包的src-source-文件下新建cpp文件，先在工作空间下的-src-下创建一个功能包\" class=\"headerlink\" title=\"与ROS2一样，创建一个功能包之后，在该包的src(source)文件下新建cpp文件，先在工作空间下的 src&#x2F; 下创建一个功能包\"></a>与ROS2一样，创建一个功能包之后，在该包的src(source)文件下新建cpp文件，先在工作空间下的 src&#x2F; 下创建一个功能包</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">roscd ros_workspace/src/</span><br><span class=\"line\">catkin_create_pkg test_pkg roscpp rospy std_msgs</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"进入-test-pkg-src-，创建一个cpp文件，\"><a href=\"#进入-test-pkg-src-，创建一个cpp文件，\" class=\"headerlink\" title=\"进入 test_pkg&#x2F;src&#x2F; ，创建一个cpp文件，\"></a>进入 test_pkg&#x2F;src&#x2F; ，创建一个cpp文件，</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd test_pkg/src</span><br><span class=\"line\">touch test.cpp</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"然后开始编写第一个cpp节点\"><a href=\"#然后开始编写第一个cpp节点\" class=\"headerlink\" title=\"然后开始编写第一个cpp节点\"></a>然后开始编写第一个cpp节点</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;ros/ros.h&quot;</span>        <span class=\"comment\">// 包含ROS的核心头文件</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;std_msgs/String.h&quot;</span> <span class=\"comment\">// 包含标准消息类型String的头文件</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;sstream&gt;</span>           <span class=\"comment\">// 包含字符串流处理的头文件</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span> **argv)</span></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 初始化ROS节点，命名为&quot;talker&quot;</span></span><br><span class=\"line\">    ros::<span class=\"built_in\">init</span>(argc, argv, <span class=\"string\">&quot;talker&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 创建一个NodeHandle对象，用于与ROS系统通信</span></span><br><span class=\"line\">    ros::NodeHandle n;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 创建一个发布者对象，发布名为&quot;chatter&quot;的消息队列长度为1000</span></span><br><span class=\"line\">    ros::Publisher chatter_pub = n.<span class=\"built_in\">advertise</span>&lt;std_msgs::String&gt;(<span class=\"string\">&quot;chatter&quot;</span>, <span class=\"number\">1000</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 设置循环频率为10Hz</span></span><br><span class=\"line\">    <span class=\"function\">ros::Rate <span class=\"title\">loop_rate</span><span class=\"params\">(<span class=\"number\">10</span>)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 计数器变量，用于在发布的消息中增加计数值</span></span><br><span class=\"line\">    <span class=\"type\">int</span> count = <span class=\"number\">0</span>;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 进入主循环，只要ROS系统正常运行就会一直执行</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span>(ros::<span class=\"built_in\">ok</span>())&#123;</span><br><span class=\"line\">        <span class=\"comment\">// 创建一个String类型的消息对象</span></span><br><span class=\"line\">        std_msgs::String msg;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 使用字符串流创建消息内容，包含&quot;hello world&quot;和当前计数值</span></span><br><span class=\"line\">        std::stringstream ss;</span><br><span class=\"line\">        ss &lt;&lt; <span class=\"string\">&quot;hello world &quot;</span> &lt;&lt; count;</span><br><span class=\"line\">        msg.data = ss.<span class=\"built_in\">str</span>();</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">// 在控制台打印出即将发布的消息内容</span></span><br><span class=\"line\">        <span class=\"built_in\">ROS_INFO</span>(<span class=\"string\">&quot;%s&quot;</span>, msg.data.<span class=\"built_in\">c_str</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 发布消息到&quot;chatter&quot;主题</span></span><br><span class=\"line\">        chatter_pub.<span class=\"built_in\">publish</span>(msg);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 处理回调函数，确保其他节点可以及时收到消息</span></span><br><span class=\"line\">        ros::<span class=\"built_in\">spinOnce</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 按照设定的频率休眠一段时间</span></span><br><span class=\"line\">        loop_rate.<span class=\"built_in\">sleep</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 增加计数器值</span></span><br><span class=\"line\">        ++count;\t\t</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 主程序返回0，表示成功结束</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"与-ros2-的cpp节点大致是一样的，都有节点的初始化，只是使用的方法不同了\"><a href=\"#与-ros2-的cpp节点大致是一样的，都有节点的初始化，只是使用的方法不同了\" class=\"headerlink\" title=\"与 ros2 的cpp节点大致是一样的，都有节点的初始化，只是使用的方法不同了\"></a>与 ros2 的cpp节点大致是一样的，都有节点的初始化，只是使用的方法不同了</h3><h3 id=\"创建并编写好结点之后，修改CMakeList文件\"><a href=\"#创建并编写好结点之后，修改CMakeList文件\" class=\"headerlink\" title=\"创建并编写好结点之后，修改CMakeList文件\"></a>创建并编写好结点之后，修改CMakeList文件</h3><figure class=\"highlight txt\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 指定src/下需要编译为可执行文件的源文件，前面节点名可以直接自己指定</span><br><span class=\"line\">add_executable($&#123;PROJECT_NAME&#125;_node  src/test.cpp)</span><br><span class=\"line\"></span><br><span class=\"line\"># 指定所要使用的链接库</span><br><span class=\"line\">target_link_libraries($&#123;PROJECT_NAME&#125;_node</span><br><span class=\"line\">$&#123;catkin_LIBRARIES&#125;</span><br><span class=\"line\">）</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"修改完CMakeList文件之后，就开始编译\"><a href=\"#修改完CMakeList文件之后，就开始编译\" class=\"headerlink\" title=\"修改完CMakeList文件之后，就开始编译\"></a>修改完CMakeList文件之后，就开始编译</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">roscd ros_workspace/</span><br><span class=\"line\">catkin_make</span><br><span class=\"line\">source ~/.bashrc</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"执行，先打开一个命令框执行-roscore\"><a href=\"#执行，先打开一个命令框执行-roscore\" class=\"headerlink\" title=\"执行，先打开一个命令框执行 roscore\"></a>执行，先打开一个命令框执行 roscore</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">roscore</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros1/1.png\"></p>\n<h3 id=\"再打开一个窗口运行节点\"><a href=\"#再打开一个窗口运行节点\" class=\"headerlink\" title=\"再打开一个窗口运行节点\"></a>再打开一个窗口运行节点</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rosrun test_pkg test_pkg_node</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros1/2.png\"></p>\n<h3 id=\"头文件的引用，在-ros1-下，cpp的-h-头文件要放在功能包目录下的include-pkg-name-路径下，\"><a href=\"#头文件的引用，在-ros1-下，cpp的-h-头文件要放在功能包目录下的include-pkg-name-路径下，\" class=\"headerlink\" title=\"头文件的引用，在 ros1 下，cpp的 .h 头文件要放在功能包目录下的include&#x2F;pkg_name&#x2F; 路径下，\"></a>头文件的引用，在 ros1 下，cpp的 .h 头文件要放在功能包目录下的include&#x2F;pkg_name&#x2F; 路径下，</h3><p><img src=\"http://picbed.yanzu.tech/img/learn_ros1/3.png\"></p>\n<h3 id=\"创建好了头文件之后，需要引用头文件的cpp文件需要导入该头文件\"><a href=\"#创建好了头文件之后，需要引用头文件的cpp文件需要导入该头文件\" class=\"headerlink\" title=\"创建好了头文件之后，需要引用头文件的cpp文件需要导入该头文件\"></a>创建好了头文件之后，需要引用头文件的cpp文件需要导入该头文件</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;pkg_name/xxx.h&quot;</span></span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编译功能包时，cpp文件中引用的头文件，ros会在该功能包的-include-文件下开始找，给出以上路径后就可以精确找到被引用的头文件\"><a href=\"#编译功能包时，cpp文件中引用的头文件，ros会在该功能包的-include-文件下开始找，给出以上路径后就可以精确找到被引用的头文件\" class=\"headerlink\" title=\"编译功能包时，cpp文件中引用的头文件，ros会在该功能包的 include&#x2F; 文件下开始找，给出以上路径后就可以精确找到被引用的头文件\"></a>编译功能包时，cpp文件中引用的头文件，ros会在该功能包的 include&#x2F; 文件下开始找，给出以上路径后就可以精确找到被引用的头文件</h3><h3 id=\"如果要引用同一工作空间其他包的头文件，那么在创建功能包时，需要依赖要引用头文件所在的包\"><a href=\"#如果要引用同一工作空间其他包的头文件，那么在创建功能包时，需要依赖要引用头文件所在的包\" class=\"headerlink\" title=\"如果要引用同一工作空间其他包的头文件，那么在创建功能包时，需要依赖要引用头文件所在的包\"></a>如果要引用同一工作空间其他包的头文件，那么在创建功能包时，需要依赖要引用头文件所在的包</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">catkin_create_pkg my_pkg roscpp rospy std_msgs test_pkg</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"包创建好之后，他的CMakeList修改没有区别跟其他的包，被引用头文件的包的CMakeList文件需要修改\"><a href=\"#包创建好之后，他的CMakeList修改没有区别跟其他的包，被引用头文件的包的CMakeList文件需要修改\" class=\"headerlink\" title=\"包创建好之后，他的CMakeList修改没有区别跟其他的包，被引用头文件的包的CMakeList文件需要修改\"></a>包创建好之后，他的CMakeList修改没有区别跟其他的包，被引用头文件的包的CMakeList文件需要修改</h3><figure class=\"highlight txt\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">catkin_package(</span><br><span class=\"line\">  INCLUDE_DIRS include</span><br><span class=\"line\">#  LIBRARIES test_pkg</span><br><span class=\"line\">#  CATKIN_DEPENDS roscpp rospy std_msgs</span><br><span class=\"line\">#  DEPENDS system_lib</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"这样修改之后，引用该包的头文件的包就能找到根据对应的路径找到需要引用的头文件了\"><a href=\"#这样修改之后，引用该包的头文件的包就能找到根据对应的路径找到需要引用的头文件了\" class=\"headerlink\" title=\"这样修改之后，引用该包的头文件的包就能找到根据对应的路径找到需要引用的头文件了\"></a>这样修改之后，引用该包的头文件的包就能找到根据对应的路径找到需要引用的头文件了</h3></blockquote>\n<h3 id=\"launch文件的编写\"><a href=\"#launch文件的编写\" class=\"headerlink\" title=\"launch文件的编写\"></a>launch文件的编写</h3><blockquote>\n<h3 id=\"因为，一个完整的ros程序，需要多个节点来完成所需实现的功能，而roslaunch一次性执行好多个节点的方法显然不够合理和高效，由此引出-launch文件。\"><a href=\"#因为，一个完整的ros程序，需要多个节点来完成所需实现的功能，而roslaunch一次性执行好多个节点的方法显然不够合理和高效，由此引出-launch文件。\" class=\"headerlink\" title=\"因为，一个完整的ros程序，需要多个节点来完成所需实现的功能，而roslaunch一次性执行好多个节点的方法显然不够合理和高效，由此引出 launch文件。\"></a>因为，一个完整的ros程序，需要多个节点来完成所需实现的功能，而roslaunch一次性执行好多个节点的方法显然不够合理和高效，由此引出 launch文件。</h3><h3 id=\"launch文件就是一个xml格式的脚本文件，把需要启动的节点都写进launch文件-中，这样就可以通过roslaunch工具来调用launch文件，执行这个脚本文件就一次性启动所有的节点程序。\"><a href=\"#launch文件就是一个xml格式的脚本文件，把需要启动的节点都写进launch文件-中，这样就可以通过roslaunch工具来调用launch文件，执行这个脚本文件就一次性启动所有的节点程序。\" class=\"headerlink\" title=\"launch文件就是一个xml格式的脚本文件，把需要启动的节点都写进launch文件 中，这样就可以通过roslaunch工具来调用launch文件，执行这个脚本文件就一次性启动所有的节点程序。\"></a>launch文件就是一个xml格式的脚本文件，把需要启动的节点都写进launch文件 中，这样就可以通过roslaunch工具来调用launch文件，执行这个脚本文件就一次性启动所有的节点程序。</h3><h3 id=\"创建一个包-robot-bringup，在其路径下创建一个文件-launch，在-launch-下创建并编辑-startup-launch-文件，用于同时启动多个节点\"><a href=\"#创建一个包-robot-bringup，在其路径下创建一个文件-launch，在-launch-下创建并编辑-startup-launch-文件，用于同时启动多个节点\" class=\"headerlink\" title=\"创建一个包 robot_bringup，在其路径下创建一个文件 launch，在 launch&#x2F; 下创建并编辑 startup.launch 文件，用于同时启动多个节点\"></a>创建一个包 robot_bringup，在其路径下创建一个文件 launch，在 launch&#x2F; 下创建并编辑 startup.launch 文件，用于同时启动多个节点</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">catkin_create_pkg robot_bringup</span><br><span class=\"line\">cd robot_bringup</span><br><span class=\"line\">mkdir launch</span><br><span class=\"line\">cd launch/</span><br><span class=\"line\">touch startup.launch</span><br><span class=\"line\">subl startup.launch</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编辑使之能同时运行-test-pkg-node-和-my-pkg-node-两个节点\"><a href=\"#编辑使之能同时运行-test-pkg-node-和-my-pkg-node-两个节点\" class=\"headerlink\" title=\"编辑使之能同时运行 test_pkg_node 和 my_pkg_node 两个节点\"></a>编辑使之能同时运行 test_pkg_node 和 my_pkg_node 两个节点</h3><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># <span class=\"tag\">&lt;<span class=\"name\">launch</span>&gt;</span> 是根标签</span><br><span class=\"line\"># <span class=\"tag\">&lt;<span class=\"name\">node</span>&gt;</span> 是节点标签,至少要有三个属性 pkg type name, type是指节点名,name给所运行的节点指定名称,他会覆盖ros::init的节点名, output 可以将单个节点的标准输出到终端</span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">launch</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">node</span> <span class=\"attr\">pkg</span>=<span class=\"string\">&quot;test_pkg&quot;</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;test_pkg_node&quot;</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;test_pkg_node&quot;</span> <span class=\"attr\">output</span>=<span class=\"string\">&quot;screen&quot;</span>/&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">node</span> <span class=\"attr\">pkg</span>=<span class=\"string\">&quot;my_pkg&quot;</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;my_pkg_node&quot;</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;my_pkg_node&quot;</span> <span class=\"attr\">output</span>=<span class=\"string\">&quot;screen&quot;</span>/&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">launch</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编译并运行\"><a href=\"#编译并运行\" class=\"headerlink\" title=\"编译并运行\"></a>编译并运行</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">catkin_make</span><br><span class=\"line\">source ~/.bashrc</span><br><span class=\"line\">roslaunch robot_bringup startup.launch</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros1/4.png\"></p>\n<h3 id=\"使用-include-标签，可以在当前-launch-文件中调用另外一个包的-launch文件\"><a href=\"#使用-include-标签，可以在当前-launch-文件中调用另外一个包的-launch文件\" class=\"headerlink\" title=\"使用 include 标签，可以在当前 launch 文件中调用另外一个包的 launch文件\"></a>使用 include 标签，可以在当前 launch 文件中调用另外一个包的 launch文件</h3><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># find 命令可以直接搜索功能包的位置，代替直接使用绝对路径</span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">include</span>  <span class=\"attr\">file</span>=<span class=\"string\">&quot;$(find package-name)/launch-file-name&quot;</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"创建新的功能包-third-pkg，在-src-中创建并编写-third-pkg-cpp\"><a href=\"#创建新的功能包-third-pkg，在-src-中创建并编写-third-pkg-cpp\" class=\"headerlink\" title=\"创建新的功能包 third_pkg，在 src 中创建并编写 third_pkg.cpp\"></a>创建新的功能包 third_pkg，在 src 中创建并编写 third_pkg.cpp</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">catkin_create_pkg third_pkg roscpp rospy std_msgs</span><br><span class=\"line\">cd third_pkg/src</span><br><span class=\"line\">touch third_pkg.cpp</span><br><span class=\"line\">subl third_pkg.cpp</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;ros/ros.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&quot;std_msgs/String.h&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span><span class=\"string\">&lt;sstream&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc,<span class=\"type\">char</span> **argv)</span></span>&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\tros::<span class=\"built_in\">init</span>(argc,argv,<span class=\"string\">&quot;third_pkg&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">\tros::NodeHandle n;<span class=\"comment\">// 更新话题的消息格式为自定义的消息格式</span></span><br><span class=\"line\"></span><br><span class=\"line\">\tros::Publisher chatter_pub = n.<span class=\"built_in\">advertise</span>&lt;std_msgs::String&gt;(<span class=\"string\">&quot;third_pkg&quot;</span>,<span class=\"number\">1000</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"function\">ros::Rate <span class=\"title\">loop_rate</span><span class=\"params\">(<span class=\"number\">10</span>)</span></span>;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"type\">int</span> count = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">while</span>(ros::<span class=\"built_in\">ok</span>())&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tstd_msgs::String msg;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tstd::stringstream ss;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tss&lt;&lt;<span class=\"string\">&quot;third pkg:&quot;</span>&lt;&lt;count;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tmsg.data = ss.<span class=\"built_in\">str</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"built_in\">ROS_INFO</span>(<span class=\"string\">&quot;%s&quot;</span>,msg.data.<span class=\"built_in\">c_str</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tchatter_pub.<span class=\"built_in\">publish</span>(msg);<span class=\"comment\">//将消息发布到话题中</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\tros::<span class=\"built_in\">spinOnce</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tloop_rate.<span class=\"built_in\">sleep</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t++count;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"修改CMakeList文件\"><a href=\"#修改CMakeList文件\" class=\"headerlink\" title=\"修改CMakeList文件\"></a>修改CMakeList文件</h3><figure class=\"highlight txt\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_executable(third_pkg_node src/third_pkg.cpp)</span><br><span class=\"line\"></span><br><span class=\"line\">target_link_libraries(third_pkg_node</span><br><span class=\"line\">  $&#123;catkin_LIBRARIES&#125;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-third-pkg-下创建一个-launch-文件夹，并在该文件中创建并编译-third-pkg-launch-文件\"><a href=\"#在-third-pkg-下创建一个-launch-文件夹，并在该文件中创建并编译-third-pkg-launch-文件\" class=\"headerlink\" title=\"在 third_pkg&#x2F; 下创建一个 launch 文件夹，并在该文件中创建并编译 third_pkg.launch 文件\"></a>在 third_pkg&#x2F; 下创建一个 launch 文件夹，并在该文件中创建并编译 third_pkg.launch 文件</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir launch</span><br><span class=\"line\">cd launch</span><br><span class=\"line\">touch third_pkg.launch</span><br><span class=\"line\">subl third_pkg.launch</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">launch</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">node</span> <span class=\"attr\">pkg</span>=<span class=\"string\">&quot;third_pkg&quot;</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;third_pkg_node&quot;</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;third_pkg_node&quot;</span> <span class=\"attr\">output</span>=<span class=\"string\">&quot;screen&quot;</span>/&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">launch</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"修改-robot-bringup-launch-下的-launch-文件\"><a href=\"#修改-robot-bringup-launch-下的-launch-文件\" class=\"headerlink\" title=\"修改 robot_bringup&#x2F;launch&#x2F; 下的 launch 文件\"></a>修改 robot_bringup&#x2F;launch&#x2F; 下的 launch 文件</h3><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">launch</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">node</span> <span class=\"attr\">pkg</span>=<span class=\"string\">&quot;test_pkg&quot;</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;test_pkg_node&quot;</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;test_pkg_node&quot;</span> <span class=\"attr\">respawn</span>=<span class=\"string\">&quot;true&quot;</span> <span class=\"attr\">output</span>=<span class=\"string\">&quot;screen&quot;</span>/&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">node</span> <span class=\"attr\">pkg</span>=<span class=\"string\">&quot;my_pkg&quot;</span> <span class=\"attr\">type</span>=<span class=\"string\">&quot;my_pkg_node&quot;</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;my_pkg_node&quot;</span> <span class=\"attr\">required</span>=<span class=\"string\">&quot;true&quot;</span> <span class=\"attr\">output</span>=<span class=\"string\">&quot;screen&quot;</span>/&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">include</span> <span class=\"attr\">file</span>=<span class=\"string\">&quot;$(find third_pkg)/launch/third_pkg.launch&quot;</span>/&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">launch</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"然后编译运行\"><a href=\"#然后编译运行\" class=\"headerlink\" title=\"然后编译运行\"></a>然后编译运行</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">catkin_make</span><br><span class=\"line\">source ~/.bashrc</span><br><span class=\"line\">roslaunch robot_bringup startup.launch</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros1/5.png\"></p>\n</blockquote>\n<h3 id=\"好久没更新了，damn\"><a href=\"#好久没更新了，damn\" class=\"headerlink\" title=\"好久没更新了，damn!\"></a>好久没更新了，damn!</h3>"},{"title":"第九弹","data":"2025-02-20T11:59:00.000Z","updated":"2025-02-20T11:59:00.000Z","type":"ROS2","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p11.png","_content":"\n# cpp话题订阅与发布\n\n## 发布速度控制小海龟画圆\n\n> ### 查看小海龟节点的话题列表\n>\n> ```shell\n> ros2 topic list -t\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_22.png)\n>\n> ### 在 chapter_3/topic_ws/src/ 下创建包 demo_cpp_topic\n>\n> ```shell\n> ros2 pkg create demo_cpp_topic --build-type ament_cmake --dependencies rclcpp geometry_msgs turtlesim --license Apache-2.0\n> ```\n>\n> ###  进入 demo_cpp_topic/src 创建 turtlesim_circle.cpp，其内容如下\n>\n> ```cpp\n> #include \"rclcpp/rclcpp.hpp\"\n> #include \"geometry_msgs/msg/twist.hpp\"\t// 引入geometry_msgs中Twist消息类型，用于控制机器人速度\n> #include <chrono>\t\t\t\t\t\t// 引入chrono库用于处理时间\n> \n> using namespace std::chrono_literals;\t// 方便使用时间单位如1s, 100ms等\n> \n> // 继承 rclcpp::Node，表示一个ROS2节点\n> class TurtleCircleNode : public rclcpp::Node{\n> \n> private:\n> \t// 定义定时器的智能指针，用于定时触发回调函数\n> \trclcpp::TimerBase::SharedPtr timer_;\n>     \n> \t// 定义发布者的智能指针，用于发布Twist消息\n> \trclcpp::Publisher<geometry_msgs::msg::Twist>::SharedPtr publisher_;\n> \t\n> public:\n>     // 构造函数，初始化节点并创建定时器和发布者\n> \texplicit TurtleCircleNode(const std::string &node_name) : Node(node_name){\n> \t\t// 创建发布者，负责将Twist消息发布到指定话题\"/turtle1/cmd_vel\"上，队列大小为10\n> \t\tpublisher_ = this->create_publisher<geometry_msgs::msg::Twist>(\"/turtle1/cmd_vel\", 10);\n> \t\t// 传入间隔周期，传入回调函数(可以使用成员函数作为回调函数，也可以使用 Lambda)\n> \t\t// 成员函数作为回调函数需要使用bind与当前对象进行绑定\n> \t\t// bind(param1, param2, param3) param1:函数的位置，param2:指向当前对象的指针, param3:参数列表占位符\n>         // 创建一个定时器，周期为1000ms（1秒），定时调用timer_callback()回调函数\n>         // 由于 timer_callback() 是一个不带参数的成员函数，所以参数列表中只有两个参数，如果成员函数有参数，那么bind的参数列表中也要相应的添加对应参数\n> \t\ttimer_ = this->create_wall_timer(1000ms, std::bind(&TurtleCircleNode::timer_callback, this));\n> \t};\n> \t// 这里使用成员函数来作为回调函数\n>     // 定时器回调函数，按设定的周期触发\n> \tvoid timer_callback(){\n> \t\t// 创建Twist消息对象\n> \t\tauto msg = geometry_msgs::msg::Twist();\n>         // 设置线速度：x轴方向速度为1.0，y轴方向为0.0（不移动）\n> \t\tmsg.linear.x = 1.0;\n> \t\tmsg.linear.y = 0.0;\n>         // 设置角速度：z轴角速度为0.5，表示沿着Z轴旋转\n> \t\tmsg.angular.z = 0.5;\n> \t\t// 发布消息，通过发布者将控制命令发布出去\n> \t\tpublisher_->publish(msg);\n> \t};\n> \n> };\n> \n> int main(int argc, char** argv){\n> \t\n> \trclcpp::init(argc, argv);\n> \tauto node = std::make_shared<TurtleCircleNode>(\"turtle_circle\");\n> \trclcpp::spin(node);\n> \t\n> \trclcpp::shutdown();\n> \treturn 0;\n> }\n> ```\n>\n> ### 如果上面的回调函数使用 Lambda 表达式实现，则写成这样\n>\n> ```cpp\n> timer_ = this->create_wall_timer(\n>     1000ms, \n>     [this]() { \n>         // Lambda 捕获当前对象，并在 Lambda 内部调用成员函数\n>         auto msg = geometry_msgs::msg::Twist();\n>         msg.linear.x = 1.0;\n>         msg.linear.y = 0.0;\n>         msg.angular.z = 0.5;\n>         publisher_->publish(msg);\n>     }\n> );\n> ```\n>\n> ### 在 CMakeLists 中添加相应语句\n>\n> ```cmake\n> # 添加可执行文件\n> add_executable(turtle_circle src/turtlesim_circle.cpp)\n> # 包含依赖\n> ament_target_dependencies(turtle_circle rclcpp geometry_msgs)\n> # 拷贝节点到 install/lib 下\n> install(TARGETS turtle_circle\n> DESTINATION lib/${PROJECT_NAME}\n> )\n> ```\n>\n> ### 编译并运行(先运行小海龟节点)\n>\n> ```shell\n> cd ~/learn_ros2/chapter_3/topic_ws/\n> colcon build\n> source install/setup.bash\n> ros2 run demo_cpp_topic turtle_circle\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_23.png)\n>\n> ### 运行起来之后，在新的终端可以查看相关信息\n>\n> ```shell\n> ros2 topic list -t\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_24.png)\n>\n> ```shell\n> ros2 node info /turtle_circle\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_25.png)\n>\n> ```shell\n> ros2 topic echo /turtle1/cmd_vel\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_26.png)\n\n## 订阅pose实现闭环控制\n\n> ### 查看当前节点的话题列表\n>\n> ```shell\n> ros2 topic list -t\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_24.png)\n>\n> ### 找到小海龟位置信息的话题，/turtle1/pose，对应的消息接口为 turtlesim/msg/Pose\n>\n> ### 查看消息接口的定义\n>\n> ```shell\n> ros2 interface show turtlesim/msg/Pose\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_27.png)\n>\n> ### 在 demo_cpp_topic/src 下创建文件 turtle_control.cpp，其内容如下\n>\n> ```cpp\n> #include \"rclcpp/rclcpp.hpp\"\n> #include \"geometry_msgs/msg/twist.hpp\"\n> #include \"turtlesim/msg/pose.hpp\"\n> \n> using namespace std::chrono_literals;\n> \n> class TurtleControlNode : public rclcpp::Node{\n> \n> private:\n> \t// 发布者的智能指针\n> \trclcpp::Publisher<geometry_msgs::msg::Twist>::SharedPtr publisher_;\n> \trclcpp::Subscription<turtlesim::msg::Pose>::SharedPtr subscriber_;\n> \tdouble target_x_{1.0};\n> \tdouble target_y_{1.0};\n> \tdouble k_{1.0};\t\t// 比例系数\n> \tdouble max_speed_{3.0};\t// 最大速度\n> \n> public:\n> \texplicit TurtleControlNode(const std::string &node_name) : Node(node_name){\n> \t\t// 创建发布者\n> \t\tpublisher_ = this->create_publisher<geometry_msgs::msg::Twist>(\"/turtle1/cmd_vel\", 10);\n> \t\tsubscriber_ = this->create_subscription<turtlesim::msg::Pose>(\"/turtle1/pose\", 10, std::bind(&TurtleControlNode::on_pose_received_, this, std::placeholders::_1));\n> \t};\n> \t\n> \tvoid on_pose_received_(const turtlesim::msg::Pose::SharedPtr pose){\n> \t\t// 获取小海龟当前位置\n> \t\tdouble current_x = pose->x;\n> \t\tdouble current_y = pose->y;\n> \t\tRCLCPP_INFO(get_logger(), \"当前位置 x: %f,y: %f \", current_x, current_y);\n> \t\t// 计算当前位置与目标位置的距离差和角度差\n> \t\tdouble distance = std::sqrt((target_x_ - current_x)*(target_x_ - current_x) + (target_y_ - current_y)*(target_y_ - current_y));\n> \t\t\n> \t\tdouble angle = std::atan2((target_y_ - current_y), (target_x_ - current_x)) - pose->theta;\n> \t\t\n> \t\t// 控制策略\n> \t\tauto msg = geometry_msgs::msg::Twist();\n> \t\tif(distance > 0.1){\n> \t\t\tif(fabs(angle) > 0.2){\n> \t\t\t\tmsg.angular.z = fabs(angle);\n> \t\t\t}\n> \t\t\telse{\n> \t\t\t\tmsg.linear.x = k_ * distance;\n> \t\t\t}\n> \t\t}\n> \t\t// 限制线速度最大值\n> \t\tif(msg.linear.x > max_speed_){\n> \t\t\tmsg.linear.x = max_speed_;\n> \t\t}\n> \t\tpublisher_->publish(msg);\n> \t};\n> \t\n> };\n> \n> int main(int argc, char** argv){\n> \t\n> \trclcpp::init(argc, argv);\n> \tauto node = std::make_shared<TurtleControlNode>(\"turtle_control\");\n> \trclcpp::spin(node);\n> \t\n> \trclcpp::shutdown();\n> \treturn 0;\n> }\n> ```\n>\n> ### 在 CMakeLists 文件中添加相应语句\n>\n> ```shell\n> # 添加可执行文件\n> add_executable(turtle_control src/turtle_control.cpp)\n> # 添加依赖\n> ament_target_dependencies(turtle_control rclcpp geometry_msgs turtlesim)\n> # 拷贝节点到 install/lib\n> install(TARGETS turtle_circle turtle_control\n> DESTINATION lib/${PROJECT_NAME}\n> )\n> ```\n>\n> ### 编译运行(先运行 turtle 节点)\n>\n> ```shell\n> cd ~/learn_ros2/chapter_3/topic_ws/\n> colcon build\n> source install/setup.bash\n> ros2 run demo_cpp_topic turtle_control\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_28.png)\n","source":"_posts/11.md","raw":"---\ntitle: 第九弹\ndata: 2025-02-20 19:59:00\nupdated: 2025-02-20 19:59:00\ntype: ROS2\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p11.png\ntags:\n  - ROS2\n  - Learning\n---\n\n# cpp话题订阅与发布\n\n## 发布速度控制小海龟画圆\n\n> ### 查看小海龟节点的话题列表\n>\n> ```shell\n> ros2 topic list -t\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_22.png)\n>\n> ### 在 chapter_3/topic_ws/src/ 下创建包 demo_cpp_topic\n>\n> ```shell\n> ros2 pkg create demo_cpp_topic --build-type ament_cmake --dependencies rclcpp geometry_msgs turtlesim --license Apache-2.0\n> ```\n>\n> ###  进入 demo_cpp_topic/src 创建 turtlesim_circle.cpp，其内容如下\n>\n> ```cpp\n> #include \"rclcpp/rclcpp.hpp\"\n> #include \"geometry_msgs/msg/twist.hpp\"\t// 引入geometry_msgs中Twist消息类型，用于控制机器人速度\n> #include <chrono>\t\t\t\t\t\t// 引入chrono库用于处理时间\n> \n> using namespace std::chrono_literals;\t// 方便使用时间单位如1s, 100ms等\n> \n> // 继承 rclcpp::Node，表示一个ROS2节点\n> class TurtleCircleNode : public rclcpp::Node{\n> \n> private:\n> \t// 定义定时器的智能指针，用于定时触发回调函数\n> \trclcpp::TimerBase::SharedPtr timer_;\n>     \n> \t// 定义发布者的智能指针，用于发布Twist消息\n> \trclcpp::Publisher<geometry_msgs::msg::Twist>::SharedPtr publisher_;\n> \t\n> public:\n>     // 构造函数，初始化节点并创建定时器和发布者\n> \texplicit TurtleCircleNode(const std::string &node_name) : Node(node_name){\n> \t\t// 创建发布者，负责将Twist消息发布到指定话题\"/turtle1/cmd_vel\"上，队列大小为10\n> \t\tpublisher_ = this->create_publisher<geometry_msgs::msg::Twist>(\"/turtle1/cmd_vel\", 10);\n> \t\t// 传入间隔周期，传入回调函数(可以使用成员函数作为回调函数，也可以使用 Lambda)\n> \t\t// 成员函数作为回调函数需要使用bind与当前对象进行绑定\n> \t\t// bind(param1, param2, param3) param1:函数的位置，param2:指向当前对象的指针, param3:参数列表占位符\n>         // 创建一个定时器，周期为1000ms（1秒），定时调用timer_callback()回调函数\n>         // 由于 timer_callback() 是一个不带参数的成员函数，所以参数列表中只有两个参数，如果成员函数有参数，那么bind的参数列表中也要相应的添加对应参数\n> \t\ttimer_ = this->create_wall_timer(1000ms, std::bind(&TurtleCircleNode::timer_callback, this));\n> \t};\n> \t// 这里使用成员函数来作为回调函数\n>     // 定时器回调函数，按设定的周期触发\n> \tvoid timer_callback(){\n> \t\t// 创建Twist消息对象\n> \t\tauto msg = geometry_msgs::msg::Twist();\n>         // 设置线速度：x轴方向速度为1.0，y轴方向为0.0（不移动）\n> \t\tmsg.linear.x = 1.0;\n> \t\tmsg.linear.y = 0.0;\n>         // 设置角速度：z轴角速度为0.5，表示沿着Z轴旋转\n> \t\tmsg.angular.z = 0.5;\n> \t\t// 发布消息，通过发布者将控制命令发布出去\n> \t\tpublisher_->publish(msg);\n> \t};\n> \n> };\n> \n> int main(int argc, char** argv){\n> \t\n> \trclcpp::init(argc, argv);\n> \tauto node = std::make_shared<TurtleCircleNode>(\"turtle_circle\");\n> \trclcpp::spin(node);\n> \t\n> \trclcpp::shutdown();\n> \treturn 0;\n> }\n> ```\n>\n> ### 如果上面的回调函数使用 Lambda 表达式实现，则写成这样\n>\n> ```cpp\n> timer_ = this->create_wall_timer(\n>     1000ms, \n>     [this]() { \n>         // Lambda 捕获当前对象，并在 Lambda 内部调用成员函数\n>         auto msg = geometry_msgs::msg::Twist();\n>         msg.linear.x = 1.0;\n>         msg.linear.y = 0.0;\n>         msg.angular.z = 0.5;\n>         publisher_->publish(msg);\n>     }\n> );\n> ```\n>\n> ### 在 CMakeLists 中添加相应语句\n>\n> ```cmake\n> # 添加可执行文件\n> add_executable(turtle_circle src/turtlesim_circle.cpp)\n> # 包含依赖\n> ament_target_dependencies(turtle_circle rclcpp geometry_msgs)\n> # 拷贝节点到 install/lib 下\n> install(TARGETS turtle_circle\n> DESTINATION lib/${PROJECT_NAME}\n> )\n> ```\n>\n> ### 编译并运行(先运行小海龟节点)\n>\n> ```shell\n> cd ~/learn_ros2/chapter_3/topic_ws/\n> colcon build\n> source install/setup.bash\n> ros2 run demo_cpp_topic turtle_circle\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_23.png)\n>\n> ### 运行起来之后，在新的终端可以查看相关信息\n>\n> ```shell\n> ros2 topic list -t\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_24.png)\n>\n> ```shell\n> ros2 node info /turtle_circle\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_25.png)\n>\n> ```shell\n> ros2 topic echo /turtle1/cmd_vel\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_26.png)\n\n## 订阅pose实现闭环控制\n\n> ### 查看当前节点的话题列表\n>\n> ```shell\n> ros2 topic list -t\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_24.png)\n>\n> ### 找到小海龟位置信息的话题，/turtle1/pose，对应的消息接口为 turtlesim/msg/Pose\n>\n> ### 查看消息接口的定义\n>\n> ```shell\n> ros2 interface show turtlesim/msg/Pose\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_27.png)\n>\n> ### 在 demo_cpp_topic/src 下创建文件 turtle_control.cpp，其内容如下\n>\n> ```cpp\n> #include \"rclcpp/rclcpp.hpp\"\n> #include \"geometry_msgs/msg/twist.hpp\"\n> #include \"turtlesim/msg/pose.hpp\"\n> \n> using namespace std::chrono_literals;\n> \n> class TurtleControlNode : public rclcpp::Node{\n> \n> private:\n> \t// 发布者的智能指针\n> \trclcpp::Publisher<geometry_msgs::msg::Twist>::SharedPtr publisher_;\n> \trclcpp::Subscription<turtlesim::msg::Pose>::SharedPtr subscriber_;\n> \tdouble target_x_{1.0};\n> \tdouble target_y_{1.0};\n> \tdouble k_{1.0};\t\t// 比例系数\n> \tdouble max_speed_{3.0};\t// 最大速度\n> \n> public:\n> \texplicit TurtleControlNode(const std::string &node_name) : Node(node_name){\n> \t\t// 创建发布者\n> \t\tpublisher_ = this->create_publisher<geometry_msgs::msg::Twist>(\"/turtle1/cmd_vel\", 10);\n> \t\tsubscriber_ = this->create_subscription<turtlesim::msg::Pose>(\"/turtle1/pose\", 10, std::bind(&TurtleControlNode::on_pose_received_, this, std::placeholders::_1));\n> \t};\n> \t\n> \tvoid on_pose_received_(const turtlesim::msg::Pose::SharedPtr pose){\n> \t\t// 获取小海龟当前位置\n> \t\tdouble current_x = pose->x;\n> \t\tdouble current_y = pose->y;\n> \t\tRCLCPP_INFO(get_logger(), \"当前位置 x: %f,y: %f \", current_x, current_y);\n> \t\t// 计算当前位置与目标位置的距离差和角度差\n> \t\tdouble distance = std::sqrt((target_x_ - current_x)*(target_x_ - current_x) + (target_y_ - current_y)*(target_y_ - current_y));\n> \t\t\n> \t\tdouble angle = std::atan2((target_y_ - current_y), (target_x_ - current_x)) - pose->theta;\n> \t\t\n> \t\t// 控制策略\n> \t\tauto msg = geometry_msgs::msg::Twist();\n> \t\tif(distance > 0.1){\n> \t\t\tif(fabs(angle) > 0.2){\n> \t\t\t\tmsg.angular.z = fabs(angle);\n> \t\t\t}\n> \t\t\telse{\n> \t\t\t\tmsg.linear.x = k_ * distance;\n> \t\t\t}\n> \t\t}\n> \t\t// 限制线速度最大值\n> \t\tif(msg.linear.x > max_speed_){\n> \t\t\tmsg.linear.x = max_speed_;\n> \t\t}\n> \t\tpublisher_->publish(msg);\n> \t};\n> \t\n> };\n> \n> int main(int argc, char** argv){\n> \t\n> \trclcpp::init(argc, argv);\n> \tauto node = std::make_shared<TurtleControlNode>(\"turtle_control\");\n> \trclcpp::spin(node);\n> \t\n> \trclcpp::shutdown();\n> \treturn 0;\n> }\n> ```\n>\n> ### 在 CMakeLists 文件中添加相应语句\n>\n> ```shell\n> # 添加可执行文件\n> add_executable(turtle_control src/turtle_control.cpp)\n> # 添加依赖\n> ament_target_dependencies(turtle_control rclcpp geometry_msgs turtlesim)\n> # 拷贝节点到 install/lib\n> install(TARGETS turtle_circle turtle_control\n> DESTINATION lib/${PROJECT_NAME}\n> )\n> ```\n>\n> ### 编译运行(先运行 turtle 节点)\n>\n> ```shell\n> cd ~/learn_ros2/chapter_3/topic_ws/\n> colcon build\n> source install/setup.bash\n> ros2 run demo_cpp_topic turtle_control\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/learn_ros2/pic_28.png)\n","slug":"11","published":1,"date":"2025-02-20T11:58:43.163Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cr0000ziku48qtn35ux","content":"<h1 id=\"cpp话题订阅与发布\"><a href=\"#cpp话题订阅与发布\" class=\"headerlink\" title=\"cpp话题订阅与发布\"></a>cpp话题订阅与发布</h1><h2 id=\"发布速度控制小海龟画圆\"><a href=\"#发布速度控制小海龟画圆\" class=\"headerlink\" title=\"发布速度控制小海龟画圆\"></a>发布速度控制小海龟画圆</h2><blockquote>\n<h3 id=\"查看小海龟节点的话题列表\"><a href=\"#查看小海龟节点的话题列表\" class=\"headerlink\" title=\"查看小海龟节点的话题列表\"></a>查看小海龟节点的话题列表</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 topic list -t</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_22.png\"></p>\n<h3 id=\"在-chapter-3-topic-ws-src-下创建包-demo-cpp-topic\"><a href=\"#在-chapter-3-topic-ws-src-下创建包-demo-cpp-topic\" class=\"headerlink\" title=\"在 chapter_3&#x2F;topic_ws&#x2F;src&#x2F; 下创建包 demo_cpp_topic\"></a>在 chapter_3&#x2F;topic_ws&#x2F;src&#x2F; 下创建包 demo_cpp_topic</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 pkg create demo_cpp_topic --build-type ament_cmake --dependencies rclcpp geometry_msgs turtlesim --license Apache-2.0</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"进入-demo-cpp-topic-src-创建-turtlesim-circle-cpp，其内容如下\"><a href=\"#进入-demo-cpp-topic-src-创建-turtlesim-circle-cpp，其内容如下\" class=\"headerlink\" title=\"进入 demo_cpp_topic&#x2F;src 创建 turtlesim_circle.cpp，其内容如下\"></a>进入 demo_cpp_topic&#x2F;src 创建 turtlesim_circle.cpp，其内容如下</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;rclcpp/rclcpp.hpp&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;geometry_msgs/msg/twist.hpp&quot;</span>\t<span class=\"comment\">// 引入geometry_msgs中Twist消息类型，用于控制机器人速度</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;chrono&gt;</span>\t\t\t\t\t\t<span class=\"comment\">// 引入chrono库用于处理时间</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std::chrono_literals;\t<span class=\"comment\">// 方便使用时间单位如1s, 100ms等</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 继承 rclcpp::Node，表示一个ROS2节点</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">TurtleCircleNode</span> : <span class=\"keyword\">public</span> rclcpp::Node&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span>:</span><br><span class=\"line\">\t<span class=\"comment\">// 定义定时器的智能指针，用于定时触发回调函数</span></span><br><span class=\"line\">\trclcpp::TimerBase::SharedPtr timer_;</span><br><span class=\"line\">    </span><br><span class=\"line\">\t<span class=\"comment\">// 定义发布者的智能指针，用于发布Twist消息</span></span><br><span class=\"line\">\trclcpp::Publisher&lt;geometry_msgs::msg::Twist&gt;::SharedPtr publisher_;</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"comment\">// 构造函数，初始化节点并创建定时器和发布者</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">TurtleCircleNode</span><span class=\"params\">(<span class=\"type\">const</span> std::string &amp;node_name)</span> : Node(node_name)&#123;</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// 创建发布者，负责将Twist消息发布到指定话题&quot;/turtle1/cmd_vel&quot;上，队列大小为10</span></span><br><span class=\"line\">\t\tpublisher_ = <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">create_publisher</span>&lt;geometry_msgs::msg::Twist&gt;(<span class=\"string\">&quot;/turtle1/cmd_vel&quot;</span>, <span class=\"number\">10</span>);</span><br><span class=\"line\">\t\t<span class=\"comment\">// 传入间隔周期，传入回调函数(可以使用成员函数作为回调函数，也可以使用 Lambda)</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// 成员函数作为回调函数需要使用bind与当前对象进行绑定</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// bind(param1, param2, param3) param1:函数的位置，param2:指向当前对象的指针, param3:参数列表占位符</span></span><br><span class=\"line\">        <span class=\"comment\">// 创建一个定时器，周期为1000ms（1秒），定时调用timer_callback()回调函数</span></span><br><span class=\"line\">        <span class=\"comment\">// 由于 timer_callback() 是一个不带参数的成员函数，所以参数列表中只有两个参数，如果成员函数有参数，那么bind的参数列表中也要相应的添加对应参数</span></span><br><span class=\"line\">\t\ttimer_ = <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">create_wall_timer</span>(<span class=\"number\">1000</span>ms, std::<span class=\"built_in\">bind</span>(&amp;TurtleCircleNode::timer_callback, <span class=\"keyword\">this</span>));</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">\t<span class=\"comment\">// 这里使用成员函数来作为回调函数</span></span><br><span class=\"line\">    <span class=\"comment\">// 定时器回调函数，按设定的周期触发</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">timer_callback</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 创建Twist消息对象</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">auto</span> msg = geometry_msgs::msg::<span class=\"built_in\">Twist</span>();</span><br><span class=\"line\">        <span class=\"comment\">// 设置线速度：x轴方向速度为1.0，y轴方向为0.0（不移动）</span></span><br><span class=\"line\">\t\tmsg.linear.x = <span class=\"number\">1.0</span>;</span><br><span class=\"line\">\t\tmsg.linear.y = <span class=\"number\">0.0</span>;</span><br><span class=\"line\">        <span class=\"comment\">// 设置角速度：z轴角速度为0.5，表示沿着Z轴旋转</span></span><br><span class=\"line\">\t\tmsg.angular.z = <span class=\"number\">0.5</span>;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 发布消息，通过发布者将控制命令发布出去</span></span><br><span class=\"line\">\t\tpublisher_-&gt;<span class=\"built_in\">publish</span>(msg);</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span>** argv)</span></span>&#123;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\trclcpp::<span class=\"built_in\">init</span>(argc, argv);</span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> node = std::<span class=\"built_in\">make_shared</span>&lt;TurtleCircleNode&gt;(<span class=\"string\">&quot;turtle_circle&quot;</span>);</span><br><span class=\"line\">\trclcpp::<span class=\"built_in\">spin</span>(node);</span><br><span class=\"line\">\t</span><br><span class=\"line\">\trclcpp::<span class=\"built_in\">shutdown</span>();</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"如果上面的回调函数使用-Lambda-表达式实现，则写成这样\"><a href=\"#如果上面的回调函数使用-Lambda-表达式实现，则写成这样\" class=\"headerlink\" title=\"如果上面的回调函数使用 Lambda 表达式实现，则写成这样\"></a>如果上面的回调函数使用 Lambda 表达式实现，则写成这样</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">timer_ = <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">create_wall_timer</span>(</span><br><span class=\"line\">    <span class=\"number\">1000</span>ms, </span><br><span class=\"line\">    [<span class=\"keyword\">this</span>]() &#123; </span><br><span class=\"line\">        <span class=\"comment\">// Lambda 捕获当前对象，并在 Lambda 内部调用成员函数</span></span><br><span class=\"line\">        <span class=\"keyword\">auto</span> msg = geometry_msgs::msg::<span class=\"built_in\">Twist</span>();</span><br><span class=\"line\">        msg.linear.x = <span class=\"number\">1.0</span>;</span><br><span class=\"line\">        msg.linear.y = <span class=\"number\">0.0</span>;</span><br><span class=\"line\">        msg.angular.z = <span class=\"number\">0.5</span>;</span><br><span class=\"line\">        publisher_-&gt;<span class=\"built_in\">publish</span>(msg);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-CMakeLists-中添加相应语句\"><a href=\"#在-CMakeLists-中添加相应语句\" class=\"headerlink\" title=\"在 CMakeLists 中添加相应语句\"></a>在 CMakeLists 中添加相应语句</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 添加可执行文件</span></span><br><span class=\"line\"><span class=\"keyword\">add_executable</span>(turtle_circle src/turtlesim_circle.cpp)</span><br><span class=\"line\"><span class=\"comment\"># 包含依赖</span></span><br><span class=\"line\">ament_target_dependencies(turtle_circle rclcpp geometry_msgs)</span><br><span class=\"line\"><span class=\"comment\"># 拷贝节点到 install/lib 下</span></span><br><span class=\"line\"><span class=\"keyword\">install</span>(TARGETS turtle_circle</span><br><span class=\"line\">DESTINATION lib/<span class=\"variable\">$&#123;PROJECT_NAME&#125;</span></span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编译并运行-先运行小海龟节点\"><a href=\"#编译并运行-先运行小海龟节点\" class=\"headerlink\" title=\"编译并运行(先运行小海龟节点)\"></a>编译并运行(先运行小海龟节点)</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/learn_ros2/chapter_3/topic_ws/</span><br><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 run demo_cpp_topic turtle_circle</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_23.png\"></p>\n<h3 id=\"运行起来之后，在新的终端可以查看相关信息\"><a href=\"#运行起来之后，在新的终端可以查看相关信息\" class=\"headerlink\" title=\"运行起来之后，在新的终端可以查看相关信息\"></a>运行起来之后，在新的终端可以查看相关信息</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 topic list -t</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_24.png\"></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 node info /turtle_circle</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_25.png\"></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 topic echo /turtle1/cmd_vel</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_26.png\"></p>\n</blockquote>\n<h2 id=\"订阅pose实现闭环控制\"><a href=\"#订阅pose实现闭环控制\" class=\"headerlink\" title=\"订阅pose实现闭环控制\"></a>订阅pose实现闭环控制</h2><blockquote>\n<h3 id=\"查看当前节点的话题列表\"><a href=\"#查看当前节点的话题列表\" class=\"headerlink\" title=\"查看当前节点的话题列表\"></a>查看当前节点的话题列表</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 topic list -t</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_24.png\"></p>\n<h3 id=\"找到小海龟位置信息的话题，-turtle1-pose，对应的消息接口为-turtlesim-msg-Pose\"><a href=\"#找到小海龟位置信息的话题，-turtle1-pose，对应的消息接口为-turtlesim-msg-Pose\" class=\"headerlink\" title=\"找到小海龟位置信息的话题，&#x2F;turtle1&#x2F;pose，对应的消息接口为 turtlesim&#x2F;msg&#x2F;Pose\"></a>找到小海龟位置信息的话题，&#x2F;turtle1&#x2F;pose，对应的消息接口为 turtlesim&#x2F;msg&#x2F;Pose</h3><h3 id=\"查看消息接口的定义\"><a href=\"#查看消息接口的定义\" class=\"headerlink\" title=\"查看消息接口的定义\"></a>查看消息接口的定义</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 interface show turtlesim/msg/Pose</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_27.png\"></p>\n<h3 id=\"在-demo-cpp-topic-src-下创建文件-turtle-control-cpp，其内容如下\"><a href=\"#在-demo-cpp-topic-src-下创建文件-turtle-control-cpp，其内容如下\" class=\"headerlink\" title=\"在 demo_cpp_topic&#x2F;src 下创建文件 turtle_control.cpp，其内容如下\"></a>在 demo_cpp_topic&#x2F;src 下创建文件 turtle_control.cpp，其内容如下</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;rclcpp/rclcpp.hpp&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;geometry_msgs/msg/twist.hpp&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;turtlesim/msg/pose.hpp&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std::chrono_literals;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">TurtleControlNode</span> : <span class=\"keyword\">public</span> rclcpp::Node&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span>:</span><br><span class=\"line\">\t<span class=\"comment\">// 发布者的智能指针</span></span><br><span class=\"line\">\trclcpp::Publisher&lt;geometry_msgs::msg::Twist&gt;::SharedPtr publisher_;</span><br><span class=\"line\">\trclcpp::Subscription&lt;turtlesim::msg::Pose&gt;::SharedPtr subscriber_;</span><br><span class=\"line\">\t<span class=\"type\">double</span> target_x_&#123;<span class=\"number\">1.0</span>&#125;;</span><br><span class=\"line\">\t<span class=\"type\">double</span> target_y_&#123;<span class=\"number\">1.0</span>&#125;;</span><br><span class=\"line\">\t<span class=\"type\">double</span> k_&#123;<span class=\"number\">1.0</span>&#125;;\t\t<span class=\"comment\">// 比例系数</span></span><br><span class=\"line\">\t<span class=\"type\">double</span> max_speed_&#123;<span class=\"number\">3.0</span>&#125;;\t<span class=\"comment\">// 最大速度</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">TurtleControlNode</span><span class=\"params\">(<span class=\"type\">const</span> std::string &amp;node_name)</span> : Node(node_name)&#123;</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// 创建发布者</span></span><br><span class=\"line\">\t\tpublisher_ = <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">create_publisher</span>&lt;geometry_msgs::msg::Twist&gt;(<span class=\"string\">&quot;/turtle1/cmd_vel&quot;</span>, <span class=\"number\">10</span>);</span><br><span class=\"line\">\t\tsubscriber_ = <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">create_subscription</span>&lt;turtlesim::msg::Pose&gt;(<span class=\"string\">&quot;/turtle1/pose&quot;</span>, <span class=\"number\">10</span>, std::<span class=\"built_in\">bind</span>(&amp;TurtleControlNode::on_pose_received_, <span class=\"keyword\">this</span>, std::placeholders::_1));</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">on_pose_received_</span><span class=\"params\">(<span class=\"type\">const</span> turtlesim::msg::Pose::SharedPtr pose)</span></span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 获取小海龟当前位置</span></span><br><span class=\"line\">\t\t<span class=\"type\">double</span> current_x = pose-&gt;x;</span><br><span class=\"line\">\t\t<span class=\"type\">double</span> current_y = pose-&gt;y;</span><br><span class=\"line\">\t\t<span class=\"built_in\">RCLCPP_INFO</span>(<span class=\"built_in\">get_logger</span>(), <span class=\"string\">&quot;当前位置 x: %f,y: %f &quot;</span>, current_x, current_y);</span><br><span class=\"line\">\t\t<span class=\"comment\">// 计算当前位置与目标位置的距离差和角度差</span></span><br><span class=\"line\">\t\t<span class=\"type\">double</span> distance = std::<span class=\"built_in\">sqrt</span>((target_x_ - current_x)*(target_x_ - current_x) + (target_y_ - current_y)*(target_y_ - current_y));</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\t<span class=\"type\">double</span> angle = std::<span class=\"built_in\">atan2</span>((target_y_ - current_y), (target_x_ - current_x)) - pose-&gt;theta;</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\t<span class=\"comment\">// 控制策略</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">auto</span> msg = geometry_msgs::msg::<span class=\"built_in\">Twist</span>();</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span>(distance &gt; <span class=\"number\">0.1</span>)&#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span>(<span class=\"built_in\">fabs</span>(angle) &gt; <span class=\"number\">0.2</span>)&#123;</span><br><span class=\"line\">\t\t\t\tmsg.angular.z = <span class=\"built_in\">fabs</span>(angle);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">\t\t\t\tmsg.linear.x = k_ * distance;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 限制线速度最大值</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span>(msg.linear.x &gt; max_speed_)&#123;</span><br><span class=\"line\">\t\t\tmsg.linear.x = max_speed_;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tpublisher_-&gt;<span class=\"built_in\">publish</span>(msg);</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">\t</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span>** argv)</span></span>&#123;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\trclcpp::<span class=\"built_in\">init</span>(argc, argv);</span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> node = std::<span class=\"built_in\">make_shared</span>&lt;TurtleControlNode&gt;(<span class=\"string\">&quot;turtle_control&quot;</span>);</span><br><span class=\"line\">\trclcpp::<span class=\"built_in\">spin</span>(node);</span><br><span class=\"line\">\t</span><br><span class=\"line\">\trclcpp::<span class=\"built_in\">shutdown</span>();</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-CMakeLists-文件中添加相应语句\"><a href=\"#在-CMakeLists-文件中添加相应语句\" class=\"headerlink\" title=\"在 CMakeLists 文件中添加相应语句\"></a>在 CMakeLists 文件中添加相应语句</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">添加可执行文件</span></span><br><span class=\"line\">add_executable(turtle_control src/turtle_control.cpp)</span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">添加依赖</span></span><br><span class=\"line\">ament_target_dependencies(turtle_control rclcpp geometry_msgs turtlesim)</span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">拷贝节点到 install/lib</span></span><br><span class=\"line\">install(TARGETS turtle_circle turtle_control</span><br><span class=\"line\">DESTINATION lib/$&#123;PROJECT_NAME&#125;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编译运行-先运行-turtle-节点\"><a href=\"#编译运行-先运行-turtle-节点\" class=\"headerlink\" title=\"编译运行(先运行 turtle 节点)\"></a>编译运行(先运行 turtle 节点)</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/learn_ros2/chapter_3/topic_ws/</span><br><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 run demo_cpp_topic turtle_control</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_28.png\"></p>\n</blockquote>\n","cover_type":"img","excerpt":"","more":"<h1 id=\"cpp话题订阅与发布\"><a href=\"#cpp话题订阅与发布\" class=\"headerlink\" title=\"cpp话题订阅与发布\"></a>cpp话题订阅与发布</h1><h2 id=\"发布速度控制小海龟画圆\"><a href=\"#发布速度控制小海龟画圆\" class=\"headerlink\" title=\"发布速度控制小海龟画圆\"></a>发布速度控制小海龟画圆</h2><blockquote>\n<h3 id=\"查看小海龟节点的话题列表\"><a href=\"#查看小海龟节点的话题列表\" class=\"headerlink\" title=\"查看小海龟节点的话题列表\"></a>查看小海龟节点的话题列表</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 topic list -t</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_22.png\"></p>\n<h3 id=\"在-chapter-3-topic-ws-src-下创建包-demo-cpp-topic\"><a href=\"#在-chapter-3-topic-ws-src-下创建包-demo-cpp-topic\" class=\"headerlink\" title=\"在 chapter_3&#x2F;topic_ws&#x2F;src&#x2F; 下创建包 demo_cpp_topic\"></a>在 chapter_3&#x2F;topic_ws&#x2F;src&#x2F; 下创建包 demo_cpp_topic</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 pkg create demo_cpp_topic --build-type ament_cmake --dependencies rclcpp geometry_msgs turtlesim --license Apache-2.0</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"进入-demo-cpp-topic-src-创建-turtlesim-circle-cpp，其内容如下\"><a href=\"#进入-demo-cpp-topic-src-创建-turtlesim-circle-cpp，其内容如下\" class=\"headerlink\" title=\"进入 demo_cpp_topic&#x2F;src 创建 turtlesim_circle.cpp，其内容如下\"></a>进入 demo_cpp_topic&#x2F;src 创建 turtlesim_circle.cpp，其内容如下</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;rclcpp/rclcpp.hpp&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;geometry_msgs/msg/twist.hpp&quot;</span>\t<span class=\"comment\">// 引入geometry_msgs中Twist消息类型，用于控制机器人速度</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;chrono&gt;</span>\t\t\t\t\t\t<span class=\"comment\">// 引入chrono库用于处理时间</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std::chrono_literals;\t<span class=\"comment\">// 方便使用时间单位如1s, 100ms等</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 继承 rclcpp::Node，表示一个ROS2节点</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">TurtleCircleNode</span> : <span class=\"keyword\">public</span> rclcpp::Node&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span>:</span><br><span class=\"line\">\t<span class=\"comment\">// 定义定时器的智能指针，用于定时触发回调函数</span></span><br><span class=\"line\">\trclcpp::TimerBase::SharedPtr timer_;</span><br><span class=\"line\">    </span><br><span class=\"line\">\t<span class=\"comment\">// 定义发布者的智能指针，用于发布Twist消息</span></span><br><span class=\"line\">\trclcpp::Publisher&lt;geometry_msgs::msg::Twist&gt;::SharedPtr publisher_;</span><br><span class=\"line\">\t</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"comment\">// 构造函数，初始化节点并创建定时器和发布者</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">TurtleCircleNode</span><span class=\"params\">(<span class=\"type\">const</span> std::string &amp;node_name)</span> : Node(node_name)&#123;</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// 创建发布者，负责将Twist消息发布到指定话题&quot;/turtle1/cmd_vel&quot;上，队列大小为10</span></span><br><span class=\"line\">\t\tpublisher_ = <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">create_publisher</span>&lt;geometry_msgs::msg::Twist&gt;(<span class=\"string\">&quot;/turtle1/cmd_vel&quot;</span>, <span class=\"number\">10</span>);</span><br><span class=\"line\">\t\t<span class=\"comment\">// 传入间隔周期，传入回调函数(可以使用成员函数作为回调函数，也可以使用 Lambda)</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// 成员函数作为回调函数需要使用bind与当前对象进行绑定</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// bind(param1, param2, param3) param1:函数的位置，param2:指向当前对象的指针, param3:参数列表占位符</span></span><br><span class=\"line\">        <span class=\"comment\">// 创建一个定时器，周期为1000ms（1秒），定时调用timer_callback()回调函数</span></span><br><span class=\"line\">        <span class=\"comment\">// 由于 timer_callback() 是一个不带参数的成员函数，所以参数列表中只有两个参数，如果成员函数有参数，那么bind的参数列表中也要相应的添加对应参数</span></span><br><span class=\"line\">\t\ttimer_ = <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">create_wall_timer</span>(<span class=\"number\">1000</span>ms, std::<span class=\"built_in\">bind</span>(&amp;TurtleCircleNode::timer_callback, <span class=\"keyword\">this</span>));</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">\t<span class=\"comment\">// 这里使用成员函数来作为回调函数</span></span><br><span class=\"line\">    <span class=\"comment\">// 定时器回调函数，按设定的周期触发</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">timer_callback</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 创建Twist消息对象</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">auto</span> msg = geometry_msgs::msg::<span class=\"built_in\">Twist</span>();</span><br><span class=\"line\">        <span class=\"comment\">// 设置线速度：x轴方向速度为1.0，y轴方向为0.0（不移动）</span></span><br><span class=\"line\">\t\tmsg.linear.x = <span class=\"number\">1.0</span>;</span><br><span class=\"line\">\t\tmsg.linear.y = <span class=\"number\">0.0</span>;</span><br><span class=\"line\">        <span class=\"comment\">// 设置角速度：z轴角速度为0.5，表示沿着Z轴旋转</span></span><br><span class=\"line\">\t\tmsg.angular.z = <span class=\"number\">0.5</span>;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 发布消息，通过发布者将控制命令发布出去</span></span><br><span class=\"line\">\t\tpublisher_-&gt;<span class=\"built_in\">publish</span>(msg);</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span>** argv)</span></span>&#123;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\trclcpp::<span class=\"built_in\">init</span>(argc, argv);</span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> node = std::<span class=\"built_in\">make_shared</span>&lt;TurtleCircleNode&gt;(<span class=\"string\">&quot;turtle_circle&quot;</span>);</span><br><span class=\"line\">\trclcpp::<span class=\"built_in\">spin</span>(node);</span><br><span class=\"line\">\t</span><br><span class=\"line\">\trclcpp::<span class=\"built_in\">shutdown</span>();</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"如果上面的回调函数使用-Lambda-表达式实现，则写成这样\"><a href=\"#如果上面的回调函数使用-Lambda-表达式实现，则写成这样\" class=\"headerlink\" title=\"如果上面的回调函数使用 Lambda 表达式实现，则写成这样\"></a>如果上面的回调函数使用 Lambda 表达式实现，则写成这样</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">timer_ = <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">create_wall_timer</span>(</span><br><span class=\"line\">    <span class=\"number\">1000</span>ms, </span><br><span class=\"line\">    [<span class=\"keyword\">this</span>]() &#123; </span><br><span class=\"line\">        <span class=\"comment\">// Lambda 捕获当前对象，并在 Lambda 内部调用成员函数</span></span><br><span class=\"line\">        <span class=\"keyword\">auto</span> msg = geometry_msgs::msg::<span class=\"built_in\">Twist</span>();</span><br><span class=\"line\">        msg.linear.x = <span class=\"number\">1.0</span>;</span><br><span class=\"line\">        msg.linear.y = <span class=\"number\">0.0</span>;</span><br><span class=\"line\">        msg.angular.z = <span class=\"number\">0.5</span>;</span><br><span class=\"line\">        publisher_-&gt;<span class=\"built_in\">publish</span>(msg);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-CMakeLists-中添加相应语句\"><a href=\"#在-CMakeLists-中添加相应语句\" class=\"headerlink\" title=\"在 CMakeLists 中添加相应语句\"></a>在 CMakeLists 中添加相应语句</h3><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 添加可执行文件</span></span><br><span class=\"line\"><span class=\"keyword\">add_executable</span>(turtle_circle src/turtlesim_circle.cpp)</span><br><span class=\"line\"><span class=\"comment\"># 包含依赖</span></span><br><span class=\"line\">ament_target_dependencies(turtle_circle rclcpp geometry_msgs)</span><br><span class=\"line\"><span class=\"comment\"># 拷贝节点到 install/lib 下</span></span><br><span class=\"line\"><span class=\"keyword\">install</span>(TARGETS turtle_circle</span><br><span class=\"line\">DESTINATION lib/<span class=\"variable\">$&#123;PROJECT_NAME&#125;</span></span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编译并运行-先运行小海龟节点\"><a href=\"#编译并运行-先运行小海龟节点\" class=\"headerlink\" title=\"编译并运行(先运行小海龟节点)\"></a>编译并运行(先运行小海龟节点)</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/learn_ros2/chapter_3/topic_ws/</span><br><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 run demo_cpp_topic turtle_circle</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_23.png\"></p>\n<h3 id=\"运行起来之后，在新的终端可以查看相关信息\"><a href=\"#运行起来之后，在新的终端可以查看相关信息\" class=\"headerlink\" title=\"运行起来之后，在新的终端可以查看相关信息\"></a>运行起来之后，在新的终端可以查看相关信息</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 topic list -t</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_24.png\"></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 node info /turtle_circle</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_25.png\"></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 topic echo /turtle1/cmd_vel</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_26.png\"></p>\n</blockquote>\n<h2 id=\"订阅pose实现闭环控制\"><a href=\"#订阅pose实现闭环控制\" class=\"headerlink\" title=\"订阅pose实现闭环控制\"></a>订阅pose实现闭环控制</h2><blockquote>\n<h3 id=\"查看当前节点的话题列表\"><a href=\"#查看当前节点的话题列表\" class=\"headerlink\" title=\"查看当前节点的话题列表\"></a>查看当前节点的话题列表</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 topic list -t</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_24.png\"></p>\n<h3 id=\"找到小海龟位置信息的话题，-turtle1-pose，对应的消息接口为-turtlesim-msg-Pose\"><a href=\"#找到小海龟位置信息的话题，-turtle1-pose，对应的消息接口为-turtlesim-msg-Pose\" class=\"headerlink\" title=\"找到小海龟位置信息的话题，&#x2F;turtle1&#x2F;pose，对应的消息接口为 turtlesim&#x2F;msg&#x2F;Pose\"></a>找到小海龟位置信息的话题，&#x2F;turtle1&#x2F;pose，对应的消息接口为 turtlesim&#x2F;msg&#x2F;Pose</h3><h3 id=\"查看消息接口的定义\"><a href=\"#查看消息接口的定义\" class=\"headerlink\" title=\"查看消息接口的定义\"></a>查看消息接口的定义</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ros2 interface show turtlesim/msg/Pose</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_27.png\"></p>\n<h3 id=\"在-demo-cpp-topic-src-下创建文件-turtle-control-cpp，其内容如下\"><a href=\"#在-demo-cpp-topic-src-下创建文件-turtle-control-cpp，其内容如下\" class=\"headerlink\" title=\"在 demo_cpp_topic&#x2F;src 下创建文件 turtle_control.cpp，其内容如下\"></a>在 demo_cpp_topic&#x2F;src 下创建文件 turtle_control.cpp，其内容如下</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;rclcpp/rclcpp.hpp&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;geometry_msgs/msg/twist.hpp&quot;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;turtlesim/msg/pose.hpp&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std::chrono_literals;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">TurtleControlNode</span> : <span class=\"keyword\">public</span> rclcpp::Node&#123;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span>:</span><br><span class=\"line\">\t<span class=\"comment\">// 发布者的智能指针</span></span><br><span class=\"line\">\trclcpp::Publisher&lt;geometry_msgs::msg::Twist&gt;::SharedPtr publisher_;</span><br><span class=\"line\">\trclcpp::Subscription&lt;turtlesim::msg::Pose&gt;::SharedPtr subscriber_;</span><br><span class=\"line\">\t<span class=\"type\">double</span> target_x_&#123;<span class=\"number\">1.0</span>&#125;;</span><br><span class=\"line\">\t<span class=\"type\">double</span> target_y_&#123;<span class=\"number\">1.0</span>&#125;;</span><br><span class=\"line\">\t<span class=\"type\">double</span> k_&#123;<span class=\"number\">1.0</span>&#125;;\t\t<span class=\"comment\">// 比例系数</span></span><br><span class=\"line\">\t<span class=\"type\">double</span> max_speed_&#123;<span class=\"number\">3.0</span>&#125;;\t<span class=\"comment\">// 最大速度</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">TurtleControlNode</span><span class=\"params\">(<span class=\"type\">const</span> std::string &amp;node_name)</span> : Node(node_name)&#123;</span></span><br><span class=\"line\">\t\t<span class=\"comment\">// 创建发布者</span></span><br><span class=\"line\">\t\tpublisher_ = <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">create_publisher</span>&lt;geometry_msgs::msg::Twist&gt;(<span class=\"string\">&quot;/turtle1/cmd_vel&quot;</span>, <span class=\"number\">10</span>);</span><br><span class=\"line\">\t\tsubscriber_ = <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">create_subscription</span>&lt;turtlesim::msg::Pose&gt;(<span class=\"string\">&quot;/turtle1/pose&quot;</span>, <span class=\"number\">10</span>, std::<span class=\"built_in\">bind</span>(&amp;TurtleControlNode::on_pose_received_, <span class=\"keyword\">this</span>, std::placeholders::_1));</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">on_pose_received_</span><span class=\"params\">(<span class=\"type\">const</span> turtlesim::msg::Pose::SharedPtr pose)</span></span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 获取小海龟当前位置</span></span><br><span class=\"line\">\t\t<span class=\"type\">double</span> current_x = pose-&gt;x;</span><br><span class=\"line\">\t\t<span class=\"type\">double</span> current_y = pose-&gt;y;</span><br><span class=\"line\">\t\t<span class=\"built_in\">RCLCPP_INFO</span>(<span class=\"built_in\">get_logger</span>(), <span class=\"string\">&quot;当前位置 x: %f,y: %f &quot;</span>, current_x, current_y);</span><br><span class=\"line\">\t\t<span class=\"comment\">// 计算当前位置与目标位置的距离差和角度差</span></span><br><span class=\"line\">\t\t<span class=\"type\">double</span> distance = std::<span class=\"built_in\">sqrt</span>((target_x_ - current_x)*(target_x_ - current_x) + (target_y_ - current_y)*(target_y_ - current_y));</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\t<span class=\"type\">double</span> angle = std::<span class=\"built_in\">atan2</span>((target_y_ - current_y), (target_x_ - current_x)) - pose-&gt;theta;</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\t<span class=\"comment\">// 控制策略</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">auto</span> msg = geometry_msgs::msg::<span class=\"built_in\">Twist</span>();</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span>(distance &gt; <span class=\"number\">0.1</span>)&#123;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span>(<span class=\"built_in\">fabs</span>(angle) &gt; <span class=\"number\">0.2</span>)&#123;</span><br><span class=\"line\">\t\t\t\tmsg.angular.z = <span class=\"built_in\">fabs</span>(angle);</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span>&#123;</span><br><span class=\"line\">\t\t\t\tmsg.linear.x = k_ * distance;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"comment\">// 限制线速度最大值</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span>(msg.linear.x &gt; max_speed_)&#123;</span><br><span class=\"line\">\t\t\tmsg.linear.x = max_speed_;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tpublisher_-&gt;<span class=\"built_in\">publish</span>(msg);</span><br><span class=\"line\">\t&#125;;</span><br><span class=\"line\">\t</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span>** argv)</span></span>&#123;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\trclcpp::<span class=\"built_in\">init</span>(argc, argv);</span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> node = std::<span class=\"built_in\">make_shared</span>&lt;TurtleControlNode&gt;(<span class=\"string\">&quot;turtle_control&quot;</span>);</span><br><span class=\"line\">\trclcpp::<span class=\"built_in\">spin</span>(node);</span><br><span class=\"line\">\t</span><br><span class=\"line\">\trclcpp::<span class=\"built_in\">shutdown</span>();</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"在-CMakeLists-文件中添加相应语句\"><a href=\"#在-CMakeLists-文件中添加相应语句\" class=\"headerlink\" title=\"在 CMakeLists 文件中添加相应语句\"></a>在 CMakeLists 文件中添加相应语句</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">添加可执行文件</span></span><br><span class=\"line\">add_executable(turtle_control src/turtle_control.cpp)</span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">添加依赖</span></span><br><span class=\"line\">ament_target_dependencies(turtle_control rclcpp geometry_msgs turtlesim)</span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">拷贝节点到 install/lib</span></span><br><span class=\"line\">install(TARGETS turtle_circle turtle_control</span><br><span class=\"line\">DESTINATION lib/$&#123;PROJECT_NAME&#125;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"编译运行-先运行-turtle-节点\"><a href=\"#编译运行-先运行-turtle-节点\" class=\"headerlink\" title=\"编译运行(先运行 turtle 节点)\"></a>编译运行(先运行 turtle 节点)</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd ~/learn_ros2/chapter_3/topic_ws/</span><br><span class=\"line\">colcon build</span><br><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 run demo_cpp_topic turtle_control</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/learn_ros2/pic_28.png\"></p>\n</blockquote>\n"},{"title":"autoware.universe安装后的官方demo运行","data":"2025-04-26T13:34:00.000Z","updated":"2025-04-26T13:34:00.000Z","type":"autoware","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p14.png","_content":"\n# autoware.universe安装后的官方demo运行\n\n> ### 先下载官网上的示例地图，autoware_map和autoware在同一路径下\n>\n> ```shell\n> gdown -O ~/autoware_map/ 'https://docs.google.com/uc?export=download&id=1499_nsbUbIeturZaDj7jhUownh5fvXHd'\n> unzip -d ~/autoware_map ~/autoware_map/sample-map-planning.zip\n> ```\n>\n> ### 然后刷新环境变量并运行\n>\n> ```shell\n> source install/setup.bash\n> ros2 launch autoware_launch planning_simulator.launch.xml map_path:=$HOME/autoware_map/sample-map-planning vehicle_model:=sample_vehicle sensor_model:=sample_sensor_kit\n> ```\n>\n> ### 运行起来之后会打开一个叫做 rviz2 的界面，\n>\n> ![](http://picbed.yanzu.tech/img/autoware/1.png)\n>\n> ### 使用 2D Pose Estimate 给定自动驾驶小车的初始位置，\n>\n> ![](http://picbed.yanzu.tech/img/autoware/2.png)\n>\n> ### 然后使用 2D Goal Pose 指定目标位置，前提是这个目标位置是从初始位置可达的才行，指定了目标位置之后，地图上会出现一条行驶轨迹\n>\n> ![](http://picbed.yanzu.tech/img/autoware/3.png)\n>\n> ### 在当前命令框路径下打开一个新的命令框，刷新一下环境变量后，执行以下命令就可以实现小车的自动驾驶了\n>\n> ```shell\n> source install/setup.bash\n> ros2 topic pub /autoware/engage autoware_vehicle_msgs/msg/Engage '{engage: true}' -1\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/autoware/4.png)\n>\n> \n\n","source":"_posts/14.md","raw":"---\ntitle: autoware.universe安装后的官方demo运行\ndata: 2025-04-26 21:34:00\nupdated: 2025-04-26 21:34:00\ntype: autoware\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p14.png\ntags:\n  - ROS2\n  - Learning\n  - autoware\n---\n\n# autoware.universe安装后的官方demo运行\n\n> ### 先下载官网上的示例地图，autoware_map和autoware在同一路径下\n>\n> ```shell\n> gdown -O ~/autoware_map/ 'https://docs.google.com/uc?export=download&id=1499_nsbUbIeturZaDj7jhUownh5fvXHd'\n> unzip -d ~/autoware_map ~/autoware_map/sample-map-planning.zip\n> ```\n>\n> ### 然后刷新环境变量并运行\n>\n> ```shell\n> source install/setup.bash\n> ros2 launch autoware_launch planning_simulator.launch.xml map_path:=$HOME/autoware_map/sample-map-planning vehicle_model:=sample_vehicle sensor_model:=sample_sensor_kit\n> ```\n>\n> ### 运行起来之后会打开一个叫做 rviz2 的界面，\n>\n> ![](http://picbed.yanzu.tech/img/autoware/1.png)\n>\n> ### 使用 2D Pose Estimate 给定自动驾驶小车的初始位置，\n>\n> ![](http://picbed.yanzu.tech/img/autoware/2.png)\n>\n> ### 然后使用 2D Goal Pose 指定目标位置，前提是这个目标位置是从初始位置可达的才行，指定了目标位置之后，地图上会出现一条行驶轨迹\n>\n> ![](http://picbed.yanzu.tech/img/autoware/3.png)\n>\n> ### 在当前命令框路径下打开一个新的命令框，刷新一下环境变量后，执行以下命令就可以实现小车的自动驾驶了\n>\n> ```shell\n> source install/setup.bash\n> ros2 topic pub /autoware/engage autoware_vehicle_msgs/msg/Engage '{engage: true}' -1\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/autoware/4.png)\n>\n> \n\n","slug":"14","published":1,"date":"2025-04-26T13:31:46.655Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cr10012iku4d07mfvt0","content":"<h1 id=\"autoware-universe安装后的官方demo运行\"><a href=\"#autoware-universe安装后的官方demo运行\" class=\"headerlink\" title=\"autoware.universe安装后的官方demo运行\"></a>autoware.universe安装后的官方demo运行</h1><blockquote>\n<h3 id=\"先下载官网上的示例地图，autoware-map和autoware在同一路径下\"><a href=\"#先下载官网上的示例地图，autoware-map和autoware在同一路径下\" class=\"headerlink\" title=\"先下载官网上的示例地图，autoware_map和autoware在同一路径下\"></a>先下载官网上的示例地图，autoware_map和autoware在同一路径下</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gdown -O ~/autoware_map/ &#x27;https://docs.google.com/uc?export=download&amp;id=1499_nsbUbIeturZaDj7jhUownh5fvXHd&#x27;</span><br><span class=\"line\">unzip -d ~/autoware_map ~/autoware_map/sample-map-planning.zip</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"然后刷新环境变量并运行\"><a href=\"#然后刷新环境变量并运行\" class=\"headerlink\" title=\"然后刷新环境变量并运行\"></a>然后刷新环境变量并运行</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 launch autoware_launch planning_simulator.launch.xml map_path:=$HOME/autoware_map/sample-map-planning vehicle_model:=sample_vehicle sensor_model:=sample_sensor_kit</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"运行起来之后会打开一个叫做-rviz2-的界面，\"><a href=\"#运行起来之后会打开一个叫做-rviz2-的界面，\" class=\"headerlink\" title=\"运行起来之后会打开一个叫做 rviz2 的界面，\"></a>运行起来之后会打开一个叫做 rviz2 的界面，</h3><p><img src=\"http://picbed.yanzu.tech/img/autoware/1.png\"></p>\n<h3 id=\"使用-2D-Pose-Estimate-给定自动驾驶小车的初始位置，\"><a href=\"#使用-2D-Pose-Estimate-给定自动驾驶小车的初始位置，\" class=\"headerlink\" title=\"使用 2D Pose Estimate 给定自动驾驶小车的初始位置，\"></a>使用 2D Pose Estimate 给定自动驾驶小车的初始位置，</h3><p><img src=\"http://picbed.yanzu.tech/img/autoware/2.png\"></p>\n<h3 id=\"然后使用-2D-Goal-Pose-指定目标位置，前提是这个目标位置是从初始位置可达的才行，指定了目标位置之后，地图上会出现一条行驶轨迹\"><a href=\"#然后使用-2D-Goal-Pose-指定目标位置，前提是这个目标位置是从初始位置可达的才行，指定了目标位置之后，地图上会出现一条行驶轨迹\" class=\"headerlink\" title=\"然后使用 2D Goal Pose 指定目标位置，前提是这个目标位置是从初始位置可达的才行，指定了目标位置之后，地图上会出现一条行驶轨迹\"></a>然后使用 2D Goal Pose 指定目标位置，前提是这个目标位置是从初始位置可达的才行，指定了目标位置之后，地图上会出现一条行驶轨迹</h3><p><img src=\"http://picbed.yanzu.tech/img/autoware/3.png\"></p>\n<h3 id=\"在当前命令框路径下打开一个新的命令框，刷新一下环境变量后，执行以下命令就可以实现小车的自动驾驶了\"><a href=\"#在当前命令框路径下打开一个新的命令框，刷新一下环境变量后，执行以下命令就可以实现小车的自动驾驶了\" class=\"headerlink\" title=\"在当前命令框路径下打开一个新的命令框，刷新一下环境变量后，执行以下命令就可以实现小车的自动驾驶了\"></a>在当前命令框路径下打开一个新的命令框，刷新一下环境变量后，执行以下命令就可以实现小车的自动驾驶了</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 topic pub /autoware/engage autoware_vehicle_msgs/msg/Engage &#x27;&#123;engage: true&#125;&#x27; -1</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/autoware/4.png\"></p>\n</blockquote>\n","cover_type":"img","excerpt":"","more":"<h1 id=\"autoware-universe安装后的官方demo运行\"><a href=\"#autoware-universe安装后的官方demo运行\" class=\"headerlink\" title=\"autoware.universe安装后的官方demo运行\"></a>autoware.universe安装后的官方demo运行</h1><blockquote>\n<h3 id=\"先下载官网上的示例地图，autoware-map和autoware在同一路径下\"><a href=\"#先下载官网上的示例地图，autoware-map和autoware在同一路径下\" class=\"headerlink\" title=\"先下载官网上的示例地图，autoware_map和autoware在同一路径下\"></a>先下载官网上的示例地图，autoware_map和autoware在同一路径下</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gdown -O ~/autoware_map/ &#x27;https://docs.google.com/uc?export=download&amp;id=1499_nsbUbIeturZaDj7jhUownh5fvXHd&#x27;</span><br><span class=\"line\">unzip -d ~/autoware_map ~/autoware_map/sample-map-planning.zip</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"然后刷新环境变量并运行\"><a href=\"#然后刷新环境变量并运行\" class=\"headerlink\" title=\"然后刷新环境变量并运行\"></a>然后刷新环境变量并运行</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 launch autoware_launch planning_simulator.launch.xml map_path:=$HOME/autoware_map/sample-map-planning vehicle_model:=sample_vehicle sensor_model:=sample_sensor_kit</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"运行起来之后会打开一个叫做-rviz2-的界面，\"><a href=\"#运行起来之后会打开一个叫做-rviz2-的界面，\" class=\"headerlink\" title=\"运行起来之后会打开一个叫做 rviz2 的界面，\"></a>运行起来之后会打开一个叫做 rviz2 的界面，</h3><p><img src=\"http://picbed.yanzu.tech/img/autoware/1.png\"></p>\n<h3 id=\"使用-2D-Pose-Estimate-给定自动驾驶小车的初始位置，\"><a href=\"#使用-2D-Pose-Estimate-给定自动驾驶小车的初始位置，\" class=\"headerlink\" title=\"使用 2D Pose Estimate 给定自动驾驶小车的初始位置，\"></a>使用 2D Pose Estimate 给定自动驾驶小车的初始位置，</h3><p><img src=\"http://picbed.yanzu.tech/img/autoware/2.png\"></p>\n<h3 id=\"然后使用-2D-Goal-Pose-指定目标位置，前提是这个目标位置是从初始位置可达的才行，指定了目标位置之后，地图上会出现一条行驶轨迹\"><a href=\"#然后使用-2D-Goal-Pose-指定目标位置，前提是这个目标位置是从初始位置可达的才行，指定了目标位置之后，地图上会出现一条行驶轨迹\" class=\"headerlink\" title=\"然后使用 2D Goal Pose 指定目标位置，前提是这个目标位置是从初始位置可达的才行，指定了目标位置之后，地图上会出现一条行驶轨迹\"></a>然后使用 2D Goal Pose 指定目标位置，前提是这个目标位置是从初始位置可达的才行，指定了目标位置之后，地图上会出现一条行驶轨迹</h3><p><img src=\"http://picbed.yanzu.tech/img/autoware/3.png\"></p>\n<h3 id=\"在当前命令框路径下打开一个新的命令框，刷新一下环境变量后，执行以下命令就可以实现小车的自动驾驶了\"><a href=\"#在当前命令框路径下打开一个新的命令框，刷新一下环境变量后，执行以下命令就可以实现小车的自动驾驶了\" class=\"headerlink\" title=\"在当前命令框路径下打开一个新的命令框，刷新一下环境变量后，执行以下命令就可以实现小车的自动驾驶了\"></a>在当前命令框路径下打开一个新的命令框，刷新一下环境变量后，执行以下命令就可以实现小车的自动驾驶了</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">source install/setup.bash</span><br><span class=\"line\">ros2 topic pub /autoware/engage autoware_vehicle_msgs/msg/Engage &#x27;&#123;engage: true&#125;&#x27; -1</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/autoware/4.png\"></p>\n</blockquote>\n"},{"title":"Anaconda安装(Win10)","data":"2025-05-21T07:34:00.000Z","updated":"2025-07-16T03:41:00.000Z","type":"DL","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p15.png","_content":"\n\n# Anaconda安装\n\n### 1.下载安装包，直接国内镜像资源，这里下载的是 Anaconda3-2022.10-Windows-x86_64.exe 版本\n\n> https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/\n\n\n\n### 2.安装好之后配置环境变量---系统变量中的path，Anaconda是安装在E:\\Anaconda3文件下的\n\n> #### 分别添加以下三个变量\n>\n> #### E:\\Anaconda3\n>\n> #### E:\\Anaconda3\\Scripts\n>\n> #### E:\\Anaconda3\\Library\\bin\n\n\n\n#### 3.在base环境下添加一个虚拟环境\n\n> #### 打开 Anaconda Prompt（Anaconda3）这个命令框\n>\n> #### 列出当前所有的环境\n>\n> ```shell\n> conda env list\n> ```\n>\n>#### 创建一个虚拟环境，并指定python版本，这里新建的虚拟环境默认是会保存在Anaconda/envs/ 路径下\n> \n>```shell\n> conda create  -n env_name python=3.9\n> ```\n> \n>#### 如果不是保存在Anaconda/envs/下，就要手动指定位置了\n>\n> ```\n>conda create  --prefix=path\\env_name python=3.9\n> ```\n> \n> #### 删除某个虚拟环境，但这个叫 env_name 的文件还在，需要手动删除\n>\n>```shell\n> conda remove -n env_name --all\n>```\n> \n> #### 进入某个虚拟环境\n> \n>```shell\n>conda activate env_name\n> ```\n\n\n\n### 4.进入某个虚拟环境\n\n> #### 列出当前虚拟环境下的所有库\n>\n> ```shell\n> conda list\n> ```\n>\n>#### 安装Numpy，并指定版本为1.21.5\n> \n>```shell\n> pip install numpy==1.21.5 -i https://pypi.tuna.tsinghua.edu.cn/simple\n> ```\n> \n>#### 安装pandas，并指定版本为1.2.4\n> \n>```shell\n> pip install Pandas==1.2.4 -i https://pypi.tuna.tsinghua.edu.cn/simple\n> ```\n> \n>#### 安装Matplotlib，并指定版本为3.5.1\n> \n>```shell\n> pip install Matplotlib==3.5.1 -i https://pypi.tuna.tsinghua.edu.cn/simple\n> ```\n> \n>#### 查看当前虚拟环境下某个库的版本，如 numpy\n> \n>```shell\n> pip show numpy\n> ```\n> \n>#### 退出当前虚拟环境\n> \n>```shell\n> conda deactivate\n> ```\n\n\n\n### 5.安装cuda\n\n> #### 这里安装的是11.3版本的\n>\n> https://developer.nvidia.com/cuda-toolkit-archive\n>\n> #### 点击跳转后，选择操作系统依次选择\n>\n> > Windows\n> > x86_64\n> > 10\n> > exe(local)\n>\n> #### 2.7G，慢慢下吧\n>\n> #### 下载好之后，将其放在E:\\CUDA\\下，并创建一个Temp文件用于存储临时缓存\n>\n> #### 开始安装\n>\n> #### 选择路径为E:\\CUDA\\Temp\\\n>\n> #### 然后等待完成\n>\n> #### 安装选项 --- 选择自定义\n>\n> #### 自定义安装选项 --- 只勾选 CUDA 且取消勾选CUDA中的 VS 选项\n>\n> #### 完成选择默认路径保存---也就是C盘（裂开）\n>\n> #### 安装完成后，可以E:\\CUDA\\这个文件删除了\n>\n> #### 配置环境变量\n>\n> > 依次添加以下四个系统变量\n> > C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\n> > C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\lib\\x64\n> > C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\bin\n> > C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\libnvvp\n>\n> #### 最后在命令行中输入以下命令查看CUDA版本\n>\n> ```shell\n> nvcc -V\n> ```\n\n\n\n### 6.安装pytorch\n\n> #### pytorch 分为三部分：torch、torchversion、torchaudio，其中torch是主要的\n>\n> #### 先进入该页面\n>\n> https://pytorch.org/get-started/previous-versions/\n>\n> #### ctrl + f 搜索 pip install torch==1.12.0\n>\n> #### 找到 CUDA 11.3版本的安装命令\n>\n> ```shell\n> pip install torch==1.12.0+cu113 torchvision==0.13.0+cu113 torchaudio==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu113\n> ```\n>\n> #### 这里并不直接执行该命令下载安装，而是手动安装\n>\n> #### 打开 https://download.pytorch.org/whl/cu113 页面\n>\n> #### 根据命令中，三个库的版本去分别下载\n>\n> #### 如，找到 torch 文件，打开，搜索 1.12.0+cu113\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/1.png)\n>\n> #### 选择cp39，也即是python3.9版本的win\n>\n> #### 其余两个同理\n>\n> #### 下载好之后，将三个 .whl 文件放在 E:\\whl\\下\n>\n> #### 然后将其安装到之前创建的虚拟环境 DL 下\n>\n> ```shell\n> conda activate DL\n> ```\n>\n> #### 然后\n>\n> ```\n> pip install E:\\whl\\torch-1.12.0+cu113-cp39-cp39-win_amd64.whl\n> pip install E:\\whl\\torchaudio-0.12.0+cu113-cp39-cp39-win_amd64.whl\n> pip install E:\\whl\\torchvision-0.13.0+cu113-cp39-cp39-win_amd64.whl\n> ```\n>\n> #### 安装好之后，可以查看是否安装成功\n>\n> ```shell\n> conda list\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/2.png)\n>\n> #### 也可以在当前虚拟环境下进入python编辑器，\n>\n> ```shell\n> python\n> ```\n>\n> #### 进入python编辑器后，导入torch\n>\n> ```python\n> import torch\n> ```\n>\n> #### 导入成功就说明安装成功了的，否则就得再折腾\n>\n> #### 再执行\n>\n> ```python\n> torch.cuda.is_available()\n> ```\n>\n> #### 会输出 True\n\n\n\n### 7.修改 Jupyter 的默认工作路径\n\n> #### Jupyter 默认的工作路径是 C:\\Uses\\用户名，将其改为 E:\\Jupyter\n>\n> #### 打开 Anaconda Prompt，执行\n>\n> ```shell\n> jupyter notebook --generate-config\n> ```\n>\n> #### 进入生成的文件的地址，编辑 jupyter_notebook_config.py 文件\n>\n> #### 查找 c.NotebookApp.notebook_dir 这一行\n>\n> #### 将其取消注释，然后 '' 里的内容就是新的文件路径，这里是 E:\\Jupyter ，这个文件要事先创建好\n>\n> #### 最后，jupyter的桌面快捷方式右键属性，快捷方式 --> 目标，将末尾的 %USERPROFILE% 删除，以及前面的空格，就可以了\n\n\n\n### 8.修改 Jupyter 的字体\n\n> #### 打开文件 \n>\n> #### E:\\Anaconda3\\Lib\\site-packages\\notebook\\static\\components\\codemirror\\lib\n>\n> #### 编辑 codemirror.css 文件\n>\n> #### 找到 font-family: \n>\n> #### 将其改为\n>\n> ```css\n> font-family: 'Fira Code Light','Consolas';\n> ```\n\n\n\n### 9.虚拟环境连接 Jupyter\n\n> #### 打开 Anaconda Prompt，进入虚拟环境 DL\n>\n> ```shell\n> conda activate DL\n> ```\n>\n> #### 安装 ipykernel\n>\n> ```shell\n> pip install ipykernel -i https://pypi.tuna.tsinghua.edu.cn/simple\n> ```\n>\n> #### 将虚拟环境导入到 Jupyter 的 kernel 中\n>\n> ```\n> python -m ipykernel install --user --name=DL\n> ```\n>\n> #### 再次打开 Jupyter 就可以选择 DL 环境了\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/3.png)\n>\n> \n>\n> #### 补充：\n>\n> > #### 要在某个虚拟环境下使用jupyter，可以这样设置\n> >\n> > #### 打开Anaconda prompt，进入该虚拟环境\n> >\n> > #### 在该环境下，下载一个包\n> >\n> > ```shell\n> > conda install nb_conda\n> > ```\n> >\n> > #### 下载好之后，再执行\n> >\n> > ```shell\n> > jupyter notebook\n> > ```\n> >\n> > #### 就可以在当前环境下打开jupyter了\n\n\n\n### 10.虚拟环境连接pycharm\n\n> #### 创建一个新文件夹 E:\\project_of_py\n>\n> #### 打开 pycharm\n>\n> #### 选择新建 New project\n>\n> #### 这里是最新版本的pycharm，25版的\n>\n> #### 点击 location，选择新创建的文件夹\n>\n> #### 再点击 Base conda，选择 Anaconda\\Scripts\\下的 conda.exe\n>\n> #### 然后直接点击 Create\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/4.png)\n>\n> #### 创建之后会进入项目界面\n>\n> #### 点击右下角的解释器名称\n>\n> #### 点击 Add New Interpreter --> Add Local Interpreter\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/5.png)\n>\n> #### 点击 Select existing\n>\n> #### Type 选择 Conda\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/6.png)\n>\n> #### 选择 Anaconda\\Scripts\\下的 conda.exe，然后点击 Reload environments，它会检测到已经创建的虚拟环境，这里是叫 DL，然后 OK\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/7.png)\n>\n> #### 右下角就会显示当前的解释器是DL\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/8.png)\n>\n> #### 随便创建一个 py 文件，测试一下这个环境能否正常执行 py 文件\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/9.png)\n>\n> #### 勾选 Run with python Console\n>\n> #### 依次点击 demo_00 --> Edit Configurations\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/10.png)\n>\n> #### 再点击 Modify options，然后勾选 Run with python Console\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/11.png)\n>\n> #### 运行demo_00之后右下角就有各变量的情况\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/12.png)\n>\n> #### 这个demo_00顺便检测了一下，pycharm 是否与 GPU 版本的 pytorch 连接成功\n>\n> ","source":"_posts/15.md","raw":"---\ntitle: Anaconda安装(Win10)\ndata: 2025-05-21 15:34:00\nupdated: 2025-07-16 11:41:00\ntype: DL\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p15.png\ntags:\n  - DL\n  - Learning\n  - Pytorch\n---\n\n\n# Anaconda安装\n\n### 1.下载安装包，直接国内镜像资源，这里下载的是 Anaconda3-2022.10-Windows-x86_64.exe 版本\n\n> https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/\n\n\n\n### 2.安装好之后配置环境变量---系统变量中的path，Anaconda是安装在E:\\Anaconda3文件下的\n\n> #### 分别添加以下三个变量\n>\n> #### E:\\Anaconda3\n>\n> #### E:\\Anaconda3\\Scripts\n>\n> #### E:\\Anaconda3\\Library\\bin\n\n\n\n#### 3.在base环境下添加一个虚拟环境\n\n> #### 打开 Anaconda Prompt（Anaconda3）这个命令框\n>\n> #### 列出当前所有的环境\n>\n> ```shell\n> conda env list\n> ```\n>\n>#### 创建一个虚拟环境，并指定python版本，这里新建的虚拟环境默认是会保存在Anaconda/envs/ 路径下\n> \n>```shell\n> conda create  -n env_name python=3.9\n> ```\n> \n>#### 如果不是保存在Anaconda/envs/下，就要手动指定位置了\n>\n> ```\n>conda create  --prefix=path\\env_name python=3.9\n> ```\n> \n> #### 删除某个虚拟环境，但这个叫 env_name 的文件还在，需要手动删除\n>\n>```shell\n> conda remove -n env_name --all\n>```\n> \n> #### 进入某个虚拟环境\n> \n>```shell\n>conda activate env_name\n> ```\n\n\n\n### 4.进入某个虚拟环境\n\n> #### 列出当前虚拟环境下的所有库\n>\n> ```shell\n> conda list\n> ```\n>\n>#### 安装Numpy，并指定版本为1.21.5\n> \n>```shell\n> pip install numpy==1.21.5 -i https://pypi.tuna.tsinghua.edu.cn/simple\n> ```\n> \n>#### 安装pandas，并指定版本为1.2.4\n> \n>```shell\n> pip install Pandas==1.2.4 -i https://pypi.tuna.tsinghua.edu.cn/simple\n> ```\n> \n>#### 安装Matplotlib，并指定版本为3.5.1\n> \n>```shell\n> pip install Matplotlib==3.5.1 -i https://pypi.tuna.tsinghua.edu.cn/simple\n> ```\n> \n>#### 查看当前虚拟环境下某个库的版本，如 numpy\n> \n>```shell\n> pip show numpy\n> ```\n> \n>#### 退出当前虚拟环境\n> \n>```shell\n> conda deactivate\n> ```\n\n\n\n### 5.安装cuda\n\n> #### 这里安装的是11.3版本的\n>\n> https://developer.nvidia.com/cuda-toolkit-archive\n>\n> #### 点击跳转后，选择操作系统依次选择\n>\n> > Windows\n> > x86_64\n> > 10\n> > exe(local)\n>\n> #### 2.7G，慢慢下吧\n>\n> #### 下载好之后，将其放在E:\\CUDA\\下，并创建一个Temp文件用于存储临时缓存\n>\n> #### 开始安装\n>\n> #### 选择路径为E:\\CUDA\\Temp\\\n>\n> #### 然后等待完成\n>\n> #### 安装选项 --- 选择自定义\n>\n> #### 自定义安装选项 --- 只勾选 CUDA 且取消勾选CUDA中的 VS 选项\n>\n> #### 完成选择默认路径保存---也就是C盘（裂开）\n>\n> #### 安装完成后，可以E:\\CUDA\\这个文件删除了\n>\n> #### 配置环境变量\n>\n> > 依次添加以下四个系统变量\n> > C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\n> > C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\lib\\x64\n> > C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\bin\n> > C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\libnvvp\n>\n> #### 最后在命令行中输入以下命令查看CUDA版本\n>\n> ```shell\n> nvcc -V\n> ```\n\n\n\n### 6.安装pytorch\n\n> #### pytorch 分为三部分：torch、torchversion、torchaudio，其中torch是主要的\n>\n> #### 先进入该页面\n>\n> https://pytorch.org/get-started/previous-versions/\n>\n> #### ctrl + f 搜索 pip install torch==1.12.0\n>\n> #### 找到 CUDA 11.3版本的安装命令\n>\n> ```shell\n> pip install torch==1.12.0+cu113 torchvision==0.13.0+cu113 torchaudio==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu113\n> ```\n>\n> #### 这里并不直接执行该命令下载安装，而是手动安装\n>\n> #### 打开 https://download.pytorch.org/whl/cu113 页面\n>\n> #### 根据命令中，三个库的版本去分别下载\n>\n> #### 如，找到 torch 文件，打开，搜索 1.12.0+cu113\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/1.png)\n>\n> #### 选择cp39，也即是python3.9版本的win\n>\n> #### 其余两个同理\n>\n> #### 下载好之后，将三个 .whl 文件放在 E:\\whl\\下\n>\n> #### 然后将其安装到之前创建的虚拟环境 DL 下\n>\n> ```shell\n> conda activate DL\n> ```\n>\n> #### 然后\n>\n> ```\n> pip install E:\\whl\\torch-1.12.0+cu113-cp39-cp39-win_amd64.whl\n> pip install E:\\whl\\torchaudio-0.12.0+cu113-cp39-cp39-win_amd64.whl\n> pip install E:\\whl\\torchvision-0.13.0+cu113-cp39-cp39-win_amd64.whl\n> ```\n>\n> #### 安装好之后，可以查看是否安装成功\n>\n> ```shell\n> conda list\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/2.png)\n>\n> #### 也可以在当前虚拟环境下进入python编辑器，\n>\n> ```shell\n> python\n> ```\n>\n> #### 进入python编辑器后，导入torch\n>\n> ```python\n> import torch\n> ```\n>\n> #### 导入成功就说明安装成功了的，否则就得再折腾\n>\n> #### 再执行\n>\n> ```python\n> torch.cuda.is_available()\n> ```\n>\n> #### 会输出 True\n\n\n\n### 7.修改 Jupyter 的默认工作路径\n\n> #### Jupyter 默认的工作路径是 C:\\Uses\\用户名，将其改为 E:\\Jupyter\n>\n> #### 打开 Anaconda Prompt，执行\n>\n> ```shell\n> jupyter notebook --generate-config\n> ```\n>\n> #### 进入生成的文件的地址，编辑 jupyter_notebook_config.py 文件\n>\n> #### 查找 c.NotebookApp.notebook_dir 这一行\n>\n> #### 将其取消注释，然后 '' 里的内容就是新的文件路径，这里是 E:\\Jupyter ，这个文件要事先创建好\n>\n> #### 最后，jupyter的桌面快捷方式右键属性，快捷方式 --> 目标，将末尾的 %USERPROFILE% 删除，以及前面的空格，就可以了\n\n\n\n### 8.修改 Jupyter 的字体\n\n> #### 打开文件 \n>\n> #### E:\\Anaconda3\\Lib\\site-packages\\notebook\\static\\components\\codemirror\\lib\n>\n> #### 编辑 codemirror.css 文件\n>\n> #### 找到 font-family: \n>\n> #### 将其改为\n>\n> ```css\n> font-family: 'Fira Code Light','Consolas';\n> ```\n\n\n\n### 9.虚拟环境连接 Jupyter\n\n> #### 打开 Anaconda Prompt，进入虚拟环境 DL\n>\n> ```shell\n> conda activate DL\n> ```\n>\n> #### 安装 ipykernel\n>\n> ```shell\n> pip install ipykernel -i https://pypi.tuna.tsinghua.edu.cn/simple\n> ```\n>\n> #### 将虚拟环境导入到 Jupyter 的 kernel 中\n>\n> ```\n> python -m ipykernel install --user --name=DL\n> ```\n>\n> #### 再次打开 Jupyter 就可以选择 DL 环境了\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/3.png)\n>\n> \n>\n> #### 补充：\n>\n> > #### 要在某个虚拟环境下使用jupyter，可以这样设置\n> >\n> > #### 打开Anaconda prompt，进入该虚拟环境\n> >\n> > #### 在该环境下，下载一个包\n> >\n> > ```shell\n> > conda install nb_conda\n> > ```\n> >\n> > #### 下载好之后，再执行\n> >\n> > ```shell\n> > jupyter notebook\n> > ```\n> >\n> > #### 就可以在当前环境下打开jupyter了\n\n\n\n### 10.虚拟环境连接pycharm\n\n> #### 创建一个新文件夹 E:\\project_of_py\n>\n> #### 打开 pycharm\n>\n> #### 选择新建 New project\n>\n> #### 这里是最新版本的pycharm，25版的\n>\n> #### 点击 location，选择新创建的文件夹\n>\n> #### 再点击 Base conda，选择 Anaconda\\Scripts\\下的 conda.exe\n>\n> #### 然后直接点击 Create\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/4.png)\n>\n> #### 创建之后会进入项目界面\n>\n> #### 点击右下角的解释器名称\n>\n> #### 点击 Add New Interpreter --> Add Local Interpreter\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/5.png)\n>\n> #### 点击 Select existing\n>\n> #### Type 选择 Conda\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/6.png)\n>\n> #### 选择 Anaconda\\Scripts\\下的 conda.exe，然后点击 Reload environments，它会检测到已经创建的虚拟环境，这里是叫 DL，然后 OK\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/7.png)\n>\n> #### 右下角就会显示当前的解释器是DL\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/8.png)\n>\n> #### 随便创建一个 py 文件，测试一下这个环境能否正常执行 py 文件\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/9.png)\n>\n> #### 勾选 Run with python Console\n>\n> #### 依次点击 demo_00 --> Edit Configurations\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/10.png)\n>\n> #### 再点击 Modify options，然后勾选 Run with python Console\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/11.png)\n>\n> #### 运行demo_00之后右下角就有各变量的情况\n>\n> ![](http://picbed.yanzu.tech/img/DL/1/12.png)\n>\n> #### 这个demo_00顺便检测了一下，pycharm 是否与 GPU 版本的 pytorch 连接成功\n>\n> ","slug":"15","published":1,"date":"2025-07-16T03:41:23.158Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cr10014iku49dte6yzx","content":"<h1 id=\"Anaconda安装\"><a href=\"#Anaconda安装\" class=\"headerlink\" title=\"Anaconda安装\"></a>Anaconda安装</h1><h3 id=\"1-下载安装包，直接国内镜像资源，这里下载的是-Anaconda3-2022-10-Windows-x86-64-exe-版本\"><a href=\"#1-下载安装包，直接国内镜像资源，这里下载的是-Anaconda3-2022-10-Windows-x86-64-exe-版本\" class=\"headerlink\" title=\"1.下载安装包，直接国内镜像资源，这里下载的是 Anaconda3-2022.10-Windows-x86_64.exe 版本\"></a>1.下载安装包，直接国内镜像资源，这里下载的是 Anaconda3-2022.10-Windows-x86_64.exe 版本</h3><blockquote>\n<p><a href=\"https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/\">https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/</a></p>\n</blockquote>\n<h3 id=\"2-安装好之后配置环境变量—系统变量中的path，Anaconda是安装在E-Anaconda3文件下的\"><a href=\"#2-安装好之后配置环境变量—系统变量中的path，Anaconda是安装在E-Anaconda3文件下的\" class=\"headerlink\" title=\"2.安装好之后配置环境变量—系统变量中的path，Anaconda是安装在E:\\Anaconda3文件下的\"></a>2.安装好之后配置环境变量—系统变量中的path，Anaconda是安装在E:\\Anaconda3文件下的</h3><blockquote>\n<h4 id=\"分别添加以下三个变量\"><a href=\"#分别添加以下三个变量\" class=\"headerlink\" title=\"分别添加以下三个变量\"></a>分别添加以下三个变量</h4><h4 id=\"E-Anaconda3\"><a href=\"#E-Anaconda3\" class=\"headerlink\" title=\"E:\\Anaconda3\"></a>E:\\Anaconda3</h4><h4 id=\"E-Anaconda3-Scripts\"><a href=\"#E-Anaconda3-Scripts\" class=\"headerlink\" title=\"E:\\Anaconda3\\Scripts\"></a>E:\\Anaconda3\\Scripts</h4><h4 id=\"E-Anaconda3-Library-bin\"><a href=\"#E-Anaconda3-Library-bin\" class=\"headerlink\" title=\"E:\\Anaconda3\\Library\\bin\"></a>E:\\Anaconda3\\Library\\bin</h4></blockquote>\n<h4 id=\"3-在base环境下添加一个虚拟环境\"><a href=\"#3-在base环境下添加一个虚拟环境\" class=\"headerlink\" title=\"3.在base环境下添加一个虚拟环境\"></a>3.在base环境下添加一个虚拟环境</h4><blockquote>\n<h4 id=\"打开-Anaconda-Prompt（Anaconda3）这个命令框\"><a href=\"#打开-Anaconda-Prompt（Anaconda3）这个命令框\" class=\"headerlink\" title=\"打开 Anaconda Prompt（Anaconda3）这个命令框\"></a>打开 Anaconda Prompt（Anaconda3）这个命令框</h4><h4 id=\"列出当前所有的环境\"><a href=\"#列出当前所有的环境\" class=\"headerlink\" title=\"列出当前所有的环境\"></a>列出当前所有的环境</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda env list</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"创建一个虚拟环境，并指定python版本，这里新建的虚拟环境默认是会保存在Anaconda-envs-路径下\"><a href=\"#创建一个虚拟环境，并指定python版本，这里新建的虚拟环境默认是会保存在Anaconda-envs-路径下\" class=\"headerlink\" title=\"创建一个虚拟环境，并指定python版本，这里新建的虚拟环境默认是会保存在Anaconda&#x2F;envs&#x2F; 路径下\"></a>创建一个虚拟环境，并指定python版本，这里新建的虚拟环境默认是会保存在Anaconda&#x2F;envs&#x2F; 路径下</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create  -n env_name python=3.9</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"如果不是保存在Anaconda-envs-下，就要手动指定位置了\"><a href=\"#如果不是保存在Anaconda-envs-下，就要手动指定位置了\" class=\"headerlink\" title=\"如果不是保存在Anaconda&#x2F;envs&#x2F;下，就要手动指定位置了\"></a>如果不是保存在Anaconda&#x2F;envs&#x2F;下，就要手动指定位置了</h4><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;conda create  --prefix=path\\env_name python=3.9</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"删除某个虚拟环境，但这个叫-env-name-的文件还在，需要手动删除\"><a href=\"#删除某个虚拟环境，但这个叫-env-name-的文件还在，需要手动删除\" class=\"headerlink\" title=\"删除某个虚拟环境，但这个叫 env_name 的文件还在，需要手动删除\"></a>删除某个虚拟环境，但这个叫 env_name 的文件还在，需要手动删除</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda remove -n env_name --all</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"进入某个虚拟环境\"><a href=\"#进入某个虚拟环境\" class=\"headerlink\" title=\"进入某个虚拟环境\"></a>进入某个虚拟环境</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">conda activate env_name</span></span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"4-进入某个虚拟环境\"><a href=\"#4-进入某个虚拟环境\" class=\"headerlink\" title=\"4.进入某个虚拟环境\"></a>4.进入某个虚拟环境</h3><blockquote>\n<h4 id=\"列出当前虚拟环境下的所有库\"><a href=\"#列出当前虚拟环境下的所有库\" class=\"headerlink\" title=\"列出当前虚拟环境下的所有库\"></a>列出当前虚拟环境下的所有库</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda list</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"安装Numpy，并指定版本为1-21-5\"><a href=\"#安装Numpy，并指定版本为1-21-5\" class=\"headerlink\" title=\"安装Numpy，并指定版本为1.21.5\"></a>安装Numpy，并指定版本为1.21.5</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install numpy==1.21.5 -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"安装pandas，并指定版本为1-2-4\"><a href=\"#安装pandas，并指定版本为1-2-4\" class=\"headerlink\" title=\"安装pandas，并指定版本为1.2.4\"></a>安装pandas，并指定版本为1.2.4</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install Pandas==1.2.4 -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"安装Matplotlib，并指定版本为3-5-1\"><a href=\"#安装Matplotlib，并指定版本为3-5-1\" class=\"headerlink\" title=\"安装Matplotlib，并指定版本为3.5.1\"></a>安装Matplotlib，并指定版本为3.5.1</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install Matplotlib==3.5.1 -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"查看当前虚拟环境下某个库的版本，如-numpy\"><a href=\"#查看当前虚拟环境下某个库的版本，如-numpy\" class=\"headerlink\" title=\"查看当前虚拟环境下某个库的版本，如 numpy\"></a>查看当前虚拟环境下某个库的版本，如 numpy</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip show numpy</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"退出当前虚拟环境\"><a href=\"#退出当前虚拟环境\" class=\"headerlink\" title=\"退出当前虚拟环境\"></a>退出当前虚拟环境</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda deactivate</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"5-安装cuda\"><a href=\"#5-安装cuda\" class=\"headerlink\" title=\"5.安装cuda\"></a>5.安装cuda</h3><blockquote>\n<h4 id=\"这里安装的是11-3版本的\"><a href=\"#这里安装的是11-3版本的\" class=\"headerlink\" title=\"这里安装的是11.3版本的\"></a>这里安装的是11.3版本的</h4><p><a href=\"https://developer.nvidia.com/cuda-toolkit-archive\">https://developer.nvidia.com/cuda-toolkit-archive</a></p>\n<h4 id=\"点击跳转后，选择操作系统依次选择\"><a href=\"#点击跳转后，选择操作系统依次选择\" class=\"headerlink\" title=\"点击跳转后，选择操作系统依次选择\"></a>点击跳转后，选择操作系统依次选择</h4><blockquote>\n<p>Windows<br>x86_64<br>10<br>exe(local)</p>\n</blockquote>\n<h4 id=\"2-7G，慢慢下吧\"><a href=\"#2-7G，慢慢下吧\" class=\"headerlink\" title=\"2.7G，慢慢下吧\"></a>2.7G，慢慢下吧</h4><h4 id=\"下载好之后，将其放在E-CUDA-下，并创建一个Temp文件用于存储临时缓存\"><a href=\"#下载好之后，将其放在E-CUDA-下，并创建一个Temp文件用于存储临时缓存\" class=\"headerlink\" title=\"下载好之后，将其放在E:\\CUDA\\下，并创建一个Temp文件用于存储临时缓存\"></a>下载好之后，将其放在E:\\CUDA\\下，并创建一个Temp文件用于存储临时缓存</h4><h4 id=\"开始安装\"><a href=\"#开始安装\" class=\"headerlink\" title=\"开始安装\"></a>开始安装</h4><h4 id=\"选择路径为E-CUDA-Temp\"><a href=\"#选择路径为E-CUDA-Temp\" class=\"headerlink\" title=\"选择路径为E:\\CUDA\\Temp\\\"></a>选择路径为E:\\CUDA\\Temp\\</h4><h4 id=\"然后等待完成\"><a href=\"#然后等待完成\" class=\"headerlink\" title=\"然后等待完成\"></a>然后等待完成</h4><h4 id=\"安装选项-—-选择自定义\"><a href=\"#安装选项-—-选择自定义\" class=\"headerlink\" title=\"安装选项 — 选择自定义\"></a>安装选项 — 选择自定义</h4><h4 id=\"自定义安装选项-—-只勾选-CUDA-且取消勾选CUDA中的-VS-选项\"><a href=\"#自定义安装选项-—-只勾选-CUDA-且取消勾选CUDA中的-VS-选项\" class=\"headerlink\" title=\"自定义安装选项 — 只勾选 CUDA 且取消勾选CUDA中的 VS 选项\"></a>自定义安装选项 — 只勾选 CUDA 且取消勾选CUDA中的 VS 选项</h4><h4 id=\"完成选择默认路径保存—也就是C盘（裂开）\"><a href=\"#完成选择默认路径保存—也就是C盘（裂开）\" class=\"headerlink\" title=\"完成选择默认路径保存—也就是C盘（裂开）\"></a>完成选择默认路径保存—也就是C盘（裂开）</h4><h4 id=\"安装完成后，可以E-CUDA-这个文件删除了\"><a href=\"#安装完成后，可以E-CUDA-这个文件删除了\" class=\"headerlink\" title=\"安装完成后，可以E:\\CUDA\\这个文件删除了\"></a>安装完成后，可以E:\\CUDA\\这个文件删除了</h4><h4 id=\"配置环境变量\"><a href=\"#配置环境变量\" class=\"headerlink\" title=\"配置环境变量\"></a>配置环境变量</h4><blockquote>\n<p>依次添加以下四个系统变量<br>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA<br>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\lib\\x64<br>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\bin<br>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\libnvvp</p>\n</blockquote>\n<h4 id=\"最后在命令行中输入以下命令查看CUDA版本\"><a href=\"#最后在命令行中输入以下命令查看CUDA版本\" class=\"headerlink\" title=\"最后在命令行中输入以下命令查看CUDA版本\"></a>最后在命令行中输入以下命令查看CUDA版本</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nvcc -V</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"6-安装pytorch\"><a href=\"#6-安装pytorch\" class=\"headerlink\" title=\"6.安装pytorch\"></a>6.安装pytorch</h3><blockquote>\n<h4 id=\"pytorch-分为三部分：torch、torchversion、torchaudio，其中torch是主要的\"><a href=\"#pytorch-分为三部分：torch、torchversion、torchaudio，其中torch是主要的\" class=\"headerlink\" title=\"pytorch 分为三部分：torch、torchversion、torchaudio，其中torch是主要的\"></a>pytorch 分为三部分：torch、torchversion、torchaudio，其中torch是主要的</h4><h4 id=\"先进入该页面\"><a href=\"#先进入该页面\" class=\"headerlink\" title=\"先进入该页面\"></a>先进入该页面</h4><p><a href=\"https://pytorch.org/get-started/previous-versions/\">https://pytorch.org/get-started/previous-versions/</a></p>\n<h4 id=\"ctrl-f-搜索-pip-install-torch-1-12-0\"><a href=\"#ctrl-f-搜索-pip-install-torch-1-12-0\" class=\"headerlink\" title=\"ctrl + f 搜索 pip install torch&#x3D;&#x3D;1.12.0\"></a>ctrl + f 搜索 pip install torch&#x3D;&#x3D;1.12.0</h4><h4 id=\"找到-CUDA-11-3版本的安装命令\"><a href=\"#找到-CUDA-11-3版本的安装命令\" class=\"headerlink\" title=\"找到 CUDA 11.3版本的安装命令\"></a>找到 CUDA 11.3版本的安装命令</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install torch==1.12.0+cu113 torchvision==0.13.0+cu113 torchaudio==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu113</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"这里并不直接执行该命令下载安装，而是手动安装\"><a href=\"#这里并不直接执行该命令下载安装，而是手动安装\" class=\"headerlink\" title=\"这里并不直接执行该命令下载安装，而是手动安装\"></a>这里并不直接执行该命令下载安装，而是手动安装</h4><h4 id=\"打开-https-download-pytorch-org-whl-cu113-页面\"><a href=\"#打开-https-download-pytorch-org-whl-cu113-页面\" class=\"headerlink\" title=\"打开 https://download.pytorch.org/whl/cu113 页面\"></a>打开 <a href=\"https://download.pytorch.org/whl/cu113\">https://download.pytorch.org/whl/cu113</a> 页面</h4><h4 id=\"根据命令中，三个库的版本去分别下载\"><a href=\"#根据命令中，三个库的版本去分别下载\" class=\"headerlink\" title=\"根据命令中，三个库的版本去分别下载\"></a>根据命令中，三个库的版本去分别下载</h4><h4 id=\"如，找到-torch-文件，打开，搜索-1-12-0-cu113\"><a href=\"#如，找到-torch-文件，打开，搜索-1-12-0-cu113\" class=\"headerlink\" title=\"如，找到 torch 文件，打开，搜索 1.12.0+cu113\"></a>如，找到 torch 文件，打开，搜索 1.12.0+cu113</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/1.png\"></p>\n<h4 id=\"选择cp39，也即是python3-9版本的win\"><a href=\"#选择cp39，也即是python3-9版本的win\" class=\"headerlink\" title=\"选择cp39，也即是python3.9版本的win\"></a>选择cp39，也即是python3.9版本的win</h4><h4 id=\"其余两个同理\"><a href=\"#其余两个同理\" class=\"headerlink\" title=\"其余两个同理\"></a>其余两个同理</h4><h4 id=\"下载好之后，将三个-whl-文件放在-E-whl-下\"><a href=\"#下载好之后，将三个-whl-文件放在-E-whl-下\" class=\"headerlink\" title=\"下载好之后，将三个 .whl 文件放在 E:\\whl\\下\"></a>下载好之后，将三个 .whl 文件放在 E:\\whl\\下</h4><h4 id=\"然后将其安装到之前创建的虚拟环境-DL-下\"><a href=\"#然后将其安装到之前创建的虚拟环境-DL-下\" class=\"headerlink\" title=\"然后将其安装到之前创建的虚拟环境 DL 下\"></a>然后将其安装到之前创建的虚拟环境 DL 下</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda activate DL</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"然后\"><a href=\"#然后\" class=\"headerlink\" title=\"然后\"></a>然后</h4><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install E:\\whl\\torch-1.12.0+cu113-cp39-cp39-win_amd64.whl</span><br><span class=\"line\">pip install E:\\whl\\torchaudio-0.12.0+cu113-cp39-cp39-win_amd64.whl</span><br><span class=\"line\">pip install E:\\whl\\torchvision-0.13.0+cu113-cp39-cp39-win_amd64.whl</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"安装好之后，可以查看是否安装成功\"><a href=\"#安装好之后，可以查看是否安装成功\" class=\"headerlink\" title=\"安装好之后，可以查看是否安装成功\"></a>安装好之后，可以查看是否安装成功</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda list</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/DL/1/2.png\"></p>\n<h4 id=\"也可以在当前虚拟环境下进入python编辑器，\"><a href=\"#也可以在当前虚拟环境下进入python编辑器，\" class=\"headerlink\" title=\"也可以在当前虚拟环境下进入python编辑器，\"></a>也可以在当前虚拟环境下进入python编辑器，</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"进入python编辑器后，导入torch\"><a href=\"#进入python编辑器后，导入torch\" class=\"headerlink\" title=\"进入python编辑器后，导入torch\"></a>进入python编辑器后，导入torch</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"导入成功就说明安装成功了的，否则就得再折腾\"><a href=\"#导入成功就说明安装成功了的，否则就得再折腾\" class=\"headerlink\" title=\"导入成功就说明安装成功了的，否则就得再折腾\"></a>导入成功就说明安装成功了的，否则就得再折腾</h4><h4 id=\"再执行\"><a href=\"#再执行\" class=\"headerlink\" title=\"再执行\"></a>再执行</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.cuda.is_available()</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"会输出-True\"><a href=\"#会输出-True\" class=\"headerlink\" title=\"会输出 True\"></a>会输出 True</h4></blockquote>\n<h3 id=\"7-修改-Jupyter-的默认工作路径\"><a href=\"#7-修改-Jupyter-的默认工作路径\" class=\"headerlink\" title=\"7.修改 Jupyter 的默认工作路径\"></a>7.修改 Jupyter 的默认工作路径</h3><blockquote>\n<h4 id=\"Jupyter-默认的工作路径是-C-Uses-用户名，将其改为-E-Jupyter\"><a href=\"#Jupyter-默认的工作路径是-C-Uses-用户名，将其改为-E-Jupyter\" class=\"headerlink\" title=\"Jupyter 默认的工作路径是 C:\\Uses\\用户名，将其改为 E:\\Jupyter\"></a>Jupyter 默认的工作路径是 C:\\Uses\\用户名，将其改为 E:\\Jupyter</h4><h4 id=\"打开-Anaconda-Prompt，执行\"><a href=\"#打开-Anaconda-Prompt，执行\" class=\"headerlink\" title=\"打开 Anaconda Prompt，执行\"></a>打开 Anaconda Prompt，执行</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jupyter notebook --generate-config</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"进入生成的文件的地址，编辑-jupyter-notebook-config-py-文件\"><a href=\"#进入生成的文件的地址，编辑-jupyter-notebook-config-py-文件\" class=\"headerlink\" title=\"进入生成的文件的地址，编辑 jupyter_notebook_config.py 文件\"></a>进入生成的文件的地址，编辑 jupyter_notebook_config.py 文件</h4><h4 id=\"查找-c-NotebookApp-notebook-dir-这一行\"><a href=\"#查找-c-NotebookApp-notebook-dir-这一行\" class=\"headerlink\" title=\"查找 c.NotebookApp.notebook_dir 这一行\"></a>查找 c.NotebookApp.notebook_dir 这一行</h4><h4 id=\"将其取消注释，然后-‘’-里的内容就是新的文件路径，这里是-E-Jupyter-，这个文件要事先创建好\"><a href=\"#将其取消注释，然后-‘’-里的内容就是新的文件路径，这里是-E-Jupyter-，这个文件要事先创建好\" class=\"headerlink\" title=\"将其取消注释，然后 ‘’ 里的内容就是新的文件路径，这里是 E:\\Jupyter ，这个文件要事先创建好\"></a>将其取消注释，然后 ‘’ 里的内容就是新的文件路径，这里是 E:\\Jupyter ，这个文件要事先创建好</h4><h4 id=\"最后，jupyter的桌面快捷方式右键属性，快捷方式-–-目标，将末尾的-USERPROFILE-删除，以及前面的空格，就可以了\"><a href=\"#最后，jupyter的桌面快捷方式右键属性，快捷方式-–-目标，将末尾的-USERPROFILE-删除，以及前面的空格，就可以了\" class=\"headerlink\" title=\"最后，jupyter的桌面快捷方式右键属性，快捷方式 –&gt; 目标，将末尾的 %USERPROFILE% 删除，以及前面的空格，就可以了\"></a>最后，jupyter的桌面快捷方式右键属性，快捷方式 –&gt; 目标，将末尾的 %USERPROFILE% 删除，以及前面的空格，就可以了</h4></blockquote>\n<h3 id=\"8-修改-Jupyter-的字体\"><a href=\"#8-修改-Jupyter-的字体\" class=\"headerlink\" title=\"8.修改 Jupyter 的字体\"></a>8.修改 Jupyter 的字体</h3><blockquote>\n<h4 id=\"打开文件\"><a href=\"#打开文件\" class=\"headerlink\" title=\"打开文件\"></a>打开文件</h4><h4 id=\"E-Anaconda3-Lib-site-packages-notebook-static-components-codemirror-lib\"><a href=\"#E-Anaconda3-Lib-site-packages-notebook-static-components-codemirror-lib\" class=\"headerlink\" title=\"E:\\Anaconda3\\Lib\\site-packages\\notebook\\static\\components\\codemirror\\lib\"></a>E:\\Anaconda3\\Lib\\site-packages\\notebook\\static\\components\\codemirror\\lib</h4><h4 id=\"编辑-codemirror-css-文件\"><a href=\"#编辑-codemirror-css-文件\" class=\"headerlink\" title=\"编辑 codemirror.css 文件\"></a>编辑 codemirror.css 文件</h4><h4 id=\"找到-font-family\"><a href=\"#找到-font-family\" class=\"headerlink\" title=\"找到 font-family:\"></a>找到 font-family:</h4><h4 id=\"将其改为\"><a href=\"#将其改为\" class=\"headerlink\" title=\"将其改为\"></a>将其改为</h4><figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">font-family</span>: <span class=\"string\">&#x27;Fira Code Light&#x27;</span>,<span class=\"string\">&#x27;Consolas&#x27;</span>;</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"9-虚拟环境连接-Jupyter\"><a href=\"#9-虚拟环境连接-Jupyter\" class=\"headerlink\" title=\"9.虚拟环境连接 Jupyter\"></a>9.虚拟环境连接 Jupyter</h3><blockquote>\n<h4 id=\"打开-Anaconda-Prompt，进入虚拟环境-DL\"><a href=\"#打开-Anaconda-Prompt，进入虚拟环境-DL\" class=\"headerlink\" title=\"打开 Anaconda Prompt，进入虚拟环境 DL\"></a>打开 Anaconda Prompt，进入虚拟环境 DL</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda activate DL</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"安装-ipykernel\"><a href=\"#安装-ipykernel\" class=\"headerlink\" title=\"安装 ipykernel\"></a>安装 ipykernel</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install ipykernel -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"将虚拟环境导入到-Jupyter-的-kernel-中\"><a href=\"#将虚拟环境导入到-Jupyter-的-kernel-中\" class=\"headerlink\" title=\"将虚拟环境导入到 Jupyter 的 kernel 中\"></a>将虚拟环境导入到 Jupyter 的 kernel 中</h4><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python -m ipykernel install --user --name=DL</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"再次打开-Jupyter-就可以选择-DL-环境了\"><a href=\"#再次打开-Jupyter-就可以选择-DL-环境了\" class=\"headerlink\" title=\"再次打开 Jupyter 就可以选择 DL 环境了\"></a>再次打开 Jupyter 就可以选择 DL 环境了</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/3.png\"></p>\n<h4 id=\"补充：\"><a href=\"#补充：\" class=\"headerlink\" title=\"补充：\"></a>补充：</h4><blockquote>\n<h4 id=\"要在某个虚拟环境下使用jupyter，可以这样设置\"><a href=\"#要在某个虚拟环境下使用jupyter，可以这样设置\" class=\"headerlink\" title=\"要在某个虚拟环境下使用jupyter，可以这样设置\"></a>要在某个虚拟环境下使用jupyter，可以这样设置</h4><h4 id=\"打开Anaconda-prompt，进入该虚拟环境\"><a href=\"#打开Anaconda-prompt，进入该虚拟环境\" class=\"headerlink\" title=\"打开Anaconda prompt，进入该虚拟环境\"></a>打开Anaconda prompt，进入该虚拟环境</h4><h4 id=\"在该环境下，下载一个包\"><a href=\"#在该环境下，下载一个包\" class=\"headerlink\" title=\"在该环境下，下载一个包\"></a>在该环境下，下载一个包</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install nb_conda</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"下载好之后，再执行\"><a href=\"#下载好之后，再执行\" class=\"headerlink\" title=\"下载好之后，再执行\"></a>下载好之后，再执行</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jupyter notebook</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"就可以在当前环境下打开jupyter了\"><a href=\"#就可以在当前环境下打开jupyter了\" class=\"headerlink\" title=\"就可以在当前环境下打开jupyter了\"></a>就可以在当前环境下打开jupyter了</h4></blockquote>\n</blockquote>\n<h3 id=\"10-虚拟环境连接pycharm\"><a href=\"#10-虚拟环境连接pycharm\" class=\"headerlink\" title=\"10.虚拟环境连接pycharm\"></a>10.虚拟环境连接pycharm</h3><blockquote>\n<h4 id=\"创建一个新文件夹-E-project-of-py\"><a href=\"#创建一个新文件夹-E-project-of-py\" class=\"headerlink\" title=\"创建一个新文件夹 E:\\project_of_py\"></a>创建一个新文件夹 E:\\project_of_py</h4><h4 id=\"打开-pycharm\"><a href=\"#打开-pycharm\" class=\"headerlink\" title=\"打开 pycharm\"></a>打开 pycharm</h4><h4 id=\"选择新建-New-project\"><a href=\"#选择新建-New-project\" class=\"headerlink\" title=\"选择新建 New project\"></a>选择新建 New project</h4><h4 id=\"这里是最新版本的pycharm，25版的\"><a href=\"#这里是最新版本的pycharm，25版的\" class=\"headerlink\" title=\"这里是最新版本的pycharm，25版的\"></a>这里是最新版本的pycharm，25版的</h4><h4 id=\"点击-location，选择新创建的文件夹\"><a href=\"#点击-location，选择新创建的文件夹\" class=\"headerlink\" title=\"点击 location，选择新创建的文件夹\"></a>点击 location，选择新创建的文件夹</h4><h4 id=\"再点击-Base-conda，选择-Anaconda-Scripts-下的-conda-exe\"><a href=\"#再点击-Base-conda，选择-Anaconda-Scripts-下的-conda-exe\" class=\"headerlink\" title=\"再点击 Base conda，选择 Anaconda\\Scripts\\下的 conda.exe\"></a>再点击 Base conda，选择 Anaconda\\Scripts\\下的 conda.exe</h4><h4 id=\"然后直接点击-Create\"><a href=\"#然后直接点击-Create\" class=\"headerlink\" title=\"然后直接点击 Create\"></a>然后直接点击 Create</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/4.png\"></p>\n<h4 id=\"创建之后会进入项目界面\"><a href=\"#创建之后会进入项目界面\" class=\"headerlink\" title=\"创建之后会进入项目界面\"></a>创建之后会进入项目界面</h4><h4 id=\"点击右下角的解释器名称\"><a href=\"#点击右下角的解释器名称\" class=\"headerlink\" title=\"点击右下角的解释器名称\"></a>点击右下角的解释器名称</h4><h4 id=\"点击-Add-New-Interpreter-–-Add-Local-Interpreter\"><a href=\"#点击-Add-New-Interpreter-–-Add-Local-Interpreter\" class=\"headerlink\" title=\"点击 Add New Interpreter –&gt; Add Local Interpreter\"></a>点击 Add New Interpreter –&gt; Add Local Interpreter</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/5.png\"></p>\n<h4 id=\"点击-Select-existing\"><a href=\"#点击-Select-existing\" class=\"headerlink\" title=\"点击 Select existing\"></a>点击 Select existing</h4><h4 id=\"Type-选择-Conda\"><a href=\"#Type-选择-Conda\" class=\"headerlink\" title=\"Type 选择 Conda\"></a>Type 选择 Conda</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/6.png\"></p>\n<h4 id=\"选择-Anaconda-Scripts-下的-conda-exe，然后点击-Reload-environments，它会检测到已经创建的虚拟环境，这里是叫-DL，然后-OK\"><a href=\"#选择-Anaconda-Scripts-下的-conda-exe，然后点击-Reload-environments，它会检测到已经创建的虚拟环境，这里是叫-DL，然后-OK\" class=\"headerlink\" title=\"选择 Anaconda\\Scripts\\下的 conda.exe，然后点击 Reload environments，它会检测到已经创建的虚拟环境，这里是叫 DL，然后 OK\"></a>选择 Anaconda\\Scripts\\下的 conda.exe，然后点击 Reload environments，它会检测到已经创建的虚拟环境，这里是叫 DL，然后 OK</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/7.png\"></p>\n<h4 id=\"右下角就会显示当前的解释器是DL\"><a href=\"#右下角就会显示当前的解释器是DL\" class=\"headerlink\" title=\"右下角就会显示当前的解释器是DL\"></a>右下角就会显示当前的解释器是DL</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/8.png\"></p>\n<h4 id=\"随便创建一个-py-文件，测试一下这个环境能否正常执行-py-文件\"><a href=\"#随便创建一个-py-文件，测试一下这个环境能否正常执行-py-文件\" class=\"headerlink\" title=\"随便创建一个 py 文件，测试一下这个环境能否正常执行 py 文件\"></a>随便创建一个 py 文件，测试一下这个环境能否正常执行 py 文件</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/9.png\"></p>\n<h4 id=\"勾选-Run-with-python-Console\"><a href=\"#勾选-Run-with-python-Console\" class=\"headerlink\" title=\"勾选 Run with python Console\"></a>勾选 Run with python Console</h4><h4 id=\"依次点击-demo-00-–-Edit-Configurations\"><a href=\"#依次点击-demo-00-–-Edit-Configurations\" class=\"headerlink\" title=\"依次点击 demo_00 –&gt; Edit Configurations\"></a>依次点击 demo_00 –&gt; Edit Configurations</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/10.png\"></p>\n<h4 id=\"再点击-Modify-options，然后勾选-Run-with-python-Console\"><a href=\"#再点击-Modify-options，然后勾选-Run-with-python-Console\" class=\"headerlink\" title=\"再点击 Modify options，然后勾选 Run with python Console\"></a>再点击 Modify options，然后勾选 Run with python Console</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/11.png\"></p>\n<h4 id=\"运行demo-00之后右下角就有各变量的情况\"><a href=\"#运行demo-00之后右下角就有各变量的情况\" class=\"headerlink\" title=\"运行demo_00之后右下角就有各变量的情况\"></a>运行demo_00之后右下角就有各变量的情况</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/12.png\"></p>\n<h4 id=\"这个demo-00顺便检测了一下，pycharm-是否与-GPU-版本的-pytorch-连接成功\"><a href=\"#这个demo-00顺便检测了一下，pycharm-是否与-GPU-版本的-pytorch-连接成功\" class=\"headerlink\" title=\"这个demo_00顺便检测了一下，pycharm 是否与 GPU 版本的 pytorch 连接成功\"></a>这个demo_00顺便检测了一下，pycharm 是否与 GPU 版本的 pytorch 连接成功</h4></blockquote>\n","cover_type":"img","excerpt":"","more":"<h1 id=\"Anaconda安装\"><a href=\"#Anaconda安装\" class=\"headerlink\" title=\"Anaconda安装\"></a>Anaconda安装</h1><h3 id=\"1-下载安装包，直接国内镜像资源，这里下载的是-Anaconda3-2022-10-Windows-x86-64-exe-版本\"><a href=\"#1-下载安装包，直接国内镜像资源，这里下载的是-Anaconda3-2022-10-Windows-x86-64-exe-版本\" class=\"headerlink\" title=\"1.下载安装包，直接国内镜像资源，这里下载的是 Anaconda3-2022.10-Windows-x86_64.exe 版本\"></a>1.下载安装包，直接国内镜像资源，这里下载的是 Anaconda3-2022.10-Windows-x86_64.exe 版本</h3><blockquote>\n<p><a href=\"https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/\">https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/</a></p>\n</blockquote>\n<h3 id=\"2-安装好之后配置环境变量—系统变量中的path，Anaconda是安装在E-Anaconda3文件下的\"><a href=\"#2-安装好之后配置环境变量—系统变量中的path，Anaconda是安装在E-Anaconda3文件下的\" class=\"headerlink\" title=\"2.安装好之后配置环境变量—系统变量中的path，Anaconda是安装在E:\\Anaconda3文件下的\"></a>2.安装好之后配置环境变量—系统变量中的path，Anaconda是安装在E:\\Anaconda3文件下的</h3><blockquote>\n<h4 id=\"分别添加以下三个变量\"><a href=\"#分别添加以下三个变量\" class=\"headerlink\" title=\"分别添加以下三个变量\"></a>分别添加以下三个变量</h4><h4 id=\"E-Anaconda3\"><a href=\"#E-Anaconda3\" class=\"headerlink\" title=\"E:\\Anaconda3\"></a>E:\\Anaconda3</h4><h4 id=\"E-Anaconda3-Scripts\"><a href=\"#E-Anaconda3-Scripts\" class=\"headerlink\" title=\"E:\\Anaconda3\\Scripts\"></a>E:\\Anaconda3\\Scripts</h4><h4 id=\"E-Anaconda3-Library-bin\"><a href=\"#E-Anaconda3-Library-bin\" class=\"headerlink\" title=\"E:\\Anaconda3\\Library\\bin\"></a>E:\\Anaconda3\\Library\\bin</h4></blockquote>\n<h4 id=\"3-在base环境下添加一个虚拟环境\"><a href=\"#3-在base环境下添加一个虚拟环境\" class=\"headerlink\" title=\"3.在base环境下添加一个虚拟环境\"></a>3.在base环境下添加一个虚拟环境</h4><blockquote>\n<h4 id=\"打开-Anaconda-Prompt（Anaconda3）这个命令框\"><a href=\"#打开-Anaconda-Prompt（Anaconda3）这个命令框\" class=\"headerlink\" title=\"打开 Anaconda Prompt（Anaconda3）这个命令框\"></a>打开 Anaconda Prompt（Anaconda3）这个命令框</h4><h4 id=\"列出当前所有的环境\"><a href=\"#列出当前所有的环境\" class=\"headerlink\" title=\"列出当前所有的环境\"></a>列出当前所有的环境</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda env list</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"创建一个虚拟环境，并指定python版本，这里新建的虚拟环境默认是会保存在Anaconda-envs-路径下\"><a href=\"#创建一个虚拟环境，并指定python版本，这里新建的虚拟环境默认是会保存在Anaconda-envs-路径下\" class=\"headerlink\" title=\"创建一个虚拟环境，并指定python版本，这里新建的虚拟环境默认是会保存在Anaconda&#x2F;envs&#x2F; 路径下\"></a>创建一个虚拟环境，并指定python版本，这里新建的虚拟环境默认是会保存在Anaconda&#x2F;envs&#x2F; 路径下</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create  -n env_name python=3.9</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"如果不是保存在Anaconda-envs-下，就要手动指定位置了\"><a href=\"#如果不是保存在Anaconda-envs-下，就要手动指定位置了\" class=\"headerlink\" title=\"如果不是保存在Anaconda&#x2F;envs&#x2F;下，就要手动指定位置了\"></a>如果不是保存在Anaconda&#x2F;envs&#x2F;下，就要手动指定位置了</h4><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;conda create  --prefix=path\\env_name python=3.9</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"删除某个虚拟环境，但这个叫-env-name-的文件还在，需要手动删除\"><a href=\"#删除某个虚拟环境，但这个叫-env-name-的文件还在，需要手动删除\" class=\"headerlink\" title=\"删除某个虚拟环境，但这个叫 env_name 的文件还在，需要手动删除\"></a>删除某个虚拟环境，但这个叫 env_name 的文件还在，需要手动删除</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda remove -n env_name --all</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"进入某个虚拟环境\"><a href=\"#进入某个虚拟环境\" class=\"headerlink\" title=\"进入某个虚拟环境\"></a>进入某个虚拟环境</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">&gt;</span><span class=\"language-bash\">conda activate env_name</span></span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"4-进入某个虚拟环境\"><a href=\"#4-进入某个虚拟环境\" class=\"headerlink\" title=\"4.进入某个虚拟环境\"></a>4.进入某个虚拟环境</h3><blockquote>\n<h4 id=\"列出当前虚拟环境下的所有库\"><a href=\"#列出当前虚拟环境下的所有库\" class=\"headerlink\" title=\"列出当前虚拟环境下的所有库\"></a>列出当前虚拟环境下的所有库</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda list</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"安装Numpy，并指定版本为1-21-5\"><a href=\"#安装Numpy，并指定版本为1-21-5\" class=\"headerlink\" title=\"安装Numpy，并指定版本为1.21.5\"></a>安装Numpy，并指定版本为1.21.5</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install numpy==1.21.5 -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"安装pandas，并指定版本为1-2-4\"><a href=\"#安装pandas，并指定版本为1-2-4\" class=\"headerlink\" title=\"安装pandas，并指定版本为1.2.4\"></a>安装pandas，并指定版本为1.2.4</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install Pandas==1.2.4 -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"安装Matplotlib，并指定版本为3-5-1\"><a href=\"#安装Matplotlib，并指定版本为3-5-1\" class=\"headerlink\" title=\"安装Matplotlib，并指定版本为3.5.1\"></a>安装Matplotlib，并指定版本为3.5.1</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install Matplotlib==3.5.1 -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"查看当前虚拟环境下某个库的版本，如-numpy\"><a href=\"#查看当前虚拟环境下某个库的版本，如-numpy\" class=\"headerlink\" title=\"查看当前虚拟环境下某个库的版本，如 numpy\"></a>查看当前虚拟环境下某个库的版本，如 numpy</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip show numpy</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"退出当前虚拟环境\"><a href=\"#退出当前虚拟环境\" class=\"headerlink\" title=\"退出当前虚拟环境\"></a>退出当前虚拟环境</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda deactivate</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"5-安装cuda\"><a href=\"#5-安装cuda\" class=\"headerlink\" title=\"5.安装cuda\"></a>5.安装cuda</h3><blockquote>\n<h4 id=\"这里安装的是11-3版本的\"><a href=\"#这里安装的是11-3版本的\" class=\"headerlink\" title=\"这里安装的是11.3版本的\"></a>这里安装的是11.3版本的</h4><p><a href=\"https://developer.nvidia.com/cuda-toolkit-archive\">https://developer.nvidia.com/cuda-toolkit-archive</a></p>\n<h4 id=\"点击跳转后，选择操作系统依次选择\"><a href=\"#点击跳转后，选择操作系统依次选择\" class=\"headerlink\" title=\"点击跳转后，选择操作系统依次选择\"></a>点击跳转后，选择操作系统依次选择</h4><blockquote>\n<p>Windows<br>x86_64<br>10<br>exe(local)</p>\n</blockquote>\n<h4 id=\"2-7G，慢慢下吧\"><a href=\"#2-7G，慢慢下吧\" class=\"headerlink\" title=\"2.7G，慢慢下吧\"></a>2.7G，慢慢下吧</h4><h4 id=\"下载好之后，将其放在E-CUDA-下，并创建一个Temp文件用于存储临时缓存\"><a href=\"#下载好之后，将其放在E-CUDA-下，并创建一个Temp文件用于存储临时缓存\" class=\"headerlink\" title=\"下载好之后，将其放在E:\\CUDA\\下，并创建一个Temp文件用于存储临时缓存\"></a>下载好之后，将其放在E:\\CUDA\\下，并创建一个Temp文件用于存储临时缓存</h4><h4 id=\"开始安装\"><a href=\"#开始安装\" class=\"headerlink\" title=\"开始安装\"></a>开始安装</h4><h4 id=\"选择路径为E-CUDA-Temp\"><a href=\"#选择路径为E-CUDA-Temp\" class=\"headerlink\" title=\"选择路径为E:\\CUDA\\Temp\\\"></a>选择路径为E:\\CUDA\\Temp\\</h4><h4 id=\"然后等待完成\"><a href=\"#然后等待完成\" class=\"headerlink\" title=\"然后等待完成\"></a>然后等待完成</h4><h4 id=\"安装选项-—-选择自定义\"><a href=\"#安装选项-—-选择自定义\" class=\"headerlink\" title=\"安装选项 — 选择自定义\"></a>安装选项 — 选择自定义</h4><h4 id=\"自定义安装选项-—-只勾选-CUDA-且取消勾选CUDA中的-VS-选项\"><a href=\"#自定义安装选项-—-只勾选-CUDA-且取消勾选CUDA中的-VS-选项\" class=\"headerlink\" title=\"自定义安装选项 — 只勾选 CUDA 且取消勾选CUDA中的 VS 选项\"></a>自定义安装选项 — 只勾选 CUDA 且取消勾选CUDA中的 VS 选项</h4><h4 id=\"完成选择默认路径保存—也就是C盘（裂开）\"><a href=\"#完成选择默认路径保存—也就是C盘（裂开）\" class=\"headerlink\" title=\"完成选择默认路径保存—也就是C盘（裂开）\"></a>完成选择默认路径保存—也就是C盘（裂开）</h4><h4 id=\"安装完成后，可以E-CUDA-这个文件删除了\"><a href=\"#安装完成后，可以E-CUDA-这个文件删除了\" class=\"headerlink\" title=\"安装完成后，可以E:\\CUDA\\这个文件删除了\"></a>安装完成后，可以E:\\CUDA\\这个文件删除了</h4><h4 id=\"配置环境变量\"><a href=\"#配置环境变量\" class=\"headerlink\" title=\"配置环境变量\"></a>配置环境变量</h4><blockquote>\n<p>依次添加以下四个系统变量<br>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA<br>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\lib\\x64<br>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\bin<br>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\libnvvp</p>\n</blockquote>\n<h4 id=\"最后在命令行中输入以下命令查看CUDA版本\"><a href=\"#最后在命令行中输入以下命令查看CUDA版本\" class=\"headerlink\" title=\"最后在命令行中输入以下命令查看CUDA版本\"></a>最后在命令行中输入以下命令查看CUDA版本</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nvcc -V</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"6-安装pytorch\"><a href=\"#6-安装pytorch\" class=\"headerlink\" title=\"6.安装pytorch\"></a>6.安装pytorch</h3><blockquote>\n<h4 id=\"pytorch-分为三部分：torch、torchversion、torchaudio，其中torch是主要的\"><a href=\"#pytorch-分为三部分：torch、torchversion、torchaudio，其中torch是主要的\" class=\"headerlink\" title=\"pytorch 分为三部分：torch、torchversion、torchaudio，其中torch是主要的\"></a>pytorch 分为三部分：torch、torchversion、torchaudio，其中torch是主要的</h4><h4 id=\"先进入该页面\"><a href=\"#先进入该页面\" class=\"headerlink\" title=\"先进入该页面\"></a>先进入该页面</h4><p><a href=\"https://pytorch.org/get-started/previous-versions/\">https://pytorch.org/get-started/previous-versions/</a></p>\n<h4 id=\"ctrl-f-搜索-pip-install-torch-1-12-0\"><a href=\"#ctrl-f-搜索-pip-install-torch-1-12-0\" class=\"headerlink\" title=\"ctrl + f 搜索 pip install torch&#x3D;&#x3D;1.12.0\"></a>ctrl + f 搜索 pip install torch&#x3D;&#x3D;1.12.0</h4><h4 id=\"找到-CUDA-11-3版本的安装命令\"><a href=\"#找到-CUDA-11-3版本的安装命令\" class=\"headerlink\" title=\"找到 CUDA 11.3版本的安装命令\"></a>找到 CUDA 11.3版本的安装命令</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install torch==1.12.0+cu113 torchvision==0.13.0+cu113 torchaudio==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu113</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"这里并不直接执行该命令下载安装，而是手动安装\"><a href=\"#这里并不直接执行该命令下载安装，而是手动安装\" class=\"headerlink\" title=\"这里并不直接执行该命令下载安装，而是手动安装\"></a>这里并不直接执行该命令下载安装，而是手动安装</h4><h4 id=\"打开-https-download-pytorch-org-whl-cu113-页面\"><a href=\"#打开-https-download-pytorch-org-whl-cu113-页面\" class=\"headerlink\" title=\"打开 https://download.pytorch.org/whl/cu113 页面\"></a>打开 <a href=\"https://download.pytorch.org/whl/cu113\">https://download.pytorch.org/whl/cu113</a> 页面</h4><h4 id=\"根据命令中，三个库的版本去分别下载\"><a href=\"#根据命令中，三个库的版本去分别下载\" class=\"headerlink\" title=\"根据命令中，三个库的版本去分别下载\"></a>根据命令中，三个库的版本去分别下载</h4><h4 id=\"如，找到-torch-文件，打开，搜索-1-12-0-cu113\"><a href=\"#如，找到-torch-文件，打开，搜索-1-12-0-cu113\" class=\"headerlink\" title=\"如，找到 torch 文件，打开，搜索 1.12.0+cu113\"></a>如，找到 torch 文件，打开，搜索 1.12.0+cu113</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/1.png\"></p>\n<h4 id=\"选择cp39，也即是python3-9版本的win\"><a href=\"#选择cp39，也即是python3-9版本的win\" class=\"headerlink\" title=\"选择cp39，也即是python3.9版本的win\"></a>选择cp39，也即是python3.9版本的win</h4><h4 id=\"其余两个同理\"><a href=\"#其余两个同理\" class=\"headerlink\" title=\"其余两个同理\"></a>其余两个同理</h4><h4 id=\"下载好之后，将三个-whl-文件放在-E-whl-下\"><a href=\"#下载好之后，将三个-whl-文件放在-E-whl-下\" class=\"headerlink\" title=\"下载好之后，将三个 .whl 文件放在 E:\\whl\\下\"></a>下载好之后，将三个 .whl 文件放在 E:\\whl\\下</h4><h4 id=\"然后将其安装到之前创建的虚拟环境-DL-下\"><a href=\"#然后将其安装到之前创建的虚拟环境-DL-下\" class=\"headerlink\" title=\"然后将其安装到之前创建的虚拟环境 DL 下\"></a>然后将其安装到之前创建的虚拟环境 DL 下</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda activate DL</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"然后\"><a href=\"#然后\" class=\"headerlink\" title=\"然后\"></a>然后</h4><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install E:\\whl\\torch-1.12.0+cu113-cp39-cp39-win_amd64.whl</span><br><span class=\"line\">pip install E:\\whl\\torchaudio-0.12.0+cu113-cp39-cp39-win_amd64.whl</span><br><span class=\"line\">pip install E:\\whl\\torchvision-0.13.0+cu113-cp39-cp39-win_amd64.whl</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"安装好之后，可以查看是否安装成功\"><a href=\"#安装好之后，可以查看是否安装成功\" class=\"headerlink\" title=\"安装好之后，可以查看是否安装成功\"></a>安装好之后，可以查看是否安装成功</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda list</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/DL/1/2.png\"></p>\n<h4 id=\"也可以在当前虚拟环境下进入python编辑器，\"><a href=\"#也可以在当前虚拟环境下进入python编辑器，\" class=\"headerlink\" title=\"也可以在当前虚拟环境下进入python编辑器，\"></a>也可以在当前虚拟环境下进入python编辑器，</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"进入python编辑器后，导入torch\"><a href=\"#进入python编辑器后，导入torch\" class=\"headerlink\" title=\"进入python编辑器后，导入torch\"></a>进入python编辑器后，导入torch</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"导入成功就说明安装成功了的，否则就得再折腾\"><a href=\"#导入成功就说明安装成功了的，否则就得再折腾\" class=\"headerlink\" title=\"导入成功就说明安装成功了的，否则就得再折腾\"></a>导入成功就说明安装成功了的，否则就得再折腾</h4><h4 id=\"再执行\"><a href=\"#再执行\" class=\"headerlink\" title=\"再执行\"></a>再执行</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.cuda.is_available()</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"会输出-True\"><a href=\"#会输出-True\" class=\"headerlink\" title=\"会输出 True\"></a>会输出 True</h4></blockquote>\n<h3 id=\"7-修改-Jupyter-的默认工作路径\"><a href=\"#7-修改-Jupyter-的默认工作路径\" class=\"headerlink\" title=\"7.修改 Jupyter 的默认工作路径\"></a>7.修改 Jupyter 的默认工作路径</h3><blockquote>\n<h4 id=\"Jupyter-默认的工作路径是-C-Uses-用户名，将其改为-E-Jupyter\"><a href=\"#Jupyter-默认的工作路径是-C-Uses-用户名，将其改为-E-Jupyter\" class=\"headerlink\" title=\"Jupyter 默认的工作路径是 C:\\Uses\\用户名，将其改为 E:\\Jupyter\"></a>Jupyter 默认的工作路径是 C:\\Uses\\用户名，将其改为 E:\\Jupyter</h4><h4 id=\"打开-Anaconda-Prompt，执行\"><a href=\"#打开-Anaconda-Prompt，执行\" class=\"headerlink\" title=\"打开 Anaconda Prompt，执行\"></a>打开 Anaconda Prompt，执行</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jupyter notebook --generate-config</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"进入生成的文件的地址，编辑-jupyter-notebook-config-py-文件\"><a href=\"#进入生成的文件的地址，编辑-jupyter-notebook-config-py-文件\" class=\"headerlink\" title=\"进入生成的文件的地址，编辑 jupyter_notebook_config.py 文件\"></a>进入生成的文件的地址，编辑 jupyter_notebook_config.py 文件</h4><h4 id=\"查找-c-NotebookApp-notebook-dir-这一行\"><a href=\"#查找-c-NotebookApp-notebook-dir-这一行\" class=\"headerlink\" title=\"查找 c.NotebookApp.notebook_dir 这一行\"></a>查找 c.NotebookApp.notebook_dir 这一行</h4><h4 id=\"将其取消注释，然后-‘’-里的内容就是新的文件路径，这里是-E-Jupyter-，这个文件要事先创建好\"><a href=\"#将其取消注释，然后-‘’-里的内容就是新的文件路径，这里是-E-Jupyter-，这个文件要事先创建好\" class=\"headerlink\" title=\"将其取消注释，然后 ‘’ 里的内容就是新的文件路径，这里是 E:\\Jupyter ，这个文件要事先创建好\"></a>将其取消注释，然后 ‘’ 里的内容就是新的文件路径，这里是 E:\\Jupyter ，这个文件要事先创建好</h4><h4 id=\"最后，jupyter的桌面快捷方式右键属性，快捷方式-–-目标，将末尾的-USERPROFILE-删除，以及前面的空格，就可以了\"><a href=\"#最后，jupyter的桌面快捷方式右键属性，快捷方式-–-目标，将末尾的-USERPROFILE-删除，以及前面的空格，就可以了\" class=\"headerlink\" title=\"最后，jupyter的桌面快捷方式右键属性，快捷方式 –&gt; 目标，将末尾的 %USERPROFILE% 删除，以及前面的空格，就可以了\"></a>最后，jupyter的桌面快捷方式右键属性，快捷方式 –&gt; 目标，将末尾的 %USERPROFILE% 删除，以及前面的空格，就可以了</h4></blockquote>\n<h3 id=\"8-修改-Jupyter-的字体\"><a href=\"#8-修改-Jupyter-的字体\" class=\"headerlink\" title=\"8.修改 Jupyter 的字体\"></a>8.修改 Jupyter 的字体</h3><blockquote>\n<h4 id=\"打开文件\"><a href=\"#打开文件\" class=\"headerlink\" title=\"打开文件\"></a>打开文件</h4><h4 id=\"E-Anaconda3-Lib-site-packages-notebook-static-components-codemirror-lib\"><a href=\"#E-Anaconda3-Lib-site-packages-notebook-static-components-codemirror-lib\" class=\"headerlink\" title=\"E:\\Anaconda3\\Lib\\site-packages\\notebook\\static\\components\\codemirror\\lib\"></a>E:\\Anaconda3\\Lib\\site-packages\\notebook\\static\\components\\codemirror\\lib</h4><h4 id=\"编辑-codemirror-css-文件\"><a href=\"#编辑-codemirror-css-文件\" class=\"headerlink\" title=\"编辑 codemirror.css 文件\"></a>编辑 codemirror.css 文件</h4><h4 id=\"找到-font-family\"><a href=\"#找到-font-family\" class=\"headerlink\" title=\"找到 font-family:\"></a>找到 font-family:</h4><h4 id=\"将其改为\"><a href=\"#将其改为\" class=\"headerlink\" title=\"将其改为\"></a>将其改为</h4><figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">font-family</span>: <span class=\"string\">&#x27;Fira Code Light&#x27;</span>,<span class=\"string\">&#x27;Consolas&#x27;</span>;</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h3 id=\"9-虚拟环境连接-Jupyter\"><a href=\"#9-虚拟环境连接-Jupyter\" class=\"headerlink\" title=\"9.虚拟环境连接 Jupyter\"></a>9.虚拟环境连接 Jupyter</h3><blockquote>\n<h4 id=\"打开-Anaconda-Prompt，进入虚拟环境-DL\"><a href=\"#打开-Anaconda-Prompt，进入虚拟环境-DL\" class=\"headerlink\" title=\"打开 Anaconda Prompt，进入虚拟环境 DL\"></a>打开 Anaconda Prompt，进入虚拟环境 DL</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda activate DL</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"安装-ipykernel\"><a href=\"#安装-ipykernel\" class=\"headerlink\" title=\"安装 ipykernel\"></a>安装 ipykernel</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install ipykernel -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"将虚拟环境导入到-Jupyter-的-kernel-中\"><a href=\"#将虚拟环境导入到-Jupyter-的-kernel-中\" class=\"headerlink\" title=\"将虚拟环境导入到 Jupyter 的 kernel 中\"></a>将虚拟环境导入到 Jupyter 的 kernel 中</h4><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python -m ipykernel install --user --name=DL</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"再次打开-Jupyter-就可以选择-DL-环境了\"><a href=\"#再次打开-Jupyter-就可以选择-DL-环境了\" class=\"headerlink\" title=\"再次打开 Jupyter 就可以选择 DL 环境了\"></a>再次打开 Jupyter 就可以选择 DL 环境了</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/3.png\"></p>\n<h4 id=\"补充：\"><a href=\"#补充：\" class=\"headerlink\" title=\"补充：\"></a>补充：</h4><blockquote>\n<h4 id=\"要在某个虚拟环境下使用jupyter，可以这样设置\"><a href=\"#要在某个虚拟环境下使用jupyter，可以这样设置\" class=\"headerlink\" title=\"要在某个虚拟环境下使用jupyter，可以这样设置\"></a>要在某个虚拟环境下使用jupyter，可以这样设置</h4><h4 id=\"打开Anaconda-prompt，进入该虚拟环境\"><a href=\"#打开Anaconda-prompt，进入该虚拟环境\" class=\"headerlink\" title=\"打开Anaconda prompt，进入该虚拟环境\"></a>打开Anaconda prompt，进入该虚拟环境</h4><h4 id=\"在该环境下，下载一个包\"><a href=\"#在该环境下，下载一个包\" class=\"headerlink\" title=\"在该环境下，下载一个包\"></a>在该环境下，下载一个包</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install nb_conda</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"下载好之后，再执行\"><a href=\"#下载好之后，再执行\" class=\"headerlink\" title=\"下载好之后，再执行\"></a>下载好之后，再执行</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jupyter notebook</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"就可以在当前环境下打开jupyter了\"><a href=\"#就可以在当前环境下打开jupyter了\" class=\"headerlink\" title=\"就可以在当前环境下打开jupyter了\"></a>就可以在当前环境下打开jupyter了</h4></blockquote>\n</blockquote>\n<h3 id=\"10-虚拟环境连接pycharm\"><a href=\"#10-虚拟环境连接pycharm\" class=\"headerlink\" title=\"10.虚拟环境连接pycharm\"></a>10.虚拟环境连接pycharm</h3><blockquote>\n<h4 id=\"创建一个新文件夹-E-project-of-py\"><a href=\"#创建一个新文件夹-E-project-of-py\" class=\"headerlink\" title=\"创建一个新文件夹 E:\\project_of_py\"></a>创建一个新文件夹 E:\\project_of_py</h4><h4 id=\"打开-pycharm\"><a href=\"#打开-pycharm\" class=\"headerlink\" title=\"打开 pycharm\"></a>打开 pycharm</h4><h4 id=\"选择新建-New-project\"><a href=\"#选择新建-New-project\" class=\"headerlink\" title=\"选择新建 New project\"></a>选择新建 New project</h4><h4 id=\"这里是最新版本的pycharm，25版的\"><a href=\"#这里是最新版本的pycharm，25版的\" class=\"headerlink\" title=\"这里是最新版本的pycharm，25版的\"></a>这里是最新版本的pycharm，25版的</h4><h4 id=\"点击-location，选择新创建的文件夹\"><a href=\"#点击-location，选择新创建的文件夹\" class=\"headerlink\" title=\"点击 location，选择新创建的文件夹\"></a>点击 location，选择新创建的文件夹</h4><h4 id=\"再点击-Base-conda，选择-Anaconda-Scripts-下的-conda-exe\"><a href=\"#再点击-Base-conda，选择-Anaconda-Scripts-下的-conda-exe\" class=\"headerlink\" title=\"再点击 Base conda，选择 Anaconda\\Scripts\\下的 conda.exe\"></a>再点击 Base conda，选择 Anaconda\\Scripts\\下的 conda.exe</h4><h4 id=\"然后直接点击-Create\"><a href=\"#然后直接点击-Create\" class=\"headerlink\" title=\"然后直接点击 Create\"></a>然后直接点击 Create</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/4.png\"></p>\n<h4 id=\"创建之后会进入项目界面\"><a href=\"#创建之后会进入项目界面\" class=\"headerlink\" title=\"创建之后会进入项目界面\"></a>创建之后会进入项目界面</h4><h4 id=\"点击右下角的解释器名称\"><a href=\"#点击右下角的解释器名称\" class=\"headerlink\" title=\"点击右下角的解释器名称\"></a>点击右下角的解释器名称</h4><h4 id=\"点击-Add-New-Interpreter-–-Add-Local-Interpreter\"><a href=\"#点击-Add-New-Interpreter-–-Add-Local-Interpreter\" class=\"headerlink\" title=\"点击 Add New Interpreter –&gt; Add Local Interpreter\"></a>点击 Add New Interpreter –&gt; Add Local Interpreter</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/5.png\"></p>\n<h4 id=\"点击-Select-existing\"><a href=\"#点击-Select-existing\" class=\"headerlink\" title=\"点击 Select existing\"></a>点击 Select existing</h4><h4 id=\"Type-选择-Conda\"><a href=\"#Type-选择-Conda\" class=\"headerlink\" title=\"Type 选择 Conda\"></a>Type 选择 Conda</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/6.png\"></p>\n<h4 id=\"选择-Anaconda-Scripts-下的-conda-exe，然后点击-Reload-environments，它会检测到已经创建的虚拟环境，这里是叫-DL，然后-OK\"><a href=\"#选择-Anaconda-Scripts-下的-conda-exe，然后点击-Reload-environments，它会检测到已经创建的虚拟环境，这里是叫-DL，然后-OK\" class=\"headerlink\" title=\"选择 Anaconda\\Scripts\\下的 conda.exe，然后点击 Reload environments，它会检测到已经创建的虚拟环境，这里是叫 DL，然后 OK\"></a>选择 Anaconda\\Scripts\\下的 conda.exe，然后点击 Reload environments，它会检测到已经创建的虚拟环境，这里是叫 DL，然后 OK</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/7.png\"></p>\n<h4 id=\"右下角就会显示当前的解释器是DL\"><a href=\"#右下角就会显示当前的解释器是DL\" class=\"headerlink\" title=\"右下角就会显示当前的解释器是DL\"></a>右下角就会显示当前的解释器是DL</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/8.png\"></p>\n<h4 id=\"随便创建一个-py-文件，测试一下这个环境能否正常执行-py-文件\"><a href=\"#随便创建一个-py-文件，测试一下这个环境能否正常执行-py-文件\" class=\"headerlink\" title=\"随便创建一个 py 文件，测试一下这个环境能否正常执行 py 文件\"></a>随便创建一个 py 文件，测试一下这个环境能否正常执行 py 文件</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/9.png\"></p>\n<h4 id=\"勾选-Run-with-python-Console\"><a href=\"#勾选-Run-with-python-Console\" class=\"headerlink\" title=\"勾选 Run with python Console\"></a>勾选 Run with python Console</h4><h4 id=\"依次点击-demo-00-–-Edit-Configurations\"><a href=\"#依次点击-demo-00-–-Edit-Configurations\" class=\"headerlink\" title=\"依次点击 demo_00 –&gt; Edit Configurations\"></a>依次点击 demo_00 –&gt; Edit Configurations</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/10.png\"></p>\n<h4 id=\"再点击-Modify-options，然后勾选-Run-with-python-Console\"><a href=\"#再点击-Modify-options，然后勾选-Run-with-python-Console\" class=\"headerlink\" title=\"再点击 Modify options，然后勾选 Run with python Console\"></a>再点击 Modify options，然后勾选 Run with python Console</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/11.png\"></p>\n<h4 id=\"运行demo-00之后右下角就有各变量的情况\"><a href=\"#运行demo-00之后右下角就有各变量的情况\" class=\"headerlink\" title=\"运行demo_00之后右下角就有各变量的情况\"></a>运行demo_00之后右下角就有各变量的情况</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/1/12.png\"></p>\n<h4 id=\"这个demo-00顺便检测了一下，pycharm-是否与-GPU-版本的-pytorch-连接成功\"><a href=\"#这个demo-00顺便检测了一下，pycharm-是否与-GPU-版本的-pytorch-连接成功\" class=\"headerlink\" title=\"这个demo_00顺便检测了一下，pycharm 是否与 GPU 版本的 pytorch 连接成功\"></a>这个demo_00顺便检测了一下，pycharm 是否与 GPU 版本的 pytorch 连接成功</h4></blockquote>\n"},{"title":"RL之路---第一弹","data":"2025-05-22T11:40:00.000Z","updated":"2025-05-22T11:40:00.000Z","type":"RL","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p16.png","_content":"\n# RL的基本数学原理---基本概念\n\n\n\n### State\n\n> 它是agent相对于当前环境的一个状态，如当前的坐标 (x,y)，速度、加速度等\n>\n> 所有的状态构成的一个集合称之为状态空间，如下图，s1~s9构成了一个状态空间，这里是2D的，那么状态主要就是location （x, y）\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/1.png)\n\n\n\n### Action\n\n> 在每一个状态下，都会有对应的一系列的动作Action，如2D平面上，在一个状态下可以采取的Action有前进、后退、左右移动、原地不动\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/2.png)\n>\n> 所有的Action构成的一个集合就称之为动作空间 Action space\n>\n> Action 和 state 是相互依赖的，不同的状态下对应不同的动作\n> $$\n> A(s_i) = {a_i}\n> $$\n> 上式意为，在状态 s_i 下，可采取的动作 a_i\n\n\n\n### State transition\n\n> 状态转换，在当前状态 s1 下，采取动作 a2(有概率采取动作 a2)，会转换到下一状态 s2，而这个下一状态 s2，其实是不确定的，它根据采取的动作而定，而且只是有概率转移到某个状态 s\n> $$\n> s_1 \\xrightarrow{a_2} s_2\n> $$\n> 状态转换定义了 agent 与环境交互的一种行为\n>\n> 在某个状态下，采取某个动作就直接确定的转换到某个状态，这种情况就类似于拓扑图，但实际上不是这样\n>\n> 实际中，在一个状态下，采取的动作都是有概率的，如在 s1 下，可以采取 a1、a2、a3 三种动作，但是这三个动作都是有概率的，p(a1)=p(a2)=0.3，p(a3)=0.4，并且每个动作对应的转换状态(下一状态)也可能是多个且一样的有概率，如采取 a1，对应的状态有s2、s3、s4，概率也是0.3、0.3、0.4，这样，事情就变得复杂多样了\n\n### State transition probability\n\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/3.png)\n>\n> \n>\n> 这里采用了条件概率来描述 state transistion 的过程，这是一个随机性的例子(deterministic case)\n\n\n\n### Policy\n\n> 它的作用是“指示” agent 在某个状态 s 下应该采取什么 Action\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/4.png)\n>\n> 这样一条路径path或者说 trajectory 就是一种策略 policy\n>\n> 策略同样还是用条件概率来表示\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/5.png)\n>\n> 不过上面的策略是一种确定性的策略，也就是在状态 s1 下，一定会采取动作 a2，并非随机性的，下面的就是随机性的策略了\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/6.png)\n>\n> 其条件概率就是这样的了\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/7.png)\n>\n> 通常概率为零的就会省略掉，策略可以通过一个表格来表示，这种表格既能描述确定性的情况，也能描述随机性的情况\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/8.png)\n>\n> 对应的代码实现，通常是用数组或者矩阵来存储这个表格以表示某个策略，通过对（0，1）这个区间进行随机采样，以 s1 为例，若 x 属于 (0,0.5) 就采取a2，若 x 属于 (0.5,1) 就采取 a3\n\n\n\n### Reward\n\n> 首先它是一个数，一个标量，agent 在当前状态下采取了一个动作Action，会得到一个奖励 reward，如果 reward 值为正，代表鼓励该行为（Action），否则就是惩罚该行为（Action）\n>\n> reward 是 agent 与人交互的一种手段，因为这个奖励机制是人为设定的，通过设定合适奖励机制引导 agent 达到预期的效果，同样的可以用条件概率来表示\n> $$\n> p(r=-1|s_1,a_1)=0.6,p(r \\neq -1|s_1,a_1)=0.4\n> $$\n> 获得 reward 的多少不是一层不变的，通常会有一个衰减系数来控制\n>\n> reward 依赖于当前状态 s 以及在当前状态下采取的动作Action\n\n\n\n### Trajectory and return\n\n> Trajectory 是一条 状态 state-->动作 Action-->奖励 reward 的链，具体例子如下图所示\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/9.png)\n>\n> \n>\n> return 就是一个 trajectory 上所有 reward 之和，上面的 return = 0 + 0 + 0 + 1 = 1\n>\n> trajectory的优劣或者说策略policy的优劣是与其对应的return相关的\n\n\n\n### discounted return\n\n> 为了获得尽可能多的奖励 reward 和总的奖励 return，一条trajectory可能会无限进行下去，return最终会发散，如下图\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/10.png)\n>\n> \n>\n> 这就引入一个discount rate，通常用 γ 表示，γ 在[0,1)之间，discount rate 与 return 结合就得到了 discount return\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/11.png)\n>\n> 当 γ < 1 时，这个无穷数列的和就等效于右边的 1/( 1 - γ)\n>\n> 通过引入这个衰减因子 γ，就使得 return 从一个发散状态，变成一个有限值 discount return。\n>\n> 另外，当 γ 趋于 0 时，那么随着 γ 的次幂越来越高，会很快的衰减至0，return 的值就主要取决于前面几项；而 γ 趋于 1 时，后面的项衰减会慢很多。换句话说，γ 越小，越注重当前或者未来临近的状态-动作-奖励，而 γ 越大，则是注重更长远的策略\n\n\n\n### episode\n\n> episode属于trajectory的一种，当 agent 采用某种策略时，它可能达到最终状态（terminal state）后就停下来了，不会一直进行下去，更不会使return发散，它是有限的，这样的任务也称之为 episodic tasks\n>\n> 有些任务是没有最终状态，它会无限的进行下去，称之为 continuing tasks\n\n\n\n### MDP\n\n> 马尔可夫框架\n>\n> 三个集合：状态集合 S、动作集合A(s)、奖励集合R(s,a)\n>\n> 两个概率分布（条件概率？）：状态转换的条件概率 p( s' | s, a)、奖励的条件概率 p( r | s, a)\n>\n> 策略：Π( a | s )\n>\n> 马尔可夫性质：memoryless property，历史无关性\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/12.png)\n>\n> 下一状态只取决于当前状态，以及当期状态下采取的动作，奖励也是如此\n>\n> 如果马尔可夫过程 Markov process 的策略确定下来了，那马尔可夫过程就变成了马尔可夫决策过程 Markov decision process\n\n","source":"_posts/16.md","raw":"---\ntitle: RL之路---第一弹\ndata: 2025-05-22 19:40:00\nupdated: 2025-05-22 19:40:00\ntype: RL\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p16.png\ntags:\n  - RL\n  - Learning\n  - math-theory\n---\n\n# RL的基本数学原理---基本概念\n\n\n\n### State\n\n> 它是agent相对于当前环境的一个状态，如当前的坐标 (x,y)，速度、加速度等\n>\n> 所有的状态构成的一个集合称之为状态空间，如下图，s1~s9构成了一个状态空间，这里是2D的，那么状态主要就是location （x, y）\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/1.png)\n\n\n\n### Action\n\n> 在每一个状态下，都会有对应的一系列的动作Action，如2D平面上，在一个状态下可以采取的Action有前进、后退、左右移动、原地不动\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/2.png)\n>\n> 所有的Action构成的一个集合就称之为动作空间 Action space\n>\n> Action 和 state 是相互依赖的，不同的状态下对应不同的动作\n> $$\n> A(s_i) = {a_i}\n> $$\n> 上式意为，在状态 s_i 下，可采取的动作 a_i\n\n\n\n### State transition\n\n> 状态转换，在当前状态 s1 下，采取动作 a2(有概率采取动作 a2)，会转换到下一状态 s2，而这个下一状态 s2，其实是不确定的，它根据采取的动作而定，而且只是有概率转移到某个状态 s\n> $$\n> s_1 \\xrightarrow{a_2} s_2\n> $$\n> 状态转换定义了 agent 与环境交互的一种行为\n>\n> 在某个状态下，采取某个动作就直接确定的转换到某个状态，这种情况就类似于拓扑图，但实际上不是这样\n>\n> 实际中，在一个状态下，采取的动作都是有概率的，如在 s1 下，可以采取 a1、a2、a3 三种动作，但是这三个动作都是有概率的，p(a1)=p(a2)=0.3，p(a3)=0.4，并且每个动作对应的转换状态(下一状态)也可能是多个且一样的有概率，如采取 a1，对应的状态有s2、s3、s4，概率也是0.3、0.3、0.4，这样，事情就变得复杂多样了\n\n### State transition probability\n\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/3.png)\n>\n> \n>\n> 这里采用了条件概率来描述 state transistion 的过程，这是一个随机性的例子(deterministic case)\n\n\n\n### Policy\n\n> 它的作用是“指示” agent 在某个状态 s 下应该采取什么 Action\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/4.png)\n>\n> 这样一条路径path或者说 trajectory 就是一种策略 policy\n>\n> 策略同样还是用条件概率来表示\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/5.png)\n>\n> 不过上面的策略是一种确定性的策略，也就是在状态 s1 下，一定会采取动作 a2，并非随机性的，下面的就是随机性的策略了\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/6.png)\n>\n> 其条件概率就是这样的了\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/7.png)\n>\n> 通常概率为零的就会省略掉，策略可以通过一个表格来表示，这种表格既能描述确定性的情况，也能描述随机性的情况\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/8.png)\n>\n> 对应的代码实现，通常是用数组或者矩阵来存储这个表格以表示某个策略，通过对（0，1）这个区间进行随机采样，以 s1 为例，若 x 属于 (0,0.5) 就采取a2，若 x 属于 (0.5,1) 就采取 a3\n\n\n\n### Reward\n\n> 首先它是一个数，一个标量，agent 在当前状态下采取了一个动作Action，会得到一个奖励 reward，如果 reward 值为正，代表鼓励该行为（Action），否则就是惩罚该行为（Action）\n>\n> reward 是 agent 与人交互的一种手段，因为这个奖励机制是人为设定的，通过设定合适奖励机制引导 agent 达到预期的效果，同样的可以用条件概率来表示\n> $$\n> p(r=-1|s_1,a_1)=0.6,p(r \\neq -1|s_1,a_1)=0.4\n> $$\n> 获得 reward 的多少不是一层不变的，通常会有一个衰减系数来控制\n>\n> reward 依赖于当前状态 s 以及在当前状态下采取的动作Action\n\n\n\n### Trajectory and return\n\n> Trajectory 是一条 状态 state-->动作 Action-->奖励 reward 的链，具体例子如下图所示\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/9.png)\n>\n> \n>\n> return 就是一个 trajectory 上所有 reward 之和，上面的 return = 0 + 0 + 0 + 1 = 1\n>\n> trajectory的优劣或者说策略policy的优劣是与其对应的return相关的\n\n\n\n### discounted return\n\n> 为了获得尽可能多的奖励 reward 和总的奖励 return，一条trajectory可能会无限进行下去，return最终会发散，如下图\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/10.png)\n>\n> \n>\n> 这就引入一个discount rate，通常用 γ 表示，γ 在[0,1)之间，discount rate 与 return 结合就得到了 discount return\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/11.png)\n>\n> 当 γ < 1 时，这个无穷数列的和就等效于右边的 1/( 1 - γ)\n>\n> 通过引入这个衰减因子 γ，就使得 return 从一个发散状态，变成一个有限值 discount return。\n>\n> 另外，当 γ 趋于 0 时，那么随着 γ 的次幂越来越高，会很快的衰减至0，return 的值就主要取决于前面几项；而 γ 趋于 1 时，后面的项衰减会慢很多。换句话说，γ 越小，越注重当前或者未来临近的状态-动作-奖励，而 γ 越大，则是注重更长远的策略\n\n\n\n### episode\n\n> episode属于trajectory的一种，当 agent 采用某种策略时，它可能达到最终状态（terminal state）后就停下来了，不会一直进行下去，更不会使return发散，它是有限的，这样的任务也称之为 episodic tasks\n>\n> 有些任务是没有最终状态，它会无限的进行下去，称之为 continuing tasks\n\n\n\n### MDP\n\n> 马尔可夫框架\n>\n> 三个集合：状态集合 S、动作集合A(s)、奖励集合R(s,a)\n>\n> 两个概率分布（条件概率？）：状态转换的条件概率 p( s' | s, a)、奖励的条件概率 p( r | s, a)\n>\n> 策略：Π( a | s )\n>\n> 马尔可夫性质：memoryless property，历史无关性\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/first_part/12.png)\n>\n> 下一状态只取决于当前状态，以及当期状态下采取的动作，奖励也是如此\n>\n> 如果马尔可夫过程 Markov process 的策略确定下来了，那马尔可夫过程就变成了马尔可夫决策过程 Markov decision process\n\n","slug":"16","published":1,"date":"2025-05-22T11:37:53.665Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cr20017iku41vjdezvp","content":"<h1 id=\"RL的基本数学原理—基本概念\"><a href=\"#RL的基本数学原理—基本概念\" class=\"headerlink\" title=\"RL的基本数学原理—基本概念\"></a>RL的基本数学原理—基本概念</h1><h3 id=\"State\"><a href=\"#State\" class=\"headerlink\" title=\"State\"></a>State</h3><blockquote>\n<p>它是agent相对于当前环境的一个状态，如当前的坐标 (x,y)，速度、加速度等</p>\n<p>所有的状态构成的一个集合称之为状态空间，如下图，s1~s9构成了一个状态空间，这里是2D的，那么状态主要就是location （x, y）</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/1.png\"></p>\n</blockquote>\n<h3 id=\"Action\"><a href=\"#Action\" class=\"headerlink\" title=\"Action\"></a>Action</h3><blockquote>\n<p>在每一个状态下，都会有对应的一系列的动作Action，如2D平面上，在一个状态下可以采取的Action有前进、后退、左右移动、原地不动</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/2.png\"></p>\n<p>所有的Action构成的一个集合就称之为动作空间 Action space</p>\n<p>Action 和 state 是相互依赖的，不同的状态下对应不同的动作<br>$$<br>A(s_i) &#x3D; {a_i}<br>$$<br>上式意为，在状态 s_i 下，可采取的动作 a_i</p>\n</blockquote>\n<h3 id=\"State-transition\"><a href=\"#State-transition\" class=\"headerlink\" title=\"State transition\"></a>State transition</h3><blockquote>\n<p>状态转换，在当前状态 s1 下，采取动作 a2(有概率采取动作 a2)，会转换到下一状态 s2，而这个下一状态 s2，其实是不确定的，它根据采取的动作而定，而且只是有概率转移到某个状态 s<br>$$<br>s_1 \\xrightarrow{a_2} s_2<br>$$<br>状态转换定义了 agent 与环境交互的一种行为</p>\n<p>在某个状态下，采取某个动作就直接确定的转换到某个状态，这种情况就类似于拓扑图，但实际上不是这样</p>\n<p>实际中，在一个状态下，采取的动作都是有概率的，如在 s1 下，可以采取 a1、a2、a3 三种动作，但是这三个动作都是有概率的，p(a1)&#x3D;p(a2)&#x3D;0.3，p(a3)&#x3D;0.4，并且每个动作对应的转换状态(下一状态)也可能是多个且一样的有概率，如采取 a1，对应的状态有s2、s3、s4，概率也是0.3、0.3、0.4，这样，事情就变得复杂多样了</p>\n</blockquote>\n<h3 id=\"State-transition-probability\"><a href=\"#State-transition-probability\" class=\"headerlink\" title=\"State transition probability\"></a>State transition probability</h3><blockquote>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/3.png\"></p>\n<p>这里采用了条件概率来描述 state transistion 的过程，这是一个随机性的例子(deterministic case)</p>\n</blockquote>\n<h3 id=\"Policy\"><a href=\"#Policy\" class=\"headerlink\" title=\"Policy\"></a>Policy</h3><blockquote>\n<p>它的作用是“指示” agent 在某个状态 s 下应该采取什么 Action</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/4.png\"></p>\n<p>这样一条路径path或者说 trajectory 就是一种策略 policy</p>\n<p>策略同样还是用条件概率来表示<br><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/5.png\"></p>\n<p>不过上面的策略是一种确定性的策略，也就是在状态 s1 下，一定会采取动作 a2，并非随机性的，下面的就是随机性的策略了</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/6.png\"></p>\n<p>其条件概率就是这样的了</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/7.png\"></p>\n<p>通常概率为零的就会省略掉，策略可以通过一个表格来表示，这种表格既能描述确定性的情况，也能描述随机性的情况</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/8.png\"></p>\n<p>对应的代码实现，通常是用数组或者矩阵来存储这个表格以表示某个策略，通过对（0，1）这个区间进行随机采样，以 s1 为例，若 x 属于 (0,0.5) 就采取a2，若 x 属于 (0.5,1) 就采取 a3</p>\n</blockquote>\n<h3 id=\"Reward\"><a href=\"#Reward\" class=\"headerlink\" title=\"Reward\"></a>Reward</h3><blockquote>\n<p>首先它是一个数，一个标量，agent 在当前状态下采取了一个动作Action，会得到一个奖励 reward，如果 reward 值为正，代表鼓励该行为（Action），否则就是惩罚该行为（Action）</p>\n<p>reward 是 agent 与人交互的一种手段，因为这个奖励机制是人为设定的，通过设定合适奖励机制引导 agent 达到预期的效果，同样的可以用条件概率来表示<br>$$<br>p(r&#x3D;-1|s_1,a_1)&#x3D;0.6,p(r \\neq -1|s_1,a_1)&#x3D;0.4<br>$$<br>获得 reward 的多少不是一层不变的，通常会有一个衰减系数来控制</p>\n<p>reward 依赖于当前状态 s 以及在当前状态下采取的动作Action</p>\n</blockquote>\n<h3 id=\"Trajectory-and-return\"><a href=\"#Trajectory-and-return\" class=\"headerlink\" title=\"Trajectory and return\"></a>Trajectory and return</h3><blockquote>\n<p>Trajectory 是一条 状态 state–&gt;动作 Action–&gt;奖励 reward 的链，具体例子如下图所示</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/9.png\"></p>\n<p>return 就是一个 trajectory 上所有 reward 之和，上面的 return &#x3D; 0 + 0 + 0 + 1 &#x3D; 1</p>\n<p>trajectory的优劣或者说策略policy的优劣是与其对应的return相关的</p>\n</blockquote>\n<h3 id=\"discounted-return\"><a href=\"#discounted-return\" class=\"headerlink\" title=\"discounted return\"></a>discounted return</h3><blockquote>\n<p>为了获得尽可能多的奖励 reward 和总的奖励 return，一条trajectory可能会无限进行下去，return最终会发散，如下图</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/10.png\"></p>\n<p>这就引入一个discount rate，通常用 γ 表示，γ 在[0,1)之间，discount rate 与 return 结合就得到了 discount return</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/11.png\"></p>\n<p>当 γ &lt; 1 时，这个无穷数列的和就等效于右边的 1&#x2F;( 1 - γ)</p>\n<p>通过引入这个衰减因子 γ，就使得 return 从一个发散状态，变成一个有限值 discount return。</p>\n<p>另外，当 γ 趋于 0 时，那么随着 γ 的次幂越来越高，会很快的衰减至0，return 的值就主要取决于前面几项；而 γ 趋于 1 时，后面的项衰减会慢很多。换句话说，γ 越小，越注重当前或者未来临近的状态-动作-奖励，而 γ 越大，则是注重更长远的策略</p>\n</blockquote>\n<h3 id=\"episode\"><a href=\"#episode\" class=\"headerlink\" title=\"episode\"></a>episode</h3><blockquote>\n<p>episode属于trajectory的一种，当 agent 采用某种策略时，它可能达到最终状态（terminal state）后就停下来了，不会一直进行下去，更不会使return发散，它是有限的，这样的任务也称之为 episodic tasks</p>\n<p>有些任务是没有最终状态，它会无限的进行下去，称之为 continuing tasks</p>\n</blockquote>\n<h3 id=\"MDP\"><a href=\"#MDP\" class=\"headerlink\" title=\"MDP\"></a>MDP</h3><blockquote>\n<p>马尔可夫框架</p>\n<p>三个集合：状态集合 S、动作集合A(s)、奖励集合R(s,a)</p>\n<p>两个概率分布（条件概率？）：状态转换的条件概率 p( s’ | s, a)、奖励的条件概率 p( r | s, a)</p>\n<p>策略：Π( a | s )</p>\n<p>马尔可夫性质：memoryless property，历史无关性</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/12.png\"></p>\n<p>下一状态只取决于当前状态，以及当期状态下采取的动作，奖励也是如此</p>\n<p>如果马尔可夫过程 Markov process 的策略确定下来了，那马尔可夫过程就变成了马尔可夫决策过程 Markov decision process</p>\n</blockquote>\n","cover_type":"img","excerpt":"","more":"<h1 id=\"RL的基本数学原理—基本概念\"><a href=\"#RL的基本数学原理—基本概念\" class=\"headerlink\" title=\"RL的基本数学原理—基本概念\"></a>RL的基本数学原理—基本概念</h1><h3 id=\"State\"><a href=\"#State\" class=\"headerlink\" title=\"State\"></a>State</h3><blockquote>\n<p>它是agent相对于当前环境的一个状态，如当前的坐标 (x,y)，速度、加速度等</p>\n<p>所有的状态构成的一个集合称之为状态空间，如下图，s1~s9构成了一个状态空间，这里是2D的，那么状态主要就是location （x, y）</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/1.png\"></p>\n</blockquote>\n<h3 id=\"Action\"><a href=\"#Action\" class=\"headerlink\" title=\"Action\"></a>Action</h3><blockquote>\n<p>在每一个状态下，都会有对应的一系列的动作Action，如2D平面上，在一个状态下可以采取的Action有前进、后退、左右移动、原地不动</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/2.png\"></p>\n<p>所有的Action构成的一个集合就称之为动作空间 Action space</p>\n<p>Action 和 state 是相互依赖的，不同的状态下对应不同的动作<br>$$<br>A(s_i) &#x3D; {a_i}<br>$$<br>上式意为，在状态 s_i 下，可采取的动作 a_i</p>\n</blockquote>\n<h3 id=\"State-transition\"><a href=\"#State-transition\" class=\"headerlink\" title=\"State transition\"></a>State transition</h3><blockquote>\n<p>状态转换，在当前状态 s1 下，采取动作 a2(有概率采取动作 a2)，会转换到下一状态 s2，而这个下一状态 s2，其实是不确定的，它根据采取的动作而定，而且只是有概率转移到某个状态 s<br>$$<br>s_1 \\xrightarrow{a_2} s_2<br>$$<br>状态转换定义了 agent 与环境交互的一种行为</p>\n<p>在某个状态下，采取某个动作就直接确定的转换到某个状态，这种情况就类似于拓扑图，但实际上不是这样</p>\n<p>实际中，在一个状态下，采取的动作都是有概率的，如在 s1 下，可以采取 a1、a2、a3 三种动作，但是这三个动作都是有概率的，p(a1)&#x3D;p(a2)&#x3D;0.3，p(a3)&#x3D;0.4，并且每个动作对应的转换状态(下一状态)也可能是多个且一样的有概率，如采取 a1，对应的状态有s2、s3、s4，概率也是0.3、0.3、0.4，这样，事情就变得复杂多样了</p>\n</blockquote>\n<h3 id=\"State-transition-probability\"><a href=\"#State-transition-probability\" class=\"headerlink\" title=\"State transition probability\"></a>State transition probability</h3><blockquote>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/3.png\"></p>\n<p>这里采用了条件概率来描述 state transistion 的过程，这是一个随机性的例子(deterministic case)</p>\n</blockquote>\n<h3 id=\"Policy\"><a href=\"#Policy\" class=\"headerlink\" title=\"Policy\"></a>Policy</h3><blockquote>\n<p>它的作用是“指示” agent 在某个状态 s 下应该采取什么 Action</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/4.png\"></p>\n<p>这样一条路径path或者说 trajectory 就是一种策略 policy</p>\n<p>策略同样还是用条件概率来表示<br><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/5.png\"></p>\n<p>不过上面的策略是一种确定性的策略，也就是在状态 s1 下，一定会采取动作 a2，并非随机性的，下面的就是随机性的策略了</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/6.png\"></p>\n<p>其条件概率就是这样的了</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/7.png\"></p>\n<p>通常概率为零的就会省略掉，策略可以通过一个表格来表示，这种表格既能描述确定性的情况，也能描述随机性的情况</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/8.png\"></p>\n<p>对应的代码实现，通常是用数组或者矩阵来存储这个表格以表示某个策略，通过对（0，1）这个区间进行随机采样，以 s1 为例，若 x 属于 (0,0.5) 就采取a2，若 x 属于 (0.5,1) 就采取 a3</p>\n</blockquote>\n<h3 id=\"Reward\"><a href=\"#Reward\" class=\"headerlink\" title=\"Reward\"></a>Reward</h3><blockquote>\n<p>首先它是一个数，一个标量，agent 在当前状态下采取了一个动作Action，会得到一个奖励 reward，如果 reward 值为正，代表鼓励该行为（Action），否则就是惩罚该行为（Action）</p>\n<p>reward 是 agent 与人交互的一种手段，因为这个奖励机制是人为设定的，通过设定合适奖励机制引导 agent 达到预期的效果，同样的可以用条件概率来表示<br>$$<br>p(r&#x3D;-1|s_1,a_1)&#x3D;0.6,p(r \\neq -1|s_1,a_1)&#x3D;0.4<br>$$<br>获得 reward 的多少不是一层不变的，通常会有一个衰减系数来控制</p>\n<p>reward 依赖于当前状态 s 以及在当前状态下采取的动作Action</p>\n</blockquote>\n<h3 id=\"Trajectory-and-return\"><a href=\"#Trajectory-and-return\" class=\"headerlink\" title=\"Trajectory and return\"></a>Trajectory and return</h3><blockquote>\n<p>Trajectory 是一条 状态 state–&gt;动作 Action–&gt;奖励 reward 的链，具体例子如下图所示</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/9.png\"></p>\n<p>return 就是一个 trajectory 上所有 reward 之和，上面的 return &#x3D; 0 + 0 + 0 + 1 &#x3D; 1</p>\n<p>trajectory的优劣或者说策略policy的优劣是与其对应的return相关的</p>\n</blockquote>\n<h3 id=\"discounted-return\"><a href=\"#discounted-return\" class=\"headerlink\" title=\"discounted return\"></a>discounted return</h3><blockquote>\n<p>为了获得尽可能多的奖励 reward 和总的奖励 return，一条trajectory可能会无限进行下去，return最终会发散，如下图</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/10.png\"></p>\n<p>这就引入一个discount rate，通常用 γ 表示，γ 在[0,1)之间，discount rate 与 return 结合就得到了 discount return</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/11.png\"></p>\n<p>当 γ &lt; 1 时，这个无穷数列的和就等效于右边的 1&#x2F;( 1 - γ)</p>\n<p>通过引入这个衰减因子 γ，就使得 return 从一个发散状态，变成一个有限值 discount return。</p>\n<p>另外，当 γ 趋于 0 时，那么随着 γ 的次幂越来越高，会很快的衰减至0，return 的值就主要取决于前面几项；而 γ 趋于 1 时，后面的项衰减会慢很多。换句话说，γ 越小，越注重当前或者未来临近的状态-动作-奖励，而 γ 越大，则是注重更长远的策略</p>\n</blockquote>\n<h3 id=\"episode\"><a href=\"#episode\" class=\"headerlink\" title=\"episode\"></a>episode</h3><blockquote>\n<p>episode属于trajectory的一种，当 agent 采用某种策略时，它可能达到最终状态（terminal state）后就停下来了，不会一直进行下去，更不会使return发散，它是有限的，这样的任务也称之为 episodic tasks</p>\n<p>有些任务是没有最终状态，它会无限的进行下去，称之为 continuing tasks</p>\n</blockquote>\n<h3 id=\"MDP\"><a href=\"#MDP\" class=\"headerlink\" title=\"MDP\"></a>MDP</h3><blockquote>\n<p>马尔可夫框架</p>\n<p>三个集合：状态集合 S、动作集合A(s)、奖励集合R(s,a)</p>\n<p>两个概率分布（条件概率？）：状态转换的条件概率 p( s’ | s, a)、奖励的条件概率 p( r | s, a)</p>\n<p>策略：Π( a | s )</p>\n<p>马尔可夫性质：memoryless property，历史无关性</p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/first_part/12.png\"></p>\n<p>下一状态只取决于当前状态，以及当期状态下采取的动作，奖励也是如此</p>\n<p>如果马尔可夫过程 Markov process 的策略确定下来了，那马尔可夫过程就变成了马尔可夫决策过程 Markov decision process</p>\n</blockquote>\n"},{"title":"科学上网？NO，是怒火中烧","data":"2025-05-24T13:31:00.000Z","updated":"2025-05-24T13:31:00.000Z","type":"安装笔记","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p18.png","_content":"\n\n# 老版本Ubuntu科学上网须知（boom！！！）\n\n\n\n### 必须记录下来\n\n\n\n> #### 绝大多数版本的clash verge或者其他形式的软件都不支持切换内核为meta了\n>\n> #### 为数不多的就是极低版本的，这里就是采用的 1.4.3 版本的\n>\n> #### 先来个地址\n>\n> > https://github.com/clash-verge-rev/clash-verge-rev/releases?page=5\n>\n> #### 直接下载对应版本的 deb ，然后 dpkg 安装\n>\n> #### 安装好之后，命令框同样会报错，提示说缺少依赖，叫 lib 什么来着\n>\n> #### 先不管报错，先打开软件\n>\n> #### 在设置这一栏，找到 Clash 内核 这行\n>\n> ![](http://picbed.yanzu.tech/img/ticks/1/1.png)\n>\n> #### 点击齿轮，点击授权 Clash Meta，然后再选择它\n>\n> ![](http://picbed.yanzu.tech/img/ticks/1/2.png)\n>\n> #### 授权成功会有提示\n>\n> ![](http://picbed.yanzu.tech/img/ticks/1/3.png)\n>\n> #### 然后就是临门一脚了，添加订阅节点也很恼火，因为不同的梯子或者同一款梯子不同版本，它支持的机场是不一样的，具体在那里看支持那些机场我也不知道，反正我知道这个梯子支持这个机场\n>\n> > https://xn--cp3a08l.com/#/register?code=dGEFYisq\n>\n> #### 我也不知道到底是不是这个地址，反正就叫这个名\n\n#### 1.4.3的链接(防止以后找不到这个版本了，自己存一份贴一个链接)\n> https://pan.baidu.com/s/1CA6RacPEwuf_aaqnrNRHcA?pwd=7kx4 提取码: 7kx4\n\n#### 老版本ubuntu太难了，各种不友好，捣腾了好几天，才总算是整好了，不能科学上网，我真的，裂开，我只能说我连猫咖都去不了，dddd 啊\n\n#### 真滴捞阿！！！！！！！！！！！！！！！！！！","source":"_posts/18.md","raw":"---\ntitle: 科学上网？NO，是怒火中烧\ndata: 2025-05-24 21:31:00\nupdated: 2025-05-24 21:31:00\ntype: 安装笔记\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p18.png\ntags:\n  - 安装\n  - 上网\n  - 踩坑\n  - Ubuntu\n---\n\n\n# 老版本Ubuntu科学上网须知（boom！！！）\n\n\n\n### 必须记录下来\n\n\n\n> #### 绝大多数版本的clash verge或者其他形式的软件都不支持切换内核为meta了\n>\n> #### 为数不多的就是极低版本的，这里就是采用的 1.4.3 版本的\n>\n> #### 先来个地址\n>\n> > https://github.com/clash-verge-rev/clash-verge-rev/releases?page=5\n>\n> #### 直接下载对应版本的 deb ，然后 dpkg 安装\n>\n> #### 安装好之后，命令框同样会报错，提示说缺少依赖，叫 lib 什么来着\n>\n> #### 先不管报错，先打开软件\n>\n> #### 在设置这一栏，找到 Clash 内核 这行\n>\n> ![](http://picbed.yanzu.tech/img/ticks/1/1.png)\n>\n> #### 点击齿轮，点击授权 Clash Meta，然后再选择它\n>\n> ![](http://picbed.yanzu.tech/img/ticks/1/2.png)\n>\n> #### 授权成功会有提示\n>\n> ![](http://picbed.yanzu.tech/img/ticks/1/3.png)\n>\n> #### 然后就是临门一脚了，添加订阅节点也很恼火，因为不同的梯子或者同一款梯子不同版本，它支持的机场是不一样的，具体在那里看支持那些机场我也不知道，反正我知道这个梯子支持这个机场\n>\n> > https://xn--cp3a08l.com/#/register?code=dGEFYisq\n>\n> #### 我也不知道到底是不是这个地址，反正就叫这个名\n\n#### 1.4.3的链接(防止以后找不到这个版本了，自己存一份贴一个链接)\n> https://pan.baidu.com/s/1CA6RacPEwuf_aaqnrNRHcA?pwd=7kx4 提取码: 7kx4\n\n#### 老版本ubuntu太难了，各种不友好，捣腾了好几天，才总算是整好了，不能科学上网，我真的，裂开，我只能说我连猫咖都去不了，dddd 啊\n\n#### 真滴捞阿！！！！！！！！！！！！！！！！！！","slug":"18","published":1,"date":"2025-05-28T00:34:37.643Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cr30019iku4a36x56zn","content":"<h1 id=\"老版本Ubuntu科学上网须知（boom！！！）\"><a href=\"#老版本Ubuntu科学上网须知（boom！！！）\" class=\"headerlink\" title=\"老版本Ubuntu科学上网须知（boom！！！）\"></a>老版本Ubuntu科学上网须知（boom！！！）</h1><h3 id=\"必须记录下来\"><a href=\"#必须记录下来\" class=\"headerlink\" title=\"必须记录下来\"></a>必须记录下来</h3><blockquote>\n<h4 id=\"绝大多数版本的clash-verge或者其他形式的软件都不支持切换内核为meta了\"><a href=\"#绝大多数版本的clash-verge或者其他形式的软件都不支持切换内核为meta了\" class=\"headerlink\" title=\"绝大多数版本的clash verge或者其他形式的软件都不支持切换内核为meta了\"></a>绝大多数版本的clash verge或者其他形式的软件都不支持切换内核为meta了</h4><h4 id=\"为数不多的就是极低版本的，这里就是采用的-1-4-3-版本的\"><a href=\"#为数不多的就是极低版本的，这里就是采用的-1-4-3-版本的\" class=\"headerlink\" title=\"为数不多的就是极低版本的，这里就是采用的 1.4.3 版本的\"></a>为数不多的就是极低版本的，这里就是采用的 1.4.3 版本的</h4><h4 id=\"先来个地址\"><a href=\"#先来个地址\" class=\"headerlink\" title=\"先来个地址\"></a>先来个地址</h4><blockquote>\n<p><a href=\"https://github.com/clash-verge-rev/clash-verge-rev/releases?page=5\">https://github.com/clash-verge-rev/clash-verge-rev/releases?page=5</a></p>\n</blockquote>\n<h4 id=\"直接下载对应版本的-deb-，然后-dpkg-安装\"><a href=\"#直接下载对应版本的-deb-，然后-dpkg-安装\" class=\"headerlink\" title=\"直接下载对应版本的 deb ，然后 dpkg 安装\"></a>直接下载对应版本的 deb ，然后 dpkg 安装</h4><h4 id=\"安装好之后，命令框同样会报错，提示说缺少依赖，叫-lib-什么来着\"><a href=\"#安装好之后，命令框同样会报错，提示说缺少依赖，叫-lib-什么来着\" class=\"headerlink\" title=\"安装好之后，命令框同样会报错，提示说缺少依赖，叫 lib 什么来着\"></a>安装好之后，命令框同样会报错，提示说缺少依赖，叫 lib 什么来着</h4><h4 id=\"先不管报错，先打开软件\"><a href=\"#先不管报错，先打开软件\" class=\"headerlink\" title=\"先不管报错，先打开软件\"></a>先不管报错，先打开软件</h4><h4 id=\"在设置这一栏，找到-Clash-内核-这行\"><a href=\"#在设置这一栏，找到-Clash-内核-这行\" class=\"headerlink\" title=\"在设置这一栏，找到 Clash 内核 这行\"></a>在设置这一栏，找到 Clash 内核 这行</h4><p><img src=\"http://picbed.yanzu.tech/img/ticks/1/1.png\"></p>\n<h4 id=\"点击齿轮，点击授权-Clash-Meta，然后再选择它\"><a href=\"#点击齿轮，点击授权-Clash-Meta，然后再选择它\" class=\"headerlink\" title=\"点击齿轮，点击授权 Clash Meta，然后再选择它\"></a>点击齿轮，点击授权 Clash Meta，然后再选择它</h4><p><img src=\"http://picbed.yanzu.tech/img/ticks/1/2.png\"></p>\n<h4 id=\"授权成功会有提示\"><a href=\"#授权成功会有提示\" class=\"headerlink\" title=\"授权成功会有提示\"></a>授权成功会有提示</h4><p><img src=\"http://picbed.yanzu.tech/img/ticks/1/3.png\"></p>\n<h4 id=\"然后就是临门一脚了，添加订阅节点也很恼火，因为不同的梯子或者同一款梯子不同版本，它支持的机场是不一样的，具体在那里看支持那些机场我也不知道，反正我知道这个梯子支持这个机场\"><a href=\"#然后就是临门一脚了，添加订阅节点也很恼火，因为不同的梯子或者同一款梯子不同版本，它支持的机场是不一样的，具体在那里看支持那些机场我也不知道，反正我知道这个梯子支持这个机场\" class=\"headerlink\" title=\"然后就是临门一脚了，添加订阅节点也很恼火，因为不同的梯子或者同一款梯子不同版本，它支持的机场是不一样的，具体在那里看支持那些机场我也不知道，反正我知道这个梯子支持这个机场\"></a>然后就是临门一脚了，添加订阅节点也很恼火，因为不同的梯子或者同一款梯子不同版本，它支持的机场是不一样的，具体在那里看支持那些机场我也不知道，反正我知道这个梯子支持这个机场</h4><blockquote>\n<p><a href=\"https://赔钱.com/#/register?code=dGEFYisq\">https://xn--cp3a08l.com/#/register?code=dGEFYisq</a></p>\n</blockquote>\n<h4 id=\"我也不知道到底是不是这个地址，反正就叫这个名\"><a href=\"#我也不知道到底是不是这个地址，反正就叫这个名\" class=\"headerlink\" title=\"我也不知道到底是不是这个地址，反正就叫这个名\"></a>我也不知道到底是不是这个地址，反正就叫这个名</h4></blockquote>\n<h4 id=\"1-4-3的链接-防止以后找不到这个版本了，自己存一份贴一个链接\"><a href=\"#1-4-3的链接-防止以后找不到这个版本了，自己存一份贴一个链接\" class=\"headerlink\" title=\"1.4.3的链接(防止以后找不到这个版本了，自己存一份贴一个链接)\"></a>1.4.3的链接(防止以后找不到这个版本了，自己存一份贴一个链接)</h4><blockquote>\n<p><a href=\"https://pan.baidu.com/s/1CA6RacPEwuf_aaqnrNRHcA?pwd=7kx4\">https://pan.baidu.com/s/1CA6RacPEwuf_aaqnrNRHcA?pwd=7kx4</a> 提取码: 7kx4</p>\n</blockquote>\n<h4 id=\"老版本ubuntu太难了，各种不友好，捣腾了好几天，才总算是整好了，不能科学上网，我真的，裂开，我只能说我连猫咖都去不了，dddd-啊\"><a href=\"#老版本ubuntu太难了，各种不友好，捣腾了好几天，才总算是整好了，不能科学上网，我真的，裂开，我只能说我连猫咖都去不了，dddd-啊\" class=\"headerlink\" title=\"老版本ubuntu太难了，各种不友好，捣腾了好几天，才总算是整好了，不能科学上网，我真的，裂开，我只能说我连猫咖都去不了，dddd 啊\"></a>老版本ubuntu太难了，各种不友好，捣腾了好几天，才总算是整好了，不能科学上网，我真的，裂开，我只能说我连猫咖都去不了，dddd 啊</h4><h4 id=\"真滴捞阿！！！！！！！！！！！！！！！！！！\"><a href=\"#真滴捞阿！！！！！！！！！！！！！！！！！！\" class=\"headerlink\" title=\"真滴捞阿！！！！！！！！！！！！！！！！！！\"></a>真滴捞阿！！！！！！！！！！！！！！！！！！</h4>","cover_type":"img","excerpt":"","more":"<h1 id=\"老版本Ubuntu科学上网须知（boom！！！）\"><a href=\"#老版本Ubuntu科学上网须知（boom！！！）\" class=\"headerlink\" title=\"老版本Ubuntu科学上网须知（boom！！！）\"></a>老版本Ubuntu科学上网须知（boom！！！）</h1><h3 id=\"必须记录下来\"><a href=\"#必须记录下来\" class=\"headerlink\" title=\"必须记录下来\"></a>必须记录下来</h3><blockquote>\n<h4 id=\"绝大多数版本的clash-verge或者其他形式的软件都不支持切换内核为meta了\"><a href=\"#绝大多数版本的clash-verge或者其他形式的软件都不支持切换内核为meta了\" class=\"headerlink\" title=\"绝大多数版本的clash verge或者其他形式的软件都不支持切换内核为meta了\"></a>绝大多数版本的clash verge或者其他形式的软件都不支持切换内核为meta了</h4><h4 id=\"为数不多的就是极低版本的，这里就是采用的-1-4-3-版本的\"><a href=\"#为数不多的就是极低版本的，这里就是采用的-1-4-3-版本的\" class=\"headerlink\" title=\"为数不多的就是极低版本的，这里就是采用的 1.4.3 版本的\"></a>为数不多的就是极低版本的，这里就是采用的 1.4.3 版本的</h4><h4 id=\"先来个地址\"><a href=\"#先来个地址\" class=\"headerlink\" title=\"先来个地址\"></a>先来个地址</h4><blockquote>\n<p><a href=\"https://github.com/clash-verge-rev/clash-verge-rev/releases?page=5\">https://github.com/clash-verge-rev/clash-verge-rev/releases?page=5</a></p>\n</blockquote>\n<h4 id=\"直接下载对应版本的-deb-，然后-dpkg-安装\"><a href=\"#直接下载对应版本的-deb-，然后-dpkg-安装\" class=\"headerlink\" title=\"直接下载对应版本的 deb ，然后 dpkg 安装\"></a>直接下载对应版本的 deb ，然后 dpkg 安装</h4><h4 id=\"安装好之后，命令框同样会报错，提示说缺少依赖，叫-lib-什么来着\"><a href=\"#安装好之后，命令框同样会报错，提示说缺少依赖，叫-lib-什么来着\" class=\"headerlink\" title=\"安装好之后，命令框同样会报错，提示说缺少依赖，叫 lib 什么来着\"></a>安装好之后，命令框同样会报错，提示说缺少依赖，叫 lib 什么来着</h4><h4 id=\"先不管报错，先打开软件\"><a href=\"#先不管报错，先打开软件\" class=\"headerlink\" title=\"先不管报错，先打开软件\"></a>先不管报错，先打开软件</h4><h4 id=\"在设置这一栏，找到-Clash-内核-这行\"><a href=\"#在设置这一栏，找到-Clash-内核-这行\" class=\"headerlink\" title=\"在设置这一栏，找到 Clash 内核 这行\"></a>在设置这一栏，找到 Clash 内核 这行</h4><p><img src=\"http://picbed.yanzu.tech/img/ticks/1/1.png\"></p>\n<h4 id=\"点击齿轮，点击授权-Clash-Meta，然后再选择它\"><a href=\"#点击齿轮，点击授权-Clash-Meta，然后再选择它\" class=\"headerlink\" title=\"点击齿轮，点击授权 Clash Meta，然后再选择它\"></a>点击齿轮，点击授权 Clash Meta，然后再选择它</h4><p><img src=\"http://picbed.yanzu.tech/img/ticks/1/2.png\"></p>\n<h4 id=\"授权成功会有提示\"><a href=\"#授权成功会有提示\" class=\"headerlink\" title=\"授权成功会有提示\"></a>授权成功会有提示</h4><p><img src=\"http://picbed.yanzu.tech/img/ticks/1/3.png\"></p>\n<h4 id=\"然后就是临门一脚了，添加订阅节点也很恼火，因为不同的梯子或者同一款梯子不同版本，它支持的机场是不一样的，具体在那里看支持那些机场我也不知道，反正我知道这个梯子支持这个机场\"><a href=\"#然后就是临门一脚了，添加订阅节点也很恼火，因为不同的梯子或者同一款梯子不同版本，它支持的机场是不一样的，具体在那里看支持那些机场我也不知道，反正我知道这个梯子支持这个机场\" class=\"headerlink\" title=\"然后就是临门一脚了，添加订阅节点也很恼火，因为不同的梯子或者同一款梯子不同版本，它支持的机场是不一样的，具体在那里看支持那些机场我也不知道，反正我知道这个梯子支持这个机场\"></a>然后就是临门一脚了，添加订阅节点也很恼火，因为不同的梯子或者同一款梯子不同版本，它支持的机场是不一样的，具体在那里看支持那些机场我也不知道，反正我知道这个梯子支持这个机场</h4><blockquote>\n<p><a href=\"https://赔钱.com/#/register?code=dGEFYisq\">https://xn--cp3a08l.com/#/register?code=dGEFYisq</a></p>\n</blockquote>\n<h4 id=\"我也不知道到底是不是这个地址，反正就叫这个名\"><a href=\"#我也不知道到底是不是这个地址，反正就叫这个名\" class=\"headerlink\" title=\"我也不知道到底是不是这个地址，反正就叫这个名\"></a>我也不知道到底是不是这个地址，反正就叫这个名</h4></blockquote>\n<h4 id=\"1-4-3的链接-防止以后找不到这个版本了，自己存一份贴一个链接\"><a href=\"#1-4-3的链接-防止以后找不到这个版本了，自己存一份贴一个链接\" class=\"headerlink\" title=\"1.4.3的链接(防止以后找不到这个版本了，自己存一份贴一个链接)\"></a>1.4.3的链接(防止以后找不到这个版本了，自己存一份贴一个链接)</h4><blockquote>\n<p><a href=\"https://pan.baidu.com/s/1CA6RacPEwuf_aaqnrNRHcA?pwd=7kx4\">https://pan.baidu.com/s/1CA6RacPEwuf_aaqnrNRHcA?pwd=7kx4</a> 提取码: 7kx4</p>\n</blockquote>\n<h4 id=\"老版本ubuntu太难了，各种不友好，捣腾了好几天，才总算是整好了，不能科学上网，我真的，裂开，我只能说我连猫咖都去不了，dddd-啊\"><a href=\"#老版本ubuntu太难了，各种不友好，捣腾了好几天，才总算是整好了，不能科学上网，我真的，裂开，我只能说我连猫咖都去不了，dddd-啊\" class=\"headerlink\" title=\"老版本ubuntu太难了，各种不友好，捣腾了好几天，才总算是整好了，不能科学上网，我真的，裂开，我只能说我连猫咖都去不了，dddd 啊\"></a>老版本ubuntu太难了，各种不友好，捣腾了好几天，才总算是整好了，不能科学上网，我真的，裂开，我只能说我连猫咖都去不了，dddd 啊</h4><h4 id=\"真滴捞阿！！！！！！！！！！！！！！！！！！\"><a href=\"#真滴捞阿！！！！！！！！！！！！！！！！！！\" class=\"headerlink\" title=\"真滴捞阿！！！！！！！！！！！！！！！！！！\"></a>真滴捞阿！！！！！！！！！！！！！！！！！！</h4>"},{"title":"DL之路---啃鱼书（1）","data":"2025-06-10T09:20:00.000Z","updated":"2025-06-10T09:20:00.000Z","type":"DL","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p20.png","_content":"\n\n# 感知机\n\n\n\n> #### 感知机（Perceptron）是一种人工神经网络，用于解决二分类问题\n>\n> #### 它模拟的是生物神经元的行为，主要包括：\n>\n> > #### 输入：多个特征值 $ x_1,x_2...x_n $ \n> >\n> > #### 权重：每个输入对应一个权重 $ \\omega_1,\\omega_2...\\omega_3 $\n> >\n> > #### 偏置：一个常数项 $ b $\n>\n> #### 偏置是用于控制神经元被激活的容易程度\n>\n> #### 权重是用于控制各个信号的重要性的\n>\n> #### 而激活函数则是决定以什么方式来激活输入信号的总和\n>\n> #### 数学表达式为：\n>\n> $$\n> y = sign(\\omega \\cdot x + b)\n> $$\n>\n> #### 其中，$ \\omega \\cdot x = \\sum_{i=1}^n \\omega_i x_i $，sign函数定义为：\n>\n> $$\n>  sign(z) = \n> \\begin{cases}\n> 1 & \\text{if } z > 0, \\\\\n> -1 & \\text otherwise.\n> \\end{cases}\n> $$\n>\n> \n>\n> #### 工作原理：找到一个线性超平面（即一条直线或一个超平面）来将数据分为两类（+1 和 -1）。通过不断调整权重 $ \\omega $ 和偏置 $ b $，使得训练数据被正确分类\n>\n> \n>\n> #### 感知机学习算法\n>\n> > #### 它的学习规则是一种迭代算法，所谓的学习就是确定合适的参数的过程\n> >\n> > > #### 初始化：随机初始化权重 $ \\omega $ 和偏置 $ b $\n> > >\n> > > #### 逐个遍历样本：对于每个训练样本 $ (x_i,y_i) $，如果预测值与真实值不一致（即 $ y_i(\\omega \\cdot x_i+b) \\le 0 $ ），就更新参数\n> > >\n> > > $$\n> > > \\omega \\leftarrow \\omega + \\eta y_i x_i \\\\\n> > > b \\leftarrow b + \\eta y_i\n> > > $$\n> > >\n> > > #### 其中，$ \\eta $ 是学习率\n> >\n> > #### 重复训练，直到所有样本被正确分类或达到最大迭代次数\n\n\n\n### code\n\n> #### 与门实现\n>\n> ```python\n> def AND(x1, x2):\n>     w1, w2, theta = 0.5, 0.5, 0.7\n>     tmp = x1 * w1 + x2 * w2\n>     if tmp <= theta:\n>         return 0\n>     elif tmp > theta:\n>         return 1\n>     \n> print(AND(0, 0))\n> print(AND(0, 1))\n> print(AND(1, 0))\n> print(AND(1, 1))\n> ```\n>\n> \n>\n> #### 或门实现\n>\n> ```python\n> def OR(x1, x2):\n>     w1, w2, theta = 0.5, 0.5, 0.4\n>     tmp = x1 * w1 + x2 * w2\n>     if tmp <= theta:\n>         return 0\n>     else:\n>         return 1\n>     \n> print(OR(0, 0))\n> print(OR(0, 1))\n> print(OR(1, 0))\n> print(OR(1, 1))\n> ```\n>\n> \n>\n> #### 与非门实现\n>\n> ```python\n> def NotAnd(x1, x2):\n>     # 直接对与门取反\n>     w1 , w2, theta = -0.5, -0.5, -0.7\n>     tmp = w1 * x1 + w2 * x2\n>     if tmp <= theta:\n>         return 0\n>     else:\n>         return 1\n>         \n> print(NotAnd(0, 0))\n> print(NotAnd(0, 1))\n> print(NotAnd(1, 0))\n> print(NotAnd(1, 1))\n> ```\n>\n> \n>\n> #### 或非门实现\n>\n> ```python\n> def NotOr(x1, x2):\n>     # 直接对或门取反\n>     w1 , w2, theta = -0.5, -0.5, -0.4\n>     tmp = w1 * x1 + w2 * x2\n>     if tmp <= theta:\n>         return 0\n>     else:\n>         return 1\n> \n> print(NotOr(0, 0))\n> print(NotOr(0, 1))\n> print(NotOr(1, 0))\n> print(NotOr(1, 1))\n> ```\n>\n> \n>\n> #### 导入权重和偏置之后\n>\n> ```python\n> import numpy as np\n> def And(x1, x2):\n>     x = np.array([x1, x2])\n>     w = np.array([0.5, 0.5])\n>     b = -0.7\n>     tmp = np.sum(w * x) + b\n>     if tmp <= 0:\n>         return 0\n>     else:\n>         return 1\n> \n> print(And(0, 0))\n> print(And(0, 1))\n> print(And(1, 0))\n> print(And(1, 1))\n> \n> \n> def NOTAnd(x1, x2):\n>     x = np.array([x1, x2])\n>     w = np.array([-0.5, -0.5])\n>     b = 0.7\n>     tmp = np.sum(w * x) + b\n>     if tmp <= 0:\n>         return 0\n>     else:\n>         return 1\n> \n> print(NOTAnd(0, 0))\n> print(NOTAnd(0, 1))\n> print(NOTAnd(1, 0))\n> print(NOTAnd(1, 1))\n> ```\n>\n> \n>\n> #### 单层感知机无法表示异或门，单层感知机只能划分线性空间，无法划分非线性空间(单层感知机也叫朴素感知机)\n>\n> #### 多层感知机（神经网络）\n>\n> ```python\n> # 与非门、或门和与门实现异或门\n> def XOR(x1, x2):\n>     s1 = NOTAnd(x1, x2)\n>     s2 = OR(x1, x2)\n>     y = And(s1, s2)\n>     return y\n> \n> print(XOR(0, 0))\n> print(XOR(0, 1))\n> print(XOR(1, 0))\n> print(XOR(1, 1))\n> ```\n\n","source":"_posts/20.md","raw":"---\ntitle: DL之路---啃鱼书（1）\ndata: 2025-06-10 17:20:00\nupdated: 2025-06-10 17:20:00\ntype: DL\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p20.png\ntags:\n  - DL\n  - Learning\n  - gnaw_book\n---\n\n\n# 感知机\n\n\n\n> #### 感知机（Perceptron）是一种人工神经网络，用于解决二分类问题\n>\n> #### 它模拟的是生物神经元的行为，主要包括：\n>\n> > #### 输入：多个特征值 $ x_1,x_2...x_n $ \n> >\n> > #### 权重：每个输入对应一个权重 $ \\omega_1,\\omega_2...\\omega_3 $\n> >\n> > #### 偏置：一个常数项 $ b $\n>\n> #### 偏置是用于控制神经元被激活的容易程度\n>\n> #### 权重是用于控制各个信号的重要性的\n>\n> #### 而激活函数则是决定以什么方式来激活输入信号的总和\n>\n> #### 数学表达式为：\n>\n> $$\n> y = sign(\\omega \\cdot x + b)\n> $$\n>\n> #### 其中，$ \\omega \\cdot x = \\sum_{i=1}^n \\omega_i x_i $，sign函数定义为：\n>\n> $$\n>  sign(z) = \n> \\begin{cases}\n> 1 & \\text{if } z > 0, \\\\\n> -1 & \\text otherwise.\n> \\end{cases}\n> $$\n>\n> \n>\n> #### 工作原理：找到一个线性超平面（即一条直线或一个超平面）来将数据分为两类（+1 和 -1）。通过不断调整权重 $ \\omega $ 和偏置 $ b $，使得训练数据被正确分类\n>\n> \n>\n> #### 感知机学习算法\n>\n> > #### 它的学习规则是一种迭代算法，所谓的学习就是确定合适的参数的过程\n> >\n> > > #### 初始化：随机初始化权重 $ \\omega $ 和偏置 $ b $\n> > >\n> > > #### 逐个遍历样本：对于每个训练样本 $ (x_i,y_i) $，如果预测值与真实值不一致（即 $ y_i(\\omega \\cdot x_i+b) \\le 0 $ ），就更新参数\n> > >\n> > > $$\n> > > \\omega \\leftarrow \\omega + \\eta y_i x_i \\\\\n> > > b \\leftarrow b + \\eta y_i\n> > > $$\n> > >\n> > > #### 其中，$ \\eta $ 是学习率\n> >\n> > #### 重复训练，直到所有样本被正确分类或达到最大迭代次数\n\n\n\n### code\n\n> #### 与门实现\n>\n> ```python\n> def AND(x1, x2):\n>     w1, w2, theta = 0.5, 0.5, 0.7\n>     tmp = x1 * w1 + x2 * w2\n>     if tmp <= theta:\n>         return 0\n>     elif tmp > theta:\n>         return 1\n>     \n> print(AND(0, 0))\n> print(AND(0, 1))\n> print(AND(1, 0))\n> print(AND(1, 1))\n> ```\n>\n> \n>\n> #### 或门实现\n>\n> ```python\n> def OR(x1, x2):\n>     w1, w2, theta = 0.5, 0.5, 0.4\n>     tmp = x1 * w1 + x2 * w2\n>     if tmp <= theta:\n>         return 0\n>     else:\n>         return 1\n>     \n> print(OR(0, 0))\n> print(OR(0, 1))\n> print(OR(1, 0))\n> print(OR(1, 1))\n> ```\n>\n> \n>\n> #### 与非门实现\n>\n> ```python\n> def NotAnd(x1, x2):\n>     # 直接对与门取反\n>     w1 , w2, theta = -0.5, -0.5, -0.7\n>     tmp = w1 * x1 + w2 * x2\n>     if tmp <= theta:\n>         return 0\n>     else:\n>         return 1\n>         \n> print(NotAnd(0, 0))\n> print(NotAnd(0, 1))\n> print(NotAnd(1, 0))\n> print(NotAnd(1, 1))\n> ```\n>\n> \n>\n> #### 或非门实现\n>\n> ```python\n> def NotOr(x1, x2):\n>     # 直接对或门取反\n>     w1 , w2, theta = -0.5, -0.5, -0.4\n>     tmp = w1 * x1 + w2 * x2\n>     if tmp <= theta:\n>         return 0\n>     else:\n>         return 1\n> \n> print(NotOr(0, 0))\n> print(NotOr(0, 1))\n> print(NotOr(1, 0))\n> print(NotOr(1, 1))\n> ```\n>\n> \n>\n> #### 导入权重和偏置之后\n>\n> ```python\n> import numpy as np\n> def And(x1, x2):\n>     x = np.array([x1, x2])\n>     w = np.array([0.5, 0.5])\n>     b = -0.7\n>     tmp = np.sum(w * x) + b\n>     if tmp <= 0:\n>         return 0\n>     else:\n>         return 1\n> \n> print(And(0, 0))\n> print(And(0, 1))\n> print(And(1, 0))\n> print(And(1, 1))\n> \n> \n> def NOTAnd(x1, x2):\n>     x = np.array([x1, x2])\n>     w = np.array([-0.5, -0.5])\n>     b = 0.7\n>     tmp = np.sum(w * x) + b\n>     if tmp <= 0:\n>         return 0\n>     else:\n>         return 1\n> \n> print(NOTAnd(0, 0))\n> print(NOTAnd(0, 1))\n> print(NOTAnd(1, 0))\n> print(NOTAnd(1, 1))\n> ```\n>\n> \n>\n> #### 单层感知机无法表示异或门，单层感知机只能划分线性空间，无法划分非线性空间(单层感知机也叫朴素感知机)\n>\n> #### 多层感知机（神经网络）\n>\n> ```python\n> # 与非门、或门和与门实现异或门\n> def XOR(x1, x2):\n>     s1 = NOTAnd(x1, x2)\n>     s2 = OR(x1, x2)\n>     y = And(s1, s2)\n>     return y\n> \n> print(XOR(0, 0))\n> print(XOR(0, 1))\n> print(XOR(1, 0))\n> print(XOR(1, 1))\n> ```\n\n","slug":"20","published":1,"date":"2025-06-11T08:01:10.058Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cr4001ciku4534b5luk","content":"<h1 id=\"感知机\"><a href=\"#感知机\" class=\"headerlink\" title=\"感知机\"></a>感知机</h1><blockquote>\n<h4 id=\"感知机（Perceptron）是一种人工神经网络，用于解决二分类问题\"><a href=\"#感知机（Perceptron）是一种人工神经网络，用于解决二分类问题\" class=\"headerlink\" title=\"感知机（Perceptron）是一种人工神经网络，用于解决二分类问题\"></a>感知机（Perceptron）是一种人工神经网络，用于解决二分类问题</h4><h4 id=\"它模拟的是生物神经元的行为，主要包括：\"><a href=\"#它模拟的是生物神经元的行为，主要包括：\" class=\"headerlink\" title=\"它模拟的是生物神经元的行为，主要包括：\"></a>它模拟的是生物神经元的行为，主要包括：</h4><blockquote>\n<h4 id=\"输入：多个特征值-x-1-x-2…x-n\"><a href=\"#输入：多个特征值-x-1-x-2…x-n\" class=\"headerlink\" title=\"输入：多个特征值 $ x_1,x_2…x_n $\"></a>输入：多个特征值 $ x_1,x_2…x_n $</h4><h4 id=\"权重：每个输入对应一个权重-omega-1-omega-2…-omega-3\"><a href=\"#权重：每个输入对应一个权重-omega-1-omega-2…-omega-3\" class=\"headerlink\" title=\"权重：每个输入对应一个权重 $ \\omega_1,\\omega_2…\\omega_3 $\"></a>权重：每个输入对应一个权重 $ \\omega_1,\\omega_2…\\omega_3 $</h4><h4 id=\"偏置：一个常数项-b\"><a href=\"#偏置：一个常数项-b\" class=\"headerlink\" title=\"偏置：一个常数项 $ b $\"></a>偏置：一个常数项 $ b $</h4></blockquote>\n<h4 id=\"偏置是用于控制神经元被激活的容易程度\"><a href=\"#偏置是用于控制神经元被激活的容易程度\" class=\"headerlink\" title=\"偏置是用于控制神经元被激活的容易程度\"></a>偏置是用于控制神经元被激活的容易程度</h4><h4 id=\"权重是用于控制各个信号的重要性的\"><a href=\"#权重是用于控制各个信号的重要性的\" class=\"headerlink\" title=\"权重是用于控制各个信号的重要性的\"></a>权重是用于控制各个信号的重要性的</h4><h4 id=\"而激活函数则是决定以什么方式来激活输入信号的总和\"><a href=\"#而激活函数则是决定以什么方式来激活输入信号的总和\" class=\"headerlink\" title=\"而激活函数则是决定以什么方式来激活输入信号的总和\"></a>而激活函数则是决定以什么方式来激活输入信号的总和</h4><h4 id=\"数学表达式为：\"><a href=\"#数学表达式为：\" class=\"headerlink\" title=\"数学表达式为：\"></a>数学表达式为：</h4><p>$$<br>y &#x3D; sign(\\omega \\cdot x + b)<br>$$</p>\n<h4 id=\"其中，-omega-cdot-x-sum-i-1-n-omega-i-x-i-，sign函数定义为：\"><a href=\"#其中，-omega-cdot-x-sum-i-1-n-omega-i-x-i-，sign函数定义为：\" class=\"headerlink\" title=\"其中，$ \\omega \\cdot x &#x3D; \\sum_{i&#x3D;1}^n \\omega_i x_i $，sign函数定义为：\"></a>其中，$ \\omega \\cdot x &#x3D; \\sum_{i&#x3D;1}^n \\omega_i x_i $，sign函数定义为：</h4><p>$$<br> sign(z) &#x3D;<br>\\begin{cases}<br>1 &amp; \\text{if } z &gt; 0, \\<br>-1 &amp; \\text otherwise.<br>\\end{cases}<br>$$</p>\n<h4 id=\"工作原理：找到一个线性超平面（即一条直线或一个超平面）来将数据分为两类（-1-和-1）。通过不断调整权重-omega-和偏置-b-，使得训练数据被正确分类\"><a href=\"#工作原理：找到一个线性超平面（即一条直线或一个超平面）来将数据分为两类（-1-和-1）。通过不断调整权重-omega-和偏置-b-，使得训练数据被正确分类\" class=\"headerlink\" title=\"工作原理：找到一个线性超平面（即一条直线或一个超平面）来将数据分为两类（+1 和 -1）。通过不断调整权重 $ \\omega $ 和偏置 $ b $，使得训练数据被正确分类\"></a>工作原理：找到一个线性超平面（即一条直线或一个超平面）来将数据分为两类（+1 和 -1）。通过不断调整权重 $ \\omega $ 和偏置 $ b $，使得训练数据被正确分类</h4><h4 id=\"感知机学习算法\"><a href=\"#感知机学习算法\" class=\"headerlink\" title=\"感知机学习算法\"></a>感知机学习算法</h4><blockquote>\n<h4 id=\"它的学习规则是一种迭代算法，所谓的学习就是确定合适的参数的过程\"><a href=\"#它的学习规则是一种迭代算法，所谓的学习就是确定合适的参数的过程\" class=\"headerlink\" title=\"它的学习规则是一种迭代算法，所谓的学习就是确定合适的参数的过程\"></a>它的学习规则是一种迭代算法，所谓的学习就是确定合适的参数的过程</h4><blockquote>\n<h4 id=\"初始化：随机初始化权重-omega-和偏置-b\"><a href=\"#初始化：随机初始化权重-omega-和偏置-b\" class=\"headerlink\" title=\"初始化：随机初始化权重 $ \\omega $ 和偏置 $ b $\"></a>初始化：随机初始化权重 $ \\omega $ 和偏置 $ b $</h4><h4 id=\"逐个遍历样本：对于每个训练样本-x-i-y-i-，如果预测值与真实值不一致（即-y-i-omega-cdot-x-i-b-le-0-），就更新参数\"><a href=\"#逐个遍历样本：对于每个训练样本-x-i-y-i-，如果预测值与真实值不一致（即-y-i-omega-cdot-x-i-b-le-0-），就更新参数\" class=\"headerlink\" title=\"逐个遍历样本：对于每个训练样本 $ (x_i,y_i) $，如果预测值与真实值不一致（即 $ y_i(\\omega \\cdot x_i+b) \\le 0 $ ），就更新参数\"></a>逐个遍历样本：对于每个训练样本 $ (x_i,y_i) $，如果预测值与真实值不一致（即 $ y_i(\\omega \\cdot x_i+b) \\le 0 $ ），就更新参数</h4><p>$$<br>\\omega \\leftarrow \\omega + \\eta y_i x_i \\<br>b \\leftarrow b + \\eta y_i<br>$$</p>\n<h4 id=\"其中，-eta-是学习率\"><a href=\"#其中，-eta-是学习率\" class=\"headerlink\" title=\"其中，$ \\eta $ 是学习率\"></a>其中，$ \\eta $ 是学习率</h4></blockquote>\n<h4 id=\"重复训练，直到所有样本被正确分类或达到最大迭代次数\"><a href=\"#重复训练，直到所有样本被正确分类或达到最大迭代次数\" class=\"headerlink\" title=\"重复训练，直到所有样本被正确分类或达到最大迭代次数\"></a>重复训练，直到所有样本被正确分类或达到最大迭代次数</h4></blockquote>\n</blockquote>\n<h3 id=\"code\"><a href=\"#code\" class=\"headerlink\" title=\"code\"></a>code</h3><blockquote>\n<h4 id=\"与门实现\"><a href=\"#与门实现\" class=\"headerlink\" title=\"与门实现\"></a>与门实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">AND</span>(<span class=\"params\">x1, x2</span>):</span><br><span class=\"line\">    w1, w2, theta = <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.7</span></span><br><span class=\"line\">    tmp = x1 * w1 + x2 * w2</span><br><span class=\"line\">    <span class=\"keyword\">if</span> tmp &lt;= theta:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">elif</span> tmp &gt; theta:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"built_in\">print</span>(AND(<span class=\"number\">0</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(AND(<span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(AND(<span class=\"number\">1</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(AND(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"或门实现\"><a href=\"#或门实现\" class=\"headerlink\" title=\"或门实现\"></a>或门实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">OR</span>(<span class=\"params\">x1, x2</span>):</span><br><span class=\"line\">    w1, w2, theta = <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.4</span></span><br><span class=\"line\">    tmp = x1 * w1 + x2 * w2</span><br><span class=\"line\">    <span class=\"keyword\">if</span> tmp &lt;= theta:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"built_in\">print</span>(OR(<span class=\"number\">0</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(OR(<span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(OR(<span class=\"number\">1</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(OR(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"与非门实现\"><a href=\"#与非门实现\" class=\"headerlink\" title=\"与非门实现\"></a>与非门实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">NotAnd</span>(<span class=\"params\">x1, x2</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 直接对与门取反</span></span><br><span class=\"line\">    w1 , w2, theta = -<span class=\"number\">0.5</span>, -<span class=\"number\">0.5</span>, -<span class=\"number\">0.7</span></span><br><span class=\"line\">    tmp = w1 * x1 + w2 * x2</span><br><span class=\"line\">    <span class=\"keyword\">if</span> tmp &lt;= theta:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\">        </span><br><span class=\"line\"><span class=\"built_in\">print</span>(NotAnd(<span class=\"number\">0</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(NotAnd(<span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(NotAnd(<span class=\"number\">1</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(NotAnd(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"或非门实现\"><a href=\"#或非门实现\" class=\"headerlink\" title=\"或非门实现\"></a>或非门实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">NotOr</span>(<span class=\"params\">x1, x2</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 直接对或门取反</span></span><br><span class=\"line\">    w1 , w2, theta = -<span class=\"number\">0.5</span>, -<span class=\"number\">0.5</span>, -<span class=\"number\">0.4</span></span><br><span class=\"line\">    tmp = w1 * x1 + w2 * x2</span><br><span class=\"line\">    <span class=\"keyword\">if</span> tmp &lt;= theta:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(NotOr(<span class=\"number\">0</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(NotOr(<span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(NotOr(<span class=\"number\">1</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(NotOr(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"导入权重和偏置之后\"><a href=\"#导入权重和偏置之后\" class=\"headerlink\" title=\"导入权重和偏置之后\"></a>导入权重和偏置之后</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">And</span>(<span class=\"params\">x1, x2</span>):</span><br><span class=\"line\">    x = np.array([x1, x2])</span><br><span class=\"line\">    w = np.array([<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>])</span><br><span class=\"line\">    b = -<span class=\"number\">0.7</span></span><br><span class=\"line\">    tmp = np.<span class=\"built_in\">sum</span>(w * x) + b</span><br><span class=\"line\">    <span class=\"keyword\">if</span> tmp &lt;= <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(And(<span class=\"number\">0</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(And(<span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(And(<span class=\"number\">1</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(And(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">NOTAnd</span>(<span class=\"params\">x1, x2</span>):</span><br><span class=\"line\">    x = np.array([x1, x2])</span><br><span class=\"line\">    w = np.array([-<span class=\"number\">0.5</span>, -<span class=\"number\">0.5</span>])</span><br><span class=\"line\">    b = <span class=\"number\">0.7</span></span><br><span class=\"line\">    tmp = np.<span class=\"built_in\">sum</span>(w * x) + b</span><br><span class=\"line\">    <span class=\"keyword\">if</span> tmp &lt;= <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(NOTAnd(<span class=\"number\">0</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(NOTAnd(<span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(NOTAnd(<span class=\"number\">1</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(NOTAnd(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"单层感知机无法表示异或门，单层感知机只能划分线性空间，无法划分非线性空间-单层感知机也叫朴素感知机\"><a href=\"#单层感知机无法表示异或门，单层感知机只能划分线性空间，无法划分非线性空间-单层感知机也叫朴素感知机\" class=\"headerlink\" title=\"单层感知机无法表示异或门，单层感知机只能划分线性空间，无法划分非线性空间(单层感知机也叫朴素感知机)\"></a>单层感知机无法表示异或门，单层感知机只能划分线性空间，无法划分非线性空间(单层感知机也叫朴素感知机)</h4><h4 id=\"多层感知机（神经网络）\"><a href=\"#多层感知机（神经网络）\" class=\"headerlink\" title=\"多层感知机（神经网络）\"></a>多层感知机（神经网络）</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 与非门、或门和与门实现异或门</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">XOR</span>(<span class=\"params\">x1, x2</span>):</span><br><span class=\"line\">    s1 = NOTAnd(x1, x2)</span><br><span class=\"line\">    s2 = OR(x1, x2)</span><br><span class=\"line\">    y = And(s1, s2)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(XOR(<span class=\"number\">0</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(XOR(<span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(XOR(<span class=\"number\">1</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(XOR(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n</blockquote>\n","cover_type":"img","excerpt":"","more":"<h1 id=\"感知机\"><a href=\"#感知机\" class=\"headerlink\" title=\"感知机\"></a>感知机</h1><blockquote>\n<h4 id=\"感知机（Perceptron）是一种人工神经网络，用于解决二分类问题\"><a href=\"#感知机（Perceptron）是一种人工神经网络，用于解决二分类问题\" class=\"headerlink\" title=\"感知机（Perceptron）是一种人工神经网络，用于解决二分类问题\"></a>感知机（Perceptron）是一种人工神经网络，用于解决二分类问题</h4><h4 id=\"它模拟的是生物神经元的行为，主要包括：\"><a href=\"#它模拟的是生物神经元的行为，主要包括：\" class=\"headerlink\" title=\"它模拟的是生物神经元的行为，主要包括：\"></a>它模拟的是生物神经元的行为，主要包括：</h4><blockquote>\n<h4 id=\"输入：多个特征值-x-1-x-2…x-n\"><a href=\"#输入：多个特征值-x-1-x-2…x-n\" class=\"headerlink\" title=\"输入：多个特征值 $ x_1,x_2…x_n $\"></a>输入：多个特征值 $ x_1,x_2…x_n $</h4><h4 id=\"权重：每个输入对应一个权重-omega-1-omega-2…-omega-3\"><a href=\"#权重：每个输入对应一个权重-omega-1-omega-2…-omega-3\" class=\"headerlink\" title=\"权重：每个输入对应一个权重 $ \\omega_1,\\omega_2…\\omega_3 $\"></a>权重：每个输入对应一个权重 $ \\omega_1,\\omega_2…\\omega_3 $</h4><h4 id=\"偏置：一个常数项-b\"><a href=\"#偏置：一个常数项-b\" class=\"headerlink\" title=\"偏置：一个常数项 $ b $\"></a>偏置：一个常数项 $ b $</h4></blockquote>\n<h4 id=\"偏置是用于控制神经元被激活的容易程度\"><a href=\"#偏置是用于控制神经元被激活的容易程度\" class=\"headerlink\" title=\"偏置是用于控制神经元被激活的容易程度\"></a>偏置是用于控制神经元被激活的容易程度</h4><h4 id=\"权重是用于控制各个信号的重要性的\"><a href=\"#权重是用于控制各个信号的重要性的\" class=\"headerlink\" title=\"权重是用于控制各个信号的重要性的\"></a>权重是用于控制各个信号的重要性的</h4><h4 id=\"而激活函数则是决定以什么方式来激活输入信号的总和\"><a href=\"#而激活函数则是决定以什么方式来激活输入信号的总和\" class=\"headerlink\" title=\"而激活函数则是决定以什么方式来激活输入信号的总和\"></a>而激活函数则是决定以什么方式来激活输入信号的总和</h4><h4 id=\"数学表达式为：\"><a href=\"#数学表达式为：\" class=\"headerlink\" title=\"数学表达式为：\"></a>数学表达式为：</h4><p>$$<br>y &#x3D; sign(\\omega \\cdot x + b)<br>$$</p>\n<h4 id=\"其中，-omega-cdot-x-sum-i-1-n-omega-i-x-i-，sign函数定义为：\"><a href=\"#其中，-omega-cdot-x-sum-i-1-n-omega-i-x-i-，sign函数定义为：\" class=\"headerlink\" title=\"其中，$ \\omega \\cdot x &#x3D; \\sum_{i&#x3D;1}^n \\omega_i x_i $，sign函数定义为：\"></a>其中，$ \\omega \\cdot x &#x3D; \\sum_{i&#x3D;1}^n \\omega_i x_i $，sign函数定义为：</h4><p>$$<br> sign(z) &#x3D;<br>\\begin{cases}<br>1 &amp; \\text{if } z &gt; 0, \\<br>-1 &amp; \\text otherwise.<br>\\end{cases}<br>$$</p>\n<h4 id=\"工作原理：找到一个线性超平面（即一条直线或一个超平面）来将数据分为两类（-1-和-1）。通过不断调整权重-omega-和偏置-b-，使得训练数据被正确分类\"><a href=\"#工作原理：找到一个线性超平面（即一条直线或一个超平面）来将数据分为两类（-1-和-1）。通过不断调整权重-omega-和偏置-b-，使得训练数据被正确分类\" class=\"headerlink\" title=\"工作原理：找到一个线性超平面（即一条直线或一个超平面）来将数据分为两类（+1 和 -1）。通过不断调整权重 $ \\omega $ 和偏置 $ b $，使得训练数据被正确分类\"></a>工作原理：找到一个线性超平面（即一条直线或一个超平面）来将数据分为两类（+1 和 -1）。通过不断调整权重 $ \\omega $ 和偏置 $ b $，使得训练数据被正确分类</h4><h4 id=\"感知机学习算法\"><a href=\"#感知机学习算法\" class=\"headerlink\" title=\"感知机学习算法\"></a>感知机学习算法</h4><blockquote>\n<h4 id=\"它的学习规则是一种迭代算法，所谓的学习就是确定合适的参数的过程\"><a href=\"#它的学习规则是一种迭代算法，所谓的学习就是确定合适的参数的过程\" class=\"headerlink\" title=\"它的学习规则是一种迭代算法，所谓的学习就是确定合适的参数的过程\"></a>它的学习规则是一种迭代算法，所谓的学习就是确定合适的参数的过程</h4><blockquote>\n<h4 id=\"初始化：随机初始化权重-omega-和偏置-b\"><a href=\"#初始化：随机初始化权重-omega-和偏置-b\" class=\"headerlink\" title=\"初始化：随机初始化权重 $ \\omega $ 和偏置 $ b $\"></a>初始化：随机初始化权重 $ \\omega $ 和偏置 $ b $</h4><h4 id=\"逐个遍历样本：对于每个训练样本-x-i-y-i-，如果预测值与真实值不一致（即-y-i-omega-cdot-x-i-b-le-0-），就更新参数\"><a href=\"#逐个遍历样本：对于每个训练样本-x-i-y-i-，如果预测值与真实值不一致（即-y-i-omega-cdot-x-i-b-le-0-），就更新参数\" class=\"headerlink\" title=\"逐个遍历样本：对于每个训练样本 $ (x_i,y_i) $，如果预测值与真实值不一致（即 $ y_i(\\omega \\cdot x_i+b) \\le 0 $ ），就更新参数\"></a>逐个遍历样本：对于每个训练样本 $ (x_i,y_i) $，如果预测值与真实值不一致（即 $ y_i(\\omega \\cdot x_i+b) \\le 0 $ ），就更新参数</h4><p>$$<br>\\omega \\leftarrow \\omega + \\eta y_i x_i \\<br>b \\leftarrow b + \\eta y_i<br>$$</p>\n<h4 id=\"其中，-eta-是学习率\"><a href=\"#其中，-eta-是学习率\" class=\"headerlink\" title=\"其中，$ \\eta $ 是学习率\"></a>其中，$ \\eta $ 是学习率</h4></blockquote>\n<h4 id=\"重复训练，直到所有样本被正确分类或达到最大迭代次数\"><a href=\"#重复训练，直到所有样本被正确分类或达到最大迭代次数\" class=\"headerlink\" title=\"重复训练，直到所有样本被正确分类或达到最大迭代次数\"></a>重复训练，直到所有样本被正确分类或达到最大迭代次数</h4></blockquote>\n</blockquote>\n<h3 id=\"code\"><a href=\"#code\" class=\"headerlink\" title=\"code\"></a>code</h3><blockquote>\n<h4 id=\"与门实现\"><a href=\"#与门实现\" class=\"headerlink\" title=\"与门实现\"></a>与门实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">AND</span>(<span class=\"params\">x1, x2</span>):</span><br><span class=\"line\">    w1, w2, theta = <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.7</span></span><br><span class=\"line\">    tmp = x1 * w1 + x2 * w2</span><br><span class=\"line\">    <span class=\"keyword\">if</span> tmp &lt;= theta:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">elif</span> tmp &gt; theta:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"built_in\">print</span>(AND(<span class=\"number\">0</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(AND(<span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(AND(<span class=\"number\">1</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(AND(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"或门实现\"><a href=\"#或门实现\" class=\"headerlink\" title=\"或门实现\"></a>或门实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">OR</span>(<span class=\"params\">x1, x2</span>):</span><br><span class=\"line\">    w1, w2, theta = <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.4</span></span><br><span class=\"line\">    tmp = x1 * w1 + x2 * w2</span><br><span class=\"line\">    <span class=\"keyword\">if</span> tmp &lt;= theta:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"built_in\">print</span>(OR(<span class=\"number\">0</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(OR(<span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(OR(<span class=\"number\">1</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(OR(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"与非门实现\"><a href=\"#与非门实现\" class=\"headerlink\" title=\"与非门实现\"></a>与非门实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">NotAnd</span>(<span class=\"params\">x1, x2</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 直接对与门取反</span></span><br><span class=\"line\">    w1 , w2, theta = -<span class=\"number\">0.5</span>, -<span class=\"number\">0.5</span>, -<span class=\"number\">0.7</span></span><br><span class=\"line\">    tmp = w1 * x1 + w2 * x2</span><br><span class=\"line\">    <span class=\"keyword\">if</span> tmp &lt;= theta:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\">        </span><br><span class=\"line\"><span class=\"built_in\">print</span>(NotAnd(<span class=\"number\">0</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(NotAnd(<span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(NotAnd(<span class=\"number\">1</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(NotAnd(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"或非门实现\"><a href=\"#或非门实现\" class=\"headerlink\" title=\"或非门实现\"></a>或非门实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">NotOr</span>(<span class=\"params\">x1, x2</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 直接对或门取反</span></span><br><span class=\"line\">    w1 , w2, theta = -<span class=\"number\">0.5</span>, -<span class=\"number\">0.5</span>, -<span class=\"number\">0.4</span></span><br><span class=\"line\">    tmp = w1 * x1 + w2 * x2</span><br><span class=\"line\">    <span class=\"keyword\">if</span> tmp &lt;= theta:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(NotOr(<span class=\"number\">0</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(NotOr(<span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(NotOr(<span class=\"number\">1</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(NotOr(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"导入权重和偏置之后\"><a href=\"#导入权重和偏置之后\" class=\"headerlink\" title=\"导入权重和偏置之后\"></a>导入权重和偏置之后</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">And</span>(<span class=\"params\">x1, x2</span>):</span><br><span class=\"line\">    x = np.array([x1, x2])</span><br><span class=\"line\">    w = np.array([<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>])</span><br><span class=\"line\">    b = -<span class=\"number\">0.7</span></span><br><span class=\"line\">    tmp = np.<span class=\"built_in\">sum</span>(w * x) + b</span><br><span class=\"line\">    <span class=\"keyword\">if</span> tmp &lt;= <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(And(<span class=\"number\">0</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(And(<span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(And(<span class=\"number\">1</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(And(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">NOTAnd</span>(<span class=\"params\">x1, x2</span>):</span><br><span class=\"line\">    x = np.array([x1, x2])</span><br><span class=\"line\">    w = np.array([-<span class=\"number\">0.5</span>, -<span class=\"number\">0.5</span>])</span><br><span class=\"line\">    b = <span class=\"number\">0.7</span></span><br><span class=\"line\">    tmp = np.<span class=\"built_in\">sum</span>(w * x) + b</span><br><span class=\"line\">    <span class=\"keyword\">if</span> tmp &lt;= <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(NOTAnd(<span class=\"number\">0</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(NOTAnd(<span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(NOTAnd(<span class=\"number\">1</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(NOTAnd(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"单层感知机无法表示异或门，单层感知机只能划分线性空间，无法划分非线性空间-单层感知机也叫朴素感知机\"><a href=\"#单层感知机无法表示异或门，单层感知机只能划分线性空间，无法划分非线性空间-单层感知机也叫朴素感知机\" class=\"headerlink\" title=\"单层感知机无法表示异或门，单层感知机只能划分线性空间，无法划分非线性空间(单层感知机也叫朴素感知机)\"></a>单层感知机无法表示异或门，单层感知机只能划分线性空间，无法划分非线性空间(单层感知机也叫朴素感知机)</h4><h4 id=\"多层感知机（神经网络）\"><a href=\"#多层感知机（神经网络）\" class=\"headerlink\" title=\"多层感知机（神经网络）\"></a>多层感知机（神经网络）</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 与非门、或门和与门实现异或门</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">XOR</span>(<span class=\"params\">x1, x2</span>):</span><br><span class=\"line\">    s1 = NOTAnd(x1, x2)</span><br><span class=\"line\">    s2 = OR(x1, x2)</span><br><span class=\"line\">    y = And(s1, s2)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(XOR(<span class=\"number\">0</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(XOR(<span class=\"number\">0</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(XOR(<span class=\"number\">1</span>, <span class=\"number\">0</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(XOR(<span class=\"number\">1</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n</blockquote>\n"},{"title":"RL之路---第二弹","data":"2025-05-29T15:46:00.000Z","updated":"2025-05-29T15:46:00.000Z","type":"RL","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p19.png","_content":"\n\n# 贝尔曼方程\n\n\n\n\n\n### 第一部分——return\n\n> #### return 能够作为策略好坏的一个评估标准\n>\n> \n>\n> #### 下面通过三种情况来说明这个问题\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/1.png)\n>\n> #### 第一种策略\n>\n> > #### 按照策略1，从 s1 出发，往下去 s3，再往右到达 s4，discounted return计算如下\n> >\n> > ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/2.png)\n> >\n> > #### gamma 是衰减系数\n>\n> \n>\n> #### 第二种策略\n>\n> #### ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/3.png)\n>\n> \n>\n> #### 第三种策略\n>\n> > ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/4.png)\n> >\n> > #### 注意，严格说来，这个return3 已经不是我们所说的return了，因为return是针对一条轨迹定义的，针对一个trajectory定义的，这里实际上是有两个轨迹，这里所做的是求取平均值，也即求expectation（期望），这个return3就是state value\n>\n> \n>\n> #### 显然，return1 > return3 > return2，那么对应的就是策略1最好，策略3居中，策略2最差\n\n\n\n\n\n### 第二部分——return的计算\n\n> #### 以图中的例子为例\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/5.png)\n>\n> #### 用 v_i来表示从 s_i 出发对应的return\n>\n> #### 方法一\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/6.png)\n>\n> #### 方法二\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/7.png)\n>\n> #### 这里的方法二说明了什么一个问题呢，从当前状态出发，return的值，就等于当前的reward + 下一状态的return * 衰减因子gamma，也就是说从一个状态出发，它的return是依赖于其他状态的return的，这种思想在强化学习中被称之为 bootstrapping，这种思想是从自身出发，不断迭代所得到的结果\n>\n> \n>\n> #### 通过矩阵的形式给出上面的四个式子\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/8.png)\n>\n> #### 对该式子进行求解得到 v，实际上这个式子就是 贝尔曼方程，但这是一个非常简单的贝尔曼方程\n\n\n\n\n\n### 第三部分——state value\n\n> #### 先来看一个单步的过程\n>\n> $$\n> S_t \\xrightarrow{A_t} R_{t+1}, S_{t+1}\n> $$\n>\n> #### 在这个过程中，都存在一个条件概率\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/9.png)\n>\n> #### 再来看一个多步的过程 trajectory\n>\n> $$\n> S_t \\xrightarrow{A_t} R_{t+1}, S_{t+1} \\xrightarrow{A_{t+1}} R_{t+2}, S_{t+2} \\xrightarrow{A_{t+2}} R_{t+3}, \\ldots\n> $$\n>\n> #### 其对应的 discounted return如下\n>\n> $$\n> G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots\n> $$\n>\n> #### 这里的G_t 也是一个随机变量\n>\n> \n>\n> #### 正式介绍 state value\n>\n> > #### state value 实际上就是 G_t 的期望值（expectation）或平均值\n> >\n> > $$\n> > v_\\pi(s) = \\mathbb{E}[G_t | S_t = s]\n> > $$\n> >\n> > #### 它是关于状态s的一个函数，所以在不同状态下，对应不同的G_t也对应不同的 state value\n> >\n> > #### 另外，它也是关于策略pi的函数，也可以写为 v(s , pi)，不同的策略会得到不同的轨迹，从而得到不同的G_t\n> >\n> > #### state value不仅仅是一个数值，当其值很大时，说明当前状态具有很高的价值\n>\n> \n>\n> #### return 与 state value的区别\n>\n> > #### return是针对单个trajectory而言的，单个trajectory的return（对应的是一种确定性，如拓扑图）\n> >\n> > #### state value是针对多个trajectory而言的，它是多个trajectory的return的平均值（对应的是一种随机性）\n\n\n\n\n\n### 第四部分——贝尔曼公式的推导\n\n> #### 贝尔曼公式就是用来计算上一部分提到的state value，它描述了不同状态下state value之间的关系\n>\n> #### 考虑这样一个trajectory\n>\n> $$\n> S_t \\xrightarrow{A_t} R_{t+1}, S_{t+1} \\xrightarrow{A_{t+1}} R_{t+2}, S_{t+2} \\xrightarrow{A_{t+2}} R_{t+3}, \\ldots\n> $$\n>\n> #### 对应的G_t（discounted return）可以表示为\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/10.png)\n>\n> #### 对应的v( s, pi)（state value）可以表示为（贝尔曼公式）\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/11.png)\n>\n> #### 上面的表达式中的第一项的详细表示\n>\n> > #### 在状态s下，采取行动a之后所得到的即时奖励r的期望值\n> >\n> > ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/12.png)\n> >\n> > #### 在当前状态s下，有多个可采取的action，每个action对应的一个概率 pi\n> >\n> > #### 当采取action a，所得到的value就是\n> >\n> > $$\n> > \\mathbb{E}[R_{t+1} | S_t = s, A_t = a]\n> > $$\n> >\n> > #### 也等同于\n> >\n> > $$\n> > \\sum_{r} p(r | s, a) r\n> > $$\n> >\n> > #### 它的意思是，在状态s下，采取动作a，得到的即时奖励r的概率为p，最后乘以r本身\n> >\n> > #### 所以这整个第一项的期望就是这么求来的，它实际上就是所能得到的即时奖励r的期望值\n>\n> \n>\n> #### 第二部分的详细表示\n>\n> > #### 在当前状态s下，下一个时刻的return的期望值\n> >\n> > ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/13.png)\n> >\n> > #### 这里隐含了一个马尔科夫性，即下一状态，只取决于当前状态，与当前状态之前的状态无关\n> >\n> > #### 其余的变换都是概率论中的一些技巧\n>\n> \n>\n> #### 然后就是正式给出贝尔曼公式的表达式（看起来好复杂！）\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/14.png)\n>\n> #### 它描述了不同状态的state value之间的关系，如上面所示，左边是s的state value，右边的式子包含s'的state value\n>\n> #### 另外，这是针对一个状态空间而言的，对应的可能是多个式子多个s，而不是一个单一的式子\n>\n> #### 其求解思想就是 bootstrapping，还依赖于几个概率，第一个是概率是policy，求解出 state value就是在评估策略的好坏，后面两个概率对应的是两个model，dynamic model或 environment model，这里的model可能是已知的也可能是未知的（model free RL）\n>\n> \n>\n> #### 通过一个例子深入理解贝尔曼公式\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/15.png)\n>\n> #### 先来s1的state value\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/16.png)\n>\n> #### 先看第一个式子，在状态s1下采取动作a3的概率是1，不采取a3的概率是0，也就是说必然采取a3，那么第一部分就等于 1\n>\n> #### 第二个式子，在状态s1下采取动作a3后状态转换为s3的概率是1，不为s3的概率为0，也就是说状态必然转换为s3，所以第三部分的值为v_pi(s3)*gamma\n>\n> #### 第三个式子，在状态s1下采取动作a3后获得的即时奖励r=0的概率是1，r不等于0的概率是0，所以第二部分的值为0\n>\n> #### 此时s1的state value对应的的贝尔曼公式为\n>\n> $$\n> v_{\\pi}(s_1) = 0 + \\gamma v_{\\pi}(s_3)\n> $$\n>\n> #### 这里所得到的形式，过程虽然复杂，但实际上就是跟 第二部分中（return的计算）方法二的表达式是一个道理，那么根据这个思想，就可以直接得出其他几个状态下的state value的贝尔曼公式\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/17.png)\n>\n> #### 有了这几个方程组，就可以通过矩阵的思想来求解了（也可以采取其他办法），得到的结果如下\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/18.png)\n>\n> #### 当 gamma 取某个值时，上面的式子也会对应有一个确定的值，而这个值的大小就说明了那些状态是值得去探索的\n>\n> #### 计算得到 state value 之后，根据其反映出来的情况，就可以根据需要去改进策略（policy evaluation），不断的改进策略以得到最优策略\n\n\n\n\n\n### 第五部分——贝尔曼公式的矩阵和向量形式\n\n> #### 贝尔曼公式是针对一个状态空间而言的，所以自然而然的与矩阵产生关联\n>\n> #### 将贝尔曼公式的形式进行一个变换，先看贝尔曼公式最初的形式\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/19.png)\n>\n> #### 将其改写为\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/20.png)\n>\n> #### 改写后的式子，第一部分表示的是：从当前状态s出发，所能得到的即时奖励r的平均值（期望）\n>\n> #### 再将上面改写后的形式进一步简化为\n>\n> $$\n> v_{\\pi} = r_{\\pi} + \\gamma P_{\\pi} v_{\\pi}\n> $$\n>\n> #### 其中\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/21.png)\n>\n> #### 举一个n=4的例子\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/22.png)\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/23.png)\n>\n> #### 上式右边第一部分 r_pi 表示：从状态s出发获得的即时奖励r的期望（平均值）\n>\n> #### 第二部分中前一个矩阵 P_pi 表示：从状态s_i转换为s_j的概率\n>\n> #### \n>\n> #### 给定一个policy，可以列出一个与之对应的贝尔曼公式，通过对贝尔曼公式进行求解得到 state value，这样的过程就称之为 policy evaluation，通过评估一个策略的优劣，进而去改进策略，最终得到最优策略\n>\n> \n>\n> #### 求解贝尔曼方程的常用方法（求逆的方法通常不推荐，因为当矩阵维数很大时，求逆的计算量很大）\n>\n> > #### 常用的是迭代法求解\n> >\n> > $$\n> > v_{k+1} = r_{\\pi} + \\gamma P_{\\pi} v_{k}\n> > $$\n> >\n> > #### 从 v_0开始，可以先假定一个值，然后就可以求出v_1，通过v_1又求出v_2，以此类推，求得v_k，v_k+1，最后就会求出这样一个序列，v_0，v_1...v_k，当 k 趋近于无穷时，就有\n> >\n> > ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/24.png)\n> >\n> > #### v_k收敛到了v_pi，证明如下\n> >\n> > ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/25.png)\n> >\n> > #### 其证明过程类似于李亚普诺夫函数的稳定性证明\n>\n> \n>\n> #### 在下面这样一个例子中，说明了一些情况\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/26.png)\n>\n> #### 可以发现，越靠近目标区域的状态的state value越大，反之则越小\n>\n> #### 另外就是，不同的策略可能会得到相同的state value\n>\n> #### 通过计算state value可以评估一个策略的优劣\n\n\n\n### 第六部分——Action value\n\n> #### state value 和 Action value 的联系与区别\n>\n> > #### 前者是agent从一个状态出发，所获得的return的平均值（期望）\n> >\n> > #### 后者是agent从一个状态出发并且选择了一个Action之后，所获得的returen的平均值（期望）\n>\n> \n>\n> #### 为什么要引入action value，因为一个状态的转换是选择了一个Action之后引起的\n>\n> \n>\n> #### 定义\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/27.png)\n>\n> #### 它依赖策略pi\n>\n> #### 它与state value 数学式上的联系，state value可以表示为下面这个形式\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/28.png)\n>\n> #### 在一个状态下，可能有多种可选的动作，每个动作都有对应的概率，上式右边部分表示，在状态s下，选择动作a后的所获得的return的平均值 * 在状态s下选择动作a的概率\n>\n> #### 注意到，右边的前面一部分就是 action value的定义，所以二者数学式的联系如下\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/29.png)\n>\n> #### 这个式子说明什么呢，说明了已知一个状态的 action value，对其求平均就可以求出其对应的 state value\n>\n> #### 另外呢，根据前面第五部分开头的表达式可以进一步给出 action value的表达式\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/30.png)\n>\n> #### 而这个式子，它有说明了，已知一个状态的 state value 可以求出其对应的 action value\n>\n> #### （没怎么懂，迷迷糊糊的！）","source":"_posts/19.md","raw":"---\ntitle: RL之路---第二弹\ndata: 2025-05-29 23:46:00\nupdated: 2025-05-29 23:46:00\ntype: RL\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p19.png\ntags:\n  - RL\n  - Learning\n  - math-theory\n---\n\n\n# 贝尔曼方程\n\n\n\n\n\n### 第一部分——return\n\n> #### return 能够作为策略好坏的一个评估标准\n>\n> \n>\n> #### 下面通过三种情况来说明这个问题\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/1.png)\n>\n> #### 第一种策略\n>\n> > #### 按照策略1，从 s1 出发，往下去 s3，再往右到达 s4，discounted return计算如下\n> >\n> > ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/2.png)\n> >\n> > #### gamma 是衰减系数\n>\n> \n>\n> #### 第二种策略\n>\n> #### ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/3.png)\n>\n> \n>\n> #### 第三种策略\n>\n> > ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/4.png)\n> >\n> > #### 注意，严格说来，这个return3 已经不是我们所说的return了，因为return是针对一条轨迹定义的，针对一个trajectory定义的，这里实际上是有两个轨迹，这里所做的是求取平均值，也即求expectation（期望），这个return3就是state value\n>\n> \n>\n> #### 显然，return1 > return3 > return2，那么对应的就是策略1最好，策略3居中，策略2最差\n\n\n\n\n\n### 第二部分——return的计算\n\n> #### 以图中的例子为例\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/5.png)\n>\n> #### 用 v_i来表示从 s_i 出发对应的return\n>\n> #### 方法一\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/6.png)\n>\n> #### 方法二\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/7.png)\n>\n> #### 这里的方法二说明了什么一个问题呢，从当前状态出发，return的值，就等于当前的reward + 下一状态的return * 衰减因子gamma，也就是说从一个状态出发，它的return是依赖于其他状态的return的，这种思想在强化学习中被称之为 bootstrapping，这种思想是从自身出发，不断迭代所得到的结果\n>\n> \n>\n> #### 通过矩阵的形式给出上面的四个式子\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/8.png)\n>\n> #### 对该式子进行求解得到 v，实际上这个式子就是 贝尔曼方程，但这是一个非常简单的贝尔曼方程\n\n\n\n\n\n### 第三部分——state value\n\n> #### 先来看一个单步的过程\n>\n> $$\n> S_t \\xrightarrow{A_t} R_{t+1}, S_{t+1}\n> $$\n>\n> #### 在这个过程中，都存在一个条件概率\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/9.png)\n>\n> #### 再来看一个多步的过程 trajectory\n>\n> $$\n> S_t \\xrightarrow{A_t} R_{t+1}, S_{t+1} \\xrightarrow{A_{t+1}} R_{t+2}, S_{t+2} \\xrightarrow{A_{t+2}} R_{t+3}, \\ldots\n> $$\n>\n> #### 其对应的 discounted return如下\n>\n> $$\n> G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots\n> $$\n>\n> #### 这里的G_t 也是一个随机变量\n>\n> \n>\n> #### 正式介绍 state value\n>\n> > #### state value 实际上就是 G_t 的期望值（expectation）或平均值\n> >\n> > $$\n> > v_\\pi(s) = \\mathbb{E}[G_t | S_t = s]\n> > $$\n> >\n> > #### 它是关于状态s的一个函数，所以在不同状态下，对应不同的G_t也对应不同的 state value\n> >\n> > #### 另外，它也是关于策略pi的函数，也可以写为 v(s , pi)，不同的策略会得到不同的轨迹，从而得到不同的G_t\n> >\n> > #### state value不仅仅是一个数值，当其值很大时，说明当前状态具有很高的价值\n>\n> \n>\n> #### return 与 state value的区别\n>\n> > #### return是针对单个trajectory而言的，单个trajectory的return（对应的是一种确定性，如拓扑图）\n> >\n> > #### state value是针对多个trajectory而言的，它是多个trajectory的return的平均值（对应的是一种随机性）\n\n\n\n\n\n### 第四部分——贝尔曼公式的推导\n\n> #### 贝尔曼公式就是用来计算上一部分提到的state value，它描述了不同状态下state value之间的关系\n>\n> #### 考虑这样一个trajectory\n>\n> $$\n> S_t \\xrightarrow{A_t} R_{t+1}, S_{t+1} \\xrightarrow{A_{t+1}} R_{t+2}, S_{t+2} \\xrightarrow{A_{t+2}} R_{t+3}, \\ldots\n> $$\n>\n> #### 对应的G_t（discounted return）可以表示为\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/10.png)\n>\n> #### 对应的v( s, pi)（state value）可以表示为（贝尔曼公式）\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/11.png)\n>\n> #### 上面的表达式中的第一项的详细表示\n>\n> > #### 在状态s下，采取行动a之后所得到的即时奖励r的期望值\n> >\n> > ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/12.png)\n> >\n> > #### 在当前状态s下，有多个可采取的action，每个action对应的一个概率 pi\n> >\n> > #### 当采取action a，所得到的value就是\n> >\n> > $$\n> > \\mathbb{E}[R_{t+1} | S_t = s, A_t = a]\n> > $$\n> >\n> > #### 也等同于\n> >\n> > $$\n> > \\sum_{r} p(r | s, a) r\n> > $$\n> >\n> > #### 它的意思是，在状态s下，采取动作a，得到的即时奖励r的概率为p，最后乘以r本身\n> >\n> > #### 所以这整个第一项的期望就是这么求来的，它实际上就是所能得到的即时奖励r的期望值\n>\n> \n>\n> #### 第二部分的详细表示\n>\n> > #### 在当前状态s下，下一个时刻的return的期望值\n> >\n> > ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/13.png)\n> >\n> > #### 这里隐含了一个马尔科夫性，即下一状态，只取决于当前状态，与当前状态之前的状态无关\n> >\n> > #### 其余的变换都是概率论中的一些技巧\n>\n> \n>\n> #### 然后就是正式给出贝尔曼公式的表达式（看起来好复杂！）\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/14.png)\n>\n> #### 它描述了不同状态的state value之间的关系，如上面所示，左边是s的state value，右边的式子包含s'的state value\n>\n> #### 另外，这是针对一个状态空间而言的，对应的可能是多个式子多个s，而不是一个单一的式子\n>\n> #### 其求解思想就是 bootstrapping，还依赖于几个概率，第一个是概率是policy，求解出 state value就是在评估策略的好坏，后面两个概率对应的是两个model，dynamic model或 environment model，这里的model可能是已知的也可能是未知的（model free RL）\n>\n> \n>\n> #### 通过一个例子深入理解贝尔曼公式\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/15.png)\n>\n> #### 先来s1的state value\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/16.png)\n>\n> #### 先看第一个式子，在状态s1下采取动作a3的概率是1，不采取a3的概率是0，也就是说必然采取a3，那么第一部分就等于 1\n>\n> #### 第二个式子，在状态s1下采取动作a3后状态转换为s3的概率是1，不为s3的概率为0，也就是说状态必然转换为s3，所以第三部分的值为v_pi(s3)*gamma\n>\n> #### 第三个式子，在状态s1下采取动作a3后获得的即时奖励r=0的概率是1，r不等于0的概率是0，所以第二部分的值为0\n>\n> #### 此时s1的state value对应的的贝尔曼公式为\n>\n> $$\n> v_{\\pi}(s_1) = 0 + \\gamma v_{\\pi}(s_3)\n> $$\n>\n> #### 这里所得到的形式，过程虽然复杂，但实际上就是跟 第二部分中（return的计算）方法二的表达式是一个道理，那么根据这个思想，就可以直接得出其他几个状态下的state value的贝尔曼公式\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/17.png)\n>\n> #### 有了这几个方程组，就可以通过矩阵的思想来求解了（也可以采取其他办法），得到的结果如下\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/18.png)\n>\n> #### 当 gamma 取某个值时，上面的式子也会对应有一个确定的值，而这个值的大小就说明了那些状态是值得去探索的\n>\n> #### 计算得到 state value 之后，根据其反映出来的情况，就可以根据需要去改进策略（policy evaluation），不断的改进策略以得到最优策略\n\n\n\n\n\n### 第五部分——贝尔曼公式的矩阵和向量形式\n\n> #### 贝尔曼公式是针对一个状态空间而言的，所以自然而然的与矩阵产生关联\n>\n> #### 将贝尔曼公式的形式进行一个变换，先看贝尔曼公式最初的形式\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/19.png)\n>\n> #### 将其改写为\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/20.png)\n>\n> #### 改写后的式子，第一部分表示的是：从当前状态s出发，所能得到的即时奖励r的平均值（期望）\n>\n> #### 再将上面改写后的形式进一步简化为\n>\n> $$\n> v_{\\pi} = r_{\\pi} + \\gamma P_{\\pi} v_{\\pi}\n> $$\n>\n> #### 其中\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/21.png)\n>\n> #### 举一个n=4的例子\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/22.png)\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/23.png)\n>\n> #### 上式右边第一部分 r_pi 表示：从状态s出发获得的即时奖励r的期望（平均值）\n>\n> #### 第二部分中前一个矩阵 P_pi 表示：从状态s_i转换为s_j的概率\n>\n> #### \n>\n> #### 给定一个policy，可以列出一个与之对应的贝尔曼公式，通过对贝尔曼公式进行求解得到 state value，这样的过程就称之为 policy evaluation，通过评估一个策略的优劣，进而去改进策略，最终得到最优策略\n>\n> \n>\n> #### 求解贝尔曼方程的常用方法（求逆的方法通常不推荐，因为当矩阵维数很大时，求逆的计算量很大）\n>\n> > #### 常用的是迭代法求解\n> >\n> > $$\n> > v_{k+1} = r_{\\pi} + \\gamma P_{\\pi} v_{k}\n> > $$\n> >\n> > #### 从 v_0开始，可以先假定一个值，然后就可以求出v_1，通过v_1又求出v_2，以此类推，求得v_k，v_k+1，最后就会求出这样一个序列，v_0，v_1...v_k，当 k 趋近于无穷时，就有\n> >\n> > ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/24.png)\n> >\n> > #### v_k收敛到了v_pi，证明如下\n> >\n> > ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/25.png)\n> >\n> > #### 其证明过程类似于李亚普诺夫函数的稳定性证明\n>\n> \n>\n> #### 在下面这样一个例子中，说明了一些情况\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/26.png)\n>\n> #### 可以发现，越靠近目标区域的状态的state value越大，反之则越小\n>\n> #### 另外就是，不同的策略可能会得到相同的state value\n>\n> #### 通过计算state value可以评估一个策略的优劣\n\n\n\n### 第六部分——Action value\n\n> #### state value 和 Action value 的联系与区别\n>\n> > #### 前者是agent从一个状态出发，所获得的return的平均值（期望）\n> >\n> > #### 后者是agent从一个状态出发并且选择了一个Action之后，所获得的returen的平均值（期望）\n>\n> \n>\n> #### 为什么要引入action value，因为一个状态的转换是选择了一个Action之后引起的\n>\n> \n>\n> #### 定义\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/27.png)\n>\n> #### 它依赖策略pi\n>\n> #### 它与state value 数学式上的联系，state value可以表示为下面这个形式\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/28.png)\n>\n> #### 在一个状态下，可能有多种可选的动作，每个动作都有对应的概率，上式右边部分表示，在状态s下，选择动作a后的所获得的return的平均值 * 在状态s下选择动作a的概率\n>\n> #### 注意到，右边的前面一部分就是 action value的定义，所以二者数学式的联系如下\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/29.png)\n>\n> #### 这个式子说明什么呢，说明了已知一个状态的 action value，对其求平均就可以求出其对应的 state value\n>\n> #### 另外呢，根据前面第五部分开头的表达式可以进一步给出 action value的表达式\n>\n> ![](http://picbed.yanzu.tech/img/RL/math_theory/2th_part/30.png)\n>\n> #### 而这个式子，它有说明了，已知一个状态的 state value 可以求出其对应的 action value\n>\n> #### （没怎么懂，迷迷糊糊的！）","slug":"19","published":1,"date":"2025-05-30T03:19:17.022Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cr4001eiku49nfh3del","content":"<h1 id=\"贝尔曼方程\"><a href=\"#贝尔曼方程\" class=\"headerlink\" title=\"贝尔曼方程\"></a>贝尔曼方程</h1><h3 id=\"第一部分——return\"><a href=\"#第一部分——return\" class=\"headerlink\" title=\"第一部分——return\"></a>第一部分——return</h3><blockquote>\n<h4 id=\"return-能够作为策略好坏的一个评估标准\"><a href=\"#return-能够作为策略好坏的一个评估标准\" class=\"headerlink\" title=\"return 能够作为策略好坏的一个评估标准\"></a>return 能够作为策略好坏的一个评估标准</h4><h4 id=\"下面通过三种情况来说明这个问题\"><a href=\"#下面通过三种情况来说明这个问题\" class=\"headerlink\" title=\"下面通过三种情况来说明这个问题\"></a>下面通过三种情况来说明这个问题</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/1.png\"></p>\n<h4 id=\"第一种策略\"><a href=\"#第一种策略\" class=\"headerlink\" title=\"第一种策略\"></a>第一种策略</h4><blockquote>\n<h4 id=\"按照策略1，从-s1-出发，往下去-s3，再往右到达-s4，discounted-return计算如下\"><a href=\"#按照策略1，从-s1-出发，往下去-s3，再往右到达-s4，discounted-return计算如下\" class=\"headerlink\" title=\"按照策略1，从 s1 出发，往下去 s3，再往右到达 s4，discounted return计算如下\"></a>按照策略1，从 s1 出发，往下去 s3，再往右到达 s4，discounted return计算如下</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/2.png\"></p>\n<h4 id=\"gamma-是衰减系数\"><a href=\"#gamma-是衰减系数\" class=\"headerlink\" title=\"gamma 是衰减系数\"></a>gamma 是衰减系数</h4></blockquote>\n<h4 id=\"第二种策略\"><a href=\"#第二种策略\" class=\"headerlink\" title=\"第二种策略\"></a>第二种策略</h4><h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/3.png\"></h4><h4 id=\"第三种策略\"><a href=\"#第三种策略\" class=\"headerlink\" title=\"第三种策略\"></a>第三种策略</h4><blockquote>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/4.png\"></p>\n<h4 id=\"注意，严格说来，这个return3-已经不是我们所说的return了，因为return是针对一条轨迹定义的，针对一个trajectory定义的，这里实际上是有两个轨迹，这里所做的是求取平均值，也即求expectation（期望），这个return3就是state-value\"><a href=\"#注意，严格说来，这个return3-已经不是我们所说的return了，因为return是针对一条轨迹定义的，针对一个trajectory定义的，这里实际上是有两个轨迹，这里所做的是求取平均值，也即求expectation（期望），这个return3就是state-value\" class=\"headerlink\" title=\"注意，严格说来，这个return3 已经不是我们所说的return了，因为return是针对一条轨迹定义的，针对一个trajectory定义的，这里实际上是有两个轨迹，这里所做的是求取平均值，也即求expectation（期望），这个return3就是state value\"></a>注意，严格说来，这个return3 已经不是我们所说的return了，因为return是针对一条轨迹定义的，针对一个trajectory定义的，这里实际上是有两个轨迹，这里所做的是求取平均值，也即求expectation（期望），这个return3就是state value</h4></blockquote>\n<h4 id=\"显然，return1-return3-return2，那么对应的就是策略1最好，策略3居中，策略2最差\"><a href=\"#显然，return1-return3-return2，那么对应的就是策略1最好，策略3居中，策略2最差\" class=\"headerlink\" title=\"显然，return1 &gt; return3 &gt; return2，那么对应的就是策略1最好，策略3居中，策略2最差\"></a>显然，return1 &gt; return3 &gt; return2，那么对应的就是策略1最好，策略3居中，策略2最差</h4></blockquote>\n<h3 id=\"第二部分——return的计算\"><a href=\"#第二部分——return的计算\" class=\"headerlink\" title=\"第二部分——return的计算\"></a>第二部分——return的计算</h3><blockquote>\n<h4 id=\"以图中的例子为例\"><a href=\"#以图中的例子为例\" class=\"headerlink\" title=\"以图中的例子为例\"></a>以图中的例子为例</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/5.png\"></p>\n<h4 id=\"用-v-i来表示从-s-i-出发对应的return\"><a href=\"#用-v-i来表示从-s-i-出发对应的return\" class=\"headerlink\" title=\"用 v_i来表示从 s_i 出发对应的return\"></a>用 v_i来表示从 s_i 出发对应的return</h4><h4 id=\"方法一\"><a href=\"#方法一\" class=\"headerlink\" title=\"方法一\"></a>方法一</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/6.png\"></p>\n<h4 id=\"方法二\"><a href=\"#方法二\" class=\"headerlink\" title=\"方法二\"></a>方法二</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/7.png\"></p>\n<h4 id=\"这里的方法二说明了什么一个问题呢，从当前状态出发，return的值，就等于当前的reward-下一状态的return-衰减因子gamma，也就是说从一个状态出发，它的return是依赖于其他状态的return的，这种思想在强化学习中被称之为-bootstrapping，这种思想是从自身出发，不断迭代所得到的结果\"><a href=\"#这里的方法二说明了什么一个问题呢，从当前状态出发，return的值，就等于当前的reward-下一状态的return-衰减因子gamma，也就是说从一个状态出发，它的return是依赖于其他状态的return的，这种思想在强化学习中被称之为-bootstrapping，这种思想是从自身出发，不断迭代所得到的结果\" class=\"headerlink\" title=\"这里的方法二说明了什么一个问题呢，从当前状态出发，return的值，就等于当前的reward + 下一状态的return * 衰减因子gamma，也就是说从一个状态出发，它的return是依赖于其他状态的return的，这种思想在强化学习中被称之为 bootstrapping，这种思想是从自身出发，不断迭代所得到的结果\"></a>这里的方法二说明了什么一个问题呢，从当前状态出发，return的值，就等于当前的reward + 下一状态的return * 衰减因子gamma，也就是说从一个状态出发，它的return是依赖于其他状态的return的，这种思想在强化学习中被称之为 bootstrapping，这种思想是从自身出发，不断迭代所得到的结果</h4><h4 id=\"通过矩阵的形式给出上面的四个式子\"><a href=\"#通过矩阵的形式给出上面的四个式子\" class=\"headerlink\" title=\"通过矩阵的形式给出上面的四个式子\"></a>通过矩阵的形式给出上面的四个式子</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/8.png\"></p>\n<h4 id=\"对该式子进行求解得到-v，实际上这个式子就是-贝尔曼方程，但这是一个非常简单的贝尔曼方程\"><a href=\"#对该式子进行求解得到-v，实际上这个式子就是-贝尔曼方程，但这是一个非常简单的贝尔曼方程\" class=\"headerlink\" title=\"对该式子进行求解得到 v，实际上这个式子就是 贝尔曼方程，但这是一个非常简单的贝尔曼方程\"></a>对该式子进行求解得到 v，实际上这个式子就是 贝尔曼方程，但这是一个非常简单的贝尔曼方程</h4></blockquote>\n<h3 id=\"第三部分——state-value\"><a href=\"#第三部分——state-value\" class=\"headerlink\" title=\"第三部分——state value\"></a>第三部分——state value</h3><blockquote>\n<h4 id=\"先来看一个单步的过程\"><a href=\"#先来看一个单步的过程\" class=\"headerlink\" title=\"先来看一个单步的过程\"></a>先来看一个单步的过程</h4><p>$$<br>S_t \\xrightarrow{A_t} R_{t+1}, S_{t+1}<br>$$</p>\n<h4 id=\"在这个过程中，都存在一个条件概率\"><a href=\"#在这个过程中，都存在一个条件概率\" class=\"headerlink\" title=\"在这个过程中，都存在一个条件概率\"></a>在这个过程中，都存在一个条件概率</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/9.png\"></p>\n<h4 id=\"再来看一个多步的过程-trajectory\"><a href=\"#再来看一个多步的过程-trajectory\" class=\"headerlink\" title=\"再来看一个多步的过程 trajectory\"></a>再来看一个多步的过程 trajectory</h4><p>$$<br>S_t \\xrightarrow{A_t} R_{t+1}, S_{t+1} \\xrightarrow{A_{t+1}} R_{t+2}, S_{t+2} \\xrightarrow{A_{t+2}} R_{t+3}, \\ldots<br>$$</p>\n<h4 id=\"其对应的-discounted-return如下\"><a href=\"#其对应的-discounted-return如下\" class=\"headerlink\" title=\"其对应的 discounted return如下\"></a>其对应的 discounted return如下</h4><p>$$<br>G_t &#x3D; R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots<br>$$</p>\n<h4 id=\"这里的G-t-也是一个随机变量\"><a href=\"#这里的G-t-也是一个随机变量\" class=\"headerlink\" title=\"这里的G_t 也是一个随机变量\"></a>这里的G_t 也是一个随机变量</h4><h4 id=\"正式介绍-state-value\"><a href=\"#正式介绍-state-value\" class=\"headerlink\" title=\"正式介绍 state value\"></a>正式介绍 state value</h4><blockquote>\n<h4 id=\"state-value-实际上就是-G-t-的期望值（expectation）或平均值\"><a href=\"#state-value-实际上就是-G-t-的期望值（expectation）或平均值\" class=\"headerlink\" title=\"state value 实际上就是 G_t 的期望值（expectation）或平均值\"></a>state value 实际上就是 G_t 的期望值（expectation）或平均值</h4><p>$$<br>v_\\pi(s) &#x3D; \\mathbb{E}[G_t | S_t &#x3D; s]<br>$$</p>\n<h4 id=\"它是关于状态s的一个函数，所以在不同状态下，对应不同的G-t也对应不同的-state-value\"><a href=\"#它是关于状态s的一个函数，所以在不同状态下，对应不同的G-t也对应不同的-state-value\" class=\"headerlink\" title=\"它是关于状态s的一个函数，所以在不同状态下，对应不同的G_t也对应不同的 state value\"></a>它是关于状态s的一个函数，所以在不同状态下，对应不同的G_t也对应不同的 state value</h4><h4 id=\"另外，它也是关于策略pi的函数，也可以写为-v-s-pi-，不同的策略会得到不同的轨迹，从而得到不同的G-t\"><a href=\"#另外，它也是关于策略pi的函数，也可以写为-v-s-pi-，不同的策略会得到不同的轨迹，从而得到不同的G-t\" class=\"headerlink\" title=\"另外，它也是关于策略pi的函数，也可以写为 v(s , pi)，不同的策略会得到不同的轨迹，从而得到不同的G_t\"></a>另外，它也是关于策略pi的函数，也可以写为 v(s , pi)，不同的策略会得到不同的轨迹，从而得到不同的G_t</h4><h4 id=\"state-value不仅仅是一个数值，当其值很大时，说明当前状态具有很高的价值\"><a href=\"#state-value不仅仅是一个数值，当其值很大时，说明当前状态具有很高的价值\" class=\"headerlink\" title=\"state value不仅仅是一个数值，当其值很大时，说明当前状态具有很高的价值\"></a>state value不仅仅是一个数值，当其值很大时，说明当前状态具有很高的价值</h4></blockquote>\n<h4 id=\"return-与-state-value的区别\"><a href=\"#return-与-state-value的区别\" class=\"headerlink\" title=\"return 与 state value的区别\"></a>return 与 state value的区别</h4><blockquote>\n<h4 id=\"return是针对单个trajectory而言的，单个trajectory的return（对应的是一种确定性，如拓扑图）\"><a href=\"#return是针对单个trajectory而言的，单个trajectory的return（对应的是一种确定性，如拓扑图）\" class=\"headerlink\" title=\"return是针对单个trajectory而言的，单个trajectory的return（对应的是一种确定性，如拓扑图）\"></a>return是针对单个trajectory而言的，单个trajectory的return（对应的是一种确定性，如拓扑图）</h4><h4 id=\"state-value是针对多个trajectory而言的，它是多个trajectory的return的平均值（对应的是一种随机性）\"><a href=\"#state-value是针对多个trajectory而言的，它是多个trajectory的return的平均值（对应的是一种随机性）\" class=\"headerlink\" title=\"state value是针对多个trajectory而言的，它是多个trajectory的return的平均值（对应的是一种随机性）\"></a>state value是针对多个trajectory而言的，它是多个trajectory的return的平均值（对应的是一种随机性）</h4></blockquote>\n</blockquote>\n<h3 id=\"第四部分——贝尔曼公式的推导\"><a href=\"#第四部分——贝尔曼公式的推导\" class=\"headerlink\" title=\"第四部分——贝尔曼公式的推导\"></a>第四部分——贝尔曼公式的推导</h3><blockquote>\n<h4 id=\"贝尔曼公式就是用来计算上一部分提到的state-value，它描述了不同状态下state-value之间的关系\"><a href=\"#贝尔曼公式就是用来计算上一部分提到的state-value，它描述了不同状态下state-value之间的关系\" class=\"headerlink\" title=\"贝尔曼公式就是用来计算上一部分提到的state value，它描述了不同状态下state value之间的关系\"></a>贝尔曼公式就是用来计算上一部分提到的state value，它描述了不同状态下state value之间的关系</h4><h4 id=\"考虑这样一个trajectory\"><a href=\"#考虑这样一个trajectory\" class=\"headerlink\" title=\"考虑这样一个trajectory\"></a>考虑这样一个trajectory</h4><p>$$<br>S_t \\xrightarrow{A_t} R_{t+1}, S_{t+1} \\xrightarrow{A_{t+1}} R_{t+2}, S_{t+2} \\xrightarrow{A_{t+2}} R_{t+3}, \\ldots<br>$$</p>\n<h4 id=\"对应的G-t（discounted-return）可以表示为\"><a href=\"#对应的G-t（discounted-return）可以表示为\" class=\"headerlink\" title=\"对应的G_t（discounted return）可以表示为\"></a>对应的G_t（discounted return）可以表示为</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/10.png\"></p>\n<h4 id=\"对应的v-s-pi-（state-value）可以表示为（贝尔曼公式）\"><a href=\"#对应的v-s-pi-（state-value）可以表示为（贝尔曼公式）\" class=\"headerlink\" title=\"对应的v( s, pi)（state value）可以表示为（贝尔曼公式）\"></a>对应的v( s, pi)（state value）可以表示为（贝尔曼公式）</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/11.png\"></p>\n<h4 id=\"上面的表达式中的第一项的详细表示\"><a href=\"#上面的表达式中的第一项的详细表示\" class=\"headerlink\" title=\"上面的表达式中的第一项的详细表示\"></a>上面的表达式中的第一项的详细表示</h4><blockquote>\n<h4 id=\"在状态s下，采取行动a之后所得到的即时奖励r的期望值\"><a href=\"#在状态s下，采取行动a之后所得到的即时奖励r的期望值\" class=\"headerlink\" title=\"在状态s下，采取行动a之后所得到的即时奖励r的期望值\"></a>在状态s下，采取行动a之后所得到的即时奖励r的期望值</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/12.png\"></p>\n<h4 id=\"在当前状态s下，有多个可采取的action，每个action对应的一个概率-pi\"><a href=\"#在当前状态s下，有多个可采取的action，每个action对应的一个概率-pi\" class=\"headerlink\" title=\"在当前状态s下，有多个可采取的action，每个action对应的一个概率 pi\"></a>在当前状态s下，有多个可采取的action，每个action对应的一个概率 pi</h4><h4 id=\"当采取action-a，所得到的value就是\"><a href=\"#当采取action-a，所得到的value就是\" class=\"headerlink\" title=\"当采取action a，所得到的value就是\"></a>当采取action a，所得到的value就是</h4><p>$$<br>\\mathbb{E}[R_{t+1} | S_t &#x3D; s, A_t &#x3D; a]<br>$$</p>\n<h4 id=\"也等同于\"><a href=\"#也等同于\" class=\"headerlink\" title=\"也等同于\"></a>也等同于</h4><p>$$<br>\\sum_{r} p(r | s, a) r<br>$$</p>\n<h4 id=\"它的意思是，在状态s下，采取动作a，得到的即时奖励r的概率为p，最后乘以r本身\"><a href=\"#它的意思是，在状态s下，采取动作a，得到的即时奖励r的概率为p，最后乘以r本身\" class=\"headerlink\" title=\"它的意思是，在状态s下，采取动作a，得到的即时奖励r的概率为p，最后乘以r本身\"></a>它的意思是，在状态s下，采取动作a，得到的即时奖励r的概率为p，最后乘以r本身</h4><h4 id=\"所以这整个第一项的期望就是这么求来的，它实际上就是所能得到的即时奖励r的期望值\"><a href=\"#所以这整个第一项的期望就是这么求来的，它实际上就是所能得到的即时奖励r的期望值\" class=\"headerlink\" title=\"所以这整个第一项的期望就是这么求来的，它实际上就是所能得到的即时奖励r的期望值\"></a>所以这整个第一项的期望就是这么求来的，它实际上就是所能得到的即时奖励r的期望值</h4></blockquote>\n<h4 id=\"第二部分的详细表示\"><a href=\"#第二部分的详细表示\" class=\"headerlink\" title=\"第二部分的详细表示\"></a>第二部分的详细表示</h4><blockquote>\n<h4 id=\"在当前状态s下，下一个时刻的return的期望值\"><a href=\"#在当前状态s下，下一个时刻的return的期望值\" class=\"headerlink\" title=\"在当前状态s下，下一个时刻的return的期望值\"></a>在当前状态s下，下一个时刻的return的期望值</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/13.png\"></p>\n<h4 id=\"这里隐含了一个马尔科夫性，即下一状态，只取决于当前状态，与当前状态之前的状态无关\"><a href=\"#这里隐含了一个马尔科夫性，即下一状态，只取决于当前状态，与当前状态之前的状态无关\" class=\"headerlink\" title=\"这里隐含了一个马尔科夫性，即下一状态，只取决于当前状态，与当前状态之前的状态无关\"></a>这里隐含了一个马尔科夫性，即下一状态，只取决于当前状态，与当前状态之前的状态无关</h4><h4 id=\"其余的变换都是概率论中的一些技巧\"><a href=\"#其余的变换都是概率论中的一些技巧\" class=\"headerlink\" title=\"其余的变换都是概率论中的一些技巧\"></a>其余的变换都是概率论中的一些技巧</h4></blockquote>\n<h4 id=\"然后就是正式给出贝尔曼公式的表达式（看起来好复杂！）\"><a href=\"#然后就是正式给出贝尔曼公式的表达式（看起来好复杂！）\" class=\"headerlink\" title=\"然后就是正式给出贝尔曼公式的表达式（看起来好复杂！）\"></a>然后就是正式给出贝尔曼公式的表达式（看起来好复杂！）</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/14.png\"></p>\n<h4 id=\"它描述了不同状态的state-value之间的关系，如上面所示，左边是s的state-value，右边的式子包含s’的state-value\"><a href=\"#它描述了不同状态的state-value之间的关系，如上面所示，左边是s的state-value，右边的式子包含s’的state-value\" class=\"headerlink\" title=\"它描述了不同状态的state value之间的关系，如上面所示，左边是s的state value，右边的式子包含s’的state value\"></a>它描述了不同状态的state value之间的关系，如上面所示，左边是s的state value，右边的式子包含s’的state value</h4><h4 id=\"另外，这是针对一个状态空间而言的，对应的可能是多个式子多个s，而不是一个单一的式子\"><a href=\"#另外，这是针对一个状态空间而言的，对应的可能是多个式子多个s，而不是一个单一的式子\" class=\"headerlink\" title=\"另外，这是针对一个状态空间而言的，对应的可能是多个式子多个s，而不是一个单一的式子\"></a>另外，这是针对一个状态空间而言的，对应的可能是多个式子多个s，而不是一个单一的式子</h4><h4 id=\"其求解思想就是-bootstrapping，还依赖于几个概率，第一个是概率是policy，求解出-state-value就是在评估策略的好坏，后面两个概率对应的是两个model，dynamic-model或-environment-model，这里的model可能是已知的也可能是未知的（model-free-RL）\"><a href=\"#其求解思想就是-bootstrapping，还依赖于几个概率，第一个是概率是policy，求解出-state-value就是在评估策略的好坏，后面两个概率对应的是两个model，dynamic-model或-environment-model，这里的model可能是已知的也可能是未知的（model-free-RL）\" class=\"headerlink\" title=\"其求解思想就是 bootstrapping，还依赖于几个概率，第一个是概率是policy，求解出 state value就是在评估策略的好坏，后面两个概率对应的是两个model，dynamic model或 environment model，这里的model可能是已知的也可能是未知的（model free RL）\"></a>其求解思想就是 bootstrapping，还依赖于几个概率，第一个是概率是policy，求解出 state value就是在评估策略的好坏，后面两个概率对应的是两个model，dynamic model或 environment model，这里的model可能是已知的也可能是未知的（model free RL）</h4><h4 id=\"通过一个例子深入理解贝尔曼公式\"><a href=\"#通过一个例子深入理解贝尔曼公式\" class=\"headerlink\" title=\"通过一个例子深入理解贝尔曼公式\"></a>通过一个例子深入理解贝尔曼公式</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/15.png\"></p>\n<h4 id=\"先来s1的state-value\"><a href=\"#先来s1的state-value\" class=\"headerlink\" title=\"先来s1的state value\"></a>先来s1的state value</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/16.png\"></p>\n<h4 id=\"先看第一个式子，在状态s1下采取动作a3的概率是1，不采取a3的概率是0，也就是说必然采取a3，那么第一部分就等于-1\"><a href=\"#先看第一个式子，在状态s1下采取动作a3的概率是1，不采取a3的概率是0，也就是说必然采取a3，那么第一部分就等于-1\" class=\"headerlink\" title=\"先看第一个式子，在状态s1下采取动作a3的概率是1，不采取a3的概率是0，也就是说必然采取a3，那么第一部分就等于 1\"></a>先看第一个式子，在状态s1下采取动作a3的概率是1，不采取a3的概率是0，也就是说必然采取a3，那么第一部分就等于 1</h4><h4 id=\"第二个式子，在状态s1下采取动作a3后状态转换为s3的概率是1，不为s3的概率为0，也就是说状态必然转换为s3，所以第三部分的值为v-pi-s3-gamma\"><a href=\"#第二个式子，在状态s1下采取动作a3后状态转换为s3的概率是1，不为s3的概率为0，也就是说状态必然转换为s3，所以第三部分的值为v-pi-s3-gamma\" class=\"headerlink\" title=\"第二个式子，在状态s1下采取动作a3后状态转换为s3的概率是1，不为s3的概率为0，也就是说状态必然转换为s3，所以第三部分的值为v_pi(s3)*gamma\"></a>第二个式子，在状态s1下采取动作a3后状态转换为s3的概率是1，不为s3的概率为0，也就是说状态必然转换为s3，所以第三部分的值为v_pi(s3)*gamma</h4><h4 id=\"第三个式子，在状态s1下采取动作a3后获得的即时奖励r-0的概率是1，r不等于0的概率是0，所以第二部分的值为0\"><a href=\"#第三个式子，在状态s1下采取动作a3后获得的即时奖励r-0的概率是1，r不等于0的概率是0，所以第二部分的值为0\" class=\"headerlink\" title=\"第三个式子，在状态s1下采取动作a3后获得的即时奖励r&#x3D;0的概率是1，r不等于0的概率是0，所以第二部分的值为0\"></a>第三个式子，在状态s1下采取动作a3后获得的即时奖励r&#x3D;0的概率是1，r不等于0的概率是0，所以第二部分的值为0</h4><h4 id=\"此时s1的state-value对应的的贝尔曼公式为\"><a href=\"#此时s1的state-value对应的的贝尔曼公式为\" class=\"headerlink\" title=\"此时s1的state value对应的的贝尔曼公式为\"></a>此时s1的state value对应的的贝尔曼公式为</h4><p>$$<br>v_{\\pi}(s_1) &#x3D; 0 + \\gamma v_{\\pi}(s_3)<br>$$</p>\n<h4 id=\"这里所得到的形式，过程虽然复杂，但实际上就是跟-第二部分中（return的计算）方法二的表达式是一个道理，那么根据这个思想，就可以直接得出其他几个状态下的state-value的贝尔曼公式\"><a href=\"#这里所得到的形式，过程虽然复杂，但实际上就是跟-第二部分中（return的计算）方法二的表达式是一个道理，那么根据这个思想，就可以直接得出其他几个状态下的state-value的贝尔曼公式\" class=\"headerlink\" title=\"这里所得到的形式，过程虽然复杂，但实际上就是跟 第二部分中（return的计算）方法二的表达式是一个道理，那么根据这个思想，就可以直接得出其他几个状态下的state value的贝尔曼公式\"></a>这里所得到的形式，过程虽然复杂，但实际上就是跟 第二部分中（return的计算）方法二的表达式是一个道理，那么根据这个思想，就可以直接得出其他几个状态下的state value的贝尔曼公式</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/17.png\"></p>\n<h4 id=\"有了这几个方程组，就可以通过矩阵的思想来求解了（也可以采取其他办法），得到的结果如下\"><a href=\"#有了这几个方程组，就可以通过矩阵的思想来求解了（也可以采取其他办法），得到的结果如下\" class=\"headerlink\" title=\"有了这几个方程组，就可以通过矩阵的思想来求解了（也可以采取其他办法），得到的结果如下\"></a>有了这几个方程组，就可以通过矩阵的思想来求解了（也可以采取其他办法），得到的结果如下</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/18.png\"></p>\n<h4 id=\"当-gamma-取某个值时，上面的式子也会对应有一个确定的值，而这个值的大小就说明了那些状态是值得去探索的\"><a href=\"#当-gamma-取某个值时，上面的式子也会对应有一个确定的值，而这个值的大小就说明了那些状态是值得去探索的\" class=\"headerlink\" title=\"当 gamma 取某个值时，上面的式子也会对应有一个确定的值，而这个值的大小就说明了那些状态是值得去探索的\"></a>当 gamma 取某个值时，上面的式子也会对应有一个确定的值，而这个值的大小就说明了那些状态是值得去探索的</h4><h4 id=\"计算得到-state-value-之后，根据其反映出来的情况，就可以根据需要去改进策略（policy-evaluation），不断的改进策略以得到最优策略\"><a href=\"#计算得到-state-value-之后，根据其反映出来的情况，就可以根据需要去改进策略（policy-evaluation），不断的改进策略以得到最优策略\" class=\"headerlink\" title=\"计算得到 state value 之后，根据其反映出来的情况，就可以根据需要去改进策略（policy evaluation），不断的改进策略以得到最优策略\"></a>计算得到 state value 之后，根据其反映出来的情况，就可以根据需要去改进策略（policy evaluation），不断的改进策略以得到最优策略</h4></blockquote>\n<h3 id=\"第五部分——贝尔曼公式的矩阵和向量形式\"><a href=\"#第五部分——贝尔曼公式的矩阵和向量形式\" class=\"headerlink\" title=\"第五部分——贝尔曼公式的矩阵和向量形式\"></a>第五部分——贝尔曼公式的矩阵和向量形式</h3><blockquote>\n<h4 id=\"贝尔曼公式是针对一个状态空间而言的，所以自然而然的与矩阵产生关联\"><a href=\"#贝尔曼公式是针对一个状态空间而言的，所以自然而然的与矩阵产生关联\" class=\"headerlink\" title=\"贝尔曼公式是针对一个状态空间而言的，所以自然而然的与矩阵产生关联\"></a>贝尔曼公式是针对一个状态空间而言的，所以自然而然的与矩阵产生关联</h4><h4 id=\"将贝尔曼公式的形式进行一个变换，先看贝尔曼公式最初的形式\"><a href=\"#将贝尔曼公式的形式进行一个变换，先看贝尔曼公式最初的形式\" class=\"headerlink\" title=\"将贝尔曼公式的形式进行一个变换，先看贝尔曼公式最初的形式\"></a>将贝尔曼公式的形式进行一个变换，先看贝尔曼公式最初的形式</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/19.png\"></p>\n<h4 id=\"将其改写为\"><a href=\"#将其改写为\" class=\"headerlink\" title=\"将其改写为\"></a>将其改写为</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/20.png\"></p>\n<h4 id=\"改写后的式子，第一部分表示的是：从当前状态s出发，所能得到的即时奖励r的平均值（期望）\"><a href=\"#改写后的式子，第一部分表示的是：从当前状态s出发，所能得到的即时奖励r的平均值（期望）\" class=\"headerlink\" title=\"改写后的式子，第一部分表示的是：从当前状态s出发，所能得到的即时奖励r的平均值（期望）\"></a>改写后的式子，第一部分表示的是：从当前状态s出发，所能得到的即时奖励r的平均值（期望）</h4><h4 id=\"再将上面改写后的形式进一步简化为\"><a href=\"#再将上面改写后的形式进一步简化为\" class=\"headerlink\" title=\"再将上面改写后的形式进一步简化为\"></a>再将上面改写后的形式进一步简化为</h4><p>$$<br>v_{\\pi} &#x3D; r_{\\pi} + \\gamma P_{\\pi} v_{\\pi}<br>$$</p>\n<h4 id=\"其中\"><a href=\"#其中\" class=\"headerlink\" title=\"其中\"></a>其中</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/21.png\"></p>\n<h4 id=\"举一个n-4的例子\"><a href=\"#举一个n-4的例子\" class=\"headerlink\" title=\"举一个n&#x3D;4的例子\"></a>举一个n&#x3D;4的例子</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/22.png\"></p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/23.png\"></p>\n<h4 id=\"上式右边第一部分-r-pi-表示：从状态s出发获得的即时奖励r的期望（平均值）\"><a href=\"#上式右边第一部分-r-pi-表示：从状态s出发获得的即时奖励r的期望（平均值）\" class=\"headerlink\" title=\"上式右边第一部分 r_pi 表示：从状态s出发获得的即时奖励r的期望（平均值）\"></a>上式右边第一部分 r_pi 表示：从状态s出发获得的即时奖励r的期望（平均值）</h4><h4 id=\"第二部分中前一个矩阵-P-pi-表示：从状态s-i转换为s-j的概率\"><a href=\"#第二部分中前一个矩阵-P-pi-表示：从状态s-i转换为s-j的概率\" class=\"headerlink\" title=\"第二部分中前一个矩阵 P_pi 表示：从状态s_i转换为s_j的概率\"></a>第二部分中前一个矩阵 P_pi 表示：从状态s_i转换为s_j的概率</h4><h4 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h4><h4 id=\"给定一个policy，可以列出一个与之对应的贝尔曼公式，通过对贝尔曼公式进行求解得到-state-value，这样的过程就称之为-policy-evaluation，通过评估一个策略的优劣，进而去改进策略，最终得到最优策略\"><a href=\"#给定一个policy，可以列出一个与之对应的贝尔曼公式，通过对贝尔曼公式进行求解得到-state-value，这样的过程就称之为-policy-evaluation，通过评估一个策略的优劣，进而去改进策略，最终得到最优策略\" class=\"headerlink\" title=\"给定一个policy，可以列出一个与之对应的贝尔曼公式，通过对贝尔曼公式进行求解得到 state value，这样的过程就称之为 policy evaluation，通过评估一个策略的优劣，进而去改进策略，最终得到最优策略\"></a>给定一个policy，可以列出一个与之对应的贝尔曼公式，通过对贝尔曼公式进行求解得到 state value，这样的过程就称之为 policy evaluation，通过评估一个策略的优劣，进而去改进策略，最终得到最优策略</h4><h4 id=\"求解贝尔曼方程的常用方法（求逆的方法通常不推荐，因为当矩阵维数很大时，求逆的计算量很大）\"><a href=\"#求解贝尔曼方程的常用方法（求逆的方法通常不推荐，因为当矩阵维数很大时，求逆的计算量很大）\" class=\"headerlink\" title=\"求解贝尔曼方程的常用方法（求逆的方法通常不推荐，因为当矩阵维数很大时，求逆的计算量很大）\"></a>求解贝尔曼方程的常用方法（求逆的方法通常不推荐，因为当矩阵维数很大时，求逆的计算量很大）</h4><blockquote>\n<h4 id=\"常用的是迭代法求解\"><a href=\"#常用的是迭代法求解\" class=\"headerlink\" title=\"常用的是迭代法求解\"></a>常用的是迭代法求解</h4><p>$$<br>v_{k+1} &#x3D; r_{\\pi} + \\gamma P_{\\pi} v_{k}<br>$$</p>\n<h4 id=\"从-v-0开始，可以先假定一个值，然后就可以求出v-1，通过v-1又求出v-2，以此类推，求得v-k，v-k-1，最后就会求出这样一个序列，v-0，v-1…v-k，当-k-趋近于无穷时，就有\"><a href=\"#从-v-0开始，可以先假定一个值，然后就可以求出v-1，通过v-1又求出v-2，以此类推，求得v-k，v-k-1，最后就会求出这样一个序列，v-0，v-1…v-k，当-k-趋近于无穷时，就有\" class=\"headerlink\" title=\"从 v_0开始，可以先假定一个值，然后就可以求出v_1，通过v_1又求出v_2，以此类推，求得v_k，v_k+1，最后就会求出这样一个序列，v_0，v_1…v_k，当 k 趋近于无穷时，就有\"></a>从 v_0开始，可以先假定一个值，然后就可以求出v_1，通过v_1又求出v_2，以此类推，求得v_k，v_k+1，最后就会求出这样一个序列，v_0，v_1…v_k，当 k 趋近于无穷时，就有</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/24.png\"></p>\n<h4 id=\"v-k收敛到了v-pi，证明如下\"><a href=\"#v-k收敛到了v-pi，证明如下\" class=\"headerlink\" title=\"v_k收敛到了v_pi，证明如下\"></a>v_k收敛到了v_pi，证明如下</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/25.png\"></p>\n<h4 id=\"其证明过程类似于李亚普诺夫函数的稳定性证明\"><a href=\"#其证明过程类似于李亚普诺夫函数的稳定性证明\" class=\"headerlink\" title=\"其证明过程类似于李亚普诺夫函数的稳定性证明\"></a>其证明过程类似于李亚普诺夫函数的稳定性证明</h4></blockquote>\n<h4 id=\"在下面这样一个例子中，说明了一些情况\"><a href=\"#在下面这样一个例子中，说明了一些情况\" class=\"headerlink\" title=\"在下面这样一个例子中，说明了一些情况\"></a>在下面这样一个例子中，说明了一些情况</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/26.png\"></p>\n<h4 id=\"可以发现，越靠近目标区域的状态的state-value越大，反之则越小\"><a href=\"#可以发现，越靠近目标区域的状态的state-value越大，反之则越小\" class=\"headerlink\" title=\"可以发现，越靠近目标区域的状态的state value越大，反之则越小\"></a>可以发现，越靠近目标区域的状态的state value越大，反之则越小</h4><h4 id=\"另外就是，不同的策略可能会得到相同的state-value\"><a href=\"#另外就是，不同的策略可能会得到相同的state-value\" class=\"headerlink\" title=\"另外就是，不同的策略可能会得到相同的state value\"></a>另外就是，不同的策略可能会得到相同的state value</h4><h4 id=\"通过计算state-value可以评估一个策略的优劣\"><a href=\"#通过计算state-value可以评估一个策略的优劣\" class=\"headerlink\" title=\"通过计算state value可以评估一个策略的优劣\"></a>通过计算state value可以评估一个策略的优劣</h4></blockquote>\n<h3 id=\"第六部分——Action-value\"><a href=\"#第六部分——Action-value\" class=\"headerlink\" title=\"第六部分——Action value\"></a>第六部分——Action value</h3><blockquote>\n<h4 id=\"state-value-和-Action-value-的联系与区别\"><a href=\"#state-value-和-Action-value-的联系与区别\" class=\"headerlink\" title=\"state value 和 Action value 的联系与区别\"></a>state value 和 Action value 的联系与区别</h4><blockquote>\n<h4 id=\"前者是agent从一个状态出发，所获得的return的平均值（期望）\"><a href=\"#前者是agent从一个状态出发，所获得的return的平均值（期望）\" class=\"headerlink\" title=\"前者是agent从一个状态出发，所获得的return的平均值（期望）\"></a>前者是agent从一个状态出发，所获得的return的平均值（期望）</h4><h4 id=\"后者是agent从一个状态出发并且选择了一个Action之后，所获得的returen的平均值（期望）\"><a href=\"#后者是agent从一个状态出发并且选择了一个Action之后，所获得的returen的平均值（期望）\" class=\"headerlink\" title=\"后者是agent从一个状态出发并且选择了一个Action之后，所获得的returen的平均值（期望）\"></a>后者是agent从一个状态出发并且选择了一个Action之后，所获得的returen的平均值（期望）</h4></blockquote>\n<h4 id=\"为什么要引入action-value，因为一个状态的转换是选择了一个Action之后引起的\"><a href=\"#为什么要引入action-value，因为一个状态的转换是选择了一个Action之后引起的\" class=\"headerlink\" title=\"为什么要引入action value，因为一个状态的转换是选择了一个Action之后引起的\"></a>为什么要引入action value，因为一个状态的转换是选择了一个Action之后引起的</h4><h4 id=\"定义\"><a href=\"#定义\" class=\"headerlink\" title=\"定义\"></a>定义</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/27.png\"></p>\n<h4 id=\"它依赖策略pi\"><a href=\"#它依赖策略pi\" class=\"headerlink\" title=\"它依赖策略pi\"></a>它依赖策略pi</h4><h4 id=\"它与state-value-数学式上的联系，state-value可以表示为下面这个形式\"><a href=\"#它与state-value-数学式上的联系，state-value可以表示为下面这个形式\" class=\"headerlink\" title=\"它与state value 数学式上的联系，state value可以表示为下面这个形式\"></a>它与state value 数学式上的联系，state value可以表示为下面这个形式</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/28.png\"></p>\n<h4 id=\"在一个状态下，可能有多种可选的动作，每个动作都有对应的概率，上式右边部分表示，在状态s下，选择动作a后的所获得的return的平均值-在状态s下选择动作a的概率\"><a href=\"#在一个状态下，可能有多种可选的动作，每个动作都有对应的概率，上式右边部分表示，在状态s下，选择动作a后的所获得的return的平均值-在状态s下选择动作a的概率\" class=\"headerlink\" title=\"在一个状态下，可能有多种可选的动作，每个动作都有对应的概率，上式右边部分表示，在状态s下，选择动作a后的所获得的return的平均值 * 在状态s下选择动作a的概率\"></a>在一个状态下，可能有多种可选的动作，每个动作都有对应的概率，上式右边部分表示，在状态s下，选择动作a后的所获得的return的平均值 * 在状态s下选择动作a的概率</h4><h4 id=\"注意到，右边的前面一部分就是-action-value的定义，所以二者数学式的联系如下\"><a href=\"#注意到，右边的前面一部分就是-action-value的定义，所以二者数学式的联系如下\" class=\"headerlink\" title=\"注意到，右边的前面一部分就是 action value的定义，所以二者数学式的联系如下\"></a>注意到，右边的前面一部分就是 action value的定义，所以二者数学式的联系如下</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/29.png\"></p>\n<h4 id=\"这个式子说明什么呢，说明了已知一个状态的-action-value，对其求平均就可以求出其对应的-state-value\"><a href=\"#这个式子说明什么呢，说明了已知一个状态的-action-value，对其求平均就可以求出其对应的-state-value\" class=\"headerlink\" title=\"这个式子说明什么呢，说明了已知一个状态的 action value，对其求平均就可以求出其对应的 state value\"></a>这个式子说明什么呢，说明了已知一个状态的 action value，对其求平均就可以求出其对应的 state value</h4><h4 id=\"另外呢，根据前面第五部分开头的表达式可以进一步给出-action-value的表达式\"><a href=\"#另外呢，根据前面第五部分开头的表达式可以进一步给出-action-value的表达式\" class=\"headerlink\" title=\"另外呢，根据前面第五部分开头的表达式可以进一步给出 action value的表达式\"></a>另外呢，根据前面第五部分开头的表达式可以进一步给出 action value的表达式</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/30.png\"></p>\n<h4 id=\"而这个式子，它有说明了，已知一个状态的-state-value-可以求出其对应的-action-value\"><a href=\"#而这个式子，它有说明了，已知一个状态的-state-value-可以求出其对应的-action-value\" class=\"headerlink\" title=\"而这个式子，它有说明了，已知一个状态的 state value 可以求出其对应的 action value\"></a>而这个式子，它有说明了，已知一个状态的 state value 可以求出其对应的 action value</h4><h4 id=\"（没怎么懂，迷迷糊糊的！）\"><a href=\"#（没怎么懂，迷迷糊糊的！）\" class=\"headerlink\" title=\"（没怎么懂，迷迷糊糊的！）\"></a>（没怎么懂，迷迷糊糊的！）</h4></blockquote>\n","cover_type":"img","excerpt":"","more":"<h1 id=\"贝尔曼方程\"><a href=\"#贝尔曼方程\" class=\"headerlink\" title=\"贝尔曼方程\"></a>贝尔曼方程</h1><h3 id=\"第一部分——return\"><a href=\"#第一部分——return\" class=\"headerlink\" title=\"第一部分——return\"></a>第一部分——return</h3><blockquote>\n<h4 id=\"return-能够作为策略好坏的一个评估标准\"><a href=\"#return-能够作为策略好坏的一个评估标准\" class=\"headerlink\" title=\"return 能够作为策略好坏的一个评估标准\"></a>return 能够作为策略好坏的一个评估标准</h4><h4 id=\"下面通过三种情况来说明这个问题\"><a href=\"#下面通过三种情况来说明这个问题\" class=\"headerlink\" title=\"下面通过三种情况来说明这个问题\"></a>下面通过三种情况来说明这个问题</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/1.png\"></p>\n<h4 id=\"第一种策略\"><a href=\"#第一种策略\" class=\"headerlink\" title=\"第一种策略\"></a>第一种策略</h4><blockquote>\n<h4 id=\"按照策略1，从-s1-出发，往下去-s3，再往右到达-s4，discounted-return计算如下\"><a href=\"#按照策略1，从-s1-出发，往下去-s3，再往右到达-s4，discounted-return计算如下\" class=\"headerlink\" title=\"按照策略1，从 s1 出发，往下去 s3，再往右到达 s4，discounted return计算如下\"></a>按照策略1，从 s1 出发，往下去 s3，再往右到达 s4，discounted return计算如下</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/2.png\"></p>\n<h4 id=\"gamma-是衰减系数\"><a href=\"#gamma-是衰减系数\" class=\"headerlink\" title=\"gamma 是衰减系数\"></a>gamma 是衰减系数</h4></blockquote>\n<h4 id=\"第二种策略\"><a href=\"#第二种策略\" class=\"headerlink\" title=\"第二种策略\"></a>第二种策略</h4><h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/3.png\"></h4><h4 id=\"第三种策略\"><a href=\"#第三种策略\" class=\"headerlink\" title=\"第三种策略\"></a>第三种策略</h4><blockquote>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/4.png\"></p>\n<h4 id=\"注意，严格说来，这个return3-已经不是我们所说的return了，因为return是针对一条轨迹定义的，针对一个trajectory定义的，这里实际上是有两个轨迹，这里所做的是求取平均值，也即求expectation（期望），这个return3就是state-value\"><a href=\"#注意，严格说来，这个return3-已经不是我们所说的return了，因为return是针对一条轨迹定义的，针对一个trajectory定义的，这里实际上是有两个轨迹，这里所做的是求取平均值，也即求expectation（期望），这个return3就是state-value\" class=\"headerlink\" title=\"注意，严格说来，这个return3 已经不是我们所说的return了，因为return是针对一条轨迹定义的，针对一个trajectory定义的，这里实际上是有两个轨迹，这里所做的是求取平均值，也即求expectation（期望），这个return3就是state value\"></a>注意，严格说来，这个return3 已经不是我们所说的return了，因为return是针对一条轨迹定义的，针对一个trajectory定义的，这里实际上是有两个轨迹，这里所做的是求取平均值，也即求expectation（期望），这个return3就是state value</h4></blockquote>\n<h4 id=\"显然，return1-return3-return2，那么对应的就是策略1最好，策略3居中，策略2最差\"><a href=\"#显然，return1-return3-return2，那么对应的就是策略1最好，策略3居中，策略2最差\" class=\"headerlink\" title=\"显然，return1 &gt; return3 &gt; return2，那么对应的就是策略1最好，策略3居中，策略2最差\"></a>显然，return1 &gt; return3 &gt; return2，那么对应的就是策略1最好，策略3居中，策略2最差</h4></blockquote>\n<h3 id=\"第二部分——return的计算\"><a href=\"#第二部分——return的计算\" class=\"headerlink\" title=\"第二部分——return的计算\"></a>第二部分——return的计算</h3><blockquote>\n<h4 id=\"以图中的例子为例\"><a href=\"#以图中的例子为例\" class=\"headerlink\" title=\"以图中的例子为例\"></a>以图中的例子为例</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/5.png\"></p>\n<h4 id=\"用-v-i来表示从-s-i-出发对应的return\"><a href=\"#用-v-i来表示从-s-i-出发对应的return\" class=\"headerlink\" title=\"用 v_i来表示从 s_i 出发对应的return\"></a>用 v_i来表示从 s_i 出发对应的return</h4><h4 id=\"方法一\"><a href=\"#方法一\" class=\"headerlink\" title=\"方法一\"></a>方法一</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/6.png\"></p>\n<h4 id=\"方法二\"><a href=\"#方法二\" class=\"headerlink\" title=\"方法二\"></a>方法二</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/7.png\"></p>\n<h4 id=\"这里的方法二说明了什么一个问题呢，从当前状态出发，return的值，就等于当前的reward-下一状态的return-衰减因子gamma，也就是说从一个状态出发，它的return是依赖于其他状态的return的，这种思想在强化学习中被称之为-bootstrapping，这种思想是从自身出发，不断迭代所得到的结果\"><a href=\"#这里的方法二说明了什么一个问题呢，从当前状态出发，return的值，就等于当前的reward-下一状态的return-衰减因子gamma，也就是说从一个状态出发，它的return是依赖于其他状态的return的，这种思想在强化学习中被称之为-bootstrapping，这种思想是从自身出发，不断迭代所得到的结果\" class=\"headerlink\" title=\"这里的方法二说明了什么一个问题呢，从当前状态出发，return的值，就等于当前的reward + 下一状态的return * 衰减因子gamma，也就是说从一个状态出发，它的return是依赖于其他状态的return的，这种思想在强化学习中被称之为 bootstrapping，这种思想是从自身出发，不断迭代所得到的结果\"></a>这里的方法二说明了什么一个问题呢，从当前状态出发，return的值，就等于当前的reward + 下一状态的return * 衰减因子gamma，也就是说从一个状态出发，它的return是依赖于其他状态的return的，这种思想在强化学习中被称之为 bootstrapping，这种思想是从自身出发，不断迭代所得到的结果</h4><h4 id=\"通过矩阵的形式给出上面的四个式子\"><a href=\"#通过矩阵的形式给出上面的四个式子\" class=\"headerlink\" title=\"通过矩阵的形式给出上面的四个式子\"></a>通过矩阵的形式给出上面的四个式子</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/8.png\"></p>\n<h4 id=\"对该式子进行求解得到-v，实际上这个式子就是-贝尔曼方程，但这是一个非常简单的贝尔曼方程\"><a href=\"#对该式子进行求解得到-v，实际上这个式子就是-贝尔曼方程，但这是一个非常简单的贝尔曼方程\" class=\"headerlink\" title=\"对该式子进行求解得到 v，实际上这个式子就是 贝尔曼方程，但这是一个非常简单的贝尔曼方程\"></a>对该式子进行求解得到 v，实际上这个式子就是 贝尔曼方程，但这是一个非常简单的贝尔曼方程</h4></blockquote>\n<h3 id=\"第三部分——state-value\"><a href=\"#第三部分——state-value\" class=\"headerlink\" title=\"第三部分——state value\"></a>第三部分——state value</h3><blockquote>\n<h4 id=\"先来看一个单步的过程\"><a href=\"#先来看一个单步的过程\" class=\"headerlink\" title=\"先来看一个单步的过程\"></a>先来看一个单步的过程</h4><p>$$<br>S_t \\xrightarrow{A_t} R_{t+1}, S_{t+1}<br>$$</p>\n<h4 id=\"在这个过程中，都存在一个条件概率\"><a href=\"#在这个过程中，都存在一个条件概率\" class=\"headerlink\" title=\"在这个过程中，都存在一个条件概率\"></a>在这个过程中，都存在一个条件概率</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/9.png\"></p>\n<h4 id=\"再来看一个多步的过程-trajectory\"><a href=\"#再来看一个多步的过程-trajectory\" class=\"headerlink\" title=\"再来看一个多步的过程 trajectory\"></a>再来看一个多步的过程 trajectory</h4><p>$$<br>S_t \\xrightarrow{A_t} R_{t+1}, S_{t+1} \\xrightarrow{A_{t+1}} R_{t+2}, S_{t+2} \\xrightarrow{A_{t+2}} R_{t+3}, \\ldots<br>$$</p>\n<h4 id=\"其对应的-discounted-return如下\"><a href=\"#其对应的-discounted-return如下\" class=\"headerlink\" title=\"其对应的 discounted return如下\"></a>其对应的 discounted return如下</h4><p>$$<br>G_t &#x3D; R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots<br>$$</p>\n<h4 id=\"这里的G-t-也是一个随机变量\"><a href=\"#这里的G-t-也是一个随机变量\" class=\"headerlink\" title=\"这里的G_t 也是一个随机变量\"></a>这里的G_t 也是一个随机变量</h4><h4 id=\"正式介绍-state-value\"><a href=\"#正式介绍-state-value\" class=\"headerlink\" title=\"正式介绍 state value\"></a>正式介绍 state value</h4><blockquote>\n<h4 id=\"state-value-实际上就是-G-t-的期望值（expectation）或平均值\"><a href=\"#state-value-实际上就是-G-t-的期望值（expectation）或平均值\" class=\"headerlink\" title=\"state value 实际上就是 G_t 的期望值（expectation）或平均值\"></a>state value 实际上就是 G_t 的期望值（expectation）或平均值</h4><p>$$<br>v_\\pi(s) &#x3D; \\mathbb{E}[G_t | S_t &#x3D; s]<br>$$</p>\n<h4 id=\"它是关于状态s的一个函数，所以在不同状态下，对应不同的G-t也对应不同的-state-value\"><a href=\"#它是关于状态s的一个函数，所以在不同状态下，对应不同的G-t也对应不同的-state-value\" class=\"headerlink\" title=\"它是关于状态s的一个函数，所以在不同状态下，对应不同的G_t也对应不同的 state value\"></a>它是关于状态s的一个函数，所以在不同状态下，对应不同的G_t也对应不同的 state value</h4><h4 id=\"另外，它也是关于策略pi的函数，也可以写为-v-s-pi-，不同的策略会得到不同的轨迹，从而得到不同的G-t\"><a href=\"#另外，它也是关于策略pi的函数，也可以写为-v-s-pi-，不同的策略会得到不同的轨迹，从而得到不同的G-t\" class=\"headerlink\" title=\"另外，它也是关于策略pi的函数，也可以写为 v(s , pi)，不同的策略会得到不同的轨迹，从而得到不同的G_t\"></a>另外，它也是关于策略pi的函数，也可以写为 v(s , pi)，不同的策略会得到不同的轨迹，从而得到不同的G_t</h4><h4 id=\"state-value不仅仅是一个数值，当其值很大时，说明当前状态具有很高的价值\"><a href=\"#state-value不仅仅是一个数值，当其值很大时，说明当前状态具有很高的价值\" class=\"headerlink\" title=\"state value不仅仅是一个数值，当其值很大时，说明当前状态具有很高的价值\"></a>state value不仅仅是一个数值，当其值很大时，说明当前状态具有很高的价值</h4></blockquote>\n<h4 id=\"return-与-state-value的区别\"><a href=\"#return-与-state-value的区别\" class=\"headerlink\" title=\"return 与 state value的区别\"></a>return 与 state value的区别</h4><blockquote>\n<h4 id=\"return是针对单个trajectory而言的，单个trajectory的return（对应的是一种确定性，如拓扑图）\"><a href=\"#return是针对单个trajectory而言的，单个trajectory的return（对应的是一种确定性，如拓扑图）\" class=\"headerlink\" title=\"return是针对单个trajectory而言的，单个trajectory的return（对应的是一种确定性，如拓扑图）\"></a>return是针对单个trajectory而言的，单个trajectory的return（对应的是一种确定性，如拓扑图）</h4><h4 id=\"state-value是针对多个trajectory而言的，它是多个trajectory的return的平均值（对应的是一种随机性）\"><a href=\"#state-value是针对多个trajectory而言的，它是多个trajectory的return的平均值（对应的是一种随机性）\" class=\"headerlink\" title=\"state value是针对多个trajectory而言的，它是多个trajectory的return的平均值（对应的是一种随机性）\"></a>state value是针对多个trajectory而言的，它是多个trajectory的return的平均值（对应的是一种随机性）</h4></blockquote>\n</blockquote>\n<h3 id=\"第四部分——贝尔曼公式的推导\"><a href=\"#第四部分——贝尔曼公式的推导\" class=\"headerlink\" title=\"第四部分——贝尔曼公式的推导\"></a>第四部分——贝尔曼公式的推导</h3><blockquote>\n<h4 id=\"贝尔曼公式就是用来计算上一部分提到的state-value，它描述了不同状态下state-value之间的关系\"><a href=\"#贝尔曼公式就是用来计算上一部分提到的state-value，它描述了不同状态下state-value之间的关系\" class=\"headerlink\" title=\"贝尔曼公式就是用来计算上一部分提到的state value，它描述了不同状态下state value之间的关系\"></a>贝尔曼公式就是用来计算上一部分提到的state value，它描述了不同状态下state value之间的关系</h4><h4 id=\"考虑这样一个trajectory\"><a href=\"#考虑这样一个trajectory\" class=\"headerlink\" title=\"考虑这样一个trajectory\"></a>考虑这样一个trajectory</h4><p>$$<br>S_t \\xrightarrow{A_t} R_{t+1}, S_{t+1} \\xrightarrow{A_{t+1}} R_{t+2}, S_{t+2} \\xrightarrow{A_{t+2}} R_{t+3}, \\ldots<br>$$</p>\n<h4 id=\"对应的G-t（discounted-return）可以表示为\"><a href=\"#对应的G-t（discounted-return）可以表示为\" class=\"headerlink\" title=\"对应的G_t（discounted return）可以表示为\"></a>对应的G_t（discounted return）可以表示为</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/10.png\"></p>\n<h4 id=\"对应的v-s-pi-（state-value）可以表示为（贝尔曼公式）\"><a href=\"#对应的v-s-pi-（state-value）可以表示为（贝尔曼公式）\" class=\"headerlink\" title=\"对应的v( s, pi)（state value）可以表示为（贝尔曼公式）\"></a>对应的v( s, pi)（state value）可以表示为（贝尔曼公式）</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/11.png\"></p>\n<h4 id=\"上面的表达式中的第一项的详细表示\"><a href=\"#上面的表达式中的第一项的详细表示\" class=\"headerlink\" title=\"上面的表达式中的第一项的详细表示\"></a>上面的表达式中的第一项的详细表示</h4><blockquote>\n<h4 id=\"在状态s下，采取行动a之后所得到的即时奖励r的期望值\"><a href=\"#在状态s下，采取行动a之后所得到的即时奖励r的期望值\" class=\"headerlink\" title=\"在状态s下，采取行动a之后所得到的即时奖励r的期望值\"></a>在状态s下，采取行动a之后所得到的即时奖励r的期望值</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/12.png\"></p>\n<h4 id=\"在当前状态s下，有多个可采取的action，每个action对应的一个概率-pi\"><a href=\"#在当前状态s下，有多个可采取的action，每个action对应的一个概率-pi\" class=\"headerlink\" title=\"在当前状态s下，有多个可采取的action，每个action对应的一个概率 pi\"></a>在当前状态s下，有多个可采取的action，每个action对应的一个概率 pi</h4><h4 id=\"当采取action-a，所得到的value就是\"><a href=\"#当采取action-a，所得到的value就是\" class=\"headerlink\" title=\"当采取action a，所得到的value就是\"></a>当采取action a，所得到的value就是</h4><p>$$<br>\\mathbb{E}[R_{t+1} | S_t &#x3D; s, A_t &#x3D; a]<br>$$</p>\n<h4 id=\"也等同于\"><a href=\"#也等同于\" class=\"headerlink\" title=\"也等同于\"></a>也等同于</h4><p>$$<br>\\sum_{r} p(r | s, a) r<br>$$</p>\n<h4 id=\"它的意思是，在状态s下，采取动作a，得到的即时奖励r的概率为p，最后乘以r本身\"><a href=\"#它的意思是，在状态s下，采取动作a，得到的即时奖励r的概率为p，最后乘以r本身\" class=\"headerlink\" title=\"它的意思是，在状态s下，采取动作a，得到的即时奖励r的概率为p，最后乘以r本身\"></a>它的意思是，在状态s下，采取动作a，得到的即时奖励r的概率为p，最后乘以r本身</h4><h4 id=\"所以这整个第一项的期望就是这么求来的，它实际上就是所能得到的即时奖励r的期望值\"><a href=\"#所以这整个第一项的期望就是这么求来的，它实际上就是所能得到的即时奖励r的期望值\" class=\"headerlink\" title=\"所以这整个第一项的期望就是这么求来的，它实际上就是所能得到的即时奖励r的期望值\"></a>所以这整个第一项的期望就是这么求来的，它实际上就是所能得到的即时奖励r的期望值</h4></blockquote>\n<h4 id=\"第二部分的详细表示\"><a href=\"#第二部分的详细表示\" class=\"headerlink\" title=\"第二部分的详细表示\"></a>第二部分的详细表示</h4><blockquote>\n<h4 id=\"在当前状态s下，下一个时刻的return的期望值\"><a href=\"#在当前状态s下，下一个时刻的return的期望值\" class=\"headerlink\" title=\"在当前状态s下，下一个时刻的return的期望值\"></a>在当前状态s下，下一个时刻的return的期望值</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/13.png\"></p>\n<h4 id=\"这里隐含了一个马尔科夫性，即下一状态，只取决于当前状态，与当前状态之前的状态无关\"><a href=\"#这里隐含了一个马尔科夫性，即下一状态，只取决于当前状态，与当前状态之前的状态无关\" class=\"headerlink\" title=\"这里隐含了一个马尔科夫性，即下一状态，只取决于当前状态，与当前状态之前的状态无关\"></a>这里隐含了一个马尔科夫性，即下一状态，只取决于当前状态，与当前状态之前的状态无关</h4><h4 id=\"其余的变换都是概率论中的一些技巧\"><a href=\"#其余的变换都是概率论中的一些技巧\" class=\"headerlink\" title=\"其余的变换都是概率论中的一些技巧\"></a>其余的变换都是概率论中的一些技巧</h4></blockquote>\n<h4 id=\"然后就是正式给出贝尔曼公式的表达式（看起来好复杂！）\"><a href=\"#然后就是正式给出贝尔曼公式的表达式（看起来好复杂！）\" class=\"headerlink\" title=\"然后就是正式给出贝尔曼公式的表达式（看起来好复杂！）\"></a>然后就是正式给出贝尔曼公式的表达式（看起来好复杂！）</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/14.png\"></p>\n<h4 id=\"它描述了不同状态的state-value之间的关系，如上面所示，左边是s的state-value，右边的式子包含s’的state-value\"><a href=\"#它描述了不同状态的state-value之间的关系，如上面所示，左边是s的state-value，右边的式子包含s’的state-value\" class=\"headerlink\" title=\"它描述了不同状态的state value之间的关系，如上面所示，左边是s的state value，右边的式子包含s’的state value\"></a>它描述了不同状态的state value之间的关系，如上面所示，左边是s的state value，右边的式子包含s’的state value</h4><h4 id=\"另外，这是针对一个状态空间而言的，对应的可能是多个式子多个s，而不是一个单一的式子\"><a href=\"#另外，这是针对一个状态空间而言的，对应的可能是多个式子多个s，而不是一个单一的式子\" class=\"headerlink\" title=\"另外，这是针对一个状态空间而言的，对应的可能是多个式子多个s，而不是一个单一的式子\"></a>另外，这是针对一个状态空间而言的，对应的可能是多个式子多个s，而不是一个单一的式子</h4><h4 id=\"其求解思想就是-bootstrapping，还依赖于几个概率，第一个是概率是policy，求解出-state-value就是在评估策略的好坏，后面两个概率对应的是两个model，dynamic-model或-environment-model，这里的model可能是已知的也可能是未知的（model-free-RL）\"><a href=\"#其求解思想就是-bootstrapping，还依赖于几个概率，第一个是概率是policy，求解出-state-value就是在评估策略的好坏，后面两个概率对应的是两个model，dynamic-model或-environment-model，这里的model可能是已知的也可能是未知的（model-free-RL）\" class=\"headerlink\" title=\"其求解思想就是 bootstrapping，还依赖于几个概率，第一个是概率是policy，求解出 state value就是在评估策略的好坏，后面两个概率对应的是两个model，dynamic model或 environment model，这里的model可能是已知的也可能是未知的（model free RL）\"></a>其求解思想就是 bootstrapping，还依赖于几个概率，第一个是概率是policy，求解出 state value就是在评估策略的好坏，后面两个概率对应的是两个model，dynamic model或 environment model，这里的model可能是已知的也可能是未知的（model free RL）</h4><h4 id=\"通过一个例子深入理解贝尔曼公式\"><a href=\"#通过一个例子深入理解贝尔曼公式\" class=\"headerlink\" title=\"通过一个例子深入理解贝尔曼公式\"></a>通过一个例子深入理解贝尔曼公式</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/15.png\"></p>\n<h4 id=\"先来s1的state-value\"><a href=\"#先来s1的state-value\" class=\"headerlink\" title=\"先来s1的state value\"></a>先来s1的state value</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/16.png\"></p>\n<h4 id=\"先看第一个式子，在状态s1下采取动作a3的概率是1，不采取a3的概率是0，也就是说必然采取a3，那么第一部分就等于-1\"><a href=\"#先看第一个式子，在状态s1下采取动作a3的概率是1，不采取a3的概率是0，也就是说必然采取a3，那么第一部分就等于-1\" class=\"headerlink\" title=\"先看第一个式子，在状态s1下采取动作a3的概率是1，不采取a3的概率是0，也就是说必然采取a3，那么第一部分就等于 1\"></a>先看第一个式子，在状态s1下采取动作a3的概率是1，不采取a3的概率是0，也就是说必然采取a3，那么第一部分就等于 1</h4><h4 id=\"第二个式子，在状态s1下采取动作a3后状态转换为s3的概率是1，不为s3的概率为0，也就是说状态必然转换为s3，所以第三部分的值为v-pi-s3-gamma\"><a href=\"#第二个式子，在状态s1下采取动作a3后状态转换为s3的概率是1，不为s3的概率为0，也就是说状态必然转换为s3，所以第三部分的值为v-pi-s3-gamma\" class=\"headerlink\" title=\"第二个式子，在状态s1下采取动作a3后状态转换为s3的概率是1，不为s3的概率为0，也就是说状态必然转换为s3，所以第三部分的值为v_pi(s3)*gamma\"></a>第二个式子，在状态s1下采取动作a3后状态转换为s3的概率是1，不为s3的概率为0，也就是说状态必然转换为s3，所以第三部分的值为v_pi(s3)*gamma</h4><h4 id=\"第三个式子，在状态s1下采取动作a3后获得的即时奖励r-0的概率是1，r不等于0的概率是0，所以第二部分的值为0\"><a href=\"#第三个式子，在状态s1下采取动作a3后获得的即时奖励r-0的概率是1，r不等于0的概率是0，所以第二部分的值为0\" class=\"headerlink\" title=\"第三个式子，在状态s1下采取动作a3后获得的即时奖励r&#x3D;0的概率是1，r不等于0的概率是0，所以第二部分的值为0\"></a>第三个式子，在状态s1下采取动作a3后获得的即时奖励r&#x3D;0的概率是1，r不等于0的概率是0，所以第二部分的值为0</h4><h4 id=\"此时s1的state-value对应的的贝尔曼公式为\"><a href=\"#此时s1的state-value对应的的贝尔曼公式为\" class=\"headerlink\" title=\"此时s1的state value对应的的贝尔曼公式为\"></a>此时s1的state value对应的的贝尔曼公式为</h4><p>$$<br>v_{\\pi}(s_1) &#x3D; 0 + \\gamma v_{\\pi}(s_3)<br>$$</p>\n<h4 id=\"这里所得到的形式，过程虽然复杂，但实际上就是跟-第二部分中（return的计算）方法二的表达式是一个道理，那么根据这个思想，就可以直接得出其他几个状态下的state-value的贝尔曼公式\"><a href=\"#这里所得到的形式，过程虽然复杂，但实际上就是跟-第二部分中（return的计算）方法二的表达式是一个道理，那么根据这个思想，就可以直接得出其他几个状态下的state-value的贝尔曼公式\" class=\"headerlink\" title=\"这里所得到的形式，过程虽然复杂，但实际上就是跟 第二部分中（return的计算）方法二的表达式是一个道理，那么根据这个思想，就可以直接得出其他几个状态下的state value的贝尔曼公式\"></a>这里所得到的形式，过程虽然复杂，但实际上就是跟 第二部分中（return的计算）方法二的表达式是一个道理，那么根据这个思想，就可以直接得出其他几个状态下的state value的贝尔曼公式</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/17.png\"></p>\n<h4 id=\"有了这几个方程组，就可以通过矩阵的思想来求解了（也可以采取其他办法），得到的结果如下\"><a href=\"#有了这几个方程组，就可以通过矩阵的思想来求解了（也可以采取其他办法），得到的结果如下\" class=\"headerlink\" title=\"有了这几个方程组，就可以通过矩阵的思想来求解了（也可以采取其他办法），得到的结果如下\"></a>有了这几个方程组，就可以通过矩阵的思想来求解了（也可以采取其他办法），得到的结果如下</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/18.png\"></p>\n<h4 id=\"当-gamma-取某个值时，上面的式子也会对应有一个确定的值，而这个值的大小就说明了那些状态是值得去探索的\"><a href=\"#当-gamma-取某个值时，上面的式子也会对应有一个确定的值，而这个值的大小就说明了那些状态是值得去探索的\" class=\"headerlink\" title=\"当 gamma 取某个值时，上面的式子也会对应有一个确定的值，而这个值的大小就说明了那些状态是值得去探索的\"></a>当 gamma 取某个值时，上面的式子也会对应有一个确定的值，而这个值的大小就说明了那些状态是值得去探索的</h4><h4 id=\"计算得到-state-value-之后，根据其反映出来的情况，就可以根据需要去改进策略（policy-evaluation），不断的改进策略以得到最优策略\"><a href=\"#计算得到-state-value-之后，根据其反映出来的情况，就可以根据需要去改进策略（policy-evaluation），不断的改进策略以得到最优策略\" class=\"headerlink\" title=\"计算得到 state value 之后，根据其反映出来的情况，就可以根据需要去改进策略（policy evaluation），不断的改进策略以得到最优策略\"></a>计算得到 state value 之后，根据其反映出来的情况，就可以根据需要去改进策略（policy evaluation），不断的改进策略以得到最优策略</h4></blockquote>\n<h3 id=\"第五部分——贝尔曼公式的矩阵和向量形式\"><a href=\"#第五部分——贝尔曼公式的矩阵和向量形式\" class=\"headerlink\" title=\"第五部分——贝尔曼公式的矩阵和向量形式\"></a>第五部分——贝尔曼公式的矩阵和向量形式</h3><blockquote>\n<h4 id=\"贝尔曼公式是针对一个状态空间而言的，所以自然而然的与矩阵产生关联\"><a href=\"#贝尔曼公式是针对一个状态空间而言的，所以自然而然的与矩阵产生关联\" class=\"headerlink\" title=\"贝尔曼公式是针对一个状态空间而言的，所以自然而然的与矩阵产生关联\"></a>贝尔曼公式是针对一个状态空间而言的，所以自然而然的与矩阵产生关联</h4><h4 id=\"将贝尔曼公式的形式进行一个变换，先看贝尔曼公式最初的形式\"><a href=\"#将贝尔曼公式的形式进行一个变换，先看贝尔曼公式最初的形式\" class=\"headerlink\" title=\"将贝尔曼公式的形式进行一个变换，先看贝尔曼公式最初的形式\"></a>将贝尔曼公式的形式进行一个变换，先看贝尔曼公式最初的形式</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/19.png\"></p>\n<h4 id=\"将其改写为\"><a href=\"#将其改写为\" class=\"headerlink\" title=\"将其改写为\"></a>将其改写为</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/20.png\"></p>\n<h4 id=\"改写后的式子，第一部分表示的是：从当前状态s出发，所能得到的即时奖励r的平均值（期望）\"><a href=\"#改写后的式子，第一部分表示的是：从当前状态s出发，所能得到的即时奖励r的平均值（期望）\" class=\"headerlink\" title=\"改写后的式子，第一部分表示的是：从当前状态s出发，所能得到的即时奖励r的平均值（期望）\"></a>改写后的式子，第一部分表示的是：从当前状态s出发，所能得到的即时奖励r的平均值（期望）</h4><h4 id=\"再将上面改写后的形式进一步简化为\"><a href=\"#再将上面改写后的形式进一步简化为\" class=\"headerlink\" title=\"再将上面改写后的形式进一步简化为\"></a>再将上面改写后的形式进一步简化为</h4><p>$$<br>v_{\\pi} &#x3D; r_{\\pi} + \\gamma P_{\\pi} v_{\\pi}<br>$$</p>\n<h4 id=\"其中\"><a href=\"#其中\" class=\"headerlink\" title=\"其中\"></a>其中</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/21.png\"></p>\n<h4 id=\"举一个n-4的例子\"><a href=\"#举一个n-4的例子\" class=\"headerlink\" title=\"举一个n&#x3D;4的例子\"></a>举一个n&#x3D;4的例子</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/22.png\"></p>\n<p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/23.png\"></p>\n<h4 id=\"上式右边第一部分-r-pi-表示：从状态s出发获得的即时奖励r的期望（平均值）\"><a href=\"#上式右边第一部分-r-pi-表示：从状态s出发获得的即时奖励r的期望（平均值）\" class=\"headerlink\" title=\"上式右边第一部分 r_pi 表示：从状态s出发获得的即时奖励r的期望（平均值）\"></a>上式右边第一部分 r_pi 表示：从状态s出发获得的即时奖励r的期望（平均值）</h4><h4 id=\"第二部分中前一个矩阵-P-pi-表示：从状态s-i转换为s-j的概率\"><a href=\"#第二部分中前一个矩阵-P-pi-表示：从状态s-i转换为s-j的概率\" class=\"headerlink\" title=\"第二部分中前一个矩阵 P_pi 表示：从状态s_i转换为s_j的概率\"></a>第二部分中前一个矩阵 P_pi 表示：从状态s_i转换为s_j的概率</h4><h4 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"\"></a></h4><h4 id=\"给定一个policy，可以列出一个与之对应的贝尔曼公式，通过对贝尔曼公式进行求解得到-state-value，这样的过程就称之为-policy-evaluation，通过评估一个策略的优劣，进而去改进策略，最终得到最优策略\"><a href=\"#给定一个policy，可以列出一个与之对应的贝尔曼公式，通过对贝尔曼公式进行求解得到-state-value，这样的过程就称之为-policy-evaluation，通过评估一个策略的优劣，进而去改进策略，最终得到最优策略\" class=\"headerlink\" title=\"给定一个policy，可以列出一个与之对应的贝尔曼公式，通过对贝尔曼公式进行求解得到 state value，这样的过程就称之为 policy evaluation，通过评估一个策略的优劣，进而去改进策略，最终得到最优策略\"></a>给定一个policy，可以列出一个与之对应的贝尔曼公式，通过对贝尔曼公式进行求解得到 state value，这样的过程就称之为 policy evaluation，通过评估一个策略的优劣，进而去改进策略，最终得到最优策略</h4><h4 id=\"求解贝尔曼方程的常用方法（求逆的方法通常不推荐，因为当矩阵维数很大时，求逆的计算量很大）\"><a href=\"#求解贝尔曼方程的常用方法（求逆的方法通常不推荐，因为当矩阵维数很大时，求逆的计算量很大）\" class=\"headerlink\" title=\"求解贝尔曼方程的常用方法（求逆的方法通常不推荐，因为当矩阵维数很大时，求逆的计算量很大）\"></a>求解贝尔曼方程的常用方法（求逆的方法通常不推荐，因为当矩阵维数很大时，求逆的计算量很大）</h4><blockquote>\n<h4 id=\"常用的是迭代法求解\"><a href=\"#常用的是迭代法求解\" class=\"headerlink\" title=\"常用的是迭代法求解\"></a>常用的是迭代法求解</h4><p>$$<br>v_{k+1} &#x3D; r_{\\pi} + \\gamma P_{\\pi} v_{k}<br>$$</p>\n<h4 id=\"从-v-0开始，可以先假定一个值，然后就可以求出v-1，通过v-1又求出v-2，以此类推，求得v-k，v-k-1，最后就会求出这样一个序列，v-0，v-1…v-k，当-k-趋近于无穷时，就有\"><a href=\"#从-v-0开始，可以先假定一个值，然后就可以求出v-1，通过v-1又求出v-2，以此类推，求得v-k，v-k-1，最后就会求出这样一个序列，v-0，v-1…v-k，当-k-趋近于无穷时，就有\" class=\"headerlink\" title=\"从 v_0开始，可以先假定一个值，然后就可以求出v_1，通过v_1又求出v_2，以此类推，求得v_k，v_k+1，最后就会求出这样一个序列，v_0，v_1…v_k，当 k 趋近于无穷时，就有\"></a>从 v_0开始，可以先假定一个值，然后就可以求出v_1，通过v_1又求出v_2，以此类推，求得v_k，v_k+1，最后就会求出这样一个序列，v_0，v_1…v_k，当 k 趋近于无穷时，就有</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/24.png\"></p>\n<h4 id=\"v-k收敛到了v-pi，证明如下\"><a href=\"#v-k收敛到了v-pi，证明如下\" class=\"headerlink\" title=\"v_k收敛到了v_pi，证明如下\"></a>v_k收敛到了v_pi，证明如下</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/25.png\"></p>\n<h4 id=\"其证明过程类似于李亚普诺夫函数的稳定性证明\"><a href=\"#其证明过程类似于李亚普诺夫函数的稳定性证明\" class=\"headerlink\" title=\"其证明过程类似于李亚普诺夫函数的稳定性证明\"></a>其证明过程类似于李亚普诺夫函数的稳定性证明</h4></blockquote>\n<h4 id=\"在下面这样一个例子中，说明了一些情况\"><a href=\"#在下面这样一个例子中，说明了一些情况\" class=\"headerlink\" title=\"在下面这样一个例子中，说明了一些情况\"></a>在下面这样一个例子中，说明了一些情况</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/26.png\"></p>\n<h4 id=\"可以发现，越靠近目标区域的状态的state-value越大，反之则越小\"><a href=\"#可以发现，越靠近目标区域的状态的state-value越大，反之则越小\" class=\"headerlink\" title=\"可以发现，越靠近目标区域的状态的state value越大，反之则越小\"></a>可以发现，越靠近目标区域的状态的state value越大，反之则越小</h4><h4 id=\"另外就是，不同的策略可能会得到相同的state-value\"><a href=\"#另外就是，不同的策略可能会得到相同的state-value\" class=\"headerlink\" title=\"另外就是，不同的策略可能会得到相同的state value\"></a>另外就是，不同的策略可能会得到相同的state value</h4><h4 id=\"通过计算state-value可以评估一个策略的优劣\"><a href=\"#通过计算state-value可以评估一个策略的优劣\" class=\"headerlink\" title=\"通过计算state value可以评估一个策略的优劣\"></a>通过计算state value可以评估一个策略的优劣</h4></blockquote>\n<h3 id=\"第六部分——Action-value\"><a href=\"#第六部分——Action-value\" class=\"headerlink\" title=\"第六部分——Action value\"></a>第六部分——Action value</h3><blockquote>\n<h4 id=\"state-value-和-Action-value-的联系与区别\"><a href=\"#state-value-和-Action-value-的联系与区别\" class=\"headerlink\" title=\"state value 和 Action value 的联系与区别\"></a>state value 和 Action value 的联系与区别</h4><blockquote>\n<h4 id=\"前者是agent从一个状态出发，所获得的return的平均值（期望）\"><a href=\"#前者是agent从一个状态出发，所获得的return的平均值（期望）\" class=\"headerlink\" title=\"前者是agent从一个状态出发，所获得的return的平均值（期望）\"></a>前者是agent从一个状态出发，所获得的return的平均值（期望）</h4><h4 id=\"后者是agent从一个状态出发并且选择了一个Action之后，所获得的returen的平均值（期望）\"><a href=\"#后者是agent从一个状态出发并且选择了一个Action之后，所获得的returen的平均值（期望）\" class=\"headerlink\" title=\"后者是agent从一个状态出发并且选择了一个Action之后，所获得的returen的平均值（期望）\"></a>后者是agent从一个状态出发并且选择了一个Action之后，所获得的returen的平均值（期望）</h4></blockquote>\n<h4 id=\"为什么要引入action-value，因为一个状态的转换是选择了一个Action之后引起的\"><a href=\"#为什么要引入action-value，因为一个状态的转换是选择了一个Action之后引起的\" class=\"headerlink\" title=\"为什么要引入action value，因为一个状态的转换是选择了一个Action之后引起的\"></a>为什么要引入action value，因为一个状态的转换是选择了一个Action之后引起的</h4><h4 id=\"定义\"><a href=\"#定义\" class=\"headerlink\" title=\"定义\"></a>定义</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/27.png\"></p>\n<h4 id=\"它依赖策略pi\"><a href=\"#它依赖策略pi\" class=\"headerlink\" title=\"它依赖策略pi\"></a>它依赖策略pi</h4><h4 id=\"它与state-value-数学式上的联系，state-value可以表示为下面这个形式\"><a href=\"#它与state-value-数学式上的联系，state-value可以表示为下面这个形式\" class=\"headerlink\" title=\"它与state value 数学式上的联系，state value可以表示为下面这个形式\"></a>它与state value 数学式上的联系，state value可以表示为下面这个形式</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/28.png\"></p>\n<h4 id=\"在一个状态下，可能有多种可选的动作，每个动作都有对应的概率，上式右边部分表示，在状态s下，选择动作a后的所获得的return的平均值-在状态s下选择动作a的概率\"><a href=\"#在一个状态下，可能有多种可选的动作，每个动作都有对应的概率，上式右边部分表示，在状态s下，选择动作a后的所获得的return的平均值-在状态s下选择动作a的概率\" class=\"headerlink\" title=\"在一个状态下，可能有多种可选的动作，每个动作都有对应的概率，上式右边部分表示，在状态s下，选择动作a后的所获得的return的平均值 * 在状态s下选择动作a的概率\"></a>在一个状态下，可能有多种可选的动作，每个动作都有对应的概率，上式右边部分表示，在状态s下，选择动作a后的所获得的return的平均值 * 在状态s下选择动作a的概率</h4><h4 id=\"注意到，右边的前面一部分就是-action-value的定义，所以二者数学式的联系如下\"><a href=\"#注意到，右边的前面一部分就是-action-value的定义，所以二者数学式的联系如下\" class=\"headerlink\" title=\"注意到，右边的前面一部分就是 action value的定义，所以二者数学式的联系如下\"></a>注意到，右边的前面一部分就是 action value的定义，所以二者数学式的联系如下</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/29.png\"></p>\n<h4 id=\"这个式子说明什么呢，说明了已知一个状态的-action-value，对其求平均就可以求出其对应的-state-value\"><a href=\"#这个式子说明什么呢，说明了已知一个状态的-action-value，对其求平均就可以求出其对应的-state-value\" class=\"headerlink\" title=\"这个式子说明什么呢，说明了已知一个状态的 action value，对其求平均就可以求出其对应的 state value\"></a>这个式子说明什么呢，说明了已知一个状态的 action value，对其求平均就可以求出其对应的 state value</h4><h4 id=\"另外呢，根据前面第五部分开头的表达式可以进一步给出-action-value的表达式\"><a href=\"#另外呢，根据前面第五部分开头的表达式可以进一步给出-action-value的表达式\" class=\"headerlink\" title=\"另外呢，根据前面第五部分开头的表达式可以进一步给出 action value的表达式\"></a>另外呢，根据前面第五部分开头的表达式可以进一步给出 action value的表达式</h4><p><img src=\"http://picbed.yanzu.tech/img/RL/math_theory/2th_part/30.png\"></p>\n<h4 id=\"而这个式子，它有说明了，已知一个状态的-state-value-可以求出其对应的-action-value\"><a href=\"#而这个式子，它有说明了，已知一个状态的-state-value-可以求出其对应的-action-value\" class=\"headerlink\" title=\"而这个式子，它有说明了，已知一个状态的 state value 可以求出其对应的 action value\"></a>而这个式子，它有说明了，已知一个状态的 state value 可以求出其对应的 action value</h4><h4 id=\"（没怎么懂，迷迷糊糊的！）\"><a href=\"#（没怎么懂，迷迷糊糊的！）\" class=\"headerlink\" title=\"（没怎么懂，迷迷糊糊的！）\"></a>（没怎么懂，迷迷糊糊的！）</h4></blockquote>\n"},{"title":"DL之路---啃鱼书（2）","data":"2025-06-11T08:05:00.000Z","updated":"2025-06-13T02:44:00.000Z","type":"DL","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p21.png","_content":"\n\n# 神经网络\n\n\n\n> #### 神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数\n>\n> \n>\n> #### 激活函数\n>\n> > #### 它会将输入信号的总和转换为输出信号，它决定了以何种方式激活输入信号的总和\n> >\n> > #### 朴素感知机和神经网络的主要区别就在于激活函数的不同\n> >\n> > #### 计算过程如下:\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/2/1.png)\n> >\n> > \n> >\n> > #### 阶跃函数：一旦输入超过阈值，就切换输出（sign符号函数，朴素感知机的激活函数）\n> >\n> > #### sigmoid函数\n> >\n> > > #### 神经网络中一个常用的激活函数就是sigmoid函数，其表达式如下：\n> > >\n> > > $$\n> > > h(x) = \\frac {1}{1 + exp(-x)}\n> > > $$\n> >\n> > \n> >\n> > #### ReLU函数\n> >\n> > #### 自变量 $ x > 0 $ 时，直接输出该值，$ x \\le 0 $  时，输出0\n> >\n> > #### 它的表达式如下：\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/2/11.png)\n> \n> \n> \n> #### 3层神经网络的实现\n> \n> > #### 首先引入一个概念符号确认\n>>\n> > #### 下面是权重符号的表示，\n>>\n> > ![](http://picbed.yanzu.tech/img/DL/2/5.png)\n>>\n> > #### 各层间信号传递的实现\n> >\n> > #### 首先是第0层到第1层之间信号传递的实现\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/2/6.png)\n> >\n> > #### 注意，偏置 $ b $ 的右下角的索引号只有一个，这是因为前一层的偏置神经元（神经元1）只有一个，右下角的索引是表示前一个神经元的，对应到权重的符号表示是 $ b^\\left[(1)\\right]_{None,1} $ ，因为本身a本身就在一个神经元上了，所以前一个索引是None\n> >\n> > #### $ a_1 $的表达式如下：\n> >\n> > $$\n> > a_1^{(1)} = w_{11}^{(1)} x_1 + w_{12}^{(1)} x_2 + b_1^{(1)}\n> > $$\n> >\n> > \n> >\n> > #### 使用矩阵的乘法运算表示第1层的加权和\n> >\n> > $$\n> > A^{(1)} = XW^{(1)} + B^{(1)}\n> > $$\n> >\n> > #### 其中，$ A^{(1)},X,B^{(1)},W^{(1)} $如下所示\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/2/12.png)\n> > \n> > #### $W_i$的每一列对应一个$ a_i $\n> \n> #### 第1层到第2层之间信号传递的实现\n> \n> ![](http://picbed.yanzu.tech/img/DL/2/7.png)\n> \n> #### 第2层到输出层之间信号传递的实现\n> \n> ![](http://picbed.yanzu.tech/img/DL/2/8.png)\n> \n> #### 输出层的激活函数一般用 $ \\sigma() $ 来表示，隐藏层的激活函数一般用 $ h() $ 来表示\n> \n> #### 另外，输出层所使用的激活函数要根据求解问题的性质来确定，一般情况下，回归问题可以使用恒等函数，二元分类问题可以使用 $sigmoid$ 函数，多元分类问题可以使用 $softmax$函数\n> \n> \n> \n> #### 输出层的设计\n> \n> > #### 机器学习的问题大致可以分为分类问题和回归问题,分类问题是数 据属于哪一个类别的问题；而回归问题是根据某个输入预测一个（连续的）数值的问题\n> >\n> > \n> >\n>> #### $softmax$函数（输出层的激活函数）\n> >\n>> > #### 分类问题中使用该函数，表达式如下：\n> > >\n>> > $$\n> > > \\sigma = y_k = \\frac {exp(a_k)} {\\sum^{n}_{i = 1} exp(a_i)}\n>> > $$\n> > >\n>> > #### 这个式子表示：假设输出层共有$n$个神经元，计算第$k$个神经元的输出$y_k$\n> > >\n>> > ![](http://picbed.yanzu.tech/img/DL/2/9.png)\n> > >\n>> > #### 可以看出，输出层的各个神经元都受到所有输入信号的影响\n> >\n>> \n> >\n>> #### 注意到，上面这种形式的$softmax$函数有一个致命问题，那就是指数部分容易发生溢出\n> >\n> > #### 为此，对其进行改进\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/2/10.png)\n> >\n> > #### 在进行$softmax$的指数的运算时，加上（或者减去）某个常数并不会改变运算的结果，这里的$C'$可以使用任何值，但是为了防止溢出，一般会使用输入信号中的最大值\n> >\n> > \n> >\n> > #### $softmax$函数的特征，其输出是0.0到1.0之间的实数，且输出值的总和是1，这是它的一个重要性质，正因如此，一般把$softmax$的输出解释为 概率\n> >\n> > #### 通过使用$softmax$函数，我们可以用概率的（统计的）方法处理问题\n> >\n> > #### 另外，即便使用了$softmax$函数，各个元素之间的大小关 系也不会改变，因为指数函数$ y = exp(x) $是单调递增函数\n> >\n> > \n> >\n> > #### 一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果，且即便使用$softmax$函数，输出值最大的神经元的位置也不会变，因此，神经网络在进行**分类**时，输出层的$softmax$函数可以省略（还有个原因是，指数函数的运算需要一定的计算机运算量）\n> >\n> > \n> >\n> > #### 求解机器学习问题的步骤可以分为学习和推理两个阶段，首先，在学习阶段进行模型的学习，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）\n> \n> \n> \n> #### 手写数字识别\n> \n> > #### one-hot表示是仅正确解标签为1，其余皆为0的数组\n> >\n> > #### pickle可以可以将程序运行中的对象保存为文件，如果加载保存过的pickle文件，可以立刻复原之前程序运行中的对象（Numpy数组），使用pickle.load()可以直接还原所有数据结构\n> >\n> > #### argmax(np.array, axis)函数可以取出数组中的最大值的索引，axis=1指定沿着行方向寻找最大值\n> >\n> > #### 正规化（normalization）：把数据限定到某个范围内的处理\n> >\n> > #### 预处理（pre-processing）：对神经网络的输入数据进行某种既定的转换\n> >\n> > #### 数据白化（whitening）：将数据整体的分布形状均匀化的方法\n> >\n> > #### 批处理\n\n### code\n\n> #### 阶跃函数的实现\n>\n> ```python\n> import numpy as np\n> def step_fun(x):\n>  y = x > 0\n>  return y.astype(np.int)\n> \n> # y = x > 0是什么意思\n> # 对numpy数组进行bool运算，它会对数组内的每个元素做bool运算，得到一个对应bool值的bool数组，不是浮点型数组了\n> x = np.array([-1.0, 1.0, 2.0])\n> print(x)\n> y = x > 0\n> print(y)\n> # astype()函数可以转换numpy数组的类型，需要对int指定精度，可以是32，也可以是64\n> y = y.astype(np.int32)\n> print(y)\n> ```\n>\n> #### 阶跃函数的图像\n>\n> ```python\n> import numpy as np\n> import matplotlib.pylab as plt\n> \n> def step_fun(x):\n>  return np.array(x > 0, dtype=np.int32)\n> \n> x = np.arange(-5.0, 5.0, 0.1)\n> y = step_fun(x)\n> plt.plot(x, y)\n> plt.ylim(-0.1, 1.1) # 指定y轴范围\n> plt.show()\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/DL/2/2.png)\n>\n> #### sigmoid函数的实现\n>\n> ```python\n> def sigmoid(x):\n>  return 1 / (1 + np.exp(-x))\n> \n> \n> x = np.array([-1.0, 1.0, 2.0])\n> print(sigmoid(x))\n> ```\n>\n> #### sigmoid函数图像\n>\n> ```python\n> x = np.arange(-5.0, 5.0, 0.1)\n> y = sigmoid(x)\n> plt.plot(x, y)\n> plt.ylim(-0.1, 1.1)\n> plt.show()\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/DL/2/3.png)\n>\n> #### 可以看到，阶跃函数和sigmoid函数的几个共同点：\n>\n> #### 一是，当输入信号为重要信息时，阶跃函数和sigmoid函数都会输出较大的值；当输入信号为不重要的信息时，两者都输出较小的值\n>\n> #### 另一个是，不管输入信号有多小，或者有多大，输出信号的值都在0到1之间\n>\n> #### 另外，两者均是非线性函数，不管是朴素感知机还是神经网络，其激活函数必须是非线性函数，不能是线性函数\n>\n> \n>\n> #### ReLU函数的实现\n>\n> ```python\n> def relu(x):\n>  return np.maximum(0, x)\n> \n> \n> x = np.array([-1.0, 1.0, 2.0])\n> print(relu(x))\n> ```\n>\n> #### ReLU函数的的图像\n>\n> ```python\n> x = np.arange(-6.0, 6.0, 1.0)\n> y = relu(x)\n> plt.plot(x, y)\n> plt.ylim(-1.0, 6.0)\n> plt.show()\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/DL/2/4.png)\n>\n> \n>\n> #### 多维数组的运算\n>\n> > ```python\n> > # 一维数组\n> > import numpy as np\n> > A = np.array([1, 2, 3, 4])\n> > print(A)\n> > print(np.ndim(A)) # 输出数组的维数\n> > print(np.shape(A)) # 输出数组的形状，其结果是一个元组\n> > print(A.shape) # 另外一种方式\n> > print(A.shape[0])\n> > \n> > # 二维数组，注意二维以及高维数组还需要一个额外的中括号括起来\n> > B = np.array([[1, 2], [3, 4], [5, 6]])\n> > print(B)\n> > print(np.ndim(B)) # 输出数组的维数\n> > print(np.shape(B)) # 输出数组的形状，其结果是一个元组\n> > print(B.shape) # 另外一种方式\n> > print(B.shape[1]) # shape[0]是数组的行数，shape[1]是数组的列数\n> > \n> > ```\n> >\n> > #### 矩阵乘法\n> >\n> > ```python\n> > # 2 * 2 的矩阵乘法\n> > A = np.array([[1, 2], [3, 4]])\n> > print(A.shape)\n> > B = np.array([[2, 3], [4, 5]])\n> > print(B.shape)\n> > print(np.dot(A, B)) # dot()接收两个numpy数组作为参数，返回数组的乘积\n> > \n> > # 2*3 和 3*2 的矩阵乘法\n> > A = np.array([[1, 2, 3], [4, 5, 6]])\n> > print(A.shape)\n> > B = np.array([[1, 2],[3, 4], [5, 6]])\n> > print(B.shape)\n> > print(np.dot(A, B)) # dot()接收两个numpy数组作为参数，返回数组的乘积\n> > print(A @ B)\n> > \n> > # 对于一维numpy数组的乘法运算，同样是借助了广播功能\n> > A = np.array([[1, 2],[3, 4], [5, 6]])\n> > print(A.shape)\n> > B = np.array([7, 8])\n> > print(B.shape)\n> > print(np.dot(A, B))\n> > ```\n> >\n> > #### 神经网络的内积\n> >\n> > ```python\n> > X = np.array([1, 2])\n> > print(X.shape)\n> > W = np.array([[1, 3, 5], [2, 4, 6]])\n> > print(W.shape)\n> > Y = np.dot(X, W)\n> > print(Y)\n> > ```\n>\n> \n>\n> #### 第0层到第1层之间信号传递的实现\n>\n> ```python\n> X = np.array([1.0, 0.5])\n> W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n> B1 = np.array([0.1, 0.2, 0.3])\n> print(W1.shape)\n> print(X.shape)\n> print(B1.shape)\n> A1 = np.dot(X, W1) + B1\n> print(A1)\n> Z1 = sigmoid(A1)\n> print(Z1)\n> ```\n>\n> #### 第1层到第2层之间信号传递的实现\n>\n> ```python\n> W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n> B2 = np.array([0.1, 0.2])\n> print(Z1.shape)\n> print(W2.shape)\n> print(B2.shape)\n> # Z2 从第一层的输出变成了第二层的输入了，新的(x1, x2, x3)\n> A2 = np.dot(Z1, W2) + B2\n> print(A2)\n> Z2 = sigmoid(A2)\n> print(Z2)\n> ```\n>\n> #### 第2层到输出层之间信号传递的实现\n>\n> ```python\n> # 注意，这两层之间的激活函数与之前隐藏层的有所不同了\n> # 这里的激活函数是恒等函数，就是输入是什么输出就是什么\n> def identify_fun(x):\n>     return x\n> \n> W3 = np.array([[0.1, 0.3], [0.2, 0.4]])\n> B3 = np.array([0.1, 0.2])\n> print(W3.shape)\n> print(B3.shape)\n> \n> A3 = np.dot(Z2, W3) + B3\n> Y = identify_fun(A3)\n> print(A3.shape)\n> print(Y)\n> ```\n>\n> #### 写成函数形式，实现3层神经网络\n>\n> ```python\n> def init_network():\n>     network = {}\n>     network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n>     network['b1'] = np.array([0.1, 0.2, 0.3])\n>     network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n>     network['b2'] = np.array([0.1, 0.2])\n>     network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n>     network['b3'] = np.array([0.1, 0.2])\n>     \n>     return network\n> \n> \n> def forward(network, x):\n>     W1, W2, W3 = network['W1'], network['W2'], network['W3']\n>     b1, b2, b3 = network['b1'], network['b2'], network['b3']\n>     \n>     a1 = np.dot(x, W1) + b1\n>     z1 = sigmoid(a1)\n>     a2 = np.dot(z1, W2) + b2\n>     z2 = sigmoid(a2)\n>     a3 = np.dot(z2, W3) + b3\n>     y = identify_fun(a3)\n>     \n>     return y\n> \n> \n> network = init_network()\n> x = np.array([1.0, 0.5])\n> y = forward(network, x)\n> print(y)\n> ```\n>\n> \n>\n> #### $softmax$函数的实现\n>\n> ```python\n> def softmax(a):\n>     exp_a = np.exp(a)\n>     sum_exp_a = np.sum(exp_a)\n>     y = exp_a / sum_exp_a\n>     \n>     return y\n> \n> \n> a = np.array([0.3, 2.9, 4.0])\n> print(softmax(a))\n> ```\n>\n> #### 改进后的$softmax$的实现\n>\n> ```python\n> def softmax(a):\n>     c = np.max(a)\n>     exp_a = np.exp(a - c)\n>     sum_exp_a = np.sum(exp_a)\n>     y = exp_a / sum_exp_a\n>     \n>     return y\n> \n> \n> a = np.array([1010, 1000, 990])\n> print(softmax(a))\n> ```\n>\n> \n>\n> #### 手写数字识别\n>\n> ```python\n> import sys\n> sys.path.append('../../py_pro/DL/') # 指定获取mnist数据集的脚本路径\n> import numpy as np\n> from dataset.mnist import load_mnist\n> # (训练图像，训练标签)，(测试图像，测试标签)\n> (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True)\n> \n> print(x_train.shape)\n> print(t_train.shape)\n> print(x_test.shape)\n> print(t_test.shape)\n> ```\n>\n> ```python\n> import os, sys\n> sys.path.append('../../py_pro/DL/') # 指定获取mnist数据集的脚本路径\n> import numpy as np\n> from dataset.mnist import load_mnist\n> from PIL import Image\n> \n> def img_show(img):\n>     pil_img = Image.fromarray(np.uint8(img))\n>     pil_img.show()\n> \n> \n> (x_train, t_train), (x_test, t_test) = load_mnist(normalize=False, flatten=True)\n> img = x_train[0]\n> label = t_train[0]\n> print(label)\n> \n> print(img.shape)\n> img = img.reshape(28, 28)\n> print(img.shape)\n> \n> img_show(img)\n> ```\n>\n> #### 神经网络的推理处理\n>\n> > #### 神经网络的输入层有784个神经元，输出层有10个神经元，784是图片像素是28*28，10是0~910种类别，有2个隐藏层，神经元数量分别是50、100，这个可以是任意值\n> >\n> > ```python\n> > import pickle\n> > def get_data():\n> >     (x_train, t_train), (x_test, t_test) = load_mnist(normalize=False, flatten=True)\n> >     return x_test, t_test\n> > \n> > \n> > def init_network():\n> >     with open('../../py_pro/DL/ch03/sample_weight.pkl', 'rb') as f:\n> >         network = pickle.load(f)\n> >     return network\n> > \n> > \n> > def predict(network, x):\n> >     W1, W2, W3 = network['W1'], network['W2'], network['W3']\n> >     b1, b2, b3 = network['b1'], network['b2'], network['b3']\n> >     a1 = np.dot(x, W1) + b1\n> >     z1 = sigmoid(a1)\n> >     a2 = np.dot(z1, W2) + b2\n> >     z2 = sigmoid(a2)\n> >     a3 = np.dot(z2, W3) + b3\n> >     y = softmax(a3)\n> > \n> >     return y\n> > \n> > x, t = get_data()\n> > network = init_network()\n> > \n> > accuracy_cnt = 0\n> > for i in range(len(x)):\n> >     y = predict(network, x[i])\n> >     p = np.argmax(y)\n> >     if p == t[i]:\n> >         accuracy_cnt += 1\n> > \n> > print('Accuracy: ' + str(float(accuracy_cnt) / len(x)))\n> > ```\n> >\n> > #### 批处理方式\n> >\n> > ```python\n> > x, t = get_data()\n> > network = init_network()\n> > \n> > batch_size = 100\n> > accuracy_cnt = 0\n> > \n> > for i in range(0, len(x), batch_size):\n> >     x_batch = x[i:i + batch_size]\n> >     y_batch = predict(network, x_batch)\n> >     p = np.argmax(y_batch, axis=1)\n> >     accuracy_cnt += np.sum(p == t[i:i+batch_size])\n> > \n> > \n> > print('Accuracy: ' + str(float(accuracy_cnt) / len(x)))\n> > ```\n> >\n> > \n>\n> \n>\n> ","source":"_posts/21.md","raw":"---\ntitle: DL之路---啃鱼书（2）\ndata: 2025-06-11 16:05:00\nupdated: 2025-06-13 10:44:00\ntype: DL\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p21.png\ntags:\n  - DL\n  - Learning\n  - gnaw_book\n---\n\n\n# 神经网络\n\n\n\n> #### 神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数\n>\n> \n>\n> #### 激活函数\n>\n> > #### 它会将输入信号的总和转换为输出信号，它决定了以何种方式激活输入信号的总和\n> >\n> > #### 朴素感知机和神经网络的主要区别就在于激活函数的不同\n> >\n> > #### 计算过程如下:\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/2/1.png)\n> >\n> > \n> >\n> > #### 阶跃函数：一旦输入超过阈值，就切换输出（sign符号函数，朴素感知机的激活函数）\n> >\n> > #### sigmoid函数\n> >\n> > > #### 神经网络中一个常用的激活函数就是sigmoid函数，其表达式如下：\n> > >\n> > > $$\n> > > h(x) = \\frac {1}{1 + exp(-x)}\n> > > $$\n> >\n> > \n> >\n> > #### ReLU函数\n> >\n> > #### 自变量 $ x > 0 $ 时，直接输出该值，$ x \\le 0 $  时，输出0\n> >\n> > #### 它的表达式如下：\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/2/11.png)\n> \n> \n> \n> #### 3层神经网络的实现\n> \n> > #### 首先引入一个概念符号确认\n>>\n> > #### 下面是权重符号的表示，\n>>\n> > ![](http://picbed.yanzu.tech/img/DL/2/5.png)\n>>\n> > #### 各层间信号传递的实现\n> >\n> > #### 首先是第0层到第1层之间信号传递的实现\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/2/6.png)\n> >\n> > #### 注意，偏置 $ b $ 的右下角的索引号只有一个，这是因为前一层的偏置神经元（神经元1）只有一个，右下角的索引是表示前一个神经元的，对应到权重的符号表示是 $ b^\\left[(1)\\right]_{None,1} $ ，因为本身a本身就在一个神经元上了，所以前一个索引是None\n> >\n> > #### $ a_1 $的表达式如下：\n> >\n> > $$\n> > a_1^{(1)} = w_{11}^{(1)} x_1 + w_{12}^{(1)} x_2 + b_1^{(1)}\n> > $$\n> >\n> > \n> >\n> > #### 使用矩阵的乘法运算表示第1层的加权和\n> >\n> > $$\n> > A^{(1)} = XW^{(1)} + B^{(1)}\n> > $$\n> >\n> > #### 其中，$ A^{(1)},X,B^{(1)},W^{(1)} $如下所示\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/2/12.png)\n> > \n> > #### $W_i$的每一列对应一个$ a_i $\n> \n> #### 第1层到第2层之间信号传递的实现\n> \n> ![](http://picbed.yanzu.tech/img/DL/2/7.png)\n> \n> #### 第2层到输出层之间信号传递的实现\n> \n> ![](http://picbed.yanzu.tech/img/DL/2/8.png)\n> \n> #### 输出层的激活函数一般用 $ \\sigma() $ 来表示，隐藏层的激活函数一般用 $ h() $ 来表示\n> \n> #### 另外，输出层所使用的激活函数要根据求解问题的性质来确定，一般情况下，回归问题可以使用恒等函数，二元分类问题可以使用 $sigmoid$ 函数，多元分类问题可以使用 $softmax$函数\n> \n> \n> \n> #### 输出层的设计\n> \n> > #### 机器学习的问题大致可以分为分类问题和回归问题,分类问题是数 据属于哪一个类别的问题；而回归问题是根据某个输入预测一个（连续的）数值的问题\n> >\n> > \n> >\n>> #### $softmax$函数（输出层的激活函数）\n> >\n>> > #### 分类问题中使用该函数，表达式如下：\n> > >\n>> > $$\n> > > \\sigma = y_k = \\frac {exp(a_k)} {\\sum^{n}_{i = 1} exp(a_i)}\n>> > $$\n> > >\n>> > #### 这个式子表示：假设输出层共有$n$个神经元，计算第$k$个神经元的输出$y_k$\n> > >\n>> > ![](http://picbed.yanzu.tech/img/DL/2/9.png)\n> > >\n>> > #### 可以看出，输出层的各个神经元都受到所有输入信号的影响\n> >\n>> \n> >\n>> #### 注意到，上面这种形式的$softmax$函数有一个致命问题，那就是指数部分容易发生溢出\n> >\n> > #### 为此，对其进行改进\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/2/10.png)\n> >\n> > #### 在进行$softmax$的指数的运算时，加上（或者减去）某个常数并不会改变运算的结果，这里的$C'$可以使用任何值，但是为了防止溢出，一般会使用输入信号中的最大值\n> >\n> > \n> >\n> > #### $softmax$函数的特征，其输出是0.0到1.0之间的实数，且输出值的总和是1，这是它的一个重要性质，正因如此，一般把$softmax$的输出解释为 概率\n> >\n> > #### 通过使用$softmax$函数，我们可以用概率的（统计的）方法处理问题\n> >\n> > #### 另外，即便使用了$softmax$函数，各个元素之间的大小关 系也不会改变，因为指数函数$ y = exp(x) $是单调递增函数\n> >\n> > \n> >\n> > #### 一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果，且即便使用$softmax$函数，输出值最大的神经元的位置也不会变，因此，神经网络在进行**分类**时，输出层的$softmax$函数可以省略（还有个原因是，指数函数的运算需要一定的计算机运算量）\n> >\n> > \n> >\n> > #### 求解机器学习问题的步骤可以分为学习和推理两个阶段，首先，在学习阶段进行模型的学习，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）\n> \n> \n> \n> #### 手写数字识别\n> \n> > #### one-hot表示是仅正确解标签为1，其余皆为0的数组\n> >\n> > #### pickle可以可以将程序运行中的对象保存为文件，如果加载保存过的pickle文件，可以立刻复原之前程序运行中的对象（Numpy数组），使用pickle.load()可以直接还原所有数据结构\n> >\n> > #### argmax(np.array, axis)函数可以取出数组中的最大值的索引，axis=1指定沿着行方向寻找最大值\n> >\n> > #### 正规化（normalization）：把数据限定到某个范围内的处理\n> >\n> > #### 预处理（pre-processing）：对神经网络的输入数据进行某种既定的转换\n> >\n> > #### 数据白化（whitening）：将数据整体的分布形状均匀化的方法\n> >\n> > #### 批处理\n\n### code\n\n> #### 阶跃函数的实现\n>\n> ```python\n> import numpy as np\n> def step_fun(x):\n>  y = x > 0\n>  return y.astype(np.int)\n> \n> # y = x > 0是什么意思\n> # 对numpy数组进行bool运算，它会对数组内的每个元素做bool运算，得到一个对应bool值的bool数组，不是浮点型数组了\n> x = np.array([-1.0, 1.0, 2.0])\n> print(x)\n> y = x > 0\n> print(y)\n> # astype()函数可以转换numpy数组的类型，需要对int指定精度，可以是32，也可以是64\n> y = y.astype(np.int32)\n> print(y)\n> ```\n>\n> #### 阶跃函数的图像\n>\n> ```python\n> import numpy as np\n> import matplotlib.pylab as plt\n> \n> def step_fun(x):\n>  return np.array(x > 0, dtype=np.int32)\n> \n> x = np.arange(-5.0, 5.0, 0.1)\n> y = step_fun(x)\n> plt.plot(x, y)\n> plt.ylim(-0.1, 1.1) # 指定y轴范围\n> plt.show()\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/DL/2/2.png)\n>\n> #### sigmoid函数的实现\n>\n> ```python\n> def sigmoid(x):\n>  return 1 / (1 + np.exp(-x))\n> \n> \n> x = np.array([-1.0, 1.0, 2.0])\n> print(sigmoid(x))\n> ```\n>\n> #### sigmoid函数图像\n>\n> ```python\n> x = np.arange(-5.0, 5.0, 0.1)\n> y = sigmoid(x)\n> plt.plot(x, y)\n> plt.ylim(-0.1, 1.1)\n> plt.show()\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/DL/2/3.png)\n>\n> #### 可以看到，阶跃函数和sigmoid函数的几个共同点：\n>\n> #### 一是，当输入信号为重要信息时，阶跃函数和sigmoid函数都会输出较大的值；当输入信号为不重要的信息时，两者都输出较小的值\n>\n> #### 另一个是，不管输入信号有多小，或者有多大，输出信号的值都在0到1之间\n>\n> #### 另外，两者均是非线性函数，不管是朴素感知机还是神经网络，其激活函数必须是非线性函数，不能是线性函数\n>\n> \n>\n> #### ReLU函数的实现\n>\n> ```python\n> def relu(x):\n>  return np.maximum(0, x)\n> \n> \n> x = np.array([-1.0, 1.0, 2.0])\n> print(relu(x))\n> ```\n>\n> #### ReLU函数的的图像\n>\n> ```python\n> x = np.arange(-6.0, 6.0, 1.0)\n> y = relu(x)\n> plt.plot(x, y)\n> plt.ylim(-1.0, 6.0)\n> plt.show()\n> ```\n>\n> ![](http://picbed.yanzu.tech/img/DL/2/4.png)\n>\n> \n>\n> #### 多维数组的运算\n>\n> > ```python\n> > # 一维数组\n> > import numpy as np\n> > A = np.array([1, 2, 3, 4])\n> > print(A)\n> > print(np.ndim(A)) # 输出数组的维数\n> > print(np.shape(A)) # 输出数组的形状，其结果是一个元组\n> > print(A.shape) # 另外一种方式\n> > print(A.shape[0])\n> > \n> > # 二维数组，注意二维以及高维数组还需要一个额外的中括号括起来\n> > B = np.array([[1, 2], [3, 4], [5, 6]])\n> > print(B)\n> > print(np.ndim(B)) # 输出数组的维数\n> > print(np.shape(B)) # 输出数组的形状，其结果是一个元组\n> > print(B.shape) # 另外一种方式\n> > print(B.shape[1]) # shape[0]是数组的行数，shape[1]是数组的列数\n> > \n> > ```\n> >\n> > #### 矩阵乘法\n> >\n> > ```python\n> > # 2 * 2 的矩阵乘法\n> > A = np.array([[1, 2], [3, 4]])\n> > print(A.shape)\n> > B = np.array([[2, 3], [4, 5]])\n> > print(B.shape)\n> > print(np.dot(A, B)) # dot()接收两个numpy数组作为参数，返回数组的乘积\n> > \n> > # 2*3 和 3*2 的矩阵乘法\n> > A = np.array([[1, 2, 3], [4, 5, 6]])\n> > print(A.shape)\n> > B = np.array([[1, 2],[3, 4], [5, 6]])\n> > print(B.shape)\n> > print(np.dot(A, B)) # dot()接收两个numpy数组作为参数，返回数组的乘积\n> > print(A @ B)\n> > \n> > # 对于一维numpy数组的乘法运算，同样是借助了广播功能\n> > A = np.array([[1, 2],[3, 4], [5, 6]])\n> > print(A.shape)\n> > B = np.array([7, 8])\n> > print(B.shape)\n> > print(np.dot(A, B))\n> > ```\n> >\n> > #### 神经网络的内积\n> >\n> > ```python\n> > X = np.array([1, 2])\n> > print(X.shape)\n> > W = np.array([[1, 3, 5], [2, 4, 6]])\n> > print(W.shape)\n> > Y = np.dot(X, W)\n> > print(Y)\n> > ```\n>\n> \n>\n> #### 第0层到第1层之间信号传递的实现\n>\n> ```python\n> X = np.array([1.0, 0.5])\n> W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n> B1 = np.array([0.1, 0.2, 0.3])\n> print(W1.shape)\n> print(X.shape)\n> print(B1.shape)\n> A1 = np.dot(X, W1) + B1\n> print(A1)\n> Z1 = sigmoid(A1)\n> print(Z1)\n> ```\n>\n> #### 第1层到第2层之间信号传递的实现\n>\n> ```python\n> W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n> B2 = np.array([0.1, 0.2])\n> print(Z1.shape)\n> print(W2.shape)\n> print(B2.shape)\n> # Z2 从第一层的输出变成了第二层的输入了，新的(x1, x2, x3)\n> A2 = np.dot(Z1, W2) + B2\n> print(A2)\n> Z2 = sigmoid(A2)\n> print(Z2)\n> ```\n>\n> #### 第2层到输出层之间信号传递的实现\n>\n> ```python\n> # 注意，这两层之间的激活函数与之前隐藏层的有所不同了\n> # 这里的激活函数是恒等函数，就是输入是什么输出就是什么\n> def identify_fun(x):\n>     return x\n> \n> W3 = np.array([[0.1, 0.3], [0.2, 0.4]])\n> B3 = np.array([0.1, 0.2])\n> print(W3.shape)\n> print(B3.shape)\n> \n> A3 = np.dot(Z2, W3) + B3\n> Y = identify_fun(A3)\n> print(A3.shape)\n> print(Y)\n> ```\n>\n> #### 写成函数形式，实现3层神经网络\n>\n> ```python\n> def init_network():\n>     network = {}\n>     network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n>     network['b1'] = np.array([0.1, 0.2, 0.3])\n>     network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n>     network['b2'] = np.array([0.1, 0.2])\n>     network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n>     network['b3'] = np.array([0.1, 0.2])\n>     \n>     return network\n> \n> \n> def forward(network, x):\n>     W1, W2, W3 = network['W1'], network['W2'], network['W3']\n>     b1, b2, b3 = network['b1'], network['b2'], network['b3']\n>     \n>     a1 = np.dot(x, W1) + b1\n>     z1 = sigmoid(a1)\n>     a2 = np.dot(z1, W2) + b2\n>     z2 = sigmoid(a2)\n>     a3 = np.dot(z2, W3) + b3\n>     y = identify_fun(a3)\n>     \n>     return y\n> \n> \n> network = init_network()\n> x = np.array([1.0, 0.5])\n> y = forward(network, x)\n> print(y)\n> ```\n>\n> \n>\n> #### $softmax$函数的实现\n>\n> ```python\n> def softmax(a):\n>     exp_a = np.exp(a)\n>     sum_exp_a = np.sum(exp_a)\n>     y = exp_a / sum_exp_a\n>     \n>     return y\n> \n> \n> a = np.array([0.3, 2.9, 4.0])\n> print(softmax(a))\n> ```\n>\n> #### 改进后的$softmax$的实现\n>\n> ```python\n> def softmax(a):\n>     c = np.max(a)\n>     exp_a = np.exp(a - c)\n>     sum_exp_a = np.sum(exp_a)\n>     y = exp_a / sum_exp_a\n>     \n>     return y\n> \n> \n> a = np.array([1010, 1000, 990])\n> print(softmax(a))\n> ```\n>\n> \n>\n> #### 手写数字识别\n>\n> ```python\n> import sys\n> sys.path.append('../../py_pro/DL/') # 指定获取mnist数据集的脚本路径\n> import numpy as np\n> from dataset.mnist import load_mnist\n> # (训练图像，训练标签)，(测试图像，测试标签)\n> (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True)\n> \n> print(x_train.shape)\n> print(t_train.shape)\n> print(x_test.shape)\n> print(t_test.shape)\n> ```\n>\n> ```python\n> import os, sys\n> sys.path.append('../../py_pro/DL/') # 指定获取mnist数据集的脚本路径\n> import numpy as np\n> from dataset.mnist import load_mnist\n> from PIL import Image\n> \n> def img_show(img):\n>     pil_img = Image.fromarray(np.uint8(img))\n>     pil_img.show()\n> \n> \n> (x_train, t_train), (x_test, t_test) = load_mnist(normalize=False, flatten=True)\n> img = x_train[0]\n> label = t_train[0]\n> print(label)\n> \n> print(img.shape)\n> img = img.reshape(28, 28)\n> print(img.shape)\n> \n> img_show(img)\n> ```\n>\n> #### 神经网络的推理处理\n>\n> > #### 神经网络的输入层有784个神经元，输出层有10个神经元，784是图片像素是28*28，10是0~910种类别，有2个隐藏层，神经元数量分别是50、100，这个可以是任意值\n> >\n> > ```python\n> > import pickle\n> > def get_data():\n> >     (x_train, t_train), (x_test, t_test) = load_mnist(normalize=False, flatten=True)\n> >     return x_test, t_test\n> > \n> > \n> > def init_network():\n> >     with open('../../py_pro/DL/ch03/sample_weight.pkl', 'rb') as f:\n> >         network = pickle.load(f)\n> >     return network\n> > \n> > \n> > def predict(network, x):\n> >     W1, W2, W3 = network['W1'], network['W2'], network['W3']\n> >     b1, b2, b3 = network['b1'], network['b2'], network['b3']\n> >     a1 = np.dot(x, W1) + b1\n> >     z1 = sigmoid(a1)\n> >     a2 = np.dot(z1, W2) + b2\n> >     z2 = sigmoid(a2)\n> >     a3 = np.dot(z2, W3) + b3\n> >     y = softmax(a3)\n> > \n> >     return y\n> > \n> > x, t = get_data()\n> > network = init_network()\n> > \n> > accuracy_cnt = 0\n> > for i in range(len(x)):\n> >     y = predict(network, x[i])\n> >     p = np.argmax(y)\n> >     if p == t[i]:\n> >         accuracy_cnt += 1\n> > \n> > print('Accuracy: ' + str(float(accuracy_cnt) / len(x)))\n> > ```\n> >\n> > #### 批处理方式\n> >\n> > ```python\n> > x, t = get_data()\n> > network = init_network()\n> > \n> > batch_size = 100\n> > accuracy_cnt = 0\n> > \n> > for i in range(0, len(x), batch_size):\n> >     x_batch = x[i:i + batch_size]\n> >     y_batch = predict(network, x_batch)\n> >     p = np.argmax(y_batch, axis=1)\n> >     accuracy_cnt += np.sum(p == t[i:i+batch_size])\n> > \n> > \n> > print('Accuracy: ' + str(float(accuracy_cnt) / len(x)))\n> > ```\n> >\n> > \n>\n> \n>\n> ","slug":"21","published":1,"date":"2025-06-11T08:37:50.735Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7cr5001hiku44y253u1z","content":"<h1 id=\"神经网络\"><a href=\"#神经网络\" class=\"headerlink\" title=\"神经网络\"></a>神经网络</h1><blockquote>\n<h4 id=\"神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数\"><a href=\"#神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数\" class=\"headerlink\" title=\"神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数\"></a>神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数</h4><h4 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h4><blockquote>\n<h4 id=\"它会将输入信号的总和转换为输出信号，它决定了以何种方式激活输入信号的总和\"><a href=\"#它会将输入信号的总和转换为输出信号，它决定了以何种方式激活输入信号的总和\" class=\"headerlink\" title=\"它会将输入信号的总和转换为输出信号，它决定了以何种方式激活输入信号的总和\"></a>它会将输入信号的总和转换为输出信号，它决定了以何种方式激活输入信号的总和</h4><h4 id=\"朴素感知机和神经网络的主要区别就在于激活函数的不同\"><a href=\"#朴素感知机和神经网络的主要区别就在于激活函数的不同\" class=\"headerlink\" title=\"朴素感知机和神经网络的主要区别就在于激活函数的不同\"></a>朴素感知机和神经网络的主要区别就在于激活函数的不同</h4><h4 id=\"计算过程如下\"><a href=\"#计算过程如下\" class=\"headerlink\" title=\"计算过程如下:\"></a>计算过程如下:</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/2/1.png\"></p>\n<h4 id=\"阶跃函数：一旦输入超过阈值，就切换输出（sign符号函数，朴素感知机的激活函数）\"><a href=\"#阶跃函数：一旦输入超过阈值，就切换输出（sign符号函数，朴素感知机的激活函数）\" class=\"headerlink\" title=\"阶跃函数：一旦输入超过阈值，就切换输出（sign符号函数，朴素感知机的激活函数）\"></a>阶跃函数：一旦输入超过阈值，就切换输出（sign符号函数，朴素感知机的激活函数）</h4><h4 id=\"sigmoid函数\"><a href=\"#sigmoid函数\" class=\"headerlink\" title=\"sigmoid函数\"></a>sigmoid函数</h4><blockquote>\n<h4 id=\"神经网络中一个常用的激活函数就是sigmoid函数，其表达式如下：\"><a href=\"#神经网络中一个常用的激活函数就是sigmoid函数，其表达式如下：\" class=\"headerlink\" title=\"神经网络中一个常用的激活函数就是sigmoid函数，其表达式如下：\"></a>神经网络中一个常用的激活函数就是sigmoid函数，其表达式如下：</h4><p>$$<br>h(x) &#x3D; \\frac {1}{1 + exp(-x)}<br>$$</p>\n</blockquote>\n<h4 id=\"ReLU函数\"><a href=\"#ReLU函数\" class=\"headerlink\" title=\"ReLU函数\"></a>ReLU函数</h4><h4 id=\"自变量-x-0-时，直接输出该值，-x-le-0-时，输出0\"><a href=\"#自变量-x-0-时，直接输出该值，-x-le-0-时，输出0\" class=\"headerlink\" title=\"自变量 $ x &gt; 0 $ 时，直接输出该值，$ x \\le 0 $  时，输出0\"></a>自变量 $ x &gt; 0 $ 时，直接输出该值，$ x \\le 0 $  时，输出0</h4><h4 id=\"它的表达式如下：\"><a href=\"#它的表达式如下：\" class=\"headerlink\" title=\"它的表达式如下：\"></a>它的表达式如下：</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/2/11.png\"></p>\n</blockquote>\n<h4 id=\"3层神经网络的实现\"><a href=\"#3层神经网络的实现\" class=\"headerlink\" title=\"3层神经网络的实现\"></a>3层神经网络的实现</h4><blockquote>\n<h4 id=\"首先引入一个概念符号确认\"><a href=\"#首先引入一个概念符号确认\" class=\"headerlink\" title=\"首先引入一个概念符号确认\"></a>首先引入一个概念符号确认</h4><h4 id=\"下面是权重符号的表示，\"><a href=\"#下面是权重符号的表示，\" class=\"headerlink\" title=\"下面是权重符号的表示，\"></a>下面是权重符号的表示，</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/2/5.png\"></p>\n<h4 id=\"各层间信号传递的实现\"><a href=\"#各层间信号传递的实现\" class=\"headerlink\" title=\"各层间信号传递的实现\"></a>各层间信号传递的实现</h4><h4 id=\"首先是第0层到第1层之间信号传递的实现\"><a href=\"#首先是第0层到第1层之间信号传递的实现\" class=\"headerlink\" title=\"首先是第0层到第1层之间信号传递的实现\"></a>首先是第0层到第1层之间信号传递的实现</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/2/6.png\"></p>\n<h4 id=\"注意，偏置-b-的右下角的索引号只有一个，这是因为前一层的偏置神经元（神经元1）只有一个，右下角的索引是表示前一个神经元的，对应到权重的符号表示是-b-left-1-right-None-1-，因为本身a本身就在一个神经元上了，所以前一个索引是None\"><a href=\"#注意，偏置-b-的右下角的索引号只有一个，这是因为前一层的偏置神经元（神经元1）只有一个，右下角的索引是表示前一个神经元的，对应到权重的符号表示是-b-left-1-right-None-1-，因为本身a本身就在一个神经元上了，所以前一个索引是None\" class=\"headerlink\" title=\"注意，偏置 $ b $ 的右下角的索引号只有一个，这是因为前一层的偏置神经元（神经元1）只有一个，右下角的索引是表示前一个神经元的，对应到权重的符号表示是 $ b^\\left[(1)\\right]_{None,1} $ ，因为本身a本身就在一个神经元上了，所以前一个索引是None\"></a>注意，偏置 $ b $ 的右下角的索引号只有一个，这是因为前一层的偏置神经元（神经元1）只有一个，右下角的索引是表示前一个神经元的，对应到权重的符号表示是 $ b^\\left[(1)\\right]_{None,1} $ ，因为本身a本身就在一个神经元上了，所以前一个索引是None</h4><h4 id=\"a-1-的表达式如下：\"><a href=\"#a-1-的表达式如下：\" class=\"headerlink\" title=\"$ a_1 $的表达式如下：\"></a>$ a_1 $的表达式如下：</h4><p>$$<br>a_1^{(1)} &#x3D; w_{11}^{(1)} x_1 + w_{12}^{(1)} x_2 + b_1^{(1)}<br>$$</p>\n<h4 id=\"使用矩阵的乘法运算表示第1层的加权和\"><a href=\"#使用矩阵的乘法运算表示第1层的加权和\" class=\"headerlink\" title=\"使用矩阵的乘法运算表示第1层的加权和\"></a>使用矩阵的乘法运算表示第1层的加权和</h4><p>$$<br>A^{(1)} &#x3D; XW^{(1)} + B^{(1)}<br>$$</p>\n<h4 id=\"其中，-A-1-X-B-1-W-1-如下所示\"><a href=\"#其中，-A-1-X-B-1-W-1-如下所示\" class=\"headerlink\" title=\"其中，$ A^{(1)},X,B^{(1)},W^{(1)} $如下所示\"></a>其中，$ A^{(1)},X,B^{(1)},W^{(1)} $如下所示</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/2/12.png\"></p>\n<h4 id=\"W-i-的每一列对应一个-a-i\"><a href=\"#W-i-的每一列对应一个-a-i\" class=\"headerlink\" title=\"$W_i$的每一列对应一个$ a_i $\"></a>$W_i$的每一列对应一个$ a_i $</h4></blockquote>\n<h4 id=\"第1层到第2层之间信号传递的实现\"><a href=\"#第1层到第2层之间信号传递的实现\" class=\"headerlink\" title=\"第1层到第2层之间信号传递的实现\"></a>第1层到第2层之间信号传递的实现</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/2/7.png\"></p>\n<h4 id=\"第2层到输出层之间信号传递的实现\"><a href=\"#第2层到输出层之间信号传递的实现\" class=\"headerlink\" title=\"第2层到输出层之间信号传递的实现\"></a>第2层到输出层之间信号传递的实现</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/2/8.png\"></p>\n<h4 id=\"输出层的激活函数一般用-sigma-来表示，隐藏层的激活函数一般用-h-来表示\"><a href=\"#输出层的激活函数一般用-sigma-来表示，隐藏层的激活函数一般用-h-来表示\" class=\"headerlink\" title=\"输出层的激活函数一般用 $ \\sigma() $ 来表示，隐藏层的激活函数一般用 $ h() $ 来表示\"></a>输出层的激活函数一般用 $ \\sigma() $ 来表示，隐藏层的激活函数一般用 $ h() $ 来表示</h4><h4 id=\"另外，输出层所使用的激活函数要根据求解问题的性质来确定，一般情况下，回归问题可以使用恒等函数，二元分类问题可以使用-sigmoid-函数，多元分类问题可以使用-softmax-函数\"><a href=\"#另外，输出层所使用的激活函数要根据求解问题的性质来确定，一般情况下，回归问题可以使用恒等函数，二元分类问题可以使用-sigmoid-函数，多元分类问题可以使用-softmax-函数\" class=\"headerlink\" title=\"另外，输出层所使用的激活函数要根据求解问题的性质来确定，一般情况下，回归问题可以使用恒等函数，二元分类问题可以使用 $sigmoid$ 函数，多元分类问题可以使用 $softmax$函数\"></a>另外，输出层所使用的激活函数要根据求解问题的性质来确定，一般情况下，回归问题可以使用恒等函数，二元分类问题可以使用 $sigmoid$ 函数，多元分类问题可以使用 $softmax$函数</h4><h4 id=\"输出层的设计\"><a href=\"#输出层的设计\" class=\"headerlink\" title=\"输出层的设计\"></a>输出层的设计</h4><blockquote>\n<h4 id=\"机器学习的问题大致可以分为分类问题和回归问题-分类问题是数-据属于哪一个类别的问题；而回归问题是根据某个输入预测一个（连续的）数值的问题\"><a href=\"#机器学习的问题大致可以分为分类问题和回归问题-分类问题是数-据属于哪一个类别的问题；而回归问题是根据某个输入预测一个（连续的）数值的问题\" class=\"headerlink\" title=\"机器学习的问题大致可以分为分类问题和回归问题,分类问题是数 据属于哪一个类别的问题；而回归问题是根据某个输入预测一个（连续的）数值的问题\"></a>机器学习的问题大致可以分为分类问题和回归问题,分类问题是数 据属于哪一个类别的问题；而回归问题是根据某个输入预测一个（连续的）数值的问题</h4><h4 id=\"softmax-函数（输出层的激活函数）\"><a href=\"#softmax-函数（输出层的激活函数）\" class=\"headerlink\" title=\"$softmax$函数（输出层的激活函数）\"></a>$softmax$函数（输出层的激活函数）</h4><blockquote>\n<h4 id=\"分类问题中使用该函数，表达式如下：\"><a href=\"#分类问题中使用该函数，表达式如下：\" class=\"headerlink\" title=\"分类问题中使用该函数，表达式如下：\"></a>分类问题中使用该函数，表达式如下：</h4><p>$$<br>\\sigma &#x3D; y_k &#x3D; \\frac {exp(a_k)} {\\sum^{n}_{i &#x3D; 1} exp(a_i)}<br>$$</p>\n<h4 id=\"这个式子表示：假设输出层共有-n-个神经元，计算第-k-个神经元的输出-y-k\"><a href=\"#这个式子表示：假设输出层共有-n-个神经元，计算第-k-个神经元的输出-y-k\" class=\"headerlink\" title=\"这个式子表示：假设输出层共有$n$个神经元，计算第$k$个神经元的输出$y_k$\"></a>这个式子表示：假设输出层共有$n$个神经元，计算第$k$个神经元的输出$y_k$</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/2/9.png\"></p>\n<h4 id=\"可以看出，输出层的各个神经元都受到所有输入信号的影响\"><a href=\"#可以看出，输出层的各个神经元都受到所有输入信号的影响\" class=\"headerlink\" title=\"可以看出，输出层的各个神经元都受到所有输入信号的影响\"></a>可以看出，输出层的各个神经元都受到所有输入信号的影响</h4></blockquote>\n<h4 id=\"注意到，上面这种形式的-softmax-函数有一个致命问题，那就是指数部分容易发生溢出\"><a href=\"#注意到，上面这种形式的-softmax-函数有一个致命问题，那就是指数部分容易发生溢出\" class=\"headerlink\" title=\"注意到，上面这种形式的$softmax$函数有一个致命问题，那就是指数部分容易发生溢出\"></a>注意到，上面这种形式的$softmax$函数有一个致命问题，那就是指数部分容易发生溢出</h4><h4 id=\"为此，对其进行改进\"><a href=\"#为此，对其进行改进\" class=\"headerlink\" title=\"为此，对其进行改进\"></a>为此，对其进行改进</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/2/10.png\"></p>\n<h4 id=\"在进行-softmax-的指数的运算时，加上（或者减去）某个常数并不会改变运算的结果，这里的-C’-可以使用任何值，但是为了防止溢出，一般会使用输入信号中的最大值\"><a href=\"#在进行-softmax-的指数的运算时，加上（或者减去）某个常数并不会改变运算的结果，这里的-C’-可以使用任何值，但是为了防止溢出，一般会使用输入信号中的最大值\" class=\"headerlink\" title=\"在进行$softmax$的指数的运算时，加上（或者减去）某个常数并不会改变运算的结果，这里的$C’$可以使用任何值，但是为了防止溢出，一般会使用输入信号中的最大值\"></a>在进行$softmax$的指数的运算时，加上（或者减去）某个常数并不会改变运算的结果，这里的$C’$可以使用任何值，但是为了防止溢出，一般会使用输入信号中的最大值</h4><h4 id=\"softmax-函数的特征，其输出是0-0到1-0之间的实数，且输出值的总和是1，这是它的一个重要性质，正因如此，一般把-softmax-的输出解释为-概率\"><a href=\"#softmax-函数的特征，其输出是0-0到1-0之间的实数，且输出值的总和是1，这是它的一个重要性质，正因如此，一般把-softmax-的输出解释为-概率\" class=\"headerlink\" title=\"$softmax$函数的特征，其输出是0.0到1.0之间的实数，且输出值的总和是1，这是它的一个重要性质，正因如此，一般把$softmax$的输出解释为 概率\"></a>$softmax$函数的特征，其输出是0.0到1.0之间的实数，且输出值的总和是1，这是它的一个重要性质，正因如此，一般把$softmax$的输出解释为 概率</h4><h4 id=\"通过使用-softmax-函数，我们可以用概率的（统计的）方法处理问题\"><a href=\"#通过使用-softmax-函数，我们可以用概率的（统计的）方法处理问题\" class=\"headerlink\" title=\"通过使用$softmax$函数，我们可以用概率的（统计的）方法处理问题\"></a>通过使用$softmax$函数，我们可以用概率的（统计的）方法处理问题</h4><h4 id=\"另外，即便使用了-softmax-函数，各个元素之间的大小关-系也不会改变，因为指数函数-y-exp-x-是单调递增函数\"><a href=\"#另外，即便使用了-softmax-函数，各个元素之间的大小关-系也不会改变，因为指数函数-y-exp-x-是单调递增函数\" class=\"headerlink\" title=\"另外，即便使用了$softmax$函数，各个元素之间的大小关 系也不会改变，因为指数函数$ y &#x3D; exp(x) $是单调递增函数\"></a>另外，即便使用了$softmax$函数，各个元素之间的大小关 系也不会改变，因为指数函数$ y &#x3D; exp(x) $是单调递增函数</h4><h4 id=\"一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果，且即便使用-softmax-函数，输出值最大的神经元的位置也不会变，因此，神经网络在进行分类时，输出层的-softmax-函数可以省略（还有个原因是，指数函数的运算需要一定的计算机运算量）\"><a href=\"#一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果，且即便使用-softmax-函数，输出值最大的神经元的位置也不会变，因此，神经网络在进行分类时，输出层的-softmax-函数可以省略（还有个原因是，指数函数的运算需要一定的计算机运算量）\" class=\"headerlink\" title=\"一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果，且即便使用$softmax$函数，输出值最大的神经元的位置也不会变，因此，神经网络在进行分类时，输出层的$softmax$函数可以省略（还有个原因是，指数函数的运算需要一定的计算机运算量）\"></a>一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果，且即便使用$softmax$函数，输出值最大的神经元的位置也不会变，因此，神经网络在进行<strong>分类</strong>时，输出层的$softmax$函数可以省略（还有个原因是，指数函数的运算需要一定的计算机运算量）</h4><h4 id=\"求解机器学习问题的步骤可以分为学习和推理两个阶段，首先，在学习阶段进行模型的学习，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）\"><a href=\"#求解机器学习问题的步骤可以分为学习和推理两个阶段，首先，在学习阶段进行模型的学习，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）\" class=\"headerlink\" title=\"求解机器学习问题的步骤可以分为学习和推理两个阶段，首先，在学习阶段进行模型的学习，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）\"></a>求解机器学习问题的步骤可以分为学习和推理两个阶段，首先，在学习阶段进行模型的学习，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）</h4></blockquote>\n<h4 id=\"手写数字识别\"><a href=\"#手写数字识别\" class=\"headerlink\" title=\"手写数字识别\"></a>手写数字识别</h4><blockquote>\n<h4 id=\"one-hot表示是仅正确解标签为1，其余皆为0的数组\"><a href=\"#one-hot表示是仅正确解标签为1，其余皆为0的数组\" class=\"headerlink\" title=\"one-hot表示是仅正确解标签为1，其余皆为0的数组\"></a>one-hot表示是仅正确解标签为1，其余皆为0的数组</h4><h4 id=\"pickle可以可以将程序运行中的对象保存为文件，如果加载保存过的pickle文件，可以立刻复原之前程序运行中的对象（Numpy数组），使用pickle-load-可以直接还原所有数据结构\"><a href=\"#pickle可以可以将程序运行中的对象保存为文件，如果加载保存过的pickle文件，可以立刻复原之前程序运行中的对象（Numpy数组），使用pickle-load-可以直接还原所有数据结构\" class=\"headerlink\" title=\"pickle可以可以将程序运行中的对象保存为文件，如果加载保存过的pickle文件，可以立刻复原之前程序运行中的对象（Numpy数组），使用pickle.load()可以直接还原所有数据结构\"></a>pickle可以可以将程序运行中的对象保存为文件，如果加载保存过的pickle文件，可以立刻复原之前程序运行中的对象（Numpy数组），使用pickle.load()可以直接还原所有数据结构</h4><h4 id=\"argmax-np-array-axis-函数可以取出数组中的最大值的索引，axis-1指定沿着行方向寻找最大值\"><a href=\"#argmax-np-array-axis-函数可以取出数组中的最大值的索引，axis-1指定沿着行方向寻找最大值\" class=\"headerlink\" title=\"argmax(np.array, axis)函数可以取出数组中的最大值的索引，axis&#x3D;1指定沿着行方向寻找最大值\"></a>argmax(np.array, axis)函数可以取出数组中的最大值的索引，axis&#x3D;1指定沿着行方向寻找最大值</h4><h4 id=\"正规化（normalization）：把数据限定到某个范围内的处理\"><a href=\"#正规化（normalization）：把数据限定到某个范围内的处理\" class=\"headerlink\" title=\"正规化（normalization）：把数据限定到某个范围内的处理\"></a>正规化（normalization）：把数据限定到某个范围内的处理</h4><h4 id=\"预处理（pre-processing）：对神经网络的输入数据进行某种既定的转换\"><a href=\"#预处理（pre-processing）：对神经网络的输入数据进行某种既定的转换\" class=\"headerlink\" title=\"预处理（pre-processing）：对神经网络的输入数据进行某种既定的转换\"></a>预处理（pre-processing）：对神经网络的输入数据进行某种既定的转换</h4><h4 id=\"数据白化（whitening）：将数据整体的分布形状均匀化的方法\"><a href=\"#数据白化（whitening）：将数据整体的分布形状均匀化的方法\" class=\"headerlink\" title=\"数据白化（whitening）：将数据整体的分布形状均匀化的方法\"></a>数据白化（whitening）：将数据整体的分布形状均匀化的方法</h4><h4 id=\"批处理\"><a href=\"#批处理\" class=\"headerlink\" title=\"批处理\"></a>批处理</h4></blockquote>\n</blockquote>\n<h3 id=\"code\"><a href=\"#code\" class=\"headerlink\" title=\"code\"></a>code</h3><blockquote>\n<h4 id=\"阶跃函数的实现\"><a href=\"#阶跃函数的实现\" class=\"headerlink\" title=\"阶跃函数的实现\"></a>阶跃函数的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">step_fun</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\"> y = x &gt; <span class=\"number\">0</span></span><br><span class=\"line\"> <span class=\"keyword\">return</span> y.astype(np.<span class=\"built_in\">int</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># y = x &gt; 0是什么意思</span></span><br><span class=\"line\"><span class=\"comment\"># 对numpy数组进行bool运算，它会对数组内的每个元素做bool运算，得到一个对应bool值的bool数组，不是浮点型数组了</span></span><br><span class=\"line\">x = np.array([-<span class=\"number\">1.0</span>, <span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)</span><br><span class=\"line\">y = x &gt; <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(y)</span><br><span class=\"line\"><span class=\"comment\"># astype()函数可以转换numpy数组的类型，需要对int指定精度，可以是32，也可以是64</span></span><br><span class=\"line\">y = y.astype(np.int32)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(y)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"阶跃函数的图像\"><a href=\"#阶跃函数的图像\" class=\"headerlink\" title=\"阶跃函数的图像\"></a>阶跃函数的图像</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pylab <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">step_fun</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\"> <span class=\"keyword\">return</span> np.array(x &gt; <span class=\"number\">0</span>, dtype=np.int32)</span><br><span class=\"line\"></span><br><span class=\"line\">x = np.arange(-<span class=\"number\">5.0</span>, <span class=\"number\">5.0</span>, <span class=\"number\">0.1</span>)</span><br><span class=\"line\">y = step_fun(x)</span><br><span class=\"line\">plt.plot(x, y)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">0.1</span>, <span class=\"number\">1.1</span>) <span class=\"comment\"># 指定y轴范围</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/DL/2/2.png\"></p>\n<h4 id=\"sigmoid函数的实现\"><a href=\"#sigmoid函数的实现\" class=\"headerlink\" title=\"sigmoid函数的实现\"></a>sigmoid函数的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sigmoid</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\"> <span class=\"keyword\">return</span> <span class=\"number\">1</span> / (<span class=\"number\">1</span> + np.exp(-x))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">x = np.array([-<span class=\"number\">1.0</span>, <span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(sigmoid(x))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"sigmoid函数图像\"><a href=\"#sigmoid函数图像\" class=\"headerlink\" title=\"sigmoid函数图像\"></a>sigmoid函数图像</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = np.arange(-<span class=\"number\">5.0</span>, <span class=\"number\">5.0</span>, <span class=\"number\">0.1</span>)</span><br><span class=\"line\">y = sigmoid(x)</span><br><span class=\"line\">plt.plot(x, y)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">0.1</span>, <span class=\"number\">1.1</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/DL/2/3.png\"></p>\n<h4 id=\"可以看到，阶跃函数和sigmoid函数的几个共同点：\"><a href=\"#可以看到，阶跃函数和sigmoid函数的几个共同点：\" class=\"headerlink\" title=\"可以看到，阶跃函数和sigmoid函数的几个共同点：\"></a>可以看到，阶跃函数和sigmoid函数的几个共同点：</h4><h4 id=\"一是，当输入信号为重要信息时，阶跃函数和sigmoid函数都会输出较大的值；当输入信号为不重要的信息时，两者都输出较小的值\"><a href=\"#一是，当输入信号为重要信息时，阶跃函数和sigmoid函数都会输出较大的值；当输入信号为不重要的信息时，两者都输出较小的值\" class=\"headerlink\" title=\"一是，当输入信号为重要信息时，阶跃函数和sigmoid函数都会输出较大的值；当输入信号为不重要的信息时，两者都输出较小的值\"></a>一是，当输入信号为重要信息时，阶跃函数和sigmoid函数都会输出较大的值；当输入信号为不重要的信息时，两者都输出较小的值</h4><h4 id=\"另一个是，不管输入信号有多小，或者有多大，输出信号的值都在0到1之间\"><a href=\"#另一个是，不管输入信号有多小，或者有多大，输出信号的值都在0到1之间\" class=\"headerlink\" title=\"另一个是，不管输入信号有多小，或者有多大，输出信号的值都在0到1之间\"></a>另一个是，不管输入信号有多小，或者有多大，输出信号的值都在0到1之间</h4><h4 id=\"另外，两者均是非线性函数，不管是朴素感知机还是神经网络，其激活函数必须是非线性函数，不能是线性函数\"><a href=\"#另外，两者均是非线性函数，不管是朴素感知机还是神经网络，其激活函数必须是非线性函数，不能是线性函数\" class=\"headerlink\" title=\"另外，两者均是非线性函数，不管是朴素感知机还是神经网络，其激活函数必须是非线性函数，不能是线性函数\"></a>另外，两者均是非线性函数，不管是朴素感知机还是神经网络，其激活函数必须是非线性函数，不能是线性函数</h4><h4 id=\"ReLU函数的实现\"><a href=\"#ReLU函数的实现\" class=\"headerlink\" title=\"ReLU函数的实现\"></a>ReLU函数的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">relu</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\"> <span class=\"keyword\">return</span> np.maximum(<span class=\"number\">0</span>, x)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">x = np.array([-<span class=\"number\">1.0</span>, <span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(relu(x))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"ReLU函数的的图像\"><a href=\"#ReLU函数的的图像\" class=\"headerlink\" title=\"ReLU函数的的图像\"></a>ReLU函数的的图像</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = np.arange(-<span class=\"number\">6.0</span>, <span class=\"number\">6.0</span>, <span class=\"number\">1.0</span>)</span><br><span class=\"line\">y = relu(x)</span><br><span class=\"line\">plt.plot(x, y)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">1.0</span>, <span class=\"number\">6.0</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/DL/2/4.png\"></p>\n<h4 id=\"多维数组的运算\"><a href=\"#多维数组的运算\" class=\"headerlink\" title=\"多维数组的运算\"></a>多维数组的运算</h4><blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 一维数组</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">A = np.array([<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(A)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(np.ndim(A)) <span class=\"comment\"># 输出数组的维数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(np.shape(A)) <span class=\"comment\"># 输出数组的形状，其结果是一个元组</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.shape) <span class=\"comment\"># 另外一种方式</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 二维数组，注意二维以及高维数组还需要一个额外的中括号括起来</span></span><br><span class=\"line\">B = np.array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>], [<span class=\"number\">3</span>, <span class=\"number\">4</span>], [<span class=\"number\">5</span>, <span class=\"number\">6</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(B)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(np.ndim(B)) <span class=\"comment\"># 输出数组的维数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(np.shape(B)) <span class=\"comment\"># 输出数组的形状，其结果是一个元组</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(B.shape) <span class=\"comment\"># 另外一种方式</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(B.shape[<span class=\"number\">1</span>]) <span class=\"comment\"># shape[0]是数组的行数，shape[1]是数组的列数</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"矩阵乘法\"><a href=\"#矩阵乘法\" class=\"headerlink\" title=\"矩阵乘法\"></a>矩阵乘法</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 2 * 2 的矩阵乘法</span></span><br><span class=\"line\">A = np.array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>], [<span class=\"number\">3</span>, <span class=\"number\">4</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.shape)</span><br><span class=\"line\">B = np.array([[<span class=\"number\">2</span>, <span class=\"number\">3</span>], [<span class=\"number\">4</span>, <span class=\"number\">5</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(B.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(np.dot(A, B)) <span class=\"comment\"># dot()接收两个numpy数组作为参数，返回数组的乘积</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2*3 和 3*2 的矩阵乘法</span></span><br><span class=\"line\">A = np.array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>], [<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.shape)</span><br><span class=\"line\">B = np.array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>],[<span class=\"number\">3</span>, <span class=\"number\">4</span>], [<span class=\"number\">5</span>, <span class=\"number\">6</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(B.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(np.dot(A, B)) <span class=\"comment\"># dot()接收两个numpy数组作为参数，返回数组的乘积</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A @ B)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 对于一维numpy数组的乘法运算，同样是借助了广播功能</span></span><br><span class=\"line\">A = np.array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>],[<span class=\"number\">3</span>, <span class=\"number\">4</span>], [<span class=\"number\">5</span>, <span class=\"number\">6</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.shape)</span><br><span class=\"line\">B = np.array([<span class=\"number\">7</span>, <span class=\"number\">8</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(B.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(np.dot(A, B))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"神经网络的内积\"><a href=\"#神经网络的内积\" class=\"headerlink\" title=\"神经网络的内积\"></a>神经网络的内积</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = np.array([<span class=\"number\">1</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(X.shape)</span><br><span class=\"line\">W = np.array([[<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">5</span>], [<span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(W.shape)</span><br><span class=\"line\">Y = np.dot(X, W)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y)</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h4 id=\"第0层到第1层之间信号传递的实现\"><a href=\"#第0层到第1层之间信号传递的实现\" class=\"headerlink\" title=\"第0层到第1层之间信号传递的实现\"></a>第0层到第1层之间信号传递的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = np.array([<span class=\"number\">1.0</span>, <span class=\"number\">0.5</span>])</span><br><span class=\"line\">W1 = np.array([[<span class=\"number\">0.1</span>, <span class=\"number\">0.3</span>, <span class=\"number\">0.5</span>], [<span class=\"number\">0.2</span>, <span class=\"number\">0.4</span>, <span class=\"number\">0.6</span>]])</span><br><span class=\"line\">B1 = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.3</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(W1.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(X.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(B1.shape)</span><br><span class=\"line\">A1 = np.dot(X, W1) + B1</span><br><span class=\"line\"><span class=\"built_in\">print</span>(A1)</span><br><span class=\"line\">Z1 = sigmoid(A1)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Z1)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"第1层到第2层之间信号传递的实现-1\"><a href=\"#第1层到第2层之间信号传递的实现-1\" class=\"headerlink\" title=\"第1层到第2层之间信号传递的实现\"></a>第1层到第2层之间信号传递的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">W2 = np.array([[<span class=\"number\">0.1</span>, <span class=\"number\">0.4</span>], [<span class=\"number\">0.2</span>, <span class=\"number\">0.5</span>], [<span class=\"number\">0.3</span>, <span class=\"number\">0.6</span>]])</span><br><span class=\"line\">B2 = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.2</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Z1.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(W2.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(B2.shape)</span><br><span class=\"line\"><span class=\"comment\"># Z2 从第一层的输出变成了第二层的输入了，新的(x1, x2, x3)</span></span><br><span class=\"line\">A2 = np.dot(Z1, W2) + B2</span><br><span class=\"line\"><span class=\"built_in\">print</span>(A2)</span><br><span class=\"line\">Z2 = sigmoid(A2)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Z2)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"第2层到输出层之间信号传递的实现-1\"><a href=\"#第2层到输出层之间信号传递的实现-1\" class=\"headerlink\" title=\"第2层到输出层之间信号传递的实现\"></a>第2层到输出层之间信号传递的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 注意，这两层之间的激活函数与之前隐藏层的有所不同了</span></span><br><span class=\"line\"><span class=\"comment\"># 这里的激活函数是恒等函数，就是输入是什么输出就是什么</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">identify_fun</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\">W3 = np.array([[<span class=\"number\">0.1</span>, <span class=\"number\">0.3</span>], [<span class=\"number\">0.2</span>, <span class=\"number\">0.4</span>]])</span><br><span class=\"line\">B3 = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.2</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(W3.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(B3.shape)</span><br><span class=\"line\"></span><br><span class=\"line\">A3 = np.dot(Z2, W3) + B3</span><br><span class=\"line\">Y = identify_fun(A3)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(A3.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"写成函数形式，实现3层神经网络\"><a href=\"#写成函数形式，实现3层神经网络\" class=\"headerlink\" title=\"写成函数形式，实现3层神经网络\"></a>写成函数形式，实现3层神经网络</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_network</span>():</span><br><span class=\"line\">    network = &#123;&#125;</span><br><span class=\"line\">    network[<span class=\"string\">&#x27;W1&#x27;</span>] = np.array([[<span class=\"number\">0.1</span>, <span class=\"number\">0.3</span>, <span class=\"number\">0.5</span>], [<span class=\"number\">0.2</span>, <span class=\"number\">0.4</span>, <span class=\"number\">0.6</span>]])</span><br><span class=\"line\">    network[<span class=\"string\">&#x27;b1&#x27;</span>] = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.3</span>])</span><br><span class=\"line\">    network[<span class=\"string\">&#x27;W2&#x27;</span>] = np.array([[<span class=\"number\">0.1</span>, <span class=\"number\">0.4</span>], [<span class=\"number\">0.2</span>, <span class=\"number\">0.5</span>], [<span class=\"number\">0.3</span>, <span class=\"number\">0.6</span>]])</span><br><span class=\"line\">    network[<span class=\"string\">&#x27;b2&#x27;</span>] = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.2</span>])</span><br><span class=\"line\">    network[<span class=\"string\">&#x27;W3&#x27;</span>] = np.array([[<span class=\"number\">0.1</span>, <span class=\"number\">0.3</span>], [<span class=\"number\">0.2</span>, <span class=\"number\">0.4</span>]])</span><br><span class=\"line\">    network[<span class=\"string\">&#x27;b3&#x27;</span>] = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.2</span>])</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> network</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">network, x</span>):</span><br><span class=\"line\">    W1, W2, W3 = network[<span class=\"string\">&#x27;W1&#x27;</span>], network[<span class=\"string\">&#x27;W2&#x27;</span>], network[<span class=\"string\">&#x27;W3&#x27;</span>]</span><br><span class=\"line\">    b1, b2, b3 = network[<span class=\"string\">&#x27;b1&#x27;</span>], network[<span class=\"string\">&#x27;b2&#x27;</span>], network[<span class=\"string\">&#x27;b3&#x27;</span>]</span><br><span class=\"line\">    </span><br><span class=\"line\">    a1 = np.dot(x, W1) + b1</span><br><span class=\"line\">    z1 = sigmoid(a1)</span><br><span class=\"line\">    a2 = np.dot(z1, W2) + b2</span><br><span class=\"line\">    z2 = sigmoid(a2)</span><br><span class=\"line\">    a3 = np.dot(z2, W3) + b3</span><br><span class=\"line\">    y = identify_fun(a3)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">network = init_network()</span><br><span class=\"line\">x = np.array([<span class=\"number\">1.0</span>, <span class=\"number\">0.5</span>])</span><br><span class=\"line\">y = forward(network, x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(y)</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"softmax-函数的实现\"><a href=\"#softmax-函数的实现\" class=\"headerlink\" title=\"$softmax$函数的实现\"></a>$softmax$函数的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">softmax</span>(<span class=\"params\">a</span>):</span><br><span class=\"line\">    exp_a = np.exp(a)</span><br><span class=\"line\">    sum_exp_a = np.<span class=\"built_in\">sum</span>(exp_a)</span><br><span class=\"line\">    y = exp_a / sum_exp_a</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">a = np.array([<span class=\"number\">0.3</span>, <span class=\"number\">2.9</span>, <span class=\"number\">4.0</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(softmax(a))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"改进后的-softmax-的实现\"><a href=\"#改进后的-softmax-的实现\" class=\"headerlink\" title=\"改进后的$softmax$的实现\"></a>改进后的$softmax$的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">softmax</span>(<span class=\"params\">a</span>):</span><br><span class=\"line\">    c = np.<span class=\"built_in\">max</span>(a)</span><br><span class=\"line\">    exp_a = np.exp(a - c)</span><br><span class=\"line\">    sum_exp_a = np.<span class=\"built_in\">sum</span>(exp_a)</span><br><span class=\"line\">    y = exp_a / sum_exp_a</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">a = np.array([<span class=\"number\">1010</span>, <span class=\"number\">1000</span>, <span class=\"number\">990</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(softmax(a))</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"手写数字识别-1\"><a href=\"#手写数字识别-1\" class=\"headerlink\" title=\"手写数字识别\"></a>手写数字识别</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">sys.path.append(<span class=\"string\">&#x27;../../py_pro/DL/&#x27;</span>) <span class=\"comment\"># 指定获取mnist数据集的脚本路径</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist</span><br><span class=\"line\"><span class=\"comment\"># (训练图像，训练标签)，(测试图像，测试标签)</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>, flatten=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x_train.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(t_train.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x_test.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(t_test.shape)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os, sys</span><br><span class=\"line\">sys.path.append(<span class=\"string\">&#x27;../../py_pro/DL/&#x27;</span>) <span class=\"comment\"># 指定获取mnist数据集的脚本路径</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist</span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">img_show</span>(<span class=\"params\">img</span>):</span><br><span class=\"line\">    pil_img = Image.fromarray(np.uint8(img))</span><br><span class=\"line\">    pil_img.show()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">False</span>, flatten=<span class=\"literal\">True</span>)</span><br><span class=\"line\">img = x_train[<span class=\"number\">0</span>]</span><br><span class=\"line\">label = t_train[<span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(label)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(img.shape)</span><br><span class=\"line\">img = img.reshape(<span class=\"number\">28</span>, <span class=\"number\">28</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(img.shape)</span><br><span class=\"line\"></span><br><span class=\"line\">img_show(img)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"神经网络的推理处理\"><a href=\"#神经网络的推理处理\" class=\"headerlink\" title=\"神经网络的推理处理\"></a>神经网络的推理处理</h4><blockquote>\n<h4 id=\"神经网络的输入层有784个神经元，输出层有10个神经元，784是图片像素是28-28，10是0-910种类别，有2个隐藏层，神经元数量分别是50、100，这个可以是任意值\"><a href=\"#神经网络的输入层有784个神经元，输出层有10个神经元，784是图片像素是28-28，10是0-910种类别，有2个隐藏层，神经元数量分别是50、100，这个可以是任意值\" class=\"headerlink\" title=\"神经网络的输入层有784个神经元，输出层有10个神经元，784是图片像素是28*28，10是0~910种类别，有2个隐藏层，神经元数量分别是50、100，这个可以是任意值\"></a>神经网络的输入层有784个神经元，输出层有10个神经元，784是图片像素是28*28，10是0~910种类别，有2个隐藏层，神经元数量分别是50、100，这个可以是任意值</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_data</span>():</span><br><span class=\"line\">    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">False</span>, flatten=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x_test, t_test</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_network</span>():</span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;../../py_pro/DL/ch03/sample_weight.pkl&#x27;</span>, <span class=\"string\">&#x27;rb&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        network = pickle.load(f)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> network</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">network, x</span>):</span><br><span class=\"line\">    W1, W2, W3 = network[<span class=\"string\">&#x27;W1&#x27;</span>], network[<span class=\"string\">&#x27;W2&#x27;</span>], network[<span class=\"string\">&#x27;W3&#x27;</span>]</span><br><span class=\"line\">    b1, b2, b3 = network[<span class=\"string\">&#x27;b1&#x27;</span>], network[<span class=\"string\">&#x27;b2&#x27;</span>], network[<span class=\"string\">&#x27;b3&#x27;</span>]</span><br><span class=\"line\">    a1 = np.dot(x, W1) + b1</span><br><span class=\"line\">    z1 = sigmoid(a1)</span><br><span class=\"line\">    a2 = np.dot(z1, W2) + b2</span><br><span class=\"line\">    z2 = sigmoid(a2)</span><br><span class=\"line\">    a3 = np.dot(z2, W3) + b3</span><br><span class=\"line\">    y = softmax(a3)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\">x, t = get_data()</span><br><span class=\"line\">network = init_network()</span><br><span class=\"line\"></span><br><span class=\"line\">accuracy_cnt = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(x)):</span><br><span class=\"line\">    y = predict(network, x[i])</span><br><span class=\"line\">    p = np.argmax(y)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> p == t[i]:</span><br><span class=\"line\">        accuracy_cnt += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Accuracy: &#x27;</span> + <span class=\"built_in\">str</span>(<span class=\"built_in\">float</span>(accuracy_cnt) / <span class=\"built_in\">len</span>(x)))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"批处理方式\"><a href=\"#批处理方式\" class=\"headerlink\" title=\"批处理方式\"></a>批处理方式</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x, t = get_data()</span><br><span class=\"line\">network = init_network()</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">100</span></span><br><span class=\"line\">accuracy_cnt = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, <span class=\"built_in\">len</span>(x), batch_size):</span><br><span class=\"line\">    x_batch = x[i:i + batch_size]</span><br><span class=\"line\">    y_batch = predict(network, x_batch)</span><br><span class=\"line\">    p = np.argmax(y_batch, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    accuracy_cnt += np.<span class=\"built_in\">sum</span>(p == t[i:i+batch_size])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Accuracy: &#x27;</span> + <span class=\"built_in\">str</span>(<span class=\"built_in\">float</span>(accuracy_cnt) / <span class=\"built_in\">len</span>(x)))</span><br></pre></td></tr></table></figure>\n\n\n</blockquote>\n</blockquote>\n","cover_type":"img","excerpt":"","more":"<h1 id=\"神经网络\"><a href=\"#神经网络\" class=\"headerlink\" title=\"神经网络\"></a>神经网络</h1><blockquote>\n<h4 id=\"神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数\"><a href=\"#神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数\" class=\"headerlink\" title=\"神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数\"></a>神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数</h4><h4 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h4><blockquote>\n<h4 id=\"它会将输入信号的总和转换为输出信号，它决定了以何种方式激活输入信号的总和\"><a href=\"#它会将输入信号的总和转换为输出信号，它决定了以何种方式激活输入信号的总和\" class=\"headerlink\" title=\"它会将输入信号的总和转换为输出信号，它决定了以何种方式激活输入信号的总和\"></a>它会将输入信号的总和转换为输出信号，它决定了以何种方式激活输入信号的总和</h4><h4 id=\"朴素感知机和神经网络的主要区别就在于激活函数的不同\"><a href=\"#朴素感知机和神经网络的主要区别就在于激活函数的不同\" class=\"headerlink\" title=\"朴素感知机和神经网络的主要区别就在于激活函数的不同\"></a>朴素感知机和神经网络的主要区别就在于激活函数的不同</h4><h4 id=\"计算过程如下\"><a href=\"#计算过程如下\" class=\"headerlink\" title=\"计算过程如下:\"></a>计算过程如下:</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/2/1.png\"></p>\n<h4 id=\"阶跃函数：一旦输入超过阈值，就切换输出（sign符号函数，朴素感知机的激活函数）\"><a href=\"#阶跃函数：一旦输入超过阈值，就切换输出（sign符号函数，朴素感知机的激活函数）\" class=\"headerlink\" title=\"阶跃函数：一旦输入超过阈值，就切换输出（sign符号函数，朴素感知机的激活函数）\"></a>阶跃函数：一旦输入超过阈值，就切换输出（sign符号函数，朴素感知机的激活函数）</h4><h4 id=\"sigmoid函数\"><a href=\"#sigmoid函数\" class=\"headerlink\" title=\"sigmoid函数\"></a>sigmoid函数</h4><blockquote>\n<h4 id=\"神经网络中一个常用的激活函数就是sigmoid函数，其表达式如下：\"><a href=\"#神经网络中一个常用的激活函数就是sigmoid函数，其表达式如下：\" class=\"headerlink\" title=\"神经网络中一个常用的激活函数就是sigmoid函数，其表达式如下：\"></a>神经网络中一个常用的激活函数就是sigmoid函数，其表达式如下：</h4><p>$$<br>h(x) &#x3D; \\frac {1}{1 + exp(-x)}<br>$$</p>\n</blockquote>\n<h4 id=\"ReLU函数\"><a href=\"#ReLU函数\" class=\"headerlink\" title=\"ReLU函数\"></a>ReLU函数</h4><h4 id=\"自变量-x-0-时，直接输出该值，-x-le-0-时，输出0\"><a href=\"#自变量-x-0-时，直接输出该值，-x-le-0-时，输出0\" class=\"headerlink\" title=\"自变量 $ x &gt; 0 $ 时，直接输出该值，$ x \\le 0 $  时，输出0\"></a>自变量 $ x &gt; 0 $ 时，直接输出该值，$ x \\le 0 $  时，输出0</h4><h4 id=\"它的表达式如下：\"><a href=\"#它的表达式如下：\" class=\"headerlink\" title=\"它的表达式如下：\"></a>它的表达式如下：</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/2/11.png\"></p>\n</blockquote>\n<h4 id=\"3层神经网络的实现\"><a href=\"#3层神经网络的实现\" class=\"headerlink\" title=\"3层神经网络的实现\"></a>3层神经网络的实现</h4><blockquote>\n<h4 id=\"首先引入一个概念符号确认\"><a href=\"#首先引入一个概念符号确认\" class=\"headerlink\" title=\"首先引入一个概念符号确认\"></a>首先引入一个概念符号确认</h4><h4 id=\"下面是权重符号的表示，\"><a href=\"#下面是权重符号的表示，\" class=\"headerlink\" title=\"下面是权重符号的表示，\"></a>下面是权重符号的表示，</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/2/5.png\"></p>\n<h4 id=\"各层间信号传递的实现\"><a href=\"#各层间信号传递的实现\" class=\"headerlink\" title=\"各层间信号传递的实现\"></a>各层间信号传递的实现</h4><h4 id=\"首先是第0层到第1层之间信号传递的实现\"><a href=\"#首先是第0层到第1层之间信号传递的实现\" class=\"headerlink\" title=\"首先是第0层到第1层之间信号传递的实现\"></a>首先是第0层到第1层之间信号传递的实现</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/2/6.png\"></p>\n<h4 id=\"注意，偏置-b-的右下角的索引号只有一个，这是因为前一层的偏置神经元（神经元1）只有一个，右下角的索引是表示前一个神经元的，对应到权重的符号表示是-b-left-1-right-None-1-，因为本身a本身就在一个神经元上了，所以前一个索引是None\"><a href=\"#注意，偏置-b-的右下角的索引号只有一个，这是因为前一层的偏置神经元（神经元1）只有一个，右下角的索引是表示前一个神经元的，对应到权重的符号表示是-b-left-1-right-None-1-，因为本身a本身就在一个神经元上了，所以前一个索引是None\" class=\"headerlink\" title=\"注意，偏置 $ b $ 的右下角的索引号只有一个，这是因为前一层的偏置神经元（神经元1）只有一个，右下角的索引是表示前一个神经元的，对应到权重的符号表示是 $ b^\\left[(1)\\right]_{None,1} $ ，因为本身a本身就在一个神经元上了，所以前一个索引是None\"></a>注意，偏置 $ b $ 的右下角的索引号只有一个，这是因为前一层的偏置神经元（神经元1）只有一个，右下角的索引是表示前一个神经元的，对应到权重的符号表示是 $ b^\\left[(1)\\right]_{None,1} $ ，因为本身a本身就在一个神经元上了，所以前一个索引是None</h4><h4 id=\"a-1-的表达式如下：\"><a href=\"#a-1-的表达式如下：\" class=\"headerlink\" title=\"$ a_1 $的表达式如下：\"></a>$ a_1 $的表达式如下：</h4><p>$$<br>a_1^{(1)} &#x3D; w_{11}^{(1)} x_1 + w_{12}^{(1)} x_2 + b_1^{(1)}<br>$$</p>\n<h4 id=\"使用矩阵的乘法运算表示第1层的加权和\"><a href=\"#使用矩阵的乘法运算表示第1层的加权和\" class=\"headerlink\" title=\"使用矩阵的乘法运算表示第1层的加权和\"></a>使用矩阵的乘法运算表示第1层的加权和</h4><p>$$<br>A^{(1)} &#x3D; XW^{(1)} + B^{(1)}<br>$$</p>\n<h4 id=\"其中，-A-1-X-B-1-W-1-如下所示\"><a href=\"#其中，-A-1-X-B-1-W-1-如下所示\" class=\"headerlink\" title=\"其中，$ A^{(1)},X,B^{(1)},W^{(1)} $如下所示\"></a>其中，$ A^{(1)},X,B^{(1)},W^{(1)} $如下所示</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/2/12.png\"></p>\n<h4 id=\"W-i-的每一列对应一个-a-i\"><a href=\"#W-i-的每一列对应一个-a-i\" class=\"headerlink\" title=\"$W_i$的每一列对应一个$ a_i $\"></a>$W_i$的每一列对应一个$ a_i $</h4></blockquote>\n<h4 id=\"第1层到第2层之间信号传递的实现\"><a href=\"#第1层到第2层之间信号传递的实现\" class=\"headerlink\" title=\"第1层到第2层之间信号传递的实现\"></a>第1层到第2层之间信号传递的实现</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/2/7.png\"></p>\n<h4 id=\"第2层到输出层之间信号传递的实现\"><a href=\"#第2层到输出层之间信号传递的实现\" class=\"headerlink\" title=\"第2层到输出层之间信号传递的实现\"></a>第2层到输出层之间信号传递的实现</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/2/8.png\"></p>\n<h4 id=\"输出层的激活函数一般用-sigma-来表示，隐藏层的激活函数一般用-h-来表示\"><a href=\"#输出层的激活函数一般用-sigma-来表示，隐藏层的激活函数一般用-h-来表示\" class=\"headerlink\" title=\"输出层的激活函数一般用 $ \\sigma() $ 来表示，隐藏层的激活函数一般用 $ h() $ 来表示\"></a>输出层的激活函数一般用 $ \\sigma() $ 来表示，隐藏层的激活函数一般用 $ h() $ 来表示</h4><h4 id=\"另外，输出层所使用的激活函数要根据求解问题的性质来确定，一般情况下，回归问题可以使用恒等函数，二元分类问题可以使用-sigmoid-函数，多元分类问题可以使用-softmax-函数\"><a href=\"#另外，输出层所使用的激活函数要根据求解问题的性质来确定，一般情况下，回归问题可以使用恒等函数，二元分类问题可以使用-sigmoid-函数，多元分类问题可以使用-softmax-函数\" class=\"headerlink\" title=\"另外，输出层所使用的激活函数要根据求解问题的性质来确定，一般情况下，回归问题可以使用恒等函数，二元分类问题可以使用 $sigmoid$ 函数，多元分类问题可以使用 $softmax$函数\"></a>另外，输出层所使用的激活函数要根据求解问题的性质来确定，一般情况下，回归问题可以使用恒等函数，二元分类问题可以使用 $sigmoid$ 函数，多元分类问题可以使用 $softmax$函数</h4><h4 id=\"输出层的设计\"><a href=\"#输出层的设计\" class=\"headerlink\" title=\"输出层的设计\"></a>输出层的设计</h4><blockquote>\n<h4 id=\"机器学习的问题大致可以分为分类问题和回归问题-分类问题是数-据属于哪一个类别的问题；而回归问题是根据某个输入预测一个（连续的）数值的问题\"><a href=\"#机器学习的问题大致可以分为分类问题和回归问题-分类问题是数-据属于哪一个类别的问题；而回归问题是根据某个输入预测一个（连续的）数值的问题\" class=\"headerlink\" title=\"机器学习的问题大致可以分为分类问题和回归问题,分类问题是数 据属于哪一个类别的问题；而回归问题是根据某个输入预测一个（连续的）数值的问题\"></a>机器学习的问题大致可以分为分类问题和回归问题,分类问题是数 据属于哪一个类别的问题；而回归问题是根据某个输入预测一个（连续的）数值的问题</h4><h4 id=\"softmax-函数（输出层的激活函数）\"><a href=\"#softmax-函数（输出层的激活函数）\" class=\"headerlink\" title=\"$softmax$函数（输出层的激活函数）\"></a>$softmax$函数（输出层的激活函数）</h4><blockquote>\n<h4 id=\"分类问题中使用该函数，表达式如下：\"><a href=\"#分类问题中使用该函数，表达式如下：\" class=\"headerlink\" title=\"分类问题中使用该函数，表达式如下：\"></a>分类问题中使用该函数，表达式如下：</h4><p>$$<br>\\sigma &#x3D; y_k &#x3D; \\frac {exp(a_k)} {\\sum^{n}_{i &#x3D; 1} exp(a_i)}<br>$$</p>\n<h4 id=\"这个式子表示：假设输出层共有-n-个神经元，计算第-k-个神经元的输出-y-k\"><a href=\"#这个式子表示：假设输出层共有-n-个神经元，计算第-k-个神经元的输出-y-k\" class=\"headerlink\" title=\"这个式子表示：假设输出层共有$n$个神经元，计算第$k$个神经元的输出$y_k$\"></a>这个式子表示：假设输出层共有$n$个神经元，计算第$k$个神经元的输出$y_k$</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/2/9.png\"></p>\n<h4 id=\"可以看出，输出层的各个神经元都受到所有输入信号的影响\"><a href=\"#可以看出，输出层的各个神经元都受到所有输入信号的影响\" class=\"headerlink\" title=\"可以看出，输出层的各个神经元都受到所有输入信号的影响\"></a>可以看出，输出层的各个神经元都受到所有输入信号的影响</h4></blockquote>\n<h4 id=\"注意到，上面这种形式的-softmax-函数有一个致命问题，那就是指数部分容易发生溢出\"><a href=\"#注意到，上面这种形式的-softmax-函数有一个致命问题，那就是指数部分容易发生溢出\" class=\"headerlink\" title=\"注意到，上面这种形式的$softmax$函数有一个致命问题，那就是指数部分容易发生溢出\"></a>注意到，上面这种形式的$softmax$函数有一个致命问题，那就是指数部分容易发生溢出</h4><h4 id=\"为此，对其进行改进\"><a href=\"#为此，对其进行改进\" class=\"headerlink\" title=\"为此，对其进行改进\"></a>为此，对其进行改进</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/2/10.png\"></p>\n<h4 id=\"在进行-softmax-的指数的运算时，加上（或者减去）某个常数并不会改变运算的结果，这里的-C’-可以使用任何值，但是为了防止溢出，一般会使用输入信号中的最大值\"><a href=\"#在进行-softmax-的指数的运算时，加上（或者减去）某个常数并不会改变运算的结果，这里的-C’-可以使用任何值，但是为了防止溢出，一般会使用输入信号中的最大值\" class=\"headerlink\" title=\"在进行$softmax$的指数的运算时，加上（或者减去）某个常数并不会改变运算的结果，这里的$C’$可以使用任何值，但是为了防止溢出，一般会使用输入信号中的最大值\"></a>在进行$softmax$的指数的运算时，加上（或者减去）某个常数并不会改变运算的结果，这里的$C’$可以使用任何值，但是为了防止溢出，一般会使用输入信号中的最大值</h4><h4 id=\"softmax-函数的特征，其输出是0-0到1-0之间的实数，且输出值的总和是1，这是它的一个重要性质，正因如此，一般把-softmax-的输出解释为-概率\"><a href=\"#softmax-函数的特征，其输出是0-0到1-0之间的实数，且输出值的总和是1，这是它的一个重要性质，正因如此，一般把-softmax-的输出解释为-概率\" class=\"headerlink\" title=\"$softmax$函数的特征，其输出是0.0到1.0之间的实数，且输出值的总和是1，这是它的一个重要性质，正因如此，一般把$softmax$的输出解释为 概率\"></a>$softmax$函数的特征，其输出是0.0到1.0之间的实数，且输出值的总和是1，这是它的一个重要性质，正因如此，一般把$softmax$的输出解释为 概率</h4><h4 id=\"通过使用-softmax-函数，我们可以用概率的（统计的）方法处理问题\"><a href=\"#通过使用-softmax-函数，我们可以用概率的（统计的）方法处理问题\" class=\"headerlink\" title=\"通过使用$softmax$函数，我们可以用概率的（统计的）方法处理问题\"></a>通过使用$softmax$函数，我们可以用概率的（统计的）方法处理问题</h4><h4 id=\"另外，即便使用了-softmax-函数，各个元素之间的大小关-系也不会改变，因为指数函数-y-exp-x-是单调递增函数\"><a href=\"#另外，即便使用了-softmax-函数，各个元素之间的大小关-系也不会改变，因为指数函数-y-exp-x-是单调递增函数\" class=\"headerlink\" title=\"另外，即便使用了$softmax$函数，各个元素之间的大小关 系也不会改变，因为指数函数$ y &#x3D; exp(x) $是单调递增函数\"></a>另外，即便使用了$softmax$函数，各个元素之间的大小关 系也不会改变，因为指数函数$ y &#x3D; exp(x) $是单调递增函数</h4><h4 id=\"一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果，且即便使用-softmax-函数，输出值最大的神经元的位置也不会变，因此，神经网络在进行分类时，输出层的-softmax-函数可以省略（还有个原因是，指数函数的运算需要一定的计算机运算量）\"><a href=\"#一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果，且即便使用-softmax-函数，输出值最大的神经元的位置也不会变，因此，神经网络在进行分类时，输出层的-softmax-函数可以省略（还有个原因是，指数函数的运算需要一定的计算机运算量）\" class=\"headerlink\" title=\"一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果，且即便使用$softmax$函数，输出值最大的神经元的位置也不会变，因此，神经网络在进行分类时，输出层的$softmax$函数可以省略（还有个原因是，指数函数的运算需要一定的计算机运算量）\"></a>一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果，且即便使用$softmax$函数，输出值最大的神经元的位置也不会变，因此，神经网络在进行<strong>分类</strong>时，输出层的$softmax$函数可以省略（还有个原因是，指数函数的运算需要一定的计算机运算量）</h4><h4 id=\"求解机器学习问题的步骤可以分为学习和推理两个阶段，首先，在学习阶段进行模型的学习，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）\"><a href=\"#求解机器学习问题的步骤可以分为学习和推理两个阶段，首先，在学习阶段进行模型的学习，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）\" class=\"headerlink\" title=\"求解机器学习问题的步骤可以分为学习和推理两个阶段，首先，在学习阶段进行模型的学习，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）\"></a>求解机器学习问题的步骤可以分为学习和推理两个阶段，首先，在学习阶段进行模型的学习，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）</h4></blockquote>\n<h4 id=\"手写数字识别\"><a href=\"#手写数字识别\" class=\"headerlink\" title=\"手写数字识别\"></a>手写数字识别</h4><blockquote>\n<h4 id=\"one-hot表示是仅正确解标签为1，其余皆为0的数组\"><a href=\"#one-hot表示是仅正确解标签为1，其余皆为0的数组\" class=\"headerlink\" title=\"one-hot表示是仅正确解标签为1，其余皆为0的数组\"></a>one-hot表示是仅正确解标签为1，其余皆为0的数组</h4><h4 id=\"pickle可以可以将程序运行中的对象保存为文件，如果加载保存过的pickle文件，可以立刻复原之前程序运行中的对象（Numpy数组），使用pickle-load-可以直接还原所有数据结构\"><a href=\"#pickle可以可以将程序运行中的对象保存为文件，如果加载保存过的pickle文件，可以立刻复原之前程序运行中的对象（Numpy数组），使用pickle-load-可以直接还原所有数据结构\" class=\"headerlink\" title=\"pickle可以可以将程序运行中的对象保存为文件，如果加载保存过的pickle文件，可以立刻复原之前程序运行中的对象（Numpy数组），使用pickle.load()可以直接还原所有数据结构\"></a>pickle可以可以将程序运行中的对象保存为文件，如果加载保存过的pickle文件，可以立刻复原之前程序运行中的对象（Numpy数组），使用pickle.load()可以直接还原所有数据结构</h4><h4 id=\"argmax-np-array-axis-函数可以取出数组中的最大值的索引，axis-1指定沿着行方向寻找最大值\"><a href=\"#argmax-np-array-axis-函数可以取出数组中的最大值的索引，axis-1指定沿着行方向寻找最大值\" class=\"headerlink\" title=\"argmax(np.array, axis)函数可以取出数组中的最大值的索引，axis&#x3D;1指定沿着行方向寻找最大值\"></a>argmax(np.array, axis)函数可以取出数组中的最大值的索引，axis&#x3D;1指定沿着行方向寻找最大值</h4><h4 id=\"正规化（normalization）：把数据限定到某个范围内的处理\"><a href=\"#正规化（normalization）：把数据限定到某个范围内的处理\" class=\"headerlink\" title=\"正规化（normalization）：把数据限定到某个范围内的处理\"></a>正规化（normalization）：把数据限定到某个范围内的处理</h4><h4 id=\"预处理（pre-processing）：对神经网络的输入数据进行某种既定的转换\"><a href=\"#预处理（pre-processing）：对神经网络的输入数据进行某种既定的转换\" class=\"headerlink\" title=\"预处理（pre-processing）：对神经网络的输入数据进行某种既定的转换\"></a>预处理（pre-processing）：对神经网络的输入数据进行某种既定的转换</h4><h4 id=\"数据白化（whitening）：将数据整体的分布形状均匀化的方法\"><a href=\"#数据白化（whitening）：将数据整体的分布形状均匀化的方法\" class=\"headerlink\" title=\"数据白化（whitening）：将数据整体的分布形状均匀化的方法\"></a>数据白化（whitening）：将数据整体的分布形状均匀化的方法</h4><h4 id=\"批处理\"><a href=\"#批处理\" class=\"headerlink\" title=\"批处理\"></a>批处理</h4></blockquote>\n</blockquote>\n<h3 id=\"code\"><a href=\"#code\" class=\"headerlink\" title=\"code\"></a>code</h3><blockquote>\n<h4 id=\"阶跃函数的实现\"><a href=\"#阶跃函数的实现\" class=\"headerlink\" title=\"阶跃函数的实现\"></a>阶跃函数的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">step_fun</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\"> y = x &gt; <span class=\"number\">0</span></span><br><span class=\"line\"> <span class=\"keyword\">return</span> y.astype(np.<span class=\"built_in\">int</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># y = x &gt; 0是什么意思</span></span><br><span class=\"line\"><span class=\"comment\"># 对numpy数组进行bool运算，它会对数组内的每个元素做bool运算，得到一个对应bool值的bool数组，不是浮点型数组了</span></span><br><span class=\"line\">x = np.array([-<span class=\"number\">1.0</span>, <span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)</span><br><span class=\"line\">y = x &gt; <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(y)</span><br><span class=\"line\"><span class=\"comment\"># astype()函数可以转换numpy数组的类型，需要对int指定精度，可以是32，也可以是64</span></span><br><span class=\"line\">y = y.astype(np.int32)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(y)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"阶跃函数的图像\"><a href=\"#阶跃函数的图像\" class=\"headerlink\" title=\"阶跃函数的图像\"></a>阶跃函数的图像</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pylab <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">step_fun</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\"> <span class=\"keyword\">return</span> np.array(x &gt; <span class=\"number\">0</span>, dtype=np.int32)</span><br><span class=\"line\"></span><br><span class=\"line\">x = np.arange(-<span class=\"number\">5.0</span>, <span class=\"number\">5.0</span>, <span class=\"number\">0.1</span>)</span><br><span class=\"line\">y = step_fun(x)</span><br><span class=\"line\">plt.plot(x, y)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">0.1</span>, <span class=\"number\">1.1</span>) <span class=\"comment\"># 指定y轴范围</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/DL/2/2.png\"></p>\n<h4 id=\"sigmoid函数的实现\"><a href=\"#sigmoid函数的实现\" class=\"headerlink\" title=\"sigmoid函数的实现\"></a>sigmoid函数的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sigmoid</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\"> <span class=\"keyword\">return</span> <span class=\"number\">1</span> / (<span class=\"number\">1</span> + np.exp(-x))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">x = np.array([-<span class=\"number\">1.0</span>, <span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(sigmoid(x))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"sigmoid函数图像\"><a href=\"#sigmoid函数图像\" class=\"headerlink\" title=\"sigmoid函数图像\"></a>sigmoid函数图像</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = np.arange(-<span class=\"number\">5.0</span>, <span class=\"number\">5.0</span>, <span class=\"number\">0.1</span>)</span><br><span class=\"line\">y = sigmoid(x)</span><br><span class=\"line\">plt.plot(x, y)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">0.1</span>, <span class=\"number\">1.1</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/DL/2/3.png\"></p>\n<h4 id=\"可以看到，阶跃函数和sigmoid函数的几个共同点：\"><a href=\"#可以看到，阶跃函数和sigmoid函数的几个共同点：\" class=\"headerlink\" title=\"可以看到，阶跃函数和sigmoid函数的几个共同点：\"></a>可以看到，阶跃函数和sigmoid函数的几个共同点：</h4><h4 id=\"一是，当输入信号为重要信息时，阶跃函数和sigmoid函数都会输出较大的值；当输入信号为不重要的信息时，两者都输出较小的值\"><a href=\"#一是，当输入信号为重要信息时，阶跃函数和sigmoid函数都会输出较大的值；当输入信号为不重要的信息时，两者都输出较小的值\" class=\"headerlink\" title=\"一是，当输入信号为重要信息时，阶跃函数和sigmoid函数都会输出较大的值；当输入信号为不重要的信息时，两者都输出较小的值\"></a>一是，当输入信号为重要信息时，阶跃函数和sigmoid函数都会输出较大的值；当输入信号为不重要的信息时，两者都输出较小的值</h4><h4 id=\"另一个是，不管输入信号有多小，或者有多大，输出信号的值都在0到1之间\"><a href=\"#另一个是，不管输入信号有多小，或者有多大，输出信号的值都在0到1之间\" class=\"headerlink\" title=\"另一个是，不管输入信号有多小，或者有多大，输出信号的值都在0到1之间\"></a>另一个是，不管输入信号有多小，或者有多大，输出信号的值都在0到1之间</h4><h4 id=\"另外，两者均是非线性函数，不管是朴素感知机还是神经网络，其激活函数必须是非线性函数，不能是线性函数\"><a href=\"#另外，两者均是非线性函数，不管是朴素感知机还是神经网络，其激活函数必须是非线性函数，不能是线性函数\" class=\"headerlink\" title=\"另外，两者均是非线性函数，不管是朴素感知机还是神经网络，其激活函数必须是非线性函数，不能是线性函数\"></a>另外，两者均是非线性函数，不管是朴素感知机还是神经网络，其激活函数必须是非线性函数，不能是线性函数</h4><h4 id=\"ReLU函数的实现\"><a href=\"#ReLU函数的实现\" class=\"headerlink\" title=\"ReLU函数的实现\"></a>ReLU函数的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">relu</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\"> <span class=\"keyword\">return</span> np.maximum(<span class=\"number\">0</span>, x)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">x = np.array([-<span class=\"number\">1.0</span>, <span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(relu(x))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"ReLU函数的的图像\"><a href=\"#ReLU函数的的图像\" class=\"headerlink\" title=\"ReLU函数的的图像\"></a>ReLU函数的的图像</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = np.arange(-<span class=\"number\">6.0</span>, <span class=\"number\">6.0</span>, <span class=\"number\">1.0</span>)</span><br><span class=\"line\">y = relu(x)</span><br><span class=\"line\">plt.plot(x, y)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">1.0</span>, <span class=\"number\">6.0</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"http://picbed.yanzu.tech/img/DL/2/4.png\"></p>\n<h4 id=\"多维数组的运算\"><a href=\"#多维数组的运算\" class=\"headerlink\" title=\"多维数组的运算\"></a>多维数组的运算</h4><blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 一维数组</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">A = np.array([<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(A)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(np.ndim(A)) <span class=\"comment\"># 输出数组的维数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(np.shape(A)) <span class=\"comment\"># 输出数组的形状，其结果是一个元组</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.shape) <span class=\"comment\"># 另外一种方式</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 二维数组，注意二维以及高维数组还需要一个额外的中括号括起来</span></span><br><span class=\"line\">B = np.array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>], [<span class=\"number\">3</span>, <span class=\"number\">4</span>], [<span class=\"number\">5</span>, <span class=\"number\">6</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(B)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(np.ndim(B)) <span class=\"comment\"># 输出数组的维数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(np.shape(B)) <span class=\"comment\"># 输出数组的形状，其结果是一个元组</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(B.shape) <span class=\"comment\"># 另外一种方式</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(B.shape[<span class=\"number\">1</span>]) <span class=\"comment\"># shape[0]是数组的行数，shape[1]是数组的列数</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"矩阵乘法\"><a href=\"#矩阵乘法\" class=\"headerlink\" title=\"矩阵乘法\"></a>矩阵乘法</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 2 * 2 的矩阵乘法</span></span><br><span class=\"line\">A = np.array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>], [<span class=\"number\">3</span>, <span class=\"number\">4</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.shape)</span><br><span class=\"line\">B = np.array([[<span class=\"number\">2</span>, <span class=\"number\">3</span>], [<span class=\"number\">4</span>, <span class=\"number\">5</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(B.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(np.dot(A, B)) <span class=\"comment\"># dot()接收两个numpy数组作为参数，返回数组的乘积</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2*3 和 3*2 的矩阵乘法</span></span><br><span class=\"line\">A = np.array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>], [<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.shape)</span><br><span class=\"line\">B = np.array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>],[<span class=\"number\">3</span>, <span class=\"number\">4</span>], [<span class=\"number\">5</span>, <span class=\"number\">6</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(B.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(np.dot(A, B)) <span class=\"comment\"># dot()接收两个numpy数组作为参数，返回数组的乘积</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(A @ B)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 对于一维numpy数组的乘法运算，同样是借助了广播功能</span></span><br><span class=\"line\">A = np.array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>],[<span class=\"number\">3</span>, <span class=\"number\">4</span>], [<span class=\"number\">5</span>, <span class=\"number\">6</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(A.shape)</span><br><span class=\"line\">B = np.array([<span class=\"number\">7</span>, <span class=\"number\">8</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(B.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(np.dot(A, B))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"神经网络的内积\"><a href=\"#神经网络的内积\" class=\"headerlink\" title=\"神经网络的内积\"></a>神经网络的内积</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = np.array([<span class=\"number\">1</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(X.shape)</span><br><span class=\"line\">W = np.array([[<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">5</span>], [<span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(W.shape)</span><br><span class=\"line\">Y = np.dot(X, W)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y)</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h4 id=\"第0层到第1层之间信号传递的实现\"><a href=\"#第0层到第1层之间信号传递的实现\" class=\"headerlink\" title=\"第0层到第1层之间信号传递的实现\"></a>第0层到第1层之间信号传递的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = np.array([<span class=\"number\">1.0</span>, <span class=\"number\">0.5</span>])</span><br><span class=\"line\">W1 = np.array([[<span class=\"number\">0.1</span>, <span class=\"number\">0.3</span>, <span class=\"number\">0.5</span>], [<span class=\"number\">0.2</span>, <span class=\"number\">0.4</span>, <span class=\"number\">0.6</span>]])</span><br><span class=\"line\">B1 = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.3</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(W1.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(X.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(B1.shape)</span><br><span class=\"line\">A1 = np.dot(X, W1) + B1</span><br><span class=\"line\"><span class=\"built_in\">print</span>(A1)</span><br><span class=\"line\">Z1 = sigmoid(A1)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Z1)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"第1层到第2层之间信号传递的实现-1\"><a href=\"#第1层到第2层之间信号传递的实现-1\" class=\"headerlink\" title=\"第1层到第2层之间信号传递的实现\"></a>第1层到第2层之间信号传递的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">W2 = np.array([[<span class=\"number\">0.1</span>, <span class=\"number\">0.4</span>], [<span class=\"number\">0.2</span>, <span class=\"number\">0.5</span>], [<span class=\"number\">0.3</span>, <span class=\"number\">0.6</span>]])</span><br><span class=\"line\">B2 = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.2</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Z1.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(W2.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(B2.shape)</span><br><span class=\"line\"><span class=\"comment\"># Z2 从第一层的输出变成了第二层的输入了，新的(x1, x2, x3)</span></span><br><span class=\"line\">A2 = np.dot(Z1, W2) + B2</span><br><span class=\"line\"><span class=\"built_in\">print</span>(A2)</span><br><span class=\"line\">Z2 = sigmoid(A2)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Z2)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"第2层到输出层之间信号传递的实现-1\"><a href=\"#第2层到输出层之间信号传递的实现-1\" class=\"headerlink\" title=\"第2层到输出层之间信号传递的实现\"></a>第2层到输出层之间信号传递的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 注意，这两层之间的激活函数与之前隐藏层的有所不同了</span></span><br><span class=\"line\"><span class=\"comment\"># 这里的激活函数是恒等函数，就是输入是什么输出就是什么</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">identify_fun</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\">W3 = np.array([[<span class=\"number\">0.1</span>, <span class=\"number\">0.3</span>], [<span class=\"number\">0.2</span>, <span class=\"number\">0.4</span>]])</span><br><span class=\"line\">B3 = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.2</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(W3.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(B3.shape)</span><br><span class=\"line\"></span><br><span class=\"line\">A3 = np.dot(Z2, W3) + B3</span><br><span class=\"line\">Y = identify_fun(A3)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(A3.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(Y)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"写成函数形式，实现3层神经网络\"><a href=\"#写成函数形式，实现3层神经网络\" class=\"headerlink\" title=\"写成函数形式，实现3层神经网络\"></a>写成函数形式，实现3层神经网络</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_network</span>():</span><br><span class=\"line\">    network = &#123;&#125;</span><br><span class=\"line\">    network[<span class=\"string\">&#x27;W1&#x27;</span>] = np.array([[<span class=\"number\">0.1</span>, <span class=\"number\">0.3</span>, <span class=\"number\">0.5</span>], [<span class=\"number\">0.2</span>, <span class=\"number\">0.4</span>, <span class=\"number\">0.6</span>]])</span><br><span class=\"line\">    network[<span class=\"string\">&#x27;b1&#x27;</span>] = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.3</span>])</span><br><span class=\"line\">    network[<span class=\"string\">&#x27;W2&#x27;</span>] = np.array([[<span class=\"number\">0.1</span>, <span class=\"number\">0.4</span>], [<span class=\"number\">0.2</span>, <span class=\"number\">0.5</span>], [<span class=\"number\">0.3</span>, <span class=\"number\">0.6</span>]])</span><br><span class=\"line\">    network[<span class=\"string\">&#x27;b2&#x27;</span>] = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.2</span>])</span><br><span class=\"line\">    network[<span class=\"string\">&#x27;W3&#x27;</span>] = np.array([[<span class=\"number\">0.1</span>, <span class=\"number\">0.3</span>], [<span class=\"number\">0.2</span>, <span class=\"number\">0.4</span>]])</span><br><span class=\"line\">    network[<span class=\"string\">&#x27;b3&#x27;</span>] = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.2</span>])</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> network</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">network, x</span>):</span><br><span class=\"line\">    W1, W2, W3 = network[<span class=\"string\">&#x27;W1&#x27;</span>], network[<span class=\"string\">&#x27;W2&#x27;</span>], network[<span class=\"string\">&#x27;W3&#x27;</span>]</span><br><span class=\"line\">    b1, b2, b3 = network[<span class=\"string\">&#x27;b1&#x27;</span>], network[<span class=\"string\">&#x27;b2&#x27;</span>], network[<span class=\"string\">&#x27;b3&#x27;</span>]</span><br><span class=\"line\">    </span><br><span class=\"line\">    a1 = np.dot(x, W1) + b1</span><br><span class=\"line\">    z1 = sigmoid(a1)</span><br><span class=\"line\">    a2 = np.dot(z1, W2) + b2</span><br><span class=\"line\">    z2 = sigmoid(a2)</span><br><span class=\"line\">    a3 = np.dot(z2, W3) + b3</span><br><span class=\"line\">    y = identify_fun(a3)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">network = init_network()</span><br><span class=\"line\">x = np.array([<span class=\"number\">1.0</span>, <span class=\"number\">0.5</span>])</span><br><span class=\"line\">y = forward(network, x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(y)</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"softmax-函数的实现\"><a href=\"#softmax-函数的实现\" class=\"headerlink\" title=\"$softmax$函数的实现\"></a>$softmax$函数的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">softmax</span>(<span class=\"params\">a</span>):</span><br><span class=\"line\">    exp_a = np.exp(a)</span><br><span class=\"line\">    sum_exp_a = np.<span class=\"built_in\">sum</span>(exp_a)</span><br><span class=\"line\">    y = exp_a / sum_exp_a</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">a = np.array([<span class=\"number\">0.3</span>, <span class=\"number\">2.9</span>, <span class=\"number\">4.0</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(softmax(a))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"改进后的-softmax-的实现\"><a href=\"#改进后的-softmax-的实现\" class=\"headerlink\" title=\"改进后的$softmax$的实现\"></a>改进后的$softmax$的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">softmax</span>(<span class=\"params\">a</span>):</span><br><span class=\"line\">    c = np.<span class=\"built_in\">max</span>(a)</span><br><span class=\"line\">    exp_a = np.exp(a - c)</span><br><span class=\"line\">    sum_exp_a = np.<span class=\"built_in\">sum</span>(exp_a)</span><br><span class=\"line\">    y = exp_a / sum_exp_a</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">a = np.array([<span class=\"number\">1010</span>, <span class=\"number\">1000</span>, <span class=\"number\">990</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(softmax(a))</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"手写数字识别-1\"><a href=\"#手写数字识别-1\" class=\"headerlink\" title=\"手写数字识别\"></a>手写数字识别</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">sys.path.append(<span class=\"string\">&#x27;../../py_pro/DL/&#x27;</span>) <span class=\"comment\"># 指定获取mnist数据集的脚本路径</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist</span><br><span class=\"line\"><span class=\"comment\"># (训练图像，训练标签)，(测试图像，测试标签)</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>, flatten=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x_train.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(t_train.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x_test.shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(t_test.shape)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os, sys</span><br><span class=\"line\">sys.path.append(<span class=\"string\">&#x27;../../py_pro/DL/&#x27;</span>) <span class=\"comment\"># 指定获取mnist数据集的脚本路径</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist</span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">img_show</span>(<span class=\"params\">img</span>):</span><br><span class=\"line\">    pil_img = Image.fromarray(np.uint8(img))</span><br><span class=\"line\">    pil_img.show()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">False</span>, flatten=<span class=\"literal\">True</span>)</span><br><span class=\"line\">img = x_train[<span class=\"number\">0</span>]</span><br><span class=\"line\">label = t_train[<span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(label)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(img.shape)</span><br><span class=\"line\">img = img.reshape(<span class=\"number\">28</span>, <span class=\"number\">28</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(img.shape)</span><br><span class=\"line\"></span><br><span class=\"line\">img_show(img)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"神经网络的推理处理\"><a href=\"#神经网络的推理处理\" class=\"headerlink\" title=\"神经网络的推理处理\"></a>神经网络的推理处理</h4><blockquote>\n<h4 id=\"神经网络的输入层有784个神经元，输出层有10个神经元，784是图片像素是28-28，10是0-910种类别，有2个隐藏层，神经元数量分别是50、100，这个可以是任意值\"><a href=\"#神经网络的输入层有784个神经元，输出层有10个神经元，784是图片像素是28-28，10是0-910种类别，有2个隐藏层，神经元数量分别是50、100，这个可以是任意值\" class=\"headerlink\" title=\"神经网络的输入层有784个神经元，输出层有10个神经元，784是图片像素是28*28，10是0~910种类别，有2个隐藏层，神经元数量分别是50、100，这个可以是任意值\"></a>神经网络的输入层有784个神经元，输出层有10个神经元，784是图片像素是28*28，10是0~910种类别，有2个隐藏层，神经元数量分别是50、100，这个可以是任意值</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_data</span>():</span><br><span class=\"line\">    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">False</span>, flatten=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x_test, t_test</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_network</span>():</span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;../../py_pro/DL/ch03/sample_weight.pkl&#x27;</span>, <span class=\"string\">&#x27;rb&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">        network = pickle.load(f)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> network</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">network, x</span>):</span><br><span class=\"line\">    W1, W2, W3 = network[<span class=\"string\">&#x27;W1&#x27;</span>], network[<span class=\"string\">&#x27;W2&#x27;</span>], network[<span class=\"string\">&#x27;W3&#x27;</span>]</span><br><span class=\"line\">    b1, b2, b3 = network[<span class=\"string\">&#x27;b1&#x27;</span>], network[<span class=\"string\">&#x27;b2&#x27;</span>], network[<span class=\"string\">&#x27;b3&#x27;</span>]</span><br><span class=\"line\">    a1 = np.dot(x, W1) + b1</span><br><span class=\"line\">    z1 = sigmoid(a1)</span><br><span class=\"line\">    a2 = np.dot(z1, W2) + b2</span><br><span class=\"line\">    z2 = sigmoid(a2)</span><br><span class=\"line\">    a3 = np.dot(z2, W3) + b3</span><br><span class=\"line\">    y = softmax(a3)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\">x, t = get_data()</span><br><span class=\"line\">network = init_network()</span><br><span class=\"line\"></span><br><span class=\"line\">accuracy_cnt = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(x)):</span><br><span class=\"line\">    y = predict(network, x[i])</span><br><span class=\"line\">    p = np.argmax(y)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> p == t[i]:</span><br><span class=\"line\">        accuracy_cnt += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Accuracy: &#x27;</span> + <span class=\"built_in\">str</span>(<span class=\"built_in\">float</span>(accuracy_cnt) / <span class=\"built_in\">len</span>(x)))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"批处理方式\"><a href=\"#批处理方式\" class=\"headerlink\" title=\"批处理方式\"></a>批处理方式</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x, t = get_data()</span><br><span class=\"line\">network = init_network()</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">100</span></span><br><span class=\"line\">accuracy_cnt = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, <span class=\"built_in\">len</span>(x), batch_size):</span><br><span class=\"line\">    x_batch = x[i:i + batch_size]</span><br><span class=\"line\">    y_batch = predict(network, x_batch)</span><br><span class=\"line\">    p = np.argmax(y_batch, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    accuracy_cnt += np.<span class=\"built_in\">sum</span>(p == t[i:i+batch_size])</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Accuracy: &#x27;</span> + <span class=\"built_in\">str</span>(<span class=\"built_in\">float</span>(accuracy_cnt) / <span class=\"built_in\">len</span>(x)))</span><br></pre></td></tr></table></figure>\n\n\n</blockquote>\n</blockquote>\n"},{"title":"DL杂谈","data":"2025-07-12T07:24:00.000Z","updated":"2025-07-12T07:24:00.000Z","type":"DL","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p25.png","_content":"\n\n# 神经网络杂谈\n\n\n\n### 什么是神经网络\n\n> #### 神经网络跟函数一样，有输入、输出，本质上也是一种映射关系，当给出一个函数表达式和一些数据，如 $y = kx + b$ ，给出了一些点或者说数据 (1,2)、(2,4)、(3,6)、(4,8)，这几组数据可以轻易的求出k和b的值来\n>\n> #### 但当给出的数据很多时，求解k和b这两个参数的问题就会变得复杂，至少是不能一眼看出某种规律。另外，函数表达式越复杂，在数据量越多的情况下，更能找出与之尽可能拟合的函数，因为数据量越大，其某种规律就可能会越明显，求得的参数就会越接近真实值（精确解）\n>\n> #### 函数表达式越复杂，其对应的表达能力会更强，如 $y = ax^2 + bx + c$ ，显然，它涵盖了 $y = bx + c,a=0$ \n>\n> #### 求出函数表达式中的参数，也就完成了神经网络的搭建，通过训练来求参数\n\n\n\n#### 怎么求参数\n\n> #### 直接求出参数的值是不太现实的，因为但表达式足够复杂，参数足够多，参数的求解是极为复杂的\n>\n> #### 但通过求数值解，使这个解尽可能的逼近真实值，当两者的误差达到了所能接受的范围，就把这个数值解近似的作为真实值，如 $y=kx$ ，可直接将k指定一个值，如 $k=1,\\hat y=x$ 然后根据所给数据求数值解，然后计算误差 $L = (\\hat y - y)^2$ \n>\n> #### 而求数值解的一个经典算法就是梯度下降法（gradient decay）\n>\n> > #### 若预测值为 $\\hat y=kx$ ，真实值是 y，那么误差 $L=(\\hat y - y)^2$ ，误差L的值越小，说明 $\\hat y$ 和 y 越接近\n> >\n> > $$\n> > L=(\\hat y - y)^2 = (kx - y)^2 = k^2x^2 -2kxy + y^2\n> > $$\n> >\n> > #### 要使L值最小，使用梯度下降法来求解，对k求偏导\n> >\n> > $$\n> > \\frac {\\partial L}{\\partial k} = 2kx^2 - 2xy\n> > $$\n> >\n> > #### 已知一组数据(x,y)=(2,4)，(x,y)=(4,8)，假设k1=1（初始值），那么带入上面的偏导中，求得偏导值（k的梯度）为-8\n> >\n> > #### 然后更新k的值，$ k = k1 - \\frac {\\partial L}{\\partial k}=1-(-8)=9 $ ，可以看到更新后的k值确实有向着期望的值靠近，但是“靠的太近了”，为此，引入一个新的因子——学习率\n> >\n> > #### 学习率（learning rate），一般设为0.1、0.01，若这里的lr=0.1，则有$ k = k1 - \\frac {\\partial L}{\\partial k} \\cdot lr=1-(-8) \\cdot 0.1=1.8 $ \n> >\n> > #### 通过不断地训练，k会逼近真实值，训练所要得到的就是损失函数取最小值时对应的k值\n>\n> #### 判断一个神经网络能不能用来训练，就要看所求解的问题对应的表达式能否求导\n\n\n\n#### 神经网络的表示形式\n\n> #### 一般的形式写作 $y = Wx + b$ ，还可以是复杂一点的 $y=W_1(W_2x+b2)+b_1$ ，对于复杂形式的求偏导采用链式法则\n>\n> #### 事实上，形如 $y = Wx + b$ 、$y=W_1(W_2x+b2)+b_1$ 、$y=W_1(W_2(W_3x+b_3)+b2)+b_1$ 等形式，其表达能力都是有限的（对应的函数都是凸函数），其函数图像都是形如 $y=x^2$ 或 $y=x^3$ 的图像，其表达能力（函数图像的复杂程度）还不够多样化，为了“打破”其凸的性质，使其更具多样性（表达能力更强）\n>\n> #### 由此引入了一个新的概念——激活函数\n>\n> #### 常见的激活函数，sigmoid、ReLU、softmax、tanh等，它破坏了 $y=Wx+b$ 的线性，使之不再是凸函数\n>\n> #### 先是用矩阵乘法来表达信息，然后使用激活函数来破坏线性，最后用一些算法求出近似的参数\n\n\n\n### 实际处理过程\n\n> $$\n> y_1 = W_1 \\cdot x + b_1 \\\\\n> y_2 = W_2 \\cdot y_1 + b_2 \\\\\n> y_3 = sigmoid(y_2) \\\\\n> L = (y_3 - y)^2 \\\\\n> $$\n>\n> #### 反向传播\n>\n> $$\n> \\begin{align}\n> \\frac {\\partial L}{\\partial W_1} & = \\frac {\\partial L}{\\partial y_3} \\cdot \\frac {\\partial y_3}{\\partial W_1} \\\\\n> & = \\frac {\\partial L}{\\partial y_3} \\cdot \\frac {\\partial y_3}{\\partial y_2} \\cdot \\frac {\\partial y_2}{\\partial y_1} \\cdot \\frac {\\partial y_1} {\\partial W_1} \\\\\n> & = 2(y_3 - y) \\cdot \\frac {exp(-y_2)}{(1 + exp(-y_2))^2} \\cdot W_2 \\cdot x\n> \\end{align}\n> $$\n>\n> #### 其他参数的梯度也是按照这种方式来写\n>\n> $$\n> \\frac{\\partial L}{\\partial b_1} = \\frac {\\partial L}{\\partial y_3} \\cdot \\frac {\\partial y_3}{\\partial y_2} \\cdot \\frac {\\partial y_2}{\\partial y_1} \\cdot \\frac{\\partial y_1}{\\partial b_1}\n> $$\n>\n> #### 可以看到，前面部分的偏导是一模一样的，只有最后一项不同\n>\n> #### 所以说，反向传播，前一层（如第三层）的梯度计算需要传入后一层（如第四层）的梯度才能计算，以此类推\n>\n> \n\n","source":"_posts/25.md","raw":"---\ntitle: DL杂谈\ndata: 2025-07-12 15:24:00\nupdated: 2025-07-12 15:24:00\ntype: DL\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p25.png\ntags:\n  - DL\n  - Learning\n  - title-tattle\n---\n\n\n# 神经网络杂谈\n\n\n\n### 什么是神经网络\n\n> #### 神经网络跟函数一样，有输入、输出，本质上也是一种映射关系，当给出一个函数表达式和一些数据，如 $y = kx + b$ ，给出了一些点或者说数据 (1,2)、(2,4)、(3,6)、(4,8)，这几组数据可以轻易的求出k和b的值来\n>\n> #### 但当给出的数据很多时，求解k和b这两个参数的问题就会变得复杂，至少是不能一眼看出某种规律。另外，函数表达式越复杂，在数据量越多的情况下，更能找出与之尽可能拟合的函数，因为数据量越大，其某种规律就可能会越明显，求得的参数就会越接近真实值（精确解）\n>\n> #### 函数表达式越复杂，其对应的表达能力会更强，如 $y = ax^2 + bx + c$ ，显然，它涵盖了 $y = bx + c,a=0$ \n>\n> #### 求出函数表达式中的参数，也就完成了神经网络的搭建，通过训练来求参数\n\n\n\n#### 怎么求参数\n\n> #### 直接求出参数的值是不太现实的，因为但表达式足够复杂，参数足够多，参数的求解是极为复杂的\n>\n> #### 但通过求数值解，使这个解尽可能的逼近真实值，当两者的误差达到了所能接受的范围，就把这个数值解近似的作为真实值，如 $y=kx$ ，可直接将k指定一个值，如 $k=1,\\hat y=x$ 然后根据所给数据求数值解，然后计算误差 $L = (\\hat y - y)^2$ \n>\n> #### 而求数值解的一个经典算法就是梯度下降法（gradient decay）\n>\n> > #### 若预测值为 $\\hat y=kx$ ，真实值是 y，那么误差 $L=(\\hat y - y)^2$ ，误差L的值越小，说明 $\\hat y$ 和 y 越接近\n> >\n> > $$\n> > L=(\\hat y - y)^2 = (kx - y)^2 = k^2x^2 -2kxy + y^2\n> > $$\n> >\n> > #### 要使L值最小，使用梯度下降法来求解，对k求偏导\n> >\n> > $$\n> > \\frac {\\partial L}{\\partial k} = 2kx^2 - 2xy\n> > $$\n> >\n> > #### 已知一组数据(x,y)=(2,4)，(x,y)=(4,8)，假设k1=1（初始值），那么带入上面的偏导中，求得偏导值（k的梯度）为-8\n> >\n> > #### 然后更新k的值，$ k = k1 - \\frac {\\partial L}{\\partial k}=1-(-8)=9 $ ，可以看到更新后的k值确实有向着期望的值靠近，但是“靠的太近了”，为此，引入一个新的因子——学习率\n> >\n> > #### 学习率（learning rate），一般设为0.1、0.01，若这里的lr=0.1，则有$ k = k1 - \\frac {\\partial L}{\\partial k} \\cdot lr=1-(-8) \\cdot 0.1=1.8 $ \n> >\n> > #### 通过不断地训练，k会逼近真实值，训练所要得到的就是损失函数取最小值时对应的k值\n>\n> #### 判断一个神经网络能不能用来训练，就要看所求解的问题对应的表达式能否求导\n\n\n\n#### 神经网络的表示形式\n\n> #### 一般的形式写作 $y = Wx + b$ ，还可以是复杂一点的 $y=W_1(W_2x+b2)+b_1$ ，对于复杂形式的求偏导采用链式法则\n>\n> #### 事实上，形如 $y = Wx + b$ 、$y=W_1(W_2x+b2)+b_1$ 、$y=W_1(W_2(W_3x+b_3)+b2)+b_1$ 等形式，其表达能力都是有限的（对应的函数都是凸函数），其函数图像都是形如 $y=x^2$ 或 $y=x^3$ 的图像，其表达能力（函数图像的复杂程度）还不够多样化，为了“打破”其凸的性质，使其更具多样性（表达能力更强）\n>\n> #### 由此引入了一个新的概念——激活函数\n>\n> #### 常见的激活函数，sigmoid、ReLU、softmax、tanh等，它破坏了 $y=Wx+b$ 的线性，使之不再是凸函数\n>\n> #### 先是用矩阵乘法来表达信息，然后使用激活函数来破坏线性，最后用一些算法求出近似的参数\n\n\n\n### 实际处理过程\n\n> $$\n> y_1 = W_1 \\cdot x + b_1 \\\\\n> y_2 = W_2 \\cdot y_1 + b_2 \\\\\n> y_3 = sigmoid(y_2) \\\\\n> L = (y_3 - y)^2 \\\\\n> $$\n>\n> #### 反向传播\n>\n> $$\n> \\begin{align}\n> \\frac {\\partial L}{\\partial W_1} & = \\frac {\\partial L}{\\partial y_3} \\cdot \\frac {\\partial y_3}{\\partial W_1} \\\\\n> & = \\frac {\\partial L}{\\partial y_3} \\cdot \\frac {\\partial y_3}{\\partial y_2} \\cdot \\frac {\\partial y_2}{\\partial y_1} \\cdot \\frac {\\partial y_1} {\\partial W_1} \\\\\n> & = 2(y_3 - y) \\cdot \\frac {exp(-y_2)}{(1 + exp(-y_2))^2} \\cdot W_2 \\cdot x\n> \\end{align}\n> $$\n>\n> #### 其他参数的梯度也是按照这种方式来写\n>\n> $$\n> \\frac{\\partial L}{\\partial b_1} = \\frac {\\partial L}{\\partial y_3} \\cdot \\frac {\\partial y_3}{\\partial y_2} \\cdot \\frac {\\partial y_2}{\\partial y_1} \\cdot \\frac{\\partial y_1}{\\partial b_1}\n> $$\n>\n> #### 可以看到，前面部分的偏导是一模一样的，只有最后一项不同\n>\n> #### 所以说，反向传播，前一层（如第三层）的梯度计算需要传入后一层（如第四层）的梯度才能计算，以此类推\n>\n> \n\n","slug":"25","published":1,"date":"2025-07-12T07:23:41.812Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7crh002xiku486hch15q","content":"<h1 id=\"神经网络杂谈\"><a href=\"#神经网络杂谈\" class=\"headerlink\" title=\"神经网络杂谈\"></a>神经网络杂谈</h1><h3 id=\"什么是神经网络\"><a href=\"#什么是神经网络\" class=\"headerlink\" title=\"什么是神经网络\"></a>什么是神经网络</h3><blockquote>\n<h4 id=\"神经网络跟函数一样，有输入、输出，本质上也是一种映射关系，当给出一个函数表达式和一些数据，如-y-kx-b-，给出了一些点或者说数据-1-2-、-2-4-、-3-6-、-4-8-，这几组数据可以轻易的求出k和b的值来\"><a href=\"#神经网络跟函数一样，有输入、输出，本质上也是一种映射关系，当给出一个函数表达式和一些数据，如-y-kx-b-，给出了一些点或者说数据-1-2-、-2-4-、-3-6-、-4-8-，这几组数据可以轻易的求出k和b的值来\" class=\"headerlink\" title=\"神经网络跟函数一样，有输入、输出，本质上也是一种映射关系，当给出一个函数表达式和一些数据，如 $y &#x3D; kx + b$ ，给出了一些点或者说数据 (1,2)、(2,4)、(3,6)、(4,8)，这几组数据可以轻易的求出k和b的值来\"></a>神经网络跟函数一样，有输入、输出，本质上也是一种映射关系，当给出一个函数表达式和一些数据，如 $y &#x3D; kx + b$ ，给出了一些点或者说数据 (1,2)、(2,4)、(3,6)、(4,8)，这几组数据可以轻易的求出k和b的值来</h4><h4 id=\"但当给出的数据很多时，求解k和b这两个参数的问题就会变得复杂，至少是不能一眼看出某种规律。另外，函数表达式越复杂，在数据量越多的情况下，更能找出与之尽可能拟合的函数，因为数据量越大，其某种规律就可能会越明显，求得的参数就会越接近真实值（精确解）\"><a href=\"#但当给出的数据很多时，求解k和b这两个参数的问题就会变得复杂，至少是不能一眼看出某种规律。另外，函数表达式越复杂，在数据量越多的情况下，更能找出与之尽可能拟合的函数，因为数据量越大，其某种规律就可能会越明显，求得的参数就会越接近真实值（精确解）\" class=\"headerlink\" title=\"但当给出的数据很多时，求解k和b这两个参数的问题就会变得复杂，至少是不能一眼看出某种规律。另外，函数表达式越复杂，在数据量越多的情况下，更能找出与之尽可能拟合的函数，因为数据量越大，其某种规律就可能会越明显，求得的参数就会越接近真实值（精确解）\"></a>但当给出的数据很多时，求解k和b这两个参数的问题就会变得复杂，至少是不能一眼看出某种规律。另外，函数表达式越复杂，在数据量越多的情况下，更能找出与之尽可能拟合的函数，因为数据量越大，其某种规律就可能会越明显，求得的参数就会越接近真实值（精确解）</h4><h4 id=\"函数表达式越复杂，其对应的表达能力会更强，如-y-ax-2-bx-c-，显然，它涵盖了-y-bx-c-a-0\"><a href=\"#函数表达式越复杂，其对应的表达能力会更强，如-y-ax-2-bx-c-，显然，它涵盖了-y-bx-c-a-0\" class=\"headerlink\" title=\"函数表达式越复杂，其对应的表达能力会更强，如 $y &#x3D; ax^2 + bx + c$ ，显然，它涵盖了 $y &#x3D; bx + c,a&#x3D;0$\"></a>函数表达式越复杂，其对应的表达能力会更强，如 $y &#x3D; ax^2 + bx + c$ ，显然，它涵盖了 $y &#x3D; bx + c,a&#x3D;0$</h4><h4 id=\"求出函数表达式中的参数，也就完成了神经网络的搭建，通过训练来求参数\"><a href=\"#求出函数表达式中的参数，也就完成了神经网络的搭建，通过训练来求参数\" class=\"headerlink\" title=\"求出函数表达式中的参数，也就完成了神经网络的搭建，通过训练来求参数\"></a>求出函数表达式中的参数，也就完成了神经网络的搭建，通过训练来求参数</h4></blockquote>\n<h4 id=\"怎么求参数\"><a href=\"#怎么求参数\" class=\"headerlink\" title=\"怎么求参数\"></a>怎么求参数</h4><blockquote>\n<h4 id=\"直接求出参数的值是不太现实的，因为但表达式足够复杂，参数足够多，参数的求解是极为复杂的\"><a href=\"#直接求出参数的值是不太现实的，因为但表达式足够复杂，参数足够多，参数的求解是极为复杂的\" class=\"headerlink\" title=\"直接求出参数的值是不太现实的，因为但表达式足够复杂，参数足够多，参数的求解是极为复杂的\"></a>直接求出参数的值是不太现实的，因为但表达式足够复杂，参数足够多，参数的求解是极为复杂的</h4><h4 id=\"但通过求数值解，使这个解尽可能的逼近真实值，当两者的误差达到了所能接受的范围，就把这个数值解近似的作为真实值，如-y-kx-，可直接将k指定一个值，如-k-1-hat-y-x-然后根据所给数据求数值解，然后计算误差-L-hat-y-y-2\"><a href=\"#但通过求数值解，使这个解尽可能的逼近真实值，当两者的误差达到了所能接受的范围，就把这个数值解近似的作为真实值，如-y-kx-，可直接将k指定一个值，如-k-1-hat-y-x-然后根据所给数据求数值解，然后计算误差-L-hat-y-y-2\" class=\"headerlink\" title=\"但通过求数值解，使这个解尽可能的逼近真实值，当两者的误差达到了所能接受的范围，就把这个数值解近似的作为真实值，如 $y&#x3D;kx$ ，可直接将k指定一个值，如 $k&#x3D;1,\\hat y&#x3D;x$ 然后根据所给数据求数值解，然后计算误差 $L &#x3D; (\\hat y - y)^2$\"></a>但通过求数值解，使这个解尽可能的逼近真实值，当两者的误差达到了所能接受的范围，就把这个数值解近似的作为真实值，如 $y&#x3D;kx$ ，可直接将k指定一个值，如 $k&#x3D;1,\\hat y&#x3D;x$ 然后根据所给数据求数值解，然后计算误差 $L &#x3D; (\\hat y - y)^2$</h4><h4 id=\"而求数值解的一个经典算法就是梯度下降法（gradient-decay）\"><a href=\"#而求数值解的一个经典算法就是梯度下降法（gradient-decay）\" class=\"headerlink\" title=\"而求数值解的一个经典算法就是梯度下降法（gradient decay）\"></a>而求数值解的一个经典算法就是梯度下降法（gradient decay）</h4><blockquote>\n<h4 id=\"若预测值为-hat-y-kx-，真实值是-y，那么误差-L-hat-y-y-2-，误差L的值越小，说明-hat-y-和-y-越接近\"><a href=\"#若预测值为-hat-y-kx-，真实值是-y，那么误差-L-hat-y-y-2-，误差L的值越小，说明-hat-y-和-y-越接近\" class=\"headerlink\" title=\"若预测值为 $\\hat y&#x3D;kx$ ，真实值是 y，那么误差 $L&#x3D;(\\hat y - y)^2$ ，误差L的值越小，说明 $\\hat y$ 和 y 越接近\"></a>若预测值为 $\\hat y&#x3D;kx$ ，真实值是 y，那么误差 $L&#x3D;(\\hat y - y)^2$ ，误差L的值越小，说明 $\\hat y$ 和 y 越接近</h4><p>$$<br>L&#x3D;(\\hat y - y)^2 &#x3D; (kx - y)^2 &#x3D; k^2x^2 -2kxy + y^2<br>$$</p>\n<h4 id=\"要使L值最小，使用梯度下降法来求解，对k求偏导\"><a href=\"#要使L值最小，使用梯度下降法来求解，对k求偏导\" class=\"headerlink\" title=\"要使L值最小，使用梯度下降法来求解，对k求偏导\"></a>要使L值最小，使用梯度下降法来求解，对k求偏导</h4><p>$$<br>\\frac {\\partial L}{\\partial k} &#x3D; 2kx^2 - 2xy<br>$$</p>\n<h4 id=\"已知一组数据-x-y-2-4-，-x-y-4-8-，假设k1-1（初始值），那么带入上面的偏导中，求得偏导值（k的梯度）为-8\"><a href=\"#已知一组数据-x-y-2-4-，-x-y-4-8-，假设k1-1（初始值），那么带入上面的偏导中，求得偏导值（k的梯度）为-8\" class=\"headerlink\" title=\"已知一组数据(x,y)&#x3D;(2,4)，(x,y)&#x3D;(4,8)，假设k1&#x3D;1（初始值），那么带入上面的偏导中，求得偏导值（k的梯度）为-8\"></a>已知一组数据(x,y)&#x3D;(2,4)，(x,y)&#x3D;(4,8)，假设k1&#x3D;1（初始值），那么带入上面的偏导中，求得偏导值（k的梯度）为-8</h4><h4 id=\"然后更新k的值，-k-k1-frac-partial-L-partial-k-1-8-9-，可以看到更新后的k值确实有向着期望的值靠近，但是“靠的太近了”，为此，引入一个新的因子——学习率\"><a href=\"#然后更新k的值，-k-k1-frac-partial-L-partial-k-1-8-9-，可以看到更新后的k值确实有向着期望的值靠近，但是“靠的太近了”，为此，引入一个新的因子——学习率\" class=\"headerlink\" title=\"然后更新k的值，$ k &#x3D; k1 - \\frac {\\partial L}{\\partial k}&#x3D;1-(-8)&#x3D;9 $ ，可以看到更新后的k值确实有向着期望的值靠近，但是“靠的太近了”，为此，引入一个新的因子——学习率\"></a>然后更新k的值，$ k &#x3D; k1 - \\frac {\\partial L}{\\partial k}&#x3D;1-(-8)&#x3D;9 $ ，可以看到更新后的k值确实有向着期望的值靠近，但是“靠的太近了”，为此，引入一个新的因子——学习率</h4><h4 id=\"学习率（learning-rate），一般设为0-1、0-01，若这里的lr-0-1，则有-k-k1-frac-partial-L-partial-k-cdot-lr-1-8-cdot-0-1-1-8\"><a href=\"#学习率（learning-rate），一般设为0-1、0-01，若这里的lr-0-1，则有-k-k1-frac-partial-L-partial-k-cdot-lr-1-8-cdot-0-1-1-8\" class=\"headerlink\" title=\"学习率（learning rate），一般设为0.1、0.01，若这里的lr&#x3D;0.1，则有$ k &#x3D; k1 - \\frac {\\partial L}{\\partial k} \\cdot lr&#x3D;1-(-8) \\cdot 0.1&#x3D;1.8 $\"></a>学习率（learning rate），一般设为0.1、0.01，若这里的lr&#x3D;0.1，则有$ k &#x3D; k1 - \\frac {\\partial L}{\\partial k} \\cdot lr&#x3D;1-(-8) \\cdot 0.1&#x3D;1.8 $</h4><h4 id=\"通过不断地训练，k会逼近真实值，训练所要得到的就是损失函数取最小值时对应的k值\"><a href=\"#通过不断地训练，k会逼近真实值，训练所要得到的就是损失函数取最小值时对应的k值\" class=\"headerlink\" title=\"通过不断地训练，k会逼近真实值，训练所要得到的就是损失函数取最小值时对应的k值\"></a>通过不断地训练，k会逼近真实值，训练所要得到的就是损失函数取最小值时对应的k值</h4></blockquote>\n<h4 id=\"判断一个神经网络能不能用来训练，就要看所求解的问题对应的表达式能否求导\"><a href=\"#判断一个神经网络能不能用来训练，就要看所求解的问题对应的表达式能否求导\" class=\"headerlink\" title=\"判断一个神经网络能不能用来训练，就要看所求解的问题对应的表达式能否求导\"></a>判断一个神经网络能不能用来训练，就要看所求解的问题对应的表达式能否求导</h4></blockquote>\n<h4 id=\"神经网络的表示形式\"><a href=\"#神经网络的表示形式\" class=\"headerlink\" title=\"神经网络的表示形式\"></a>神经网络的表示形式</h4><blockquote>\n<h4 id=\"一般的形式写作-y-Wx-b-，还可以是复杂一点的-y-W-1-W-2x-b2-b-1-，对于复杂形式的求偏导采用链式法则\"><a href=\"#一般的形式写作-y-Wx-b-，还可以是复杂一点的-y-W-1-W-2x-b2-b-1-，对于复杂形式的求偏导采用链式法则\" class=\"headerlink\" title=\"一般的形式写作 $y &#x3D; Wx + b$ ，还可以是复杂一点的 $y&#x3D;W_1(W_2x+b2)+b_1$ ，对于复杂形式的求偏导采用链式法则\"></a>一般的形式写作 $y &#x3D; Wx + b$ ，还可以是复杂一点的 $y&#x3D;W_1(W_2x+b2)+b_1$ ，对于复杂形式的求偏导采用链式法则</h4><h4 id=\"事实上，形如-y-Wx-b-、-y-W-1-W-2x-b2-b-1-、-y-W-1-W-2-W-3x-b-3-b2-b-1-等形式，其表达能力都是有限的（对应的函数都是凸函数），其函数图像都是形如-y-x-2-或-y-x-3-的图像，其表达能力（函数图像的复杂程度）还不够多样化，为了“打破”其凸的性质，使其更具多样性（表达能力更强）\"><a href=\"#事实上，形如-y-Wx-b-、-y-W-1-W-2x-b2-b-1-、-y-W-1-W-2-W-3x-b-3-b2-b-1-等形式，其表达能力都是有限的（对应的函数都是凸函数），其函数图像都是形如-y-x-2-或-y-x-3-的图像，其表达能力（函数图像的复杂程度）还不够多样化，为了“打破”其凸的性质，使其更具多样性（表达能力更强）\" class=\"headerlink\" title=\"事实上，形如 $y &#x3D; Wx + b$ 、$y&#x3D;W_1(W_2x+b2)+b_1$ 、$y&#x3D;W_1(W_2(W_3x+b_3)+b2)+b_1$ 等形式，其表达能力都是有限的（对应的函数都是凸函数），其函数图像都是形如 $y&#x3D;x^2$ 或 $y&#x3D;x^3$ 的图像，其表达能力（函数图像的复杂程度）还不够多样化，为了“打破”其凸的性质，使其更具多样性（表达能力更强）\"></a>事实上，形如 $y &#x3D; Wx + b$ 、$y&#x3D;W_1(W_2x+b2)+b_1$ 、$y&#x3D;W_1(W_2(W_3x+b_3)+b2)+b_1$ 等形式，其表达能力都是有限的（对应的函数都是凸函数），其函数图像都是形如 $y&#x3D;x^2$ 或 $y&#x3D;x^3$ 的图像，其表达能力（函数图像的复杂程度）还不够多样化，为了“打破”其凸的性质，使其更具多样性（表达能力更强）</h4><h4 id=\"由此引入了一个新的概念——激活函数\"><a href=\"#由此引入了一个新的概念——激活函数\" class=\"headerlink\" title=\"由此引入了一个新的概念——激活函数\"></a>由此引入了一个新的概念——激活函数</h4><h4 id=\"常见的激活函数，sigmoid、ReLU、softmax、tanh等，它破坏了-y-Wx-b-的线性，使之不再是凸函数\"><a href=\"#常见的激活函数，sigmoid、ReLU、softmax、tanh等，它破坏了-y-Wx-b-的线性，使之不再是凸函数\" class=\"headerlink\" title=\"常见的激活函数，sigmoid、ReLU、softmax、tanh等，它破坏了 $y&#x3D;Wx+b$ 的线性，使之不再是凸函数\"></a>常见的激活函数，sigmoid、ReLU、softmax、tanh等，它破坏了 $y&#x3D;Wx+b$ 的线性，使之不再是凸函数</h4><h4 id=\"先是用矩阵乘法来表达信息，然后使用激活函数来破坏线性，最后用一些算法求出近似的参数\"><a href=\"#先是用矩阵乘法来表达信息，然后使用激活函数来破坏线性，最后用一些算法求出近似的参数\" class=\"headerlink\" title=\"先是用矩阵乘法来表达信息，然后使用激活函数来破坏线性，最后用一些算法求出近似的参数\"></a>先是用矩阵乘法来表达信息，然后使用激活函数来破坏线性，最后用一些算法求出近似的参数</h4></blockquote>\n<h3 id=\"实际处理过程\"><a href=\"#实际处理过程\" class=\"headerlink\" title=\"实际处理过程\"></a>实际处理过程</h3><blockquote>\n<p>$$<br>y_1 &#x3D; W_1 \\cdot x + b_1 \\<br>y_2 &#x3D; W_2 \\cdot y_1 + b_2 \\<br>y_3 &#x3D; sigmoid(y_2) \\<br>L &#x3D; (y_3 - y)^2 \\<br>$$</p>\n<h4 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h4><p>$$<br>\\begin{align}<br>\\frac {\\partial L}{\\partial W_1} &amp; &#x3D; \\frac {\\partial L}{\\partial y_3} \\cdot \\frac {\\partial y_3}{\\partial W_1} \\<br>&amp; &#x3D; \\frac {\\partial L}{\\partial y_3} \\cdot \\frac {\\partial y_3}{\\partial y_2} \\cdot \\frac {\\partial y_2}{\\partial y_1} \\cdot \\frac {\\partial y_1} {\\partial W_1} \\<br>&amp; &#x3D; 2(y_3 - y) \\cdot \\frac {exp(-y_2)}{(1 + exp(-y_2))^2} \\cdot W_2 \\cdot x<br>\\end{align}<br>$$</p>\n<h4 id=\"其他参数的梯度也是按照这种方式来写\"><a href=\"#其他参数的梯度也是按照这种方式来写\" class=\"headerlink\" title=\"其他参数的梯度也是按照这种方式来写\"></a>其他参数的梯度也是按照这种方式来写</h4><p>$$<br>\\frac{\\partial L}{\\partial b_1} &#x3D; \\frac {\\partial L}{\\partial y_3} \\cdot \\frac {\\partial y_3}{\\partial y_2} \\cdot \\frac {\\partial y_2}{\\partial y_1} \\cdot \\frac{\\partial y_1}{\\partial b_1}<br>$$</p>\n<h4 id=\"可以看到，前面部分的偏导是一模一样的，只有最后一项不同\"><a href=\"#可以看到，前面部分的偏导是一模一样的，只有最后一项不同\" class=\"headerlink\" title=\"可以看到，前面部分的偏导是一模一样的，只有最后一项不同\"></a>可以看到，前面部分的偏导是一模一样的，只有最后一项不同</h4><h4 id=\"所以说，反向传播，前一层（如第三层）的梯度计算需要传入后一层（如第四层）的梯度才能计算，以此类推\"><a href=\"#所以说，反向传播，前一层（如第三层）的梯度计算需要传入后一层（如第四层）的梯度才能计算，以此类推\" class=\"headerlink\" title=\"所以说，反向传播，前一层（如第三层）的梯度计算需要传入后一层（如第四层）的梯度才能计算，以此类推\"></a>所以说，反向传播，前一层（如第三层）的梯度计算需要传入后一层（如第四层）的梯度才能计算，以此类推</h4></blockquote>\n","cover_type":"img","excerpt":"","more":"<h1 id=\"神经网络杂谈\"><a href=\"#神经网络杂谈\" class=\"headerlink\" title=\"神经网络杂谈\"></a>神经网络杂谈</h1><h3 id=\"什么是神经网络\"><a href=\"#什么是神经网络\" class=\"headerlink\" title=\"什么是神经网络\"></a>什么是神经网络</h3><blockquote>\n<h4 id=\"神经网络跟函数一样，有输入、输出，本质上也是一种映射关系，当给出一个函数表达式和一些数据，如-y-kx-b-，给出了一些点或者说数据-1-2-、-2-4-、-3-6-、-4-8-，这几组数据可以轻易的求出k和b的值来\"><a href=\"#神经网络跟函数一样，有输入、输出，本质上也是一种映射关系，当给出一个函数表达式和一些数据，如-y-kx-b-，给出了一些点或者说数据-1-2-、-2-4-、-3-6-、-4-8-，这几组数据可以轻易的求出k和b的值来\" class=\"headerlink\" title=\"神经网络跟函数一样，有输入、输出，本质上也是一种映射关系，当给出一个函数表达式和一些数据，如 $y &#x3D; kx + b$ ，给出了一些点或者说数据 (1,2)、(2,4)、(3,6)、(4,8)，这几组数据可以轻易的求出k和b的值来\"></a>神经网络跟函数一样，有输入、输出，本质上也是一种映射关系，当给出一个函数表达式和一些数据，如 $y &#x3D; kx + b$ ，给出了一些点或者说数据 (1,2)、(2,4)、(3,6)、(4,8)，这几组数据可以轻易的求出k和b的值来</h4><h4 id=\"但当给出的数据很多时，求解k和b这两个参数的问题就会变得复杂，至少是不能一眼看出某种规律。另外，函数表达式越复杂，在数据量越多的情况下，更能找出与之尽可能拟合的函数，因为数据量越大，其某种规律就可能会越明显，求得的参数就会越接近真实值（精确解）\"><a href=\"#但当给出的数据很多时，求解k和b这两个参数的问题就会变得复杂，至少是不能一眼看出某种规律。另外，函数表达式越复杂，在数据量越多的情况下，更能找出与之尽可能拟合的函数，因为数据量越大，其某种规律就可能会越明显，求得的参数就会越接近真实值（精确解）\" class=\"headerlink\" title=\"但当给出的数据很多时，求解k和b这两个参数的问题就会变得复杂，至少是不能一眼看出某种规律。另外，函数表达式越复杂，在数据量越多的情况下，更能找出与之尽可能拟合的函数，因为数据量越大，其某种规律就可能会越明显，求得的参数就会越接近真实值（精确解）\"></a>但当给出的数据很多时，求解k和b这两个参数的问题就会变得复杂，至少是不能一眼看出某种规律。另外，函数表达式越复杂，在数据量越多的情况下，更能找出与之尽可能拟合的函数，因为数据量越大，其某种规律就可能会越明显，求得的参数就会越接近真实值（精确解）</h4><h4 id=\"函数表达式越复杂，其对应的表达能力会更强，如-y-ax-2-bx-c-，显然，它涵盖了-y-bx-c-a-0\"><a href=\"#函数表达式越复杂，其对应的表达能力会更强，如-y-ax-2-bx-c-，显然，它涵盖了-y-bx-c-a-0\" class=\"headerlink\" title=\"函数表达式越复杂，其对应的表达能力会更强，如 $y &#x3D; ax^2 + bx + c$ ，显然，它涵盖了 $y &#x3D; bx + c,a&#x3D;0$\"></a>函数表达式越复杂，其对应的表达能力会更强，如 $y &#x3D; ax^2 + bx + c$ ，显然，它涵盖了 $y &#x3D; bx + c,a&#x3D;0$</h4><h4 id=\"求出函数表达式中的参数，也就完成了神经网络的搭建，通过训练来求参数\"><a href=\"#求出函数表达式中的参数，也就完成了神经网络的搭建，通过训练来求参数\" class=\"headerlink\" title=\"求出函数表达式中的参数，也就完成了神经网络的搭建，通过训练来求参数\"></a>求出函数表达式中的参数，也就完成了神经网络的搭建，通过训练来求参数</h4></blockquote>\n<h4 id=\"怎么求参数\"><a href=\"#怎么求参数\" class=\"headerlink\" title=\"怎么求参数\"></a>怎么求参数</h4><blockquote>\n<h4 id=\"直接求出参数的值是不太现实的，因为但表达式足够复杂，参数足够多，参数的求解是极为复杂的\"><a href=\"#直接求出参数的值是不太现实的，因为但表达式足够复杂，参数足够多，参数的求解是极为复杂的\" class=\"headerlink\" title=\"直接求出参数的值是不太现实的，因为但表达式足够复杂，参数足够多，参数的求解是极为复杂的\"></a>直接求出参数的值是不太现实的，因为但表达式足够复杂，参数足够多，参数的求解是极为复杂的</h4><h4 id=\"但通过求数值解，使这个解尽可能的逼近真实值，当两者的误差达到了所能接受的范围，就把这个数值解近似的作为真实值，如-y-kx-，可直接将k指定一个值，如-k-1-hat-y-x-然后根据所给数据求数值解，然后计算误差-L-hat-y-y-2\"><a href=\"#但通过求数值解，使这个解尽可能的逼近真实值，当两者的误差达到了所能接受的范围，就把这个数值解近似的作为真实值，如-y-kx-，可直接将k指定一个值，如-k-1-hat-y-x-然后根据所给数据求数值解，然后计算误差-L-hat-y-y-2\" class=\"headerlink\" title=\"但通过求数值解，使这个解尽可能的逼近真实值，当两者的误差达到了所能接受的范围，就把这个数值解近似的作为真实值，如 $y&#x3D;kx$ ，可直接将k指定一个值，如 $k&#x3D;1,\\hat y&#x3D;x$ 然后根据所给数据求数值解，然后计算误差 $L &#x3D; (\\hat y - y)^2$\"></a>但通过求数值解，使这个解尽可能的逼近真实值，当两者的误差达到了所能接受的范围，就把这个数值解近似的作为真实值，如 $y&#x3D;kx$ ，可直接将k指定一个值，如 $k&#x3D;1,\\hat y&#x3D;x$ 然后根据所给数据求数值解，然后计算误差 $L &#x3D; (\\hat y - y)^2$</h4><h4 id=\"而求数值解的一个经典算法就是梯度下降法（gradient-decay）\"><a href=\"#而求数值解的一个经典算法就是梯度下降法（gradient-decay）\" class=\"headerlink\" title=\"而求数值解的一个经典算法就是梯度下降法（gradient decay）\"></a>而求数值解的一个经典算法就是梯度下降法（gradient decay）</h4><blockquote>\n<h4 id=\"若预测值为-hat-y-kx-，真实值是-y，那么误差-L-hat-y-y-2-，误差L的值越小，说明-hat-y-和-y-越接近\"><a href=\"#若预测值为-hat-y-kx-，真实值是-y，那么误差-L-hat-y-y-2-，误差L的值越小，说明-hat-y-和-y-越接近\" class=\"headerlink\" title=\"若预测值为 $\\hat y&#x3D;kx$ ，真实值是 y，那么误差 $L&#x3D;(\\hat y - y)^2$ ，误差L的值越小，说明 $\\hat y$ 和 y 越接近\"></a>若预测值为 $\\hat y&#x3D;kx$ ，真实值是 y，那么误差 $L&#x3D;(\\hat y - y)^2$ ，误差L的值越小，说明 $\\hat y$ 和 y 越接近</h4><p>$$<br>L&#x3D;(\\hat y - y)^2 &#x3D; (kx - y)^2 &#x3D; k^2x^2 -2kxy + y^2<br>$$</p>\n<h4 id=\"要使L值最小，使用梯度下降法来求解，对k求偏导\"><a href=\"#要使L值最小，使用梯度下降法来求解，对k求偏导\" class=\"headerlink\" title=\"要使L值最小，使用梯度下降法来求解，对k求偏导\"></a>要使L值最小，使用梯度下降法来求解，对k求偏导</h4><p>$$<br>\\frac {\\partial L}{\\partial k} &#x3D; 2kx^2 - 2xy<br>$$</p>\n<h4 id=\"已知一组数据-x-y-2-4-，-x-y-4-8-，假设k1-1（初始值），那么带入上面的偏导中，求得偏导值（k的梯度）为-8\"><a href=\"#已知一组数据-x-y-2-4-，-x-y-4-8-，假设k1-1（初始值），那么带入上面的偏导中，求得偏导值（k的梯度）为-8\" class=\"headerlink\" title=\"已知一组数据(x,y)&#x3D;(2,4)，(x,y)&#x3D;(4,8)，假设k1&#x3D;1（初始值），那么带入上面的偏导中，求得偏导值（k的梯度）为-8\"></a>已知一组数据(x,y)&#x3D;(2,4)，(x,y)&#x3D;(4,8)，假设k1&#x3D;1（初始值），那么带入上面的偏导中，求得偏导值（k的梯度）为-8</h4><h4 id=\"然后更新k的值，-k-k1-frac-partial-L-partial-k-1-8-9-，可以看到更新后的k值确实有向着期望的值靠近，但是“靠的太近了”，为此，引入一个新的因子——学习率\"><a href=\"#然后更新k的值，-k-k1-frac-partial-L-partial-k-1-8-9-，可以看到更新后的k值确实有向着期望的值靠近，但是“靠的太近了”，为此，引入一个新的因子——学习率\" class=\"headerlink\" title=\"然后更新k的值，$ k &#x3D; k1 - \\frac {\\partial L}{\\partial k}&#x3D;1-(-8)&#x3D;9 $ ，可以看到更新后的k值确实有向着期望的值靠近，但是“靠的太近了”，为此，引入一个新的因子——学习率\"></a>然后更新k的值，$ k &#x3D; k1 - \\frac {\\partial L}{\\partial k}&#x3D;1-(-8)&#x3D;9 $ ，可以看到更新后的k值确实有向着期望的值靠近，但是“靠的太近了”，为此，引入一个新的因子——学习率</h4><h4 id=\"学习率（learning-rate），一般设为0-1、0-01，若这里的lr-0-1，则有-k-k1-frac-partial-L-partial-k-cdot-lr-1-8-cdot-0-1-1-8\"><a href=\"#学习率（learning-rate），一般设为0-1、0-01，若这里的lr-0-1，则有-k-k1-frac-partial-L-partial-k-cdot-lr-1-8-cdot-0-1-1-8\" class=\"headerlink\" title=\"学习率（learning rate），一般设为0.1、0.01，若这里的lr&#x3D;0.1，则有$ k &#x3D; k1 - \\frac {\\partial L}{\\partial k} \\cdot lr&#x3D;1-(-8) \\cdot 0.1&#x3D;1.8 $\"></a>学习率（learning rate），一般设为0.1、0.01，若这里的lr&#x3D;0.1，则有$ k &#x3D; k1 - \\frac {\\partial L}{\\partial k} \\cdot lr&#x3D;1-(-8) \\cdot 0.1&#x3D;1.8 $</h4><h4 id=\"通过不断地训练，k会逼近真实值，训练所要得到的就是损失函数取最小值时对应的k值\"><a href=\"#通过不断地训练，k会逼近真实值，训练所要得到的就是损失函数取最小值时对应的k值\" class=\"headerlink\" title=\"通过不断地训练，k会逼近真实值，训练所要得到的就是损失函数取最小值时对应的k值\"></a>通过不断地训练，k会逼近真实值，训练所要得到的就是损失函数取最小值时对应的k值</h4></blockquote>\n<h4 id=\"判断一个神经网络能不能用来训练，就要看所求解的问题对应的表达式能否求导\"><a href=\"#判断一个神经网络能不能用来训练，就要看所求解的问题对应的表达式能否求导\" class=\"headerlink\" title=\"判断一个神经网络能不能用来训练，就要看所求解的问题对应的表达式能否求导\"></a>判断一个神经网络能不能用来训练，就要看所求解的问题对应的表达式能否求导</h4></blockquote>\n<h4 id=\"神经网络的表示形式\"><a href=\"#神经网络的表示形式\" class=\"headerlink\" title=\"神经网络的表示形式\"></a>神经网络的表示形式</h4><blockquote>\n<h4 id=\"一般的形式写作-y-Wx-b-，还可以是复杂一点的-y-W-1-W-2x-b2-b-1-，对于复杂形式的求偏导采用链式法则\"><a href=\"#一般的形式写作-y-Wx-b-，还可以是复杂一点的-y-W-1-W-2x-b2-b-1-，对于复杂形式的求偏导采用链式法则\" class=\"headerlink\" title=\"一般的形式写作 $y &#x3D; Wx + b$ ，还可以是复杂一点的 $y&#x3D;W_1(W_2x+b2)+b_1$ ，对于复杂形式的求偏导采用链式法则\"></a>一般的形式写作 $y &#x3D; Wx + b$ ，还可以是复杂一点的 $y&#x3D;W_1(W_2x+b2)+b_1$ ，对于复杂形式的求偏导采用链式法则</h4><h4 id=\"事实上，形如-y-Wx-b-、-y-W-1-W-2x-b2-b-1-、-y-W-1-W-2-W-3x-b-3-b2-b-1-等形式，其表达能力都是有限的（对应的函数都是凸函数），其函数图像都是形如-y-x-2-或-y-x-3-的图像，其表达能力（函数图像的复杂程度）还不够多样化，为了“打破”其凸的性质，使其更具多样性（表达能力更强）\"><a href=\"#事实上，形如-y-Wx-b-、-y-W-1-W-2x-b2-b-1-、-y-W-1-W-2-W-3x-b-3-b2-b-1-等形式，其表达能力都是有限的（对应的函数都是凸函数），其函数图像都是形如-y-x-2-或-y-x-3-的图像，其表达能力（函数图像的复杂程度）还不够多样化，为了“打破”其凸的性质，使其更具多样性（表达能力更强）\" class=\"headerlink\" title=\"事实上，形如 $y &#x3D; Wx + b$ 、$y&#x3D;W_1(W_2x+b2)+b_1$ 、$y&#x3D;W_1(W_2(W_3x+b_3)+b2)+b_1$ 等形式，其表达能力都是有限的（对应的函数都是凸函数），其函数图像都是形如 $y&#x3D;x^2$ 或 $y&#x3D;x^3$ 的图像，其表达能力（函数图像的复杂程度）还不够多样化，为了“打破”其凸的性质，使其更具多样性（表达能力更强）\"></a>事实上，形如 $y &#x3D; Wx + b$ 、$y&#x3D;W_1(W_2x+b2)+b_1$ 、$y&#x3D;W_1(W_2(W_3x+b_3)+b2)+b_1$ 等形式，其表达能力都是有限的（对应的函数都是凸函数），其函数图像都是形如 $y&#x3D;x^2$ 或 $y&#x3D;x^3$ 的图像，其表达能力（函数图像的复杂程度）还不够多样化，为了“打破”其凸的性质，使其更具多样性（表达能力更强）</h4><h4 id=\"由此引入了一个新的概念——激活函数\"><a href=\"#由此引入了一个新的概念——激活函数\" class=\"headerlink\" title=\"由此引入了一个新的概念——激活函数\"></a>由此引入了一个新的概念——激活函数</h4><h4 id=\"常见的激活函数，sigmoid、ReLU、softmax、tanh等，它破坏了-y-Wx-b-的线性，使之不再是凸函数\"><a href=\"#常见的激活函数，sigmoid、ReLU、softmax、tanh等，它破坏了-y-Wx-b-的线性，使之不再是凸函数\" class=\"headerlink\" title=\"常见的激活函数，sigmoid、ReLU、softmax、tanh等，它破坏了 $y&#x3D;Wx+b$ 的线性，使之不再是凸函数\"></a>常见的激活函数，sigmoid、ReLU、softmax、tanh等，它破坏了 $y&#x3D;Wx+b$ 的线性，使之不再是凸函数</h4><h4 id=\"先是用矩阵乘法来表达信息，然后使用激活函数来破坏线性，最后用一些算法求出近似的参数\"><a href=\"#先是用矩阵乘法来表达信息，然后使用激活函数来破坏线性，最后用一些算法求出近似的参数\" class=\"headerlink\" title=\"先是用矩阵乘法来表达信息，然后使用激活函数来破坏线性，最后用一些算法求出近似的参数\"></a>先是用矩阵乘法来表达信息，然后使用激活函数来破坏线性，最后用一些算法求出近似的参数</h4></blockquote>\n<h3 id=\"实际处理过程\"><a href=\"#实际处理过程\" class=\"headerlink\" title=\"实际处理过程\"></a>实际处理过程</h3><blockquote>\n<p>$$<br>y_1 &#x3D; W_1 \\cdot x + b_1 \\<br>y_2 &#x3D; W_2 \\cdot y_1 + b_2 \\<br>y_3 &#x3D; sigmoid(y_2) \\<br>L &#x3D; (y_3 - y)^2 \\<br>$$</p>\n<h4 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h4><p>$$<br>\\begin{align}<br>\\frac {\\partial L}{\\partial W_1} &amp; &#x3D; \\frac {\\partial L}{\\partial y_3} \\cdot \\frac {\\partial y_3}{\\partial W_1} \\<br>&amp; &#x3D; \\frac {\\partial L}{\\partial y_3} \\cdot \\frac {\\partial y_3}{\\partial y_2} \\cdot \\frac {\\partial y_2}{\\partial y_1} \\cdot \\frac {\\partial y_1} {\\partial W_1} \\<br>&amp; &#x3D; 2(y_3 - y) \\cdot \\frac {exp(-y_2)}{(1 + exp(-y_2))^2} \\cdot W_2 \\cdot x<br>\\end{align}<br>$$</p>\n<h4 id=\"其他参数的梯度也是按照这种方式来写\"><a href=\"#其他参数的梯度也是按照这种方式来写\" class=\"headerlink\" title=\"其他参数的梯度也是按照这种方式来写\"></a>其他参数的梯度也是按照这种方式来写</h4><p>$$<br>\\frac{\\partial L}{\\partial b_1} &#x3D; \\frac {\\partial L}{\\partial y_3} \\cdot \\frac {\\partial y_3}{\\partial y_2} \\cdot \\frac {\\partial y_2}{\\partial y_1} \\cdot \\frac{\\partial y_1}{\\partial b_1}<br>$$</p>\n<h4 id=\"可以看到，前面部分的偏导是一模一样的，只有最后一项不同\"><a href=\"#可以看到，前面部分的偏导是一模一样的，只有最后一项不同\" class=\"headerlink\" title=\"可以看到，前面部分的偏导是一模一样的，只有最后一项不同\"></a>可以看到，前面部分的偏导是一模一样的，只有最后一项不同</h4><h4 id=\"所以说，反向传播，前一层（如第三层）的梯度计算需要传入后一层（如第四层）的梯度才能计算，以此类推\"><a href=\"#所以说，反向传播，前一层（如第三层）的梯度计算需要传入后一层（如第四层）的梯度才能计算，以此类推\" class=\"headerlink\" title=\"所以说，反向传播，前一层（如第三层）的梯度计算需要传入后一层（如第四层）的梯度才能计算，以此类推\"></a>所以说，反向传播，前一层（如第三层）的梯度计算需要传入后一层（如第四层）的梯度才能计算，以此类推</h4></blockquote>\n"},{"title":"看论文---第一弹","data":"2025-05-24T01:50:00.000Z","updated":"2025-05-26T12:08:00.000Z","type":"RL","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p17.png","_content":"\n# Goal-Driven Autonomous Exploration Through Deep Reinforcement  Learning---通过深度强化学习实现目标驱动的自主探索\n\n\n\n### 摘要\n\n> > #### 本文提出了一种基于深度强化学习（DRL）的自主导航系统，用于在未知环境中进行目标驱动的探索。系统从环境中获取可能的导航方向的兴趣点（POI），并基于可用数据选择一个最优的航路点。机器人根据这些航路点被引导向全局目标，缓解了反应式导航中的局部最优问题。随后，在仿真环境中通过DRL框架学习用于局部导航的运动策略。我们开发了一个导航系统，将该学习到的策略集成到运动规划系统中，作为局部导航层，用于驱动机器人在航路点之间移动，最终到达全局目标。整个自主导航过程中无需任何先验知识，同时在机器人移动过程中会记录环境地图。实验表明，该方法在无需地图或先验信息的情况下，在复杂的静态和动态环境中相较于其他探索方法具有优势。\n>\n> #### 自主导航系统\n>\n> #### 用于通过DRL对未知环境进行目标驱动的探索\n>\n> #### 获取可能导航方向的兴趣点(POI)\n>\n> #### 根据可用数据选择最佳航路点\n>\n> #### 缓解反应式导航中的局部最优问题\n>\n> \n>\n> #### 采用的是TD3算法（双延迟深度确定性策略梯度）\n>\n> > #### 它是在DDPG算法的基础上进行的扩展\n> >\n> > #### DDPG（Deep Deterministic Policy Gradient）深度确定性策略梯度：它是一种同时学习Q函数和策略的方法，它使用非策略数据和贝尔曼方程来学习Q函数，并使用Q函数来学习策略\n> >\n> > #### 详细介绍：https://spinningup.openai.com/en/latest/algorithms/ddpg.html#background\n> >\n> > \n> >\n> > #### DDPG 对超参数和其他调优手段**非常敏感**，表现常常不稳定，它的一个常见失败模式是：所学习的 Q 函数会**严重高估 Q 值**，进而导致策略崩溃，因为策略会利用 Q 函数中的误差\n> >\n> > #### 而双延迟深度确定性策略梯度算法（TD3）引入三个关键技巧来解决DDPG的问题：\n> >\n> > > 1. #### 截断的双Q学习：TD3 同时学习两个 Q 函数（因此称为“双”Q），在计算贝尔曼误差损失函数中的目标值时，取两个Q值中较小的一个，以降低过估计的风险\n> > >\n> > > 2. #### 延迟的策略更新：TD3 中，策略网络（Actor）和目标网络的更新频率**低于 Q 函数**的更新频率，建议是，更新两次 Q 网络，只更新 **一次策略网络**，以提高训练稳定性\n> > >\n> > > 3. #### 目标策略平滑：TD3 在计算目标 Q 值时，会在目标动作上添加噪声，一般是高斯噪声，使得动作空间内的 Q 值更平滑，从而防止策略去“钻空子”利用 Q 函数中的误差\n> >\n> > #### 详细介绍：https://spinningup.openai.com/en/latest/algorithms/td3.html\n\n\n\n### 介绍\n\n> #### **完全自主的目标驱动探索**是一个包含两个方面的问题\n>\n> > #### 首先，探索机器人需要决策去哪一个方向，以最大可能性到达全局目标。在没有先验信息或无法观测到全局目标的情况下，系统需要直接从传感器数据中指示可能的导航方向。从这些兴趣点（POI）中，需要选择一个最优点作为航路点，以最优的方式引导机器人接近全局目标。\n> >\n> > #### 其次，在不依赖地图数据的前提下，还需要获得一种适用于不确定环境的机器人运动策略。\n> >\n> > #### 这就引入了DRL\n> >\n> > \n> >\n> > #### 但是DRL存在一个问题：DRL具有反应性特征且缺乏全局信息，其容易陷入局部最优问题尤其是在大规模导航任务中表现尤为明显。\n> >\n> > > #### DRL的反应性特征\n> > >\n> > > > #### DRL中的大多策略都是基于当前状态做出决策的，也即 reactive policy（反应式策略），也就是具有马尔可夫性\n> > > >\n> > > > #### 这会导致：\n> > > >\n> > > > - #### agent只依据当前观测（摄像头图像、雷达扫描）来决定或判断下一动作\n> > > >\n> > > > - #### 没有全局地图或任务结果的长期记忆，缺少更高层次的计划或决策能力\n> > > >\n> > > > - #### 动作的选择往往基于短期奖励最大化，而非长期全局最优\n> > > >\n> > > > #### 这种策略适用于快速反应、实时避障等场景\n> > >\n> > > \n> > >\n> > > #### 全局信息的缺乏\n> > >\n> > > > #### 大多数DRL系统的输入都是局部传感器数据（当前位置的摄像头图像、雷达扫描），因此DRL也只能感知局部环境。另外，强化学习RL本身就是一种局部策略的学习方法，它更适合解决的是马尔可夫决策过程MDP问题。还有就是，DRL的训练目标就是最大化累计奖励，而非构建认知地图。\n> > >\n> > > \n> > >\n> > > #### 局部最优问题\n> > >\n> > > > #### 系统在当前状态下选择了一个看似收益最大（或最短路径）的行为，但这个选择却可能阻碍了其通向真正全局最优解的路径。\n>\n> \n>\n> #### 本文提出一种完全自主的探索系统，用于在无需人工控制或环境先验信息的情况下实现导航至全局目标。\n>\n> #### 该系统从机器人周围的局部环境中提取**兴趣点（POI）**，对其进行评估，并从中选取一个作为航路点（waypoint）。\n>\n> #### 这些航路点用于引导基于深度强化学习（DRL）的运动策略朝向全局目标，从而缓解局部最优问题。\n>\n> #### 机器人依据该策略进行运动，无需对周围环境进行完整建图。\n>\n> \n>\n> #### POI\n>\n> > #### 这里所说的 POI 兴趣点，是指机器人环境中那些具有导航价值的特定点或位置，这些点通常是机器人决策“去哪儿”的候选目标。它们帮助机器人在未知环境里选择下一步行动方向，特别是在没有完整地图信息时。POI 可以作为导航路径的候选点，也即中间航路点waypoint。\n> >\n> > \n> >\n> > #### POI的确定\n> >\n> > > #### 激光读数间隙产生的POI：当连续两束激光雷达测距数值差距较大时，说明可能有一个开口或通道，这个位置就被视作一个 POI。\n> > >\n> > > #### 非数值激光读数产生的POI：激光雷达超出测量范围时返回非数值，代表远处有自由空间，这些边缘位置也被标记为兴趣点。\n>\n> \n>\n> #### 导航系统的实现：左侧展示了机器人的设置，中间和右侧分别可视化了全局导航与局部导航的组成部分及其数据流。\n>\n> ![](http://picbed.yanzu.tech/img/paper_read/1/1.jpg)\n\n\n\n### 实现\n\n> #### 导航结构分两部分：\n>\n> > #### 具有**最优航路点选择机制**的**全局导航与建图**模块\n> >\n> > #### 基于**深度强化学习**的**局部导航**模块\n>\n> \n>\n> #### 系统首先从环境中提取**兴趣点（POI）**，并依据设定的评估标准选择一个**最优航路点**。\n>\n> #### 在每一个导航步骤中，系统会将该航路点以**相对于机器人当前位置与朝向的极坐标形式**输入神经网络。\n>\n> #### 随后，网络基于当前的传感器数据计算并执行一个动作，以朝向该航路点运动。\n>\n> #### 在机器人沿着多个航路点逐步向全局目标移动的过程中，同时完成对环境的**地图构建（建图）**。\n>\n> \n>\n> ### A.全局导航\n>\n> > #### 为了使机器人能够朝向全局目标导航，必须从当前可用的兴趣点（POI）中选择**用于局部导航的中间航路点**。\n> >\n> > #### 机器人不仅需要被引导前往目标，还必须在行进过程中**探索周围环境**，以便在遇到死路时能够识别出可能的替代路径。\n> >\n> > #### 鉴于没有预先提供的环境信息，所有可能的 POI 必须从**机器人当前的周边环境中提取**，并**存储在内存中**以供后续使用。\n> >\n> > #### 如果在后续步骤中发现某个兴趣点（POI）位于障碍物附近，则该点会从内存中删除。\n> >\n> > #### 在机器人已经访问过的位置，激光雷达不会再提取新的 POI。\n> >\n> > #### 此外，如果某个 POI 被选为航路点但在若干步内始终无法到达，它也会被删除，并重新选择一个新的航路点。\n> >\n> > \n> >\n> > #### 获取新的POI的方法：就是上面提到的POI的确定\n> >\n> > \n> >\n> > #### 从当前环境中提取POI，蓝色圆圈表示通过相应方法提取的兴趣点，绿色圆圈表示当前的航路点。\n> >\n> > #### (a) 蓝色 POI 是通过激光雷达测距数据之间的间隙提取的；\n> >\n> > #### (b)蓝色 POI 是从非数值型的激光读数中提取的。\n> >\n> > ![](http://picbed.yanzu.tech/img/paper_read/1/2.jpg)\n> >\n> > \n> >\n> > #### 在时刻 *t*，从当前可用的兴趣点（POI）中，使用**基于信息的距离受限探索方法**（Information-based Distance Limited Exploration，简称 **IDLE**）来选择最优的航路点。IDLE 方法通过以下方式评估每个候选 POI 的适应度：\n> >\n> > $$\n> > h(c_i) = \\tanh\\left(\\frac{e^{\\left(\\frac{d(p_t, c_i)}{l_2 - l_1}\\right)^2}}{e^{\\left(\\frac{l_2}{l_2 - l_1}\\right)^2}}\\right) l_2 + d(c_i, g) + e^{I_{i,t}}\n> > \\tag{1}\n> > $$\n> >\n> > #### 每个候选兴趣点 c (索引为 i) 的得分 h 由三部分组成\n> >\n> > #### 其中，机器人在时刻 t 的位置 p_t 与候选兴趣点 *c_i* 之间的欧几里得距离分量 d(p_t, c_i) 被表示为一个双曲正切函数（tanh）形式：\n> >\n> > $$\n> > \\tanh\\left(\\frac{e^{\\left(\\frac{d(p_t, c_i)}{l_2 - l_1}\\right)^2}}{e^{\\left(\\frac{l_2}{l_2 - l_1}\\right)^2}}\\right) l_2\n> > \\tag{2}\n> > $$\n> >\n> > #### 其中，*e* 是自然对数的底（欧拉数），*l_1* 和 *l_2* 是两个距离阈值，用于对得分进行衰减。这两个距离阈值根据深度强化学习（DRL）训练环境的区域大小设定。 这部分是一个距离惩罚项\n> >\n> > #### 注：\n> >\n> > > #### 分子中指数部分的 d(p_t,c_i)/(l_2 - l_1) 这是在对距离 d 做归一化处理，把距离尺度转化为无单位的比值，l_2 - l_1 是距离窗口，也就是期望 d 落在这个区间里，这样做的目的是让距离 d 的大小相对于“允许的距离范围” （l_2 - l_1） 来表达，而不是绝对距离大小，方便在指数和平方计算中保持数值的合理范围。\n> > >\n> > > #### l_2 / (l_2 - l_1) 是对 l_2 进行归一化处理，这个值确定了指数表达式的“最大值”或者“参考值”，它让指数中的分子和分母在相同的尺度下进行比较，实现对距离得分的合理缩放和归一化。本质上也是实现了e^x / e^y 这个分式的归一化处理。\n> > >\n> > > #### 通过平方，可以让距离差异更敏感，远离阈值的点权重变化更大。\n> > >\n> > > #### 通过指数放大这个变化，距离较远或较近的点的得分迅速减小或增大，达到对距离分量的**平滑衰减**效果。\n> > >\n> > > #### 这种平滑衰减避免了距离直接线性权重导致的极端值影响，使得分分布更合理和稳定。\n> > >\n> > > #### 使用双曲正切函数 tanh() 将指数结果映射到 (0, 1) 之间，起到非线性缩放和平滑归一化的作用。\n> > >\n> > > #### 最后乘以 l_2 对距离分量进行尺度调整，映射到 (0, l_2)之间\n> > >\n> > > #### 这一部分得到的值，是一个非线性压缩过的距离得分项，目的就是使距离处于 l_1~l_2 之间的候选兴趣点的得分平滑上升，有利于选点策略更稳定\n> >\n> > #### 第二个分量 d(c_i, g) 表示候选兴趣点 c_i 与全局目标点 g 之间的欧几里得距离（全局目标距离）。\n> >\n> > #### 最后，时刻 t 的地图信息得分（信息增益激励项）表示为：\n> >\n> > $$\n> > e^{I_{i,t}}\n> > \\tag{3}\n> > $$\n> >\n> > #### 其中，*I_{i,t}* 的计算方式如下：\n> >\n> > $$\n> > I_{i,t} = \\frac{\\sum\\limits_{w=-\\frac{k}{2}}^{\\frac{k}{2}} \\sum\\limits_{h=-\\frac{k}{2}}^{\\frac{k}{2}} C(x+w)(y+h)}{k^2}\n> > \\tag{4}\n> > $$\n> >\n> > #### 其中，k 表示用于计算候选点周围信息的卷积核大小，候选点的坐标为 x 和 y ，而 w 和 h 分别表示卷积核的宽度和高度。\n> >\n> > #### 在公式 (1) 中，具有最小 IDLE 得分的兴趣点（POI）被选为用于局部导航的最优航路点。\n> >\n> > #### 注：\n> >\n> > > #### I_{i,t} 表示候选兴趣点（POI）c_i 在时间 t 的**信息得分**，它用于衡量此点周围环境的“信息丰富度”或“探索潜力”，得分越高，说明此 POI 附近还有**未探索、未知或值得探索的区域**。\n> > >\n> > > #### C(x, y) 是地图上 (x, y) 点的**置信值或不确定度值**，可能来源于占据栅格地图中的熵值、不确定度、未知区域标记、置信概率等，通常，这个值越高，表示该位置的信息越少、越值得探索。\n> > >\n> > > #### 双重求和：它是在点 (x, y) 的周围取了一个大小为 k * k 的滑动窗口，然后对该窗口内所有位置的 C 值进行求和，也就是在以候选点为中心的邻域中，统计该邻域“信息量”。\n> > >\n> > > #### 除以 k^2 是做了一个**均值操作**，即把窗口内的信息总量归一化为平均信息值，得到的值落在 [0, 1]内，便于指数函数处理。\n> > >\n> > > #### 其目的就是鼓励探索未知区域\n> >\n> > \n> >\n> > #### 注意到，h 的三个组成部分好像不能直接相加，第一部分得到的是一个得分值，第二部分是一个距离值，第三部分是一个信息增益激励值，貌似是不能直接相加的，但第一部分进行了一个 *l_2 的操作，是一个伪距离，最后一部分也进行了指数化操作，所以是可以相加的\n>\n> \n>\n> ### B.局部导航\n>\n> > #### 使用基于 双延迟深度确定性策略梯度(TD3，一种 Actor-Critic 架构)的神经网络架构来训练运动策略\n> >\n> > #### 局部环境信息和目标航点相对于 agent 位置的极坐标一起，作为状态输入 s 传入 TD3 的Actor 网络中\n> >\n> > #### 该 Actor 网络由两个**全连接（FC）层**组成，每一层后面都接有 **ReLU（修正线性单元）激活函数**。\n> >\n> > #### 最后一层与输出层相连，输出两个动作参数 a，分别表示机器人的线速度 a_1 和角速度 a_2\n> >\n> > #### 输出层采用 **tanh 激活函数**，将输出限制在区间 (−1,1) 内\n> >\n> > #### 在将动作应用于环境之前，它们会按以下方式缩放为实际速度值：\n> >\n> >\n> > $$\n> > a = \\left[ v_{\\max} \\left( \\frac{a_1 + 1}{2} \\right), \\omega_{\\max} a_2 \\right],\n> > \\tag{5}\n> > $$\n> >\n> > #### 最大线速度 v_max，最大角速度 ω_max\n> >\n> > #### 由于激光雷达只记录机器人前方的数据，因此**不考虑向后的运动**，并将**线速度调整为仅为正值**。\n> >\n> > \n> >\n> > #### 状态-动作对的 Q 值 Q(s,a) 由两个 **Critic 网络**进行评估。这两个 Critic 网络具有相同的结构，但其参数更新是**延迟进行**的，从而允许它们在参数上产生差异（避免完全同步）。\n> >\n> > #### Critic 网络以状态 s 和动作 a 的组合作为输入\n> >\n> > #### 其中，状态 s 首先被送入一个**全连接层**，并接上一个 ReLU 激活函数，输出为 L_s\n> >\n> > #### 该层的输出 L_s 以及动作 a，将分别送入两个大小相同的变换全连接层（TFC）中，分别对应变换结果为 τ1 和 τ2，随后，这两个结果按如下方式进行组合：\n> >\n> > $$\n> > L_c = L_sW_{\\tau_1} + aW_{\\tau_2} + b_{\\tau_2},\n> > \\tag{6}\n> > $$\n> >\n> > #### 其中，L_c 是组合全连接层（CFC）的输出，W_τ_1 和 W_τ_2 分别是 τ1 和 τ2 的权重，b_τ_2 是 τ_2 的偏执项，然后在这个组合层上使用ReLU激活函数，接着，该输出连接到一个最终输出节点，该节点包含**一个参数**，表示对应状态-动作对的 **Q 值**。最终，从两个 Critic 网络中**选择较小的 Q 值**，作为最后的 Critic 输出，以此来**限制对状态-动作值的过高估计**。\n> >\n> > #### 完整的网络架构如图\n> >\n> > ![](http://picbed.yanzu.tech/img/paper_read/1/3.jpg)\n> >\n> > #### TD3 网络结构包括 actor 和 critic 两个部分。每一层的类型以及其对应的参数数量在层中都有描述。TFC 层指的是变换全连接层 τ，CFC 层指的是组合全连接层 Lc。\n> >\n> > #### 策略的奖励依据以下函数进行评估(奖励函数)\n> >\n> > $$\n> > r(s_t, a_t) = \n> > \\begin{cases} \n> > r_g & \\text{if } D_t < \\eta D \\\\\n> > r_c & \\text{if collision} \\\\\n> > v - |\\omega| & \\text{otherwise},\n> > \\end{cases},\n> > \\tag{7}\n> > $$\n> >\n> > #### 在时间步 t时，状态-动作对 (s_t, a_t) 的奖励 r 取决于以下三种情况： \n> >\n> > - #### 如果当前时间步与目标点的距离 D_t 小于阈值 η_D，则给予一个正的目标奖励 r_g（也就是鼓励）\n> >\n> > - #### 如果检测到碰撞，则给予一个负的碰撞惩罚 r_c（惩罚）\n> >\n> > - #### 如果以上两种情况均未发生，则根据当前的线速度 v 和角速度 ω 给予即时奖励。\n> >\n> > \n> >\n> > #### 为了引导导航策略朝向给定目标，一个延迟归因奖励方法被采用，其计算如下：\n> >\n> > $$\n> > r_{t-i} = r(s_{t-i}, a_{t-i}) + \\frac{r_g}{i}, \\quad \\forall i \\in \\{1, 2, 3, \\ldots, n\\},\n> > \\tag{8}\n> > $$\n> >\n> > #### 其中，n 表示前 n 个时间步中需要更新奖励的步数。这意味着，正的目标奖励不仅被分配给达到目标的那个状态-动作对，还以递减的方式分配给其之前的 n 个时间步。通过这种方式，网络学习到了一种局部导航策略，该策略能够仅依赖激光输入，避开障碍物的同时到达局部目标。\n>\n> \n>\n> ### C.探索与建图\n>\n> > #### 机器人沿着路径点被引导向全局目标前进。一旦靠近全局目标，机器人将自主导航至该目标。在此过程中，环境被逐步探索和建图。建图依赖于激光雷达和机器人里程计传感器数据，生成环境的占据栅格地图。完整的自主探索与建图算法的伪代码在算法1中进行了描述。\n\n\n\n### 实验部分\n\n> #### 算法过程：\n>\n> > ```\n> > # 输入参数\n> > global_goal\t\t# 全局目标点，即最终导航的目标位置\n> > δ\t\t\t# 导航至全局目标的距离阈值,用于判断机器人是否“足够接近”目标点\n> > \n> > # 主循环 直到达到全局目标\n> > while(reached_global_goal != True){\t# 判断是否已达到全局目标\n> > \n> > \tread sensor data # 读取传感器数据：如激光雷达、相机、里程计等\n> > \t\n> > \tupdate map from sensor data # 根据传感器数据更新地图：构建或完善占据栅格地图\n> > \t\n> > \tObtain new POI # 获取新的兴趣点 POI,可能是探索边界或未知区域的候选目标点\n> > \t\n> > \t# 判断当前是否已接近目标区域\n> > \tif (D_t < δ_D){\t# 如果 agent与目标的距离处于接近目标区域\n> > \t\n> > \t\tif(waypoint = global_goal)\t# 若当前导航的子目标 waypoint 已经是 global_goal\n> > \t\t\treachedGlobalGoal = True\t# 那么任务完成\n> > \t\t\t\n> > \t\telse{\t# 否则，进一步判断当前是否靠近全局目标\n> > \t\t\n> > \t\t\tif(d(p_t, g) < δ)\t# 如果当前位置p_t与目标g的距离d(p_t,g) 小于 δ\n> > \t\t\t\twaypoint <-- global_goal\t# 那么就把当前的 waypoint 设置为 global_goal\n> > \t\t\t\t\n> > \t\t\telse\t# 否则，从所有兴趣点中选择下一个最优子目标点\n> > \t\t\t\tfor i in POI\t# 遍历POI中所有的兴趣点\n> > \t\t\t\t\tcaculate h(i) from (1) # 根据式子(1)计算每个兴趣点的启发值h(i)\n> > \t\t\t\twaypoint <-- POI_min(h(i))\t# 将h(i)值最小对应的兴趣点作为新的 waypoint\n> > \t\t\tend if\n> > \t\tend if\n> > \tend if\n> > \t\n> > \tObtain an action from TD3\t# 从 TD3 策略网络中获取当前动作,利用强化学习模型TD3预测最优动作\n> > \tPerform action\t# 执行该动作\n> > end while\n> > ```\n> >\n> > #### \n>\n> \n>\n> ### A.系统设置\n>\n> > #### 原作者系统配置：\n> >\n> > > #### 显卡：NVIDIA GTX 1080\n> > >\n> > > #### 运行内存：32G\n> > >\n> > > #### CPU： Intel Core i7-6800K \n> >\n> > #### 训练参数设置\n> >\n> > > #### TD3网络在Gazebo上进行训练，并通过ROS进行控制，训练了800回合，约8h\n> > >\n> > > #### 每个训练回合在机器人到达目标、发生碰撞或执行了 500 步动作后结束\n> > >\n> > > #### 最大线速度 v_max 和最大角速度 ω_max 分别设置为 0.5 m/s 和 1rad/s\n> > >\n> > > #### 延迟奖励在最后 n=10 步中更新，参数更新延迟设置为每 2 个回合\n> >\n> > \n> >\n> > #### 训练在一个 10x10 米的模拟环境中进行，如图所示\n> >\n> > ![](http://picbed.yanzu.tech/img/paper_read/1/4.jpg)\n> >\n> > #### 训练环境示例。蓝色区域表示输入的激光雷达读数和测距。四个箱形障碍物在每一轮训练中都会改变位置，如图(a)、(b)和(c)所示，以实现训练数据的随机化。\n> >\n> > #### 为促进策略的泛化与探索，向传感器读数和动作值中添加了高斯噪声。为了构建多样化的训练环境，在每个回合开始时，箱形障碍物的位置会被随机化。其位置变化的示例如图 (a)、(b)、(c) 所示。同时，机器人的起始位置与目标位置也在每一回合中被随机设置。\n> >\n> > \n> >\n> > #### ROS 中的 SLAM Toolbox 软件包用于生成和更新环境的全局地图，并用于机器人在地图中的定位\n> >\n> > #### ROS的本地规划器包（TrajectoryPlanner）代替了神经网络\n> >\n> > \n> >\n> > #### 目标驱动自主探索（GDAE, Goal-Driven Autonomous Exploration）\n> >\n> > #### 最近前沿探索策略（Nearest Frontier, NF）\n> >\n> > #### 目标驱动强化学习（GD-RL, Goal-Driven Reinforcement Learning）\n> >\n> > #### 本地规划器自主探索（LP-AE，Local Planner Autonomous Exploration）\n> >\n> > #### 路径规划器（PP，Path Planner）它是基于 Dijkstra 的生成路径的方法\n>\n> \n>\n> ### B.定量实验\n>\n> \n>\n> ### C.定性实验\n>\n> \n\n\n\n### 结论\n\n> #### 基于深度强化学习（DRL）的目标驱动型全自主探索系统（GDAE）\n>\n> #### 无需人工干预，即可导航至指定目标并记录环境信息（导航去目标点的过程中建图）\n>\n> #### 系统有效结合了反应式的本地导航策略和全局导航策略\n>\n> #### 将神经网络模块引入端到端系统：机器人无需生成显式路径即可移动，而这一机制的不足则通过引入全局导航策略得到了弥补\n>\n> #### 系统的导航性能接近于基于已知地图路径规划器所得的最优解\n>\n> #### GDAE 系统依赖**直接的传感器输入**而非从不确定地图生成路径，因此在可靠性方面表现更佳\n>\n> #### 若希望进一步**泛化至不同类型的机器人**，可以将**机器人动力学作为神经网络的一个输入状态**，并据此开展训练。只需提供机器人动力学信息，即可在不同平台上实现最优的本地导航。\n>\n> \n>\n> #### 接下来的研究：\n>\n> #### 引入**长短时记忆（LSTM）结构**也可能有助于缓解局部最优问题，并辅助规避当前传感器视野之外的障碍物\n\n\n\n### 代码部分\n\n> #### 两个量的定义\n>\n> #### Episode：\n>\n> > #### 它是后续步骤的集合，直到达到其中一个终止条件（终止条件：达到目标、与障碍物碰撞或达到最大步数max_ep）\n> >\n> > #### 它可以理解是一个训练过程，如果达到终止条件就开始新的一个训练过程，并且伴随着环境的改变，包括区域内的墙体位置改变、障碍物的位置改变、agent的初始位置改变和目标点位置的改变\n>\n> #### Epoch：\n>\n> > #### 执行评估之间的后续事件数（episode）或者时间步长（timesteps）\n> >\n> > #### 一个epoch运行5000个步骤step（对应代码中的 eval_freq 这个参数），步长为0.1秒，也就是说一个epoch大概要运行8分多钟\n>\n> ","source":"_posts/17.md","raw":"---\ntitle: 看论文---第一弹\ndata: 2025-05-24 09:50:00\nupdated: 2025-05-26 20:08:00\ntype: RL\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p17.png\ntags:\n  - RL\n  - Learning\n  - paper-reading\n  - ROS1\n---\n\n# Goal-Driven Autonomous Exploration Through Deep Reinforcement  Learning---通过深度强化学习实现目标驱动的自主探索\n\n\n\n### 摘要\n\n> > #### 本文提出了一种基于深度强化学习（DRL）的自主导航系统，用于在未知环境中进行目标驱动的探索。系统从环境中获取可能的导航方向的兴趣点（POI），并基于可用数据选择一个最优的航路点。机器人根据这些航路点被引导向全局目标，缓解了反应式导航中的局部最优问题。随后，在仿真环境中通过DRL框架学习用于局部导航的运动策略。我们开发了一个导航系统，将该学习到的策略集成到运动规划系统中，作为局部导航层，用于驱动机器人在航路点之间移动，最终到达全局目标。整个自主导航过程中无需任何先验知识，同时在机器人移动过程中会记录环境地图。实验表明，该方法在无需地图或先验信息的情况下，在复杂的静态和动态环境中相较于其他探索方法具有优势。\n>\n> #### 自主导航系统\n>\n> #### 用于通过DRL对未知环境进行目标驱动的探索\n>\n> #### 获取可能导航方向的兴趣点(POI)\n>\n> #### 根据可用数据选择最佳航路点\n>\n> #### 缓解反应式导航中的局部最优问题\n>\n> \n>\n> #### 采用的是TD3算法（双延迟深度确定性策略梯度）\n>\n> > #### 它是在DDPG算法的基础上进行的扩展\n> >\n> > #### DDPG（Deep Deterministic Policy Gradient）深度确定性策略梯度：它是一种同时学习Q函数和策略的方法，它使用非策略数据和贝尔曼方程来学习Q函数，并使用Q函数来学习策略\n> >\n> > #### 详细介绍：https://spinningup.openai.com/en/latest/algorithms/ddpg.html#background\n> >\n> > \n> >\n> > #### DDPG 对超参数和其他调优手段**非常敏感**，表现常常不稳定，它的一个常见失败模式是：所学习的 Q 函数会**严重高估 Q 值**，进而导致策略崩溃，因为策略会利用 Q 函数中的误差\n> >\n> > #### 而双延迟深度确定性策略梯度算法（TD3）引入三个关键技巧来解决DDPG的问题：\n> >\n> > > 1. #### 截断的双Q学习：TD3 同时学习两个 Q 函数（因此称为“双”Q），在计算贝尔曼误差损失函数中的目标值时，取两个Q值中较小的一个，以降低过估计的风险\n> > >\n> > > 2. #### 延迟的策略更新：TD3 中，策略网络（Actor）和目标网络的更新频率**低于 Q 函数**的更新频率，建议是，更新两次 Q 网络，只更新 **一次策略网络**，以提高训练稳定性\n> > >\n> > > 3. #### 目标策略平滑：TD3 在计算目标 Q 值时，会在目标动作上添加噪声，一般是高斯噪声，使得动作空间内的 Q 值更平滑，从而防止策略去“钻空子”利用 Q 函数中的误差\n> >\n> > #### 详细介绍：https://spinningup.openai.com/en/latest/algorithms/td3.html\n\n\n\n### 介绍\n\n> #### **完全自主的目标驱动探索**是一个包含两个方面的问题\n>\n> > #### 首先，探索机器人需要决策去哪一个方向，以最大可能性到达全局目标。在没有先验信息或无法观测到全局目标的情况下，系统需要直接从传感器数据中指示可能的导航方向。从这些兴趣点（POI）中，需要选择一个最优点作为航路点，以最优的方式引导机器人接近全局目标。\n> >\n> > #### 其次，在不依赖地图数据的前提下，还需要获得一种适用于不确定环境的机器人运动策略。\n> >\n> > #### 这就引入了DRL\n> >\n> > \n> >\n> > #### 但是DRL存在一个问题：DRL具有反应性特征且缺乏全局信息，其容易陷入局部最优问题尤其是在大规模导航任务中表现尤为明显。\n> >\n> > > #### DRL的反应性特征\n> > >\n> > > > #### DRL中的大多策略都是基于当前状态做出决策的，也即 reactive policy（反应式策略），也就是具有马尔可夫性\n> > > >\n> > > > #### 这会导致：\n> > > >\n> > > > - #### agent只依据当前观测（摄像头图像、雷达扫描）来决定或判断下一动作\n> > > >\n> > > > - #### 没有全局地图或任务结果的长期记忆，缺少更高层次的计划或决策能力\n> > > >\n> > > > - #### 动作的选择往往基于短期奖励最大化，而非长期全局最优\n> > > >\n> > > > #### 这种策略适用于快速反应、实时避障等场景\n> > >\n> > > \n> > >\n> > > #### 全局信息的缺乏\n> > >\n> > > > #### 大多数DRL系统的输入都是局部传感器数据（当前位置的摄像头图像、雷达扫描），因此DRL也只能感知局部环境。另外，强化学习RL本身就是一种局部策略的学习方法，它更适合解决的是马尔可夫决策过程MDP问题。还有就是，DRL的训练目标就是最大化累计奖励，而非构建认知地图。\n> > >\n> > > \n> > >\n> > > #### 局部最优问题\n> > >\n> > > > #### 系统在当前状态下选择了一个看似收益最大（或最短路径）的行为，但这个选择却可能阻碍了其通向真正全局最优解的路径。\n>\n> \n>\n> #### 本文提出一种完全自主的探索系统，用于在无需人工控制或环境先验信息的情况下实现导航至全局目标。\n>\n> #### 该系统从机器人周围的局部环境中提取**兴趣点（POI）**，对其进行评估，并从中选取一个作为航路点（waypoint）。\n>\n> #### 这些航路点用于引导基于深度强化学习（DRL）的运动策略朝向全局目标，从而缓解局部最优问题。\n>\n> #### 机器人依据该策略进行运动，无需对周围环境进行完整建图。\n>\n> \n>\n> #### POI\n>\n> > #### 这里所说的 POI 兴趣点，是指机器人环境中那些具有导航价值的特定点或位置，这些点通常是机器人决策“去哪儿”的候选目标。它们帮助机器人在未知环境里选择下一步行动方向，特别是在没有完整地图信息时。POI 可以作为导航路径的候选点，也即中间航路点waypoint。\n> >\n> > \n> >\n> > #### POI的确定\n> >\n> > > #### 激光读数间隙产生的POI：当连续两束激光雷达测距数值差距较大时，说明可能有一个开口或通道，这个位置就被视作一个 POI。\n> > >\n> > > #### 非数值激光读数产生的POI：激光雷达超出测量范围时返回非数值，代表远处有自由空间，这些边缘位置也被标记为兴趣点。\n>\n> \n>\n> #### 导航系统的实现：左侧展示了机器人的设置，中间和右侧分别可视化了全局导航与局部导航的组成部分及其数据流。\n>\n> ![](http://picbed.yanzu.tech/img/paper_read/1/1.jpg)\n\n\n\n### 实现\n\n> #### 导航结构分两部分：\n>\n> > #### 具有**最优航路点选择机制**的**全局导航与建图**模块\n> >\n> > #### 基于**深度强化学习**的**局部导航**模块\n>\n> \n>\n> #### 系统首先从环境中提取**兴趣点（POI）**，并依据设定的评估标准选择一个**最优航路点**。\n>\n> #### 在每一个导航步骤中，系统会将该航路点以**相对于机器人当前位置与朝向的极坐标形式**输入神经网络。\n>\n> #### 随后，网络基于当前的传感器数据计算并执行一个动作，以朝向该航路点运动。\n>\n> #### 在机器人沿着多个航路点逐步向全局目标移动的过程中，同时完成对环境的**地图构建（建图）**。\n>\n> \n>\n> ### A.全局导航\n>\n> > #### 为了使机器人能够朝向全局目标导航，必须从当前可用的兴趣点（POI）中选择**用于局部导航的中间航路点**。\n> >\n> > #### 机器人不仅需要被引导前往目标，还必须在行进过程中**探索周围环境**，以便在遇到死路时能够识别出可能的替代路径。\n> >\n> > #### 鉴于没有预先提供的环境信息，所有可能的 POI 必须从**机器人当前的周边环境中提取**，并**存储在内存中**以供后续使用。\n> >\n> > #### 如果在后续步骤中发现某个兴趣点（POI）位于障碍物附近，则该点会从内存中删除。\n> >\n> > #### 在机器人已经访问过的位置，激光雷达不会再提取新的 POI。\n> >\n> > #### 此外，如果某个 POI 被选为航路点但在若干步内始终无法到达，它也会被删除，并重新选择一个新的航路点。\n> >\n> > \n> >\n> > #### 获取新的POI的方法：就是上面提到的POI的确定\n> >\n> > \n> >\n> > #### 从当前环境中提取POI，蓝色圆圈表示通过相应方法提取的兴趣点，绿色圆圈表示当前的航路点。\n> >\n> > #### (a) 蓝色 POI 是通过激光雷达测距数据之间的间隙提取的；\n> >\n> > #### (b)蓝色 POI 是从非数值型的激光读数中提取的。\n> >\n> > ![](http://picbed.yanzu.tech/img/paper_read/1/2.jpg)\n> >\n> > \n> >\n> > #### 在时刻 *t*，从当前可用的兴趣点（POI）中，使用**基于信息的距离受限探索方法**（Information-based Distance Limited Exploration，简称 **IDLE**）来选择最优的航路点。IDLE 方法通过以下方式评估每个候选 POI 的适应度：\n> >\n> > $$\n> > h(c_i) = \\tanh\\left(\\frac{e^{\\left(\\frac{d(p_t, c_i)}{l_2 - l_1}\\right)^2}}{e^{\\left(\\frac{l_2}{l_2 - l_1}\\right)^2}}\\right) l_2 + d(c_i, g) + e^{I_{i,t}}\n> > \\tag{1}\n> > $$\n> >\n> > #### 每个候选兴趣点 c (索引为 i) 的得分 h 由三部分组成\n> >\n> > #### 其中，机器人在时刻 t 的位置 p_t 与候选兴趣点 *c_i* 之间的欧几里得距离分量 d(p_t, c_i) 被表示为一个双曲正切函数（tanh）形式：\n> >\n> > $$\n> > \\tanh\\left(\\frac{e^{\\left(\\frac{d(p_t, c_i)}{l_2 - l_1}\\right)^2}}{e^{\\left(\\frac{l_2}{l_2 - l_1}\\right)^2}}\\right) l_2\n> > \\tag{2}\n> > $$\n> >\n> > #### 其中，*e* 是自然对数的底（欧拉数），*l_1* 和 *l_2* 是两个距离阈值，用于对得分进行衰减。这两个距离阈值根据深度强化学习（DRL）训练环境的区域大小设定。 这部分是一个距离惩罚项\n> >\n> > #### 注：\n> >\n> > > #### 分子中指数部分的 d(p_t,c_i)/(l_2 - l_1) 这是在对距离 d 做归一化处理，把距离尺度转化为无单位的比值，l_2 - l_1 是距离窗口，也就是期望 d 落在这个区间里，这样做的目的是让距离 d 的大小相对于“允许的距离范围” （l_2 - l_1） 来表达，而不是绝对距离大小，方便在指数和平方计算中保持数值的合理范围。\n> > >\n> > > #### l_2 / (l_2 - l_1) 是对 l_2 进行归一化处理，这个值确定了指数表达式的“最大值”或者“参考值”，它让指数中的分子和分母在相同的尺度下进行比较，实现对距离得分的合理缩放和归一化。本质上也是实现了e^x / e^y 这个分式的归一化处理。\n> > >\n> > > #### 通过平方，可以让距离差异更敏感，远离阈值的点权重变化更大。\n> > >\n> > > #### 通过指数放大这个变化，距离较远或较近的点的得分迅速减小或增大，达到对距离分量的**平滑衰减**效果。\n> > >\n> > > #### 这种平滑衰减避免了距离直接线性权重导致的极端值影响，使得分分布更合理和稳定。\n> > >\n> > > #### 使用双曲正切函数 tanh() 将指数结果映射到 (0, 1) 之间，起到非线性缩放和平滑归一化的作用。\n> > >\n> > > #### 最后乘以 l_2 对距离分量进行尺度调整，映射到 (0, l_2)之间\n> > >\n> > > #### 这一部分得到的值，是一个非线性压缩过的距离得分项，目的就是使距离处于 l_1~l_2 之间的候选兴趣点的得分平滑上升，有利于选点策略更稳定\n> >\n> > #### 第二个分量 d(c_i, g) 表示候选兴趣点 c_i 与全局目标点 g 之间的欧几里得距离（全局目标距离）。\n> >\n> > #### 最后，时刻 t 的地图信息得分（信息增益激励项）表示为：\n> >\n> > $$\n> > e^{I_{i,t}}\n> > \\tag{3}\n> > $$\n> >\n> > #### 其中，*I_{i,t}* 的计算方式如下：\n> >\n> > $$\n> > I_{i,t} = \\frac{\\sum\\limits_{w=-\\frac{k}{2}}^{\\frac{k}{2}} \\sum\\limits_{h=-\\frac{k}{2}}^{\\frac{k}{2}} C(x+w)(y+h)}{k^2}\n> > \\tag{4}\n> > $$\n> >\n> > #### 其中，k 表示用于计算候选点周围信息的卷积核大小，候选点的坐标为 x 和 y ，而 w 和 h 分别表示卷积核的宽度和高度。\n> >\n> > #### 在公式 (1) 中，具有最小 IDLE 得分的兴趣点（POI）被选为用于局部导航的最优航路点。\n> >\n> > #### 注：\n> >\n> > > #### I_{i,t} 表示候选兴趣点（POI）c_i 在时间 t 的**信息得分**，它用于衡量此点周围环境的“信息丰富度”或“探索潜力”，得分越高，说明此 POI 附近还有**未探索、未知或值得探索的区域**。\n> > >\n> > > #### C(x, y) 是地图上 (x, y) 点的**置信值或不确定度值**，可能来源于占据栅格地图中的熵值、不确定度、未知区域标记、置信概率等，通常，这个值越高，表示该位置的信息越少、越值得探索。\n> > >\n> > > #### 双重求和：它是在点 (x, y) 的周围取了一个大小为 k * k 的滑动窗口，然后对该窗口内所有位置的 C 值进行求和，也就是在以候选点为中心的邻域中，统计该邻域“信息量”。\n> > >\n> > > #### 除以 k^2 是做了一个**均值操作**，即把窗口内的信息总量归一化为平均信息值，得到的值落在 [0, 1]内，便于指数函数处理。\n> > >\n> > > #### 其目的就是鼓励探索未知区域\n> >\n> > \n> >\n> > #### 注意到，h 的三个组成部分好像不能直接相加，第一部分得到的是一个得分值，第二部分是一个距离值，第三部分是一个信息增益激励值，貌似是不能直接相加的，但第一部分进行了一个 *l_2 的操作，是一个伪距离，最后一部分也进行了指数化操作，所以是可以相加的\n>\n> \n>\n> ### B.局部导航\n>\n> > #### 使用基于 双延迟深度确定性策略梯度(TD3，一种 Actor-Critic 架构)的神经网络架构来训练运动策略\n> >\n> > #### 局部环境信息和目标航点相对于 agent 位置的极坐标一起，作为状态输入 s 传入 TD3 的Actor 网络中\n> >\n> > #### 该 Actor 网络由两个**全连接（FC）层**组成，每一层后面都接有 **ReLU（修正线性单元）激活函数**。\n> >\n> > #### 最后一层与输出层相连，输出两个动作参数 a，分别表示机器人的线速度 a_1 和角速度 a_2\n> >\n> > #### 输出层采用 **tanh 激活函数**，将输出限制在区间 (−1,1) 内\n> >\n> > #### 在将动作应用于环境之前，它们会按以下方式缩放为实际速度值：\n> >\n> >\n> > $$\n> > a = \\left[ v_{\\max} \\left( \\frac{a_1 + 1}{2} \\right), \\omega_{\\max} a_2 \\right],\n> > \\tag{5}\n> > $$\n> >\n> > #### 最大线速度 v_max，最大角速度 ω_max\n> >\n> > #### 由于激光雷达只记录机器人前方的数据，因此**不考虑向后的运动**，并将**线速度调整为仅为正值**。\n> >\n> > \n> >\n> > #### 状态-动作对的 Q 值 Q(s,a) 由两个 **Critic 网络**进行评估。这两个 Critic 网络具有相同的结构，但其参数更新是**延迟进行**的，从而允许它们在参数上产生差异（避免完全同步）。\n> >\n> > #### Critic 网络以状态 s 和动作 a 的组合作为输入\n> >\n> > #### 其中，状态 s 首先被送入一个**全连接层**，并接上一个 ReLU 激活函数，输出为 L_s\n> >\n> > #### 该层的输出 L_s 以及动作 a，将分别送入两个大小相同的变换全连接层（TFC）中，分别对应变换结果为 τ1 和 τ2，随后，这两个结果按如下方式进行组合：\n> >\n> > $$\n> > L_c = L_sW_{\\tau_1} + aW_{\\tau_2} + b_{\\tau_2},\n> > \\tag{6}\n> > $$\n> >\n> > #### 其中，L_c 是组合全连接层（CFC）的输出，W_τ_1 和 W_τ_2 分别是 τ1 和 τ2 的权重，b_τ_2 是 τ_2 的偏执项，然后在这个组合层上使用ReLU激活函数，接着，该输出连接到一个最终输出节点，该节点包含**一个参数**，表示对应状态-动作对的 **Q 值**。最终，从两个 Critic 网络中**选择较小的 Q 值**，作为最后的 Critic 输出，以此来**限制对状态-动作值的过高估计**。\n> >\n> > #### 完整的网络架构如图\n> >\n> > ![](http://picbed.yanzu.tech/img/paper_read/1/3.jpg)\n> >\n> > #### TD3 网络结构包括 actor 和 critic 两个部分。每一层的类型以及其对应的参数数量在层中都有描述。TFC 层指的是变换全连接层 τ，CFC 层指的是组合全连接层 Lc。\n> >\n> > #### 策略的奖励依据以下函数进行评估(奖励函数)\n> >\n> > $$\n> > r(s_t, a_t) = \n> > \\begin{cases} \n> > r_g & \\text{if } D_t < \\eta D \\\\\n> > r_c & \\text{if collision} \\\\\n> > v - |\\omega| & \\text{otherwise},\n> > \\end{cases},\n> > \\tag{7}\n> > $$\n> >\n> > #### 在时间步 t时，状态-动作对 (s_t, a_t) 的奖励 r 取决于以下三种情况： \n> >\n> > - #### 如果当前时间步与目标点的距离 D_t 小于阈值 η_D，则给予一个正的目标奖励 r_g（也就是鼓励）\n> >\n> > - #### 如果检测到碰撞，则给予一个负的碰撞惩罚 r_c（惩罚）\n> >\n> > - #### 如果以上两种情况均未发生，则根据当前的线速度 v 和角速度 ω 给予即时奖励。\n> >\n> > \n> >\n> > #### 为了引导导航策略朝向给定目标，一个延迟归因奖励方法被采用，其计算如下：\n> >\n> > $$\n> > r_{t-i} = r(s_{t-i}, a_{t-i}) + \\frac{r_g}{i}, \\quad \\forall i \\in \\{1, 2, 3, \\ldots, n\\},\n> > \\tag{8}\n> > $$\n> >\n> > #### 其中，n 表示前 n 个时间步中需要更新奖励的步数。这意味着，正的目标奖励不仅被分配给达到目标的那个状态-动作对，还以递减的方式分配给其之前的 n 个时间步。通过这种方式，网络学习到了一种局部导航策略，该策略能够仅依赖激光输入，避开障碍物的同时到达局部目标。\n>\n> \n>\n> ### C.探索与建图\n>\n> > #### 机器人沿着路径点被引导向全局目标前进。一旦靠近全局目标，机器人将自主导航至该目标。在此过程中，环境被逐步探索和建图。建图依赖于激光雷达和机器人里程计传感器数据，生成环境的占据栅格地图。完整的自主探索与建图算法的伪代码在算法1中进行了描述。\n\n\n\n### 实验部分\n\n> #### 算法过程：\n>\n> > ```\n> > # 输入参数\n> > global_goal\t\t# 全局目标点，即最终导航的目标位置\n> > δ\t\t\t# 导航至全局目标的距离阈值,用于判断机器人是否“足够接近”目标点\n> > \n> > # 主循环 直到达到全局目标\n> > while(reached_global_goal != True){\t# 判断是否已达到全局目标\n> > \n> > \tread sensor data # 读取传感器数据：如激光雷达、相机、里程计等\n> > \t\n> > \tupdate map from sensor data # 根据传感器数据更新地图：构建或完善占据栅格地图\n> > \t\n> > \tObtain new POI # 获取新的兴趣点 POI,可能是探索边界或未知区域的候选目标点\n> > \t\n> > \t# 判断当前是否已接近目标区域\n> > \tif (D_t < δ_D){\t# 如果 agent与目标的距离处于接近目标区域\n> > \t\n> > \t\tif(waypoint = global_goal)\t# 若当前导航的子目标 waypoint 已经是 global_goal\n> > \t\t\treachedGlobalGoal = True\t# 那么任务完成\n> > \t\t\t\n> > \t\telse{\t# 否则，进一步判断当前是否靠近全局目标\n> > \t\t\n> > \t\t\tif(d(p_t, g) < δ)\t# 如果当前位置p_t与目标g的距离d(p_t,g) 小于 δ\n> > \t\t\t\twaypoint <-- global_goal\t# 那么就把当前的 waypoint 设置为 global_goal\n> > \t\t\t\t\n> > \t\t\telse\t# 否则，从所有兴趣点中选择下一个最优子目标点\n> > \t\t\t\tfor i in POI\t# 遍历POI中所有的兴趣点\n> > \t\t\t\t\tcaculate h(i) from (1) # 根据式子(1)计算每个兴趣点的启发值h(i)\n> > \t\t\t\twaypoint <-- POI_min(h(i))\t# 将h(i)值最小对应的兴趣点作为新的 waypoint\n> > \t\t\tend if\n> > \t\tend if\n> > \tend if\n> > \t\n> > \tObtain an action from TD3\t# 从 TD3 策略网络中获取当前动作,利用强化学习模型TD3预测最优动作\n> > \tPerform action\t# 执行该动作\n> > end while\n> > ```\n> >\n> > #### \n>\n> \n>\n> ### A.系统设置\n>\n> > #### 原作者系统配置：\n> >\n> > > #### 显卡：NVIDIA GTX 1080\n> > >\n> > > #### 运行内存：32G\n> > >\n> > > #### CPU： Intel Core i7-6800K \n> >\n> > #### 训练参数设置\n> >\n> > > #### TD3网络在Gazebo上进行训练，并通过ROS进行控制，训练了800回合，约8h\n> > >\n> > > #### 每个训练回合在机器人到达目标、发生碰撞或执行了 500 步动作后结束\n> > >\n> > > #### 最大线速度 v_max 和最大角速度 ω_max 分别设置为 0.5 m/s 和 1rad/s\n> > >\n> > > #### 延迟奖励在最后 n=10 步中更新，参数更新延迟设置为每 2 个回合\n> >\n> > \n> >\n> > #### 训练在一个 10x10 米的模拟环境中进行，如图所示\n> >\n> > ![](http://picbed.yanzu.tech/img/paper_read/1/4.jpg)\n> >\n> > #### 训练环境示例。蓝色区域表示输入的激光雷达读数和测距。四个箱形障碍物在每一轮训练中都会改变位置，如图(a)、(b)和(c)所示，以实现训练数据的随机化。\n> >\n> > #### 为促进策略的泛化与探索，向传感器读数和动作值中添加了高斯噪声。为了构建多样化的训练环境，在每个回合开始时，箱形障碍物的位置会被随机化。其位置变化的示例如图 (a)、(b)、(c) 所示。同时，机器人的起始位置与目标位置也在每一回合中被随机设置。\n> >\n> > \n> >\n> > #### ROS 中的 SLAM Toolbox 软件包用于生成和更新环境的全局地图，并用于机器人在地图中的定位\n> >\n> > #### ROS的本地规划器包（TrajectoryPlanner）代替了神经网络\n> >\n> > \n> >\n> > #### 目标驱动自主探索（GDAE, Goal-Driven Autonomous Exploration）\n> >\n> > #### 最近前沿探索策略（Nearest Frontier, NF）\n> >\n> > #### 目标驱动强化学习（GD-RL, Goal-Driven Reinforcement Learning）\n> >\n> > #### 本地规划器自主探索（LP-AE，Local Planner Autonomous Exploration）\n> >\n> > #### 路径规划器（PP，Path Planner）它是基于 Dijkstra 的生成路径的方法\n>\n> \n>\n> ### B.定量实验\n>\n> \n>\n> ### C.定性实验\n>\n> \n\n\n\n### 结论\n\n> #### 基于深度强化学习（DRL）的目标驱动型全自主探索系统（GDAE）\n>\n> #### 无需人工干预，即可导航至指定目标并记录环境信息（导航去目标点的过程中建图）\n>\n> #### 系统有效结合了反应式的本地导航策略和全局导航策略\n>\n> #### 将神经网络模块引入端到端系统：机器人无需生成显式路径即可移动，而这一机制的不足则通过引入全局导航策略得到了弥补\n>\n> #### 系统的导航性能接近于基于已知地图路径规划器所得的最优解\n>\n> #### GDAE 系统依赖**直接的传感器输入**而非从不确定地图生成路径，因此在可靠性方面表现更佳\n>\n> #### 若希望进一步**泛化至不同类型的机器人**，可以将**机器人动力学作为神经网络的一个输入状态**，并据此开展训练。只需提供机器人动力学信息，即可在不同平台上实现最优的本地导航。\n>\n> \n>\n> #### 接下来的研究：\n>\n> #### 引入**长短时记忆（LSTM）结构**也可能有助于缓解局部最优问题，并辅助规避当前传感器视野之外的障碍物\n\n\n\n### 代码部分\n\n> #### 两个量的定义\n>\n> #### Episode：\n>\n> > #### 它是后续步骤的集合，直到达到其中一个终止条件（终止条件：达到目标、与障碍物碰撞或达到最大步数max_ep）\n> >\n> > #### 它可以理解是一个训练过程，如果达到终止条件就开始新的一个训练过程，并且伴随着环境的改变，包括区域内的墙体位置改变、障碍物的位置改变、agent的初始位置改变和目标点位置的改变\n>\n> #### Epoch：\n>\n> > #### 执行评估之间的后续事件数（episode）或者时间步长（timesteps）\n> >\n> > #### 一个epoch运行5000个步骤step（对应代码中的 eval_freq 这个参数），步长为0.1秒，也就是说一个epoch大概要运行8分多钟\n>\n> ","slug":"17","published":1,"date":"2025-05-26T04:07:22.695Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7crh002yiku40nxy02tk","content":"<h1 id=\"Goal-Driven-Autonomous-Exploration-Through-Deep-Reinforcement-Learning—通过深度强化学习实现目标驱动的自主探索\"><a href=\"#Goal-Driven-Autonomous-Exploration-Through-Deep-Reinforcement-Learning—通过深度强化学习实现目标驱动的自主探索\" class=\"headerlink\" title=\"Goal-Driven Autonomous Exploration Through Deep Reinforcement  Learning—通过深度强化学习实现目标驱动的自主探索\"></a>Goal-Driven Autonomous Exploration Through Deep Reinforcement  Learning—通过深度强化学习实现目标驱动的自主探索</h1><h3 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h3><blockquote>\n<blockquote>\n<h4 id=\"本文提出了一种基于深度强化学习（DRL）的自主导航系统，用于在未知环境中进行目标驱动的探索。系统从环境中获取可能的导航方向的兴趣点（POI），并基于可用数据选择一个最优的航路点。机器人根据这些航路点被引导向全局目标，缓解了反应式导航中的局部最优问题。随后，在仿真环境中通过DRL框架学习用于局部导航的运动策略。我们开发了一个导航系统，将该学习到的策略集成到运动规划系统中，作为局部导航层，用于驱动机器人在航路点之间移动，最终到达全局目标。整个自主导航过程中无需任何先验知识，同时在机器人移动过程中会记录环境地图。实验表明，该方法在无需地图或先验信息的情况下，在复杂的静态和动态环境中相较于其他探索方法具有优势。\"><a href=\"#本文提出了一种基于深度强化学习（DRL）的自主导航系统，用于在未知环境中进行目标驱动的探索。系统从环境中获取可能的导航方向的兴趣点（POI），并基于可用数据选择一个最优的航路点。机器人根据这些航路点被引导向全局目标，缓解了反应式导航中的局部最优问题。随后，在仿真环境中通过DRL框架学习用于局部导航的运动策略。我们开发了一个导航系统，将该学习到的策略集成到运动规划系统中，作为局部导航层，用于驱动机器人在航路点之间移动，最终到达全局目标。整个自主导航过程中无需任何先验知识，同时在机器人移动过程中会记录环境地图。实验表明，该方法在无需地图或先验信息的情况下，在复杂的静态和动态环境中相较于其他探索方法具有优势。\" class=\"headerlink\" title=\"本文提出了一种基于深度强化学习（DRL）的自主导航系统，用于在未知环境中进行目标驱动的探索。系统从环境中获取可能的导航方向的兴趣点（POI），并基于可用数据选择一个最优的航路点。机器人根据这些航路点被引导向全局目标，缓解了反应式导航中的局部最优问题。随后，在仿真环境中通过DRL框架学习用于局部导航的运动策略。我们开发了一个导航系统，将该学习到的策略集成到运动规划系统中，作为局部导航层，用于驱动机器人在航路点之间移动，最终到达全局目标。整个自主导航过程中无需任何先验知识，同时在机器人移动过程中会记录环境地图。实验表明，该方法在无需地图或先验信息的情况下，在复杂的静态和动态环境中相较于其他探索方法具有优势。\"></a>本文提出了一种基于深度强化学习（DRL）的自主导航系统，用于在未知环境中进行目标驱动的探索。系统从环境中获取可能的导航方向的兴趣点（POI），并基于可用数据选择一个最优的航路点。机器人根据这些航路点被引导向全局目标，缓解了反应式导航中的局部最优问题。随后，在仿真环境中通过DRL框架学习用于局部导航的运动策略。我们开发了一个导航系统，将该学习到的策略集成到运动规划系统中，作为局部导航层，用于驱动机器人在航路点之间移动，最终到达全局目标。整个自主导航过程中无需任何先验知识，同时在机器人移动过程中会记录环境地图。实验表明，该方法在无需地图或先验信息的情况下，在复杂的静态和动态环境中相较于其他探索方法具有优势。</h4></blockquote>\n<h4 id=\"自主导航系统\"><a href=\"#自主导航系统\" class=\"headerlink\" title=\"自主导航系统\"></a>自主导航系统</h4><h4 id=\"用于通过DRL对未知环境进行目标驱动的探索\"><a href=\"#用于通过DRL对未知环境进行目标驱动的探索\" class=\"headerlink\" title=\"用于通过DRL对未知环境进行目标驱动的探索\"></a>用于通过DRL对未知环境进行目标驱动的探索</h4><h4 id=\"获取可能导航方向的兴趣点-POI\"><a href=\"#获取可能导航方向的兴趣点-POI\" class=\"headerlink\" title=\"获取可能导航方向的兴趣点(POI)\"></a>获取可能导航方向的兴趣点(POI)</h4><h4 id=\"根据可用数据选择最佳航路点\"><a href=\"#根据可用数据选择最佳航路点\" class=\"headerlink\" title=\"根据可用数据选择最佳航路点\"></a>根据可用数据选择最佳航路点</h4><h4 id=\"缓解反应式导航中的局部最优问题\"><a href=\"#缓解反应式导航中的局部最优问题\" class=\"headerlink\" title=\"缓解反应式导航中的局部最优问题\"></a>缓解反应式导航中的局部最优问题</h4><h4 id=\"采用的是TD3算法（双延迟深度确定性策略梯度）\"><a href=\"#采用的是TD3算法（双延迟深度确定性策略梯度）\" class=\"headerlink\" title=\"采用的是TD3算法（双延迟深度确定性策略梯度）\"></a>采用的是TD3算法（双延迟深度确定性策略梯度）</h4><blockquote>\n<h4 id=\"它是在DDPG算法的基础上进行的扩展\"><a href=\"#它是在DDPG算法的基础上进行的扩展\" class=\"headerlink\" title=\"它是在DDPG算法的基础上进行的扩展\"></a>它是在DDPG算法的基础上进行的扩展</h4><h4 id=\"DDPG（Deep-Deterministic-Policy-Gradient）深度确定性策略梯度：它是一种同时学习Q函数和策略的方法，它使用非策略数据和贝尔曼方程来学习Q函数，并使用Q函数来学习策略\"><a href=\"#DDPG（Deep-Deterministic-Policy-Gradient）深度确定性策略梯度：它是一种同时学习Q函数和策略的方法，它使用非策略数据和贝尔曼方程来学习Q函数，并使用Q函数来学习策略\" class=\"headerlink\" title=\"DDPG（Deep Deterministic Policy Gradient）深度确定性策略梯度：它是一种同时学习Q函数和策略的方法，它使用非策略数据和贝尔曼方程来学习Q函数，并使用Q函数来学习策略\"></a>DDPG（Deep Deterministic Policy Gradient）深度确定性策略梯度：它是一种同时学习Q函数和策略的方法，它使用非策略数据和贝尔曼方程来学习Q函数，并使用Q函数来学习策略</h4><h4 id=\"详细介绍：https-spinningup-openai-com-en-latest-algorithms-ddpg-html-background\"><a href=\"#详细介绍：https-spinningup-openai-com-en-latest-algorithms-ddpg-html-background\" class=\"headerlink\" title=\"详细介绍：https://spinningup.openai.com/en/latest/algorithms/ddpg.html#background\"></a>详细介绍：<a href=\"https://spinningup.openai.com/en/latest/algorithms/ddpg.html#background\">https://spinningup.openai.com/en/latest/algorithms/ddpg.html#background</a></h4><h4 id=\"DDPG-对超参数和其他调优手段非常敏感，表现常常不稳定，它的一个常见失败模式是：所学习的-Q-函数会严重高估-Q-值，进而导致策略崩溃，因为策略会利用-Q-函数中的误差\"><a href=\"#DDPG-对超参数和其他调优手段非常敏感，表现常常不稳定，它的一个常见失败模式是：所学习的-Q-函数会严重高估-Q-值，进而导致策略崩溃，因为策略会利用-Q-函数中的误差\" class=\"headerlink\" title=\"DDPG 对超参数和其他调优手段非常敏感，表现常常不稳定，它的一个常见失败模式是：所学习的 Q 函数会严重高估 Q 值，进而导致策略崩溃，因为策略会利用 Q 函数中的误差\"></a>DDPG 对超参数和其他调优手段<strong>非常敏感</strong>，表现常常不稳定，它的一个常见失败模式是：所学习的 Q 函数会<strong>严重高估 Q 值</strong>，进而导致策略崩溃，因为策略会利用 Q 函数中的误差</h4><h4 id=\"而双延迟深度确定性策略梯度算法（TD3）引入三个关键技巧来解决DDPG的问题：\"><a href=\"#而双延迟深度确定性策略梯度算法（TD3）引入三个关键技巧来解决DDPG的问题：\" class=\"headerlink\" title=\"而双延迟深度确定性策略梯度算法（TD3）引入三个关键技巧来解决DDPG的问题：\"></a>而双延迟深度确定性策略梯度算法（TD3）引入三个关键技巧来解决DDPG的问题：</h4><blockquote>\n<ol>\n<li><h4 id=\"截断的双Q学习：TD3-同时学习两个-Q-函数（因此称为“双”Q），在计算贝尔曼误差损失函数中的目标值时，取两个Q值中较小的一个，以降低过估计的风险\"><a href=\"#截断的双Q学习：TD3-同时学习两个-Q-函数（因此称为“双”Q），在计算贝尔曼误差损失函数中的目标值时，取两个Q值中较小的一个，以降低过估计的风险\" class=\"headerlink\" title=\"截断的双Q学习：TD3 同时学习两个 Q 函数（因此称为“双”Q），在计算贝尔曼误差损失函数中的目标值时，取两个Q值中较小的一个，以降低过估计的风险\"></a>截断的双Q学习：TD3 同时学习两个 Q 函数（因此称为“双”Q），在计算贝尔曼误差损失函数中的目标值时，取两个Q值中较小的一个，以降低过估计的风险</h4></li>\n<li><h4 id=\"延迟的策略更新：TD3-中，策略网络（Actor）和目标网络的更新频率低于-Q-函数的更新频率，建议是，更新两次-Q-网络，只更新-一次策略网络，以提高训练稳定性\"><a href=\"#延迟的策略更新：TD3-中，策略网络（Actor）和目标网络的更新频率低于-Q-函数的更新频率，建议是，更新两次-Q-网络，只更新-一次策略网络，以提高训练稳定性\" class=\"headerlink\" title=\"延迟的策略更新：TD3 中，策略网络（Actor）和目标网络的更新频率低于 Q 函数的更新频率，建议是，更新两次 Q 网络，只更新 一次策略网络，以提高训练稳定性\"></a>延迟的策略更新：TD3 中，策略网络（Actor）和目标网络的更新频率<strong>低于 Q 函数</strong>的更新频率，建议是，更新两次 Q 网络，只更新 <strong>一次策略网络</strong>，以提高训练稳定性</h4></li>\n<li><h4 id=\"目标策略平滑：TD3-在计算目标-Q-值时，会在目标动作上添加噪声，一般是高斯噪声，使得动作空间内的-Q-值更平滑，从而防止策略去“钻空子”利用-Q-函数中的误差\"><a href=\"#目标策略平滑：TD3-在计算目标-Q-值时，会在目标动作上添加噪声，一般是高斯噪声，使得动作空间内的-Q-值更平滑，从而防止策略去“钻空子”利用-Q-函数中的误差\" class=\"headerlink\" title=\"目标策略平滑：TD3 在计算目标 Q 值时，会在目标动作上添加噪声，一般是高斯噪声，使得动作空间内的 Q 值更平滑，从而防止策略去“钻空子”利用 Q 函数中的误差\"></a>目标策略平滑：TD3 在计算目标 Q 值时，会在目标动作上添加噪声，一般是高斯噪声，使得动作空间内的 Q 值更平滑，从而防止策略去“钻空子”利用 Q 函数中的误差</h4></li>\n</ol>\n</blockquote>\n<h4 id=\"详细介绍：https-spinningup-openai-com-en-latest-algorithms-td3-html\"><a href=\"#详细介绍：https-spinningup-openai-com-en-latest-algorithms-td3-html\" class=\"headerlink\" title=\"详细介绍：https://spinningup.openai.com/en/latest/algorithms/td3.html\"></a>详细介绍：<a href=\"https://spinningup.openai.com/en/latest/algorithms/td3.html\">https://spinningup.openai.com/en/latest/algorithms/td3.html</a></h4></blockquote>\n</blockquote>\n<h3 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h3><blockquote>\n<h4 id=\"完全自主的目标驱动探索是一个包含两个方面的问题\"><a href=\"#完全自主的目标驱动探索是一个包含两个方面的问题\" class=\"headerlink\" title=\"完全自主的目标驱动探索是一个包含两个方面的问题\"></a><strong>完全自主的目标驱动探索</strong>是一个包含两个方面的问题</h4><blockquote>\n<h4 id=\"首先，探索机器人需要决策去哪一个方向，以最大可能性到达全局目标。在没有先验信息或无法观测到全局目标的情况下，系统需要直接从传感器数据中指示可能的导航方向。从这些兴趣点（POI）中，需要选择一个最优点作为航路点，以最优的方式引导机器人接近全局目标。\"><a href=\"#首先，探索机器人需要决策去哪一个方向，以最大可能性到达全局目标。在没有先验信息或无法观测到全局目标的情况下，系统需要直接从传感器数据中指示可能的导航方向。从这些兴趣点（POI）中，需要选择一个最优点作为航路点，以最优的方式引导机器人接近全局目标。\" class=\"headerlink\" title=\"首先，探索机器人需要决策去哪一个方向，以最大可能性到达全局目标。在没有先验信息或无法观测到全局目标的情况下，系统需要直接从传感器数据中指示可能的导航方向。从这些兴趣点（POI）中，需要选择一个最优点作为航路点，以最优的方式引导机器人接近全局目标。\"></a>首先，探索机器人需要决策去哪一个方向，以最大可能性到达全局目标。在没有先验信息或无法观测到全局目标的情况下，系统需要直接从传感器数据中指示可能的导航方向。从这些兴趣点（POI）中，需要选择一个最优点作为航路点，以最优的方式引导机器人接近全局目标。</h4><h4 id=\"其次，在不依赖地图数据的前提下，还需要获得一种适用于不确定环境的机器人运动策略。\"><a href=\"#其次，在不依赖地图数据的前提下，还需要获得一种适用于不确定环境的机器人运动策略。\" class=\"headerlink\" title=\"其次，在不依赖地图数据的前提下，还需要获得一种适用于不确定环境的机器人运动策略。\"></a>其次，在不依赖地图数据的前提下，还需要获得一种适用于不确定环境的机器人运动策略。</h4><h4 id=\"这就引入了DRL\"><a href=\"#这就引入了DRL\" class=\"headerlink\" title=\"这就引入了DRL\"></a>这就引入了DRL</h4><h4 id=\"但是DRL存在一个问题：DRL具有反应性特征且缺乏全局信息，其容易陷入局部最优问题尤其是在大规模导航任务中表现尤为明显。\"><a href=\"#但是DRL存在一个问题：DRL具有反应性特征且缺乏全局信息，其容易陷入局部最优问题尤其是在大规模导航任务中表现尤为明显。\" class=\"headerlink\" title=\"但是DRL存在一个问题：DRL具有反应性特征且缺乏全局信息，其容易陷入局部最优问题尤其是在大规模导航任务中表现尤为明显。\"></a>但是DRL存在一个问题：DRL具有反应性特征且缺乏全局信息，其容易陷入局部最优问题尤其是在大规模导航任务中表现尤为明显。</h4><blockquote>\n<h4 id=\"DRL的反应性特征\"><a href=\"#DRL的反应性特征\" class=\"headerlink\" title=\"DRL的反应性特征\"></a>DRL的反应性特征</h4><blockquote>\n<h4 id=\"DRL中的大多策略都是基于当前状态做出决策的，也即-reactive-policy（反应式策略），也就是具有马尔可夫性\"><a href=\"#DRL中的大多策略都是基于当前状态做出决策的，也即-reactive-policy（反应式策略），也就是具有马尔可夫性\" class=\"headerlink\" title=\"DRL中的大多策略都是基于当前状态做出决策的，也即 reactive policy（反应式策略），也就是具有马尔可夫性\"></a>DRL中的大多策略都是基于当前状态做出决策的，也即 reactive policy（反应式策略），也就是具有马尔可夫性</h4><h4 id=\"这会导致：\"><a href=\"#这会导致：\" class=\"headerlink\" title=\"这会导致：\"></a>这会导致：</h4><ul>\n<li><h4 id=\"agent只依据当前观测（摄像头图像、雷达扫描）来决定或判断下一动作\"><a href=\"#agent只依据当前观测（摄像头图像、雷达扫描）来决定或判断下一动作\" class=\"headerlink\" title=\"agent只依据当前观测（摄像头图像、雷达扫描）来决定或判断下一动作\"></a>agent只依据当前观测（摄像头图像、雷达扫描）来决定或判断下一动作</h4></li>\n<li><h4 id=\"没有全局地图或任务结果的长期记忆，缺少更高层次的计划或决策能力\"><a href=\"#没有全局地图或任务结果的长期记忆，缺少更高层次的计划或决策能力\" class=\"headerlink\" title=\"没有全局地图或任务结果的长期记忆，缺少更高层次的计划或决策能力\"></a>没有全局地图或任务结果的长期记忆，缺少更高层次的计划或决策能力</h4></li>\n<li><h4 id=\"动作的选择往往基于短期奖励最大化，而非长期全局最优\"><a href=\"#动作的选择往往基于短期奖励最大化，而非长期全局最优\" class=\"headerlink\" title=\"动作的选择往往基于短期奖励最大化，而非长期全局最优\"></a>动作的选择往往基于短期奖励最大化，而非长期全局最优</h4></li>\n</ul>\n<h4 id=\"这种策略适用于快速反应、实时避障等场景\"><a href=\"#这种策略适用于快速反应、实时避障等场景\" class=\"headerlink\" title=\"这种策略适用于快速反应、实时避障等场景\"></a>这种策略适用于快速反应、实时避障等场景</h4></blockquote>\n<h4 id=\"全局信息的缺乏\"><a href=\"#全局信息的缺乏\" class=\"headerlink\" title=\"全局信息的缺乏\"></a>全局信息的缺乏</h4><blockquote>\n<h4 id=\"大多数DRL系统的输入都是局部传感器数据（当前位置的摄像头图像、雷达扫描），因此DRL也只能感知局部环境。另外，强化学习RL本身就是一种局部策略的学习方法，它更适合解决的是马尔可夫决策过程MDP问题。还有就是，DRL的训练目标就是最大化累计奖励，而非构建认知地图。\"><a href=\"#大多数DRL系统的输入都是局部传感器数据（当前位置的摄像头图像、雷达扫描），因此DRL也只能感知局部环境。另外，强化学习RL本身就是一种局部策略的学习方法，它更适合解决的是马尔可夫决策过程MDP问题。还有就是，DRL的训练目标就是最大化累计奖励，而非构建认知地图。\" class=\"headerlink\" title=\"大多数DRL系统的输入都是局部传感器数据（当前位置的摄像头图像、雷达扫描），因此DRL也只能感知局部环境。另外，强化学习RL本身就是一种局部策略的学习方法，它更适合解决的是马尔可夫决策过程MDP问题。还有就是，DRL的训练目标就是最大化累计奖励，而非构建认知地图。\"></a>大多数DRL系统的输入都是局部传感器数据（当前位置的摄像头图像、雷达扫描），因此DRL也只能感知局部环境。另外，强化学习RL本身就是一种局部策略的学习方法，它更适合解决的是马尔可夫决策过程MDP问题。还有就是，DRL的训练目标就是最大化累计奖励，而非构建认知地图。</h4></blockquote>\n<h4 id=\"局部最优问题\"><a href=\"#局部最优问题\" class=\"headerlink\" title=\"局部最优问题\"></a>局部最优问题</h4><blockquote>\n<h4 id=\"系统在当前状态下选择了一个看似收益最大（或最短路径）的行为，但这个选择却可能阻碍了其通向真正全局最优解的路径。\"><a href=\"#系统在当前状态下选择了一个看似收益最大（或最短路径）的行为，但这个选择却可能阻碍了其通向真正全局最优解的路径。\" class=\"headerlink\" title=\"系统在当前状态下选择了一个看似收益最大（或最短路径）的行为，但这个选择却可能阻碍了其通向真正全局最优解的路径。\"></a>系统在当前状态下选择了一个看似收益最大（或最短路径）的行为，但这个选择却可能阻碍了其通向真正全局最优解的路径。</h4></blockquote>\n</blockquote>\n</blockquote>\n<h4 id=\"本文提出一种完全自主的探索系统，用于在无需人工控制或环境先验信息的情况下实现导航至全局目标。\"><a href=\"#本文提出一种完全自主的探索系统，用于在无需人工控制或环境先验信息的情况下实现导航至全局目标。\" class=\"headerlink\" title=\"本文提出一种完全自主的探索系统，用于在无需人工控制或环境先验信息的情况下实现导航至全局目标。\"></a>本文提出一种完全自主的探索系统，用于在无需人工控制或环境先验信息的情况下实现导航至全局目标。</h4><h4 id=\"该系统从机器人周围的局部环境中提取兴趣点（POI），对其进行评估，并从中选取一个作为航路点（waypoint）。\"><a href=\"#该系统从机器人周围的局部环境中提取兴趣点（POI），对其进行评估，并从中选取一个作为航路点（waypoint）。\" class=\"headerlink\" title=\"该系统从机器人周围的局部环境中提取兴趣点（POI），对其进行评估，并从中选取一个作为航路点（waypoint）。\"></a>该系统从机器人周围的局部环境中提取<strong>兴趣点（POI）</strong>，对其进行评估，并从中选取一个作为航路点（waypoint）。</h4><h4 id=\"这些航路点用于引导基于深度强化学习（DRL）的运动策略朝向全局目标，从而缓解局部最优问题。\"><a href=\"#这些航路点用于引导基于深度强化学习（DRL）的运动策略朝向全局目标，从而缓解局部最优问题。\" class=\"headerlink\" title=\"这些航路点用于引导基于深度强化学习（DRL）的运动策略朝向全局目标，从而缓解局部最优问题。\"></a>这些航路点用于引导基于深度强化学习（DRL）的运动策略朝向全局目标，从而缓解局部最优问题。</h4><h4 id=\"机器人依据该策略进行运动，无需对周围环境进行完整建图。\"><a href=\"#机器人依据该策略进行运动，无需对周围环境进行完整建图。\" class=\"headerlink\" title=\"机器人依据该策略进行运动，无需对周围环境进行完整建图。\"></a>机器人依据该策略进行运动，无需对周围环境进行完整建图。</h4><h4 id=\"POI\"><a href=\"#POI\" class=\"headerlink\" title=\"POI\"></a>POI</h4><blockquote>\n<h4 id=\"这里所说的-POI-兴趣点，是指机器人环境中那些具有导航价值的特定点或位置，这些点通常是机器人决策“去哪儿”的候选目标。它们帮助机器人在未知环境里选择下一步行动方向，特别是在没有完整地图信息时。POI-可以作为导航路径的候选点，也即中间航路点waypoint。\"><a href=\"#这里所说的-POI-兴趣点，是指机器人环境中那些具有导航价值的特定点或位置，这些点通常是机器人决策“去哪儿”的候选目标。它们帮助机器人在未知环境里选择下一步行动方向，特别是在没有完整地图信息时。POI-可以作为导航路径的候选点，也即中间航路点waypoint。\" class=\"headerlink\" title=\"这里所说的 POI 兴趣点，是指机器人环境中那些具有导航价值的特定点或位置，这些点通常是机器人决策“去哪儿”的候选目标。它们帮助机器人在未知环境里选择下一步行动方向，特别是在没有完整地图信息时。POI 可以作为导航路径的候选点，也即中间航路点waypoint。\"></a>这里所说的 POI 兴趣点，是指机器人环境中那些具有导航价值的特定点或位置，这些点通常是机器人决策“去哪儿”的候选目标。它们帮助机器人在未知环境里选择下一步行动方向，特别是在没有完整地图信息时。POI 可以作为导航路径的候选点，也即中间航路点waypoint。</h4><h4 id=\"POI的确定\"><a href=\"#POI的确定\" class=\"headerlink\" title=\"POI的确定\"></a>POI的确定</h4><blockquote>\n<h4 id=\"激光读数间隙产生的POI：当连续两束激光雷达测距数值差距较大时，说明可能有一个开口或通道，这个位置就被视作一个-POI。\"><a href=\"#激光读数间隙产生的POI：当连续两束激光雷达测距数值差距较大时，说明可能有一个开口或通道，这个位置就被视作一个-POI。\" class=\"headerlink\" title=\"激光读数间隙产生的POI：当连续两束激光雷达测距数值差距较大时，说明可能有一个开口或通道，这个位置就被视作一个 POI。\"></a>激光读数间隙产生的POI：当连续两束激光雷达测距数值差距较大时，说明可能有一个开口或通道，这个位置就被视作一个 POI。</h4><h4 id=\"非数值激光读数产生的POI：激光雷达超出测量范围时返回非数值，代表远处有自由空间，这些边缘位置也被标记为兴趣点。\"><a href=\"#非数值激光读数产生的POI：激光雷达超出测量范围时返回非数值，代表远处有自由空间，这些边缘位置也被标记为兴趣点。\" class=\"headerlink\" title=\"非数值激光读数产生的POI：激光雷达超出测量范围时返回非数值，代表远处有自由空间，这些边缘位置也被标记为兴趣点。\"></a>非数值激光读数产生的POI：激光雷达超出测量范围时返回非数值，代表远处有自由空间，这些边缘位置也被标记为兴趣点。</h4></blockquote>\n</blockquote>\n<h4 id=\"导航系统的实现：左侧展示了机器人的设置，中间和右侧分别可视化了全局导航与局部导航的组成部分及其数据流。\"><a href=\"#导航系统的实现：左侧展示了机器人的设置，中间和右侧分别可视化了全局导航与局部导航的组成部分及其数据流。\" class=\"headerlink\" title=\"导航系统的实现：左侧展示了机器人的设置，中间和右侧分别可视化了全局导航与局部导航的组成部分及其数据流。\"></a>导航系统的实现：左侧展示了机器人的设置，中间和右侧分别可视化了全局导航与局部导航的组成部分及其数据流。</h4><p><img src=\"http://picbed.yanzu.tech/img/paper_read/1/1.jpg\"></p>\n</blockquote>\n<h3 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h3><blockquote>\n<h4 id=\"导航结构分两部分：\"><a href=\"#导航结构分两部分：\" class=\"headerlink\" title=\"导航结构分两部分：\"></a>导航结构分两部分：</h4><blockquote>\n<h4 id=\"具有最优航路点选择机制的全局导航与建图模块\"><a href=\"#具有最优航路点选择机制的全局导航与建图模块\" class=\"headerlink\" title=\"具有最优航路点选择机制的全局导航与建图模块\"></a>具有<strong>最优航路点选择机制</strong>的<strong>全局导航与建图</strong>模块</h4><h4 id=\"基于深度强化学习的局部导航模块\"><a href=\"#基于深度强化学习的局部导航模块\" class=\"headerlink\" title=\"基于深度强化学习的局部导航模块\"></a>基于<strong>深度强化学习</strong>的<strong>局部导航</strong>模块</h4></blockquote>\n<h4 id=\"系统首先从环境中提取兴趣点（POI），并依据设定的评估标准选择一个最优航路点。\"><a href=\"#系统首先从环境中提取兴趣点（POI），并依据设定的评估标准选择一个最优航路点。\" class=\"headerlink\" title=\"系统首先从环境中提取兴趣点（POI），并依据设定的评估标准选择一个最优航路点。\"></a>系统首先从环境中提取<strong>兴趣点（POI）</strong>，并依据设定的评估标准选择一个<strong>最优航路点</strong>。</h4><h4 id=\"在每一个导航步骤中，系统会将该航路点以相对于机器人当前位置与朝向的极坐标形式输入神经网络。\"><a href=\"#在每一个导航步骤中，系统会将该航路点以相对于机器人当前位置与朝向的极坐标形式输入神经网络。\" class=\"headerlink\" title=\"在每一个导航步骤中，系统会将该航路点以相对于机器人当前位置与朝向的极坐标形式输入神经网络。\"></a>在每一个导航步骤中，系统会将该航路点以<strong>相对于机器人当前位置与朝向的极坐标形式</strong>输入神经网络。</h4><h4 id=\"随后，网络基于当前的传感器数据计算并执行一个动作，以朝向该航路点运动。\"><a href=\"#随后，网络基于当前的传感器数据计算并执行一个动作，以朝向该航路点运动。\" class=\"headerlink\" title=\"随后，网络基于当前的传感器数据计算并执行一个动作，以朝向该航路点运动。\"></a>随后，网络基于当前的传感器数据计算并执行一个动作，以朝向该航路点运动。</h4><h4 id=\"在机器人沿着多个航路点逐步向全局目标移动的过程中，同时完成对环境的地图构建（建图）。\"><a href=\"#在机器人沿着多个航路点逐步向全局目标移动的过程中，同时完成对环境的地图构建（建图）。\" class=\"headerlink\" title=\"在机器人沿着多个航路点逐步向全局目标移动的过程中，同时完成对环境的地图构建（建图）。\"></a>在机器人沿着多个航路点逐步向全局目标移动的过程中，同时完成对环境的<strong>地图构建（建图）</strong>。</h4><h3 id=\"A-全局导航\"><a href=\"#A-全局导航\" class=\"headerlink\" title=\"A.全局导航\"></a>A.全局导航</h3><blockquote>\n<h4 id=\"为了使机器人能够朝向全局目标导航，必须从当前可用的兴趣点（POI）中选择用于局部导航的中间航路点。\"><a href=\"#为了使机器人能够朝向全局目标导航，必须从当前可用的兴趣点（POI）中选择用于局部导航的中间航路点。\" class=\"headerlink\" title=\"为了使机器人能够朝向全局目标导航，必须从当前可用的兴趣点（POI）中选择用于局部导航的中间航路点。\"></a>为了使机器人能够朝向全局目标导航，必须从当前可用的兴趣点（POI）中选择<strong>用于局部导航的中间航路点</strong>。</h4><h4 id=\"机器人不仅需要被引导前往目标，还必须在行进过程中探索周围环境，以便在遇到死路时能够识别出可能的替代路径。\"><a href=\"#机器人不仅需要被引导前往目标，还必须在行进过程中探索周围环境，以便在遇到死路时能够识别出可能的替代路径。\" class=\"headerlink\" title=\"机器人不仅需要被引导前往目标，还必须在行进过程中探索周围环境，以便在遇到死路时能够识别出可能的替代路径。\"></a>机器人不仅需要被引导前往目标，还必须在行进过程中<strong>探索周围环境</strong>，以便在遇到死路时能够识别出可能的替代路径。</h4><h4 id=\"鉴于没有预先提供的环境信息，所有可能的-POI-必须从机器人当前的周边环境中提取，并存储在内存中以供后续使用。\"><a href=\"#鉴于没有预先提供的环境信息，所有可能的-POI-必须从机器人当前的周边环境中提取，并存储在内存中以供后续使用。\" class=\"headerlink\" title=\"鉴于没有预先提供的环境信息，所有可能的 POI 必须从机器人当前的周边环境中提取，并存储在内存中以供后续使用。\"></a>鉴于没有预先提供的环境信息，所有可能的 POI 必须从<strong>机器人当前的周边环境中提取</strong>，并<strong>存储在内存中</strong>以供后续使用。</h4><h4 id=\"如果在后续步骤中发现某个兴趣点（POI）位于障碍物附近，则该点会从内存中删除。\"><a href=\"#如果在后续步骤中发现某个兴趣点（POI）位于障碍物附近，则该点会从内存中删除。\" class=\"headerlink\" title=\"如果在后续步骤中发现某个兴趣点（POI）位于障碍物附近，则该点会从内存中删除。\"></a>如果在后续步骤中发现某个兴趣点（POI）位于障碍物附近，则该点会从内存中删除。</h4><h4 id=\"在机器人已经访问过的位置，激光雷达不会再提取新的-POI。\"><a href=\"#在机器人已经访问过的位置，激光雷达不会再提取新的-POI。\" class=\"headerlink\" title=\"在机器人已经访问过的位置，激光雷达不会再提取新的 POI。\"></a>在机器人已经访问过的位置，激光雷达不会再提取新的 POI。</h4><h4 id=\"此外，如果某个-POI-被选为航路点但在若干步内始终无法到达，它也会被删除，并重新选择一个新的航路点。\"><a href=\"#此外，如果某个-POI-被选为航路点但在若干步内始终无法到达，它也会被删除，并重新选择一个新的航路点。\" class=\"headerlink\" title=\"此外，如果某个 POI 被选为航路点但在若干步内始终无法到达，它也会被删除，并重新选择一个新的航路点。\"></a>此外，如果某个 POI 被选为航路点但在若干步内始终无法到达，它也会被删除，并重新选择一个新的航路点。</h4><h4 id=\"获取新的POI的方法：就是上面提到的POI的确定\"><a href=\"#获取新的POI的方法：就是上面提到的POI的确定\" class=\"headerlink\" title=\"获取新的POI的方法：就是上面提到的POI的确定\"></a>获取新的POI的方法：就是上面提到的POI的确定</h4><h4 id=\"从当前环境中提取POI，蓝色圆圈表示通过相应方法提取的兴趣点，绿色圆圈表示当前的航路点。\"><a href=\"#从当前环境中提取POI，蓝色圆圈表示通过相应方法提取的兴趣点，绿色圆圈表示当前的航路点。\" class=\"headerlink\" title=\"从当前环境中提取POI，蓝色圆圈表示通过相应方法提取的兴趣点，绿色圆圈表示当前的航路点。\"></a>从当前环境中提取POI，蓝色圆圈表示通过相应方法提取的兴趣点，绿色圆圈表示当前的航路点。</h4><h4 id=\"a-蓝色-POI-是通过激光雷达测距数据之间的间隙提取的；\"><a href=\"#a-蓝色-POI-是通过激光雷达测距数据之间的间隙提取的；\" class=\"headerlink\" title=\"(a) 蓝色 POI 是通过激光雷达测距数据之间的间隙提取的；\"></a>(a) 蓝色 POI 是通过激光雷达测距数据之间的间隙提取的；</h4><h4 id=\"b-蓝色-POI-是从非数值型的激光读数中提取的。\"><a href=\"#b-蓝色-POI-是从非数值型的激光读数中提取的。\" class=\"headerlink\" title=\"(b)蓝色 POI 是从非数值型的激光读数中提取的。\"></a>(b)蓝色 POI 是从非数值型的激光读数中提取的。</h4><p><img src=\"http://picbed.yanzu.tech/img/paper_read/1/2.jpg\"></p>\n<h4 id=\"在时刻-t，从当前可用的兴趣点（POI）中，使用基于信息的距离受限探索方法（Information-based-Distance-Limited-Exploration，简称-IDLE）来选择最优的航路点。IDLE-方法通过以下方式评估每个候选-POI-的适应度：\"><a href=\"#在时刻-t，从当前可用的兴趣点（POI）中，使用基于信息的距离受限探索方法（Information-based-Distance-Limited-Exploration，简称-IDLE）来选择最优的航路点。IDLE-方法通过以下方式评估每个候选-POI-的适应度：\" class=\"headerlink\" title=\"在时刻 t，从当前可用的兴趣点（POI）中，使用基于信息的距离受限探索方法（Information-based Distance Limited Exploration，简称 IDLE）来选择最优的航路点。IDLE 方法通过以下方式评估每个候选 POI 的适应度：\"></a>在时刻 <em>t</em>，从当前可用的兴趣点（POI）中，使用<strong>基于信息的距离受限探索方法</strong>（Information-based Distance Limited Exploration，简称 <strong>IDLE</strong>）来选择最优的航路点。IDLE 方法通过以下方式评估每个候选 POI 的适应度：</h4><p>$$<br>h(c_i) &#x3D; \\tanh\\left(\\frac{e^{\\left(\\frac{d(p_t, c_i)}{l_2 - l_1}\\right)^2}}{e^{\\left(\\frac{l_2}{l_2 - l_1}\\right)^2}}\\right) l_2 + d(c_i, g) + e^{I_{i,t}}<br>\\tag{1}<br>$$</p>\n<h4 id=\"每个候选兴趣点-c-索引为-i-的得分-h-由三部分组成\"><a href=\"#每个候选兴趣点-c-索引为-i-的得分-h-由三部分组成\" class=\"headerlink\" title=\"每个候选兴趣点 c (索引为 i) 的得分 h 由三部分组成\"></a>每个候选兴趣点 c (索引为 i) 的得分 h 由三部分组成</h4><h4 id=\"其中，机器人在时刻-t-的位置-p-t-与候选兴趣点-c-i-之间的欧几里得距离分量-d-p-t-c-i-被表示为一个双曲正切函数（tanh）形式：\"><a href=\"#其中，机器人在时刻-t-的位置-p-t-与候选兴趣点-c-i-之间的欧几里得距离分量-d-p-t-c-i-被表示为一个双曲正切函数（tanh）形式：\" class=\"headerlink\" title=\"其中，机器人在时刻 t 的位置 p_t 与候选兴趣点 c_i 之间的欧几里得距离分量 d(p_t, c_i) 被表示为一个双曲正切函数（tanh）形式：\"></a>其中，机器人在时刻 t 的位置 p_t 与候选兴趣点 <em>c_i</em> 之间的欧几里得距离分量 d(p_t, c_i) 被表示为一个双曲正切函数（tanh）形式：</h4><p>$$<br>\\tanh\\left(\\frac{e^{\\left(\\frac{d(p_t, c_i)}{l_2 - l_1}\\right)^2}}{e^{\\left(\\frac{l_2}{l_2 - l_1}\\right)^2}}\\right) l_2<br>\\tag{2}<br>$$</p>\n<h4 id=\"其中，e-是自然对数的底（欧拉数），l-1-和-l-2-是两个距离阈值，用于对得分进行衰减。这两个距离阈值根据深度强化学习（DRL）训练环境的区域大小设定。-这部分是一个距离惩罚项\"><a href=\"#其中，e-是自然对数的底（欧拉数），l-1-和-l-2-是两个距离阈值，用于对得分进行衰减。这两个距离阈值根据深度强化学习（DRL）训练环境的区域大小设定。-这部分是一个距离惩罚项\" class=\"headerlink\" title=\"其中，e 是自然对数的底（欧拉数），l_1 和 l_2 是两个距离阈值，用于对得分进行衰减。这两个距离阈值根据深度强化学习（DRL）训练环境的区域大小设定。 这部分是一个距离惩罚项\"></a>其中，<em>e</em> 是自然对数的底（欧拉数），<em>l_1</em> 和 <em>l_2</em> 是两个距离阈值，用于对得分进行衰减。这两个距离阈值根据深度强化学习（DRL）训练环境的区域大小设定。 这部分是一个距离惩罚项</h4><h4 id=\"注：\"><a href=\"#注：\" class=\"headerlink\" title=\"注：\"></a>注：</h4><blockquote>\n<h4 id=\"分子中指数部分的-d-p-t-c-i-l-2-l-1-这是在对距离-d-做归一化处理，把距离尺度转化为无单位的比值，l-2-l-1-是距离窗口，也就是期望-d-落在这个区间里，这样做的目的是让距离-d-的大小相对于“允许的距离范围”-（l-2-l-1）-来表达，而不是绝对距离大小，方便在指数和平方计算中保持数值的合理范围。\"><a href=\"#分子中指数部分的-d-p-t-c-i-l-2-l-1-这是在对距离-d-做归一化处理，把距离尺度转化为无单位的比值，l-2-l-1-是距离窗口，也就是期望-d-落在这个区间里，这样做的目的是让距离-d-的大小相对于“允许的距离范围”-（l-2-l-1）-来表达，而不是绝对距离大小，方便在指数和平方计算中保持数值的合理范围。\" class=\"headerlink\" title=\"分子中指数部分的 d(p_t,c_i)&#x2F;(l_2 - l_1) 这是在对距离 d 做归一化处理，把距离尺度转化为无单位的比值，l_2 - l_1 是距离窗口，也就是期望 d 落在这个区间里，这样做的目的是让距离 d 的大小相对于“允许的距离范围” （l_2 - l_1） 来表达，而不是绝对距离大小，方便在指数和平方计算中保持数值的合理范围。\"></a>分子中指数部分的 d(p_t,c_i)&#x2F;(l_2 - l_1) 这是在对距离 d 做归一化处理，把距离尺度转化为无单位的比值，l_2 - l_1 是距离窗口，也就是期望 d 落在这个区间里，这样做的目的是让距离 d 的大小相对于“允许的距离范围” （l_2 - l_1） 来表达，而不是绝对距离大小，方便在指数和平方计算中保持数值的合理范围。</h4><h4 id=\"l-2-l-2-l-1-是对-l-2-进行归一化处理，这个值确定了指数表达式的“最大值”或者“参考值”，它让指数中的分子和分母在相同的尺度下进行比较，实现对距离得分的合理缩放和归一化。本质上也是实现了e-x-e-y-这个分式的归一化处理。\"><a href=\"#l-2-l-2-l-1-是对-l-2-进行归一化处理，这个值确定了指数表达式的“最大值”或者“参考值”，它让指数中的分子和分母在相同的尺度下进行比较，实现对距离得分的合理缩放和归一化。本质上也是实现了e-x-e-y-这个分式的归一化处理。\" class=\"headerlink\" title=\"l_2 &#x2F; (l_2 - l_1) 是对 l_2 进行归一化处理，这个值确定了指数表达式的“最大值”或者“参考值”，它让指数中的分子和分母在相同的尺度下进行比较，实现对距离得分的合理缩放和归一化。本质上也是实现了e^x &#x2F; e^y 这个分式的归一化处理。\"></a>l_2 &#x2F; (l_2 - l_1) 是对 l_2 进行归一化处理，这个值确定了指数表达式的“最大值”或者“参考值”，它让指数中的分子和分母在相同的尺度下进行比较，实现对距离得分的合理缩放和归一化。本质上也是实现了e^x &#x2F; e^y 这个分式的归一化处理。</h4><h4 id=\"通过平方，可以让距离差异更敏感，远离阈值的点权重变化更大。\"><a href=\"#通过平方，可以让距离差异更敏感，远离阈值的点权重变化更大。\" class=\"headerlink\" title=\"通过平方，可以让距离差异更敏感，远离阈值的点权重变化更大。\"></a>通过平方，可以让距离差异更敏感，远离阈值的点权重变化更大。</h4><h4 id=\"通过指数放大这个变化，距离较远或较近的点的得分迅速减小或增大，达到对距离分量的平滑衰减效果。\"><a href=\"#通过指数放大这个变化，距离较远或较近的点的得分迅速减小或增大，达到对距离分量的平滑衰减效果。\" class=\"headerlink\" title=\"通过指数放大这个变化，距离较远或较近的点的得分迅速减小或增大，达到对距离分量的平滑衰减效果。\"></a>通过指数放大这个变化，距离较远或较近的点的得分迅速减小或增大，达到对距离分量的<strong>平滑衰减</strong>效果。</h4><h4 id=\"这种平滑衰减避免了距离直接线性权重导致的极端值影响，使得分分布更合理和稳定。\"><a href=\"#这种平滑衰减避免了距离直接线性权重导致的极端值影响，使得分分布更合理和稳定。\" class=\"headerlink\" title=\"这种平滑衰减避免了距离直接线性权重导致的极端值影响，使得分分布更合理和稳定。\"></a>这种平滑衰减避免了距离直接线性权重导致的极端值影响，使得分分布更合理和稳定。</h4><h4 id=\"使用双曲正切函数-tanh-将指数结果映射到-0-1-之间，起到非线性缩放和平滑归一化的作用。\"><a href=\"#使用双曲正切函数-tanh-将指数结果映射到-0-1-之间，起到非线性缩放和平滑归一化的作用。\" class=\"headerlink\" title=\"使用双曲正切函数 tanh() 将指数结果映射到 (0, 1) 之间，起到非线性缩放和平滑归一化的作用。\"></a>使用双曲正切函数 tanh() 将指数结果映射到 (0, 1) 之间，起到非线性缩放和平滑归一化的作用。</h4><h4 id=\"最后乘以-l-2-对距离分量进行尺度调整，映射到-0-l-2-之间\"><a href=\"#最后乘以-l-2-对距离分量进行尺度调整，映射到-0-l-2-之间\" class=\"headerlink\" title=\"最后乘以 l_2 对距离分量进行尺度调整，映射到 (0, l_2)之间\"></a>最后乘以 l_2 对距离分量进行尺度调整，映射到 (0, l_2)之间</h4><h4 id=\"这一部分得到的值，是一个非线性压缩过的距离得分项，目的就是使距离处于-l-1-l-2-之间的候选兴趣点的得分平滑上升，有利于选点策略更稳定\"><a href=\"#这一部分得到的值，是一个非线性压缩过的距离得分项，目的就是使距离处于-l-1-l-2-之间的候选兴趣点的得分平滑上升，有利于选点策略更稳定\" class=\"headerlink\" title=\"这一部分得到的值，是一个非线性压缩过的距离得分项，目的就是使距离处于 l_1~l_2 之间的候选兴趣点的得分平滑上升，有利于选点策略更稳定\"></a>这一部分得到的值，是一个非线性压缩过的距离得分项，目的就是使距离处于 l_1~l_2 之间的候选兴趣点的得分平滑上升，有利于选点策略更稳定</h4></blockquote>\n<h4 id=\"第二个分量-d-c-i-g-表示候选兴趣点-c-i-与全局目标点-g-之间的欧几里得距离（全局目标距离）。\"><a href=\"#第二个分量-d-c-i-g-表示候选兴趣点-c-i-与全局目标点-g-之间的欧几里得距离（全局目标距离）。\" class=\"headerlink\" title=\"第二个分量 d(c_i, g) 表示候选兴趣点 c_i 与全局目标点 g 之间的欧几里得距离（全局目标距离）。\"></a>第二个分量 d(c_i, g) 表示候选兴趣点 c_i 与全局目标点 g 之间的欧几里得距离（全局目标距离）。</h4><h4 id=\"最后，时刻-t-的地图信息得分（信息增益激励项）表示为：\"><a href=\"#最后，时刻-t-的地图信息得分（信息增益激励项）表示为：\" class=\"headerlink\" title=\"最后，时刻 t 的地图信息得分（信息增益激励项）表示为：\"></a>最后，时刻 t 的地图信息得分（信息增益激励项）表示为：</h4><p>$$<br>e^{I_{i,t}}<br>\\tag{3}<br>$$</p>\n<h4 id=\"其中，I-i-t-的计算方式如下：\"><a href=\"#其中，I-i-t-的计算方式如下：\" class=\"headerlink\" title=\"其中，I_{i,t} 的计算方式如下：\"></a>其中，<em>I_{i,t}</em> 的计算方式如下：</h4><p>$$<br>I_{i,t} &#x3D; \\frac{\\sum\\limits_{w&#x3D;-\\frac{k}{2}}^{\\frac{k}{2}} \\sum\\limits_{h&#x3D;-\\frac{k}{2}}^{\\frac{k}{2}} C(x+w)(y+h)}{k^2}<br>\\tag{4}<br>$$</p>\n<h4 id=\"其中，k-表示用于计算候选点周围信息的卷积核大小，候选点的坐标为-x-和-y-，而-w-和-h-分别表示卷积核的宽度和高度。\"><a href=\"#其中，k-表示用于计算候选点周围信息的卷积核大小，候选点的坐标为-x-和-y-，而-w-和-h-分别表示卷积核的宽度和高度。\" class=\"headerlink\" title=\"其中，k 表示用于计算候选点周围信息的卷积核大小，候选点的坐标为 x 和 y ，而 w 和 h 分别表示卷积核的宽度和高度。\"></a>其中，k 表示用于计算候选点周围信息的卷积核大小，候选点的坐标为 x 和 y ，而 w 和 h 分别表示卷积核的宽度和高度。</h4><h4 id=\"在公式-1-中，具有最小-IDLE-得分的兴趣点（POI）被选为用于局部导航的最优航路点。\"><a href=\"#在公式-1-中，具有最小-IDLE-得分的兴趣点（POI）被选为用于局部导航的最优航路点。\" class=\"headerlink\" title=\"在公式 (1) 中，具有最小 IDLE 得分的兴趣点（POI）被选为用于局部导航的最优航路点。\"></a>在公式 (1) 中，具有最小 IDLE 得分的兴趣点（POI）被选为用于局部导航的最优航路点。</h4><h4 id=\"注：-1\"><a href=\"#注：-1\" class=\"headerlink\" title=\"注：\"></a>注：</h4><blockquote>\n<h4 id=\"I-i-t-表示候选兴趣点（POI）c-i-在时间-t-的信息得分，它用于衡量此点周围环境的“信息丰富度”或“探索潜力”，得分越高，说明此-POI-附近还有未探索、未知或值得探索的区域。\"><a href=\"#I-i-t-表示候选兴趣点（POI）c-i-在时间-t-的信息得分，它用于衡量此点周围环境的“信息丰富度”或“探索潜力”，得分越高，说明此-POI-附近还有未探索、未知或值得探索的区域。\" class=\"headerlink\" title=\"I_{i,t} 表示候选兴趣点（POI）c_i 在时间 t 的信息得分，它用于衡量此点周围环境的“信息丰富度”或“探索潜力”，得分越高，说明此 POI 附近还有未探索、未知或值得探索的区域。\"></a>I_{i,t} 表示候选兴趣点（POI）c_i 在时间 t 的<strong>信息得分</strong>，它用于衡量此点周围环境的“信息丰富度”或“探索潜力”，得分越高，说明此 POI 附近还有<strong>未探索、未知或值得探索的区域</strong>。</h4><h4 id=\"C-x-y-是地图上-x-y-点的置信值或不确定度值，可能来源于占据栅格地图中的熵值、不确定度、未知区域标记、置信概率等，通常，这个值越高，表示该位置的信息越少、越值得探索。\"><a href=\"#C-x-y-是地图上-x-y-点的置信值或不确定度值，可能来源于占据栅格地图中的熵值、不确定度、未知区域标记、置信概率等，通常，这个值越高，表示该位置的信息越少、越值得探索。\" class=\"headerlink\" title=\"C(x, y) 是地图上 (x, y) 点的置信值或不确定度值，可能来源于占据栅格地图中的熵值、不确定度、未知区域标记、置信概率等，通常，这个值越高，表示该位置的信息越少、越值得探索。\"></a>C(x, y) 是地图上 (x, y) 点的<strong>置信值或不确定度值</strong>，可能来源于占据栅格地图中的熵值、不确定度、未知区域标记、置信概率等，通常，这个值越高，表示该位置的信息越少、越值得探索。</h4><h4 id=\"双重求和：它是在点-x-y-的周围取了一个大小为-k-k-的滑动窗口，然后对该窗口内所有位置的-C-值进行求和，也就是在以候选点为中心的邻域中，统计该邻域“信息量”。\"><a href=\"#双重求和：它是在点-x-y-的周围取了一个大小为-k-k-的滑动窗口，然后对该窗口内所有位置的-C-值进行求和，也就是在以候选点为中心的邻域中，统计该邻域“信息量”。\" class=\"headerlink\" title=\"双重求和：它是在点 (x, y) 的周围取了一个大小为 k * k 的滑动窗口，然后对该窗口内所有位置的 C 值进行求和，也就是在以候选点为中心的邻域中，统计该邻域“信息量”。\"></a>双重求和：它是在点 (x, y) 的周围取了一个大小为 k * k 的滑动窗口，然后对该窗口内所有位置的 C 值进行求和，也就是在以候选点为中心的邻域中，统计该邻域“信息量”。</h4><h4 id=\"除以-k-2-是做了一个均值操作，即把窗口内的信息总量归一化为平均信息值，得到的值落在-0-1-内，便于指数函数处理。\"><a href=\"#除以-k-2-是做了一个均值操作，即把窗口内的信息总量归一化为平均信息值，得到的值落在-0-1-内，便于指数函数处理。\" class=\"headerlink\" title=\"除以 k^2 是做了一个均值操作，即把窗口内的信息总量归一化为平均信息值，得到的值落在 [0, 1]内，便于指数函数处理。\"></a>除以 k^2 是做了一个<strong>均值操作</strong>，即把窗口内的信息总量归一化为平均信息值，得到的值落在 [0, 1]内，便于指数函数处理。</h4><h4 id=\"其目的就是鼓励探索未知区域\"><a href=\"#其目的就是鼓励探索未知区域\" class=\"headerlink\" title=\"其目的就是鼓励探索未知区域\"></a>其目的就是鼓励探索未知区域</h4></blockquote>\n<h4 id=\"注意到，h-的三个组成部分好像不能直接相加，第一部分得到的是一个得分值，第二部分是一个距离值，第三部分是一个信息增益激励值，貌似是不能直接相加的，但第一部分进行了一个-l-2-的操作，是一个伪距离，最后一部分也进行了指数化操作，所以是可以相加的\"><a href=\"#注意到，h-的三个组成部分好像不能直接相加，第一部分得到的是一个得分值，第二部分是一个距离值，第三部分是一个信息增益激励值，貌似是不能直接相加的，但第一部分进行了一个-l-2-的操作，是一个伪距离，最后一部分也进行了指数化操作，所以是可以相加的\" class=\"headerlink\" title=\"注意到，h 的三个组成部分好像不能直接相加，第一部分得到的是一个得分值，第二部分是一个距离值，第三部分是一个信息增益激励值，貌似是不能直接相加的，但第一部分进行了一个 *l_2 的操作，是一个伪距离，最后一部分也进行了指数化操作，所以是可以相加的\"></a>注意到，h 的三个组成部分好像不能直接相加，第一部分得到的是一个得分值，第二部分是一个距离值，第三部分是一个信息增益激励值，貌似是不能直接相加的，但第一部分进行了一个 *l_2 的操作，是一个伪距离，最后一部分也进行了指数化操作，所以是可以相加的</h4></blockquote>\n<h3 id=\"B-局部导航\"><a href=\"#B-局部导航\" class=\"headerlink\" title=\"B.局部导航\"></a>B.局部导航</h3><blockquote>\n<h4 id=\"使用基于-双延迟深度确定性策略梯度-TD3，一种-Actor-Critic-架构-的神经网络架构来训练运动策略\"><a href=\"#使用基于-双延迟深度确定性策略梯度-TD3，一种-Actor-Critic-架构-的神经网络架构来训练运动策略\" class=\"headerlink\" title=\"使用基于 双延迟深度确定性策略梯度(TD3，一种 Actor-Critic 架构)的神经网络架构来训练运动策略\"></a>使用基于 双延迟深度确定性策略梯度(TD3，一种 Actor-Critic 架构)的神经网络架构来训练运动策略</h4><h4 id=\"局部环境信息和目标航点相对于-agent-位置的极坐标一起，作为状态输入-s-传入-TD3-的Actor-网络中\"><a href=\"#局部环境信息和目标航点相对于-agent-位置的极坐标一起，作为状态输入-s-传入-TD3-的Actor-网络中\" class=\"headerlink\" title=\"局部环境信息和目标航点相对于 agent 位置的极坐标一起，作为状态输入 s 传入 TD3 的Actor 网络中\"></a>局部环境信息和目标航点相对于 agent 位置的极坐标一起，作为状态输入 s 传入 TD3 的Actor 网络中</h4><h4 id=\"该-Actor-网络由两个全连接（FC）层组成，每一层后面都接有-ReLU（修正线性单元）激活函数。\"><a href=\"#该-Actor-网络由两个全连接（FC）层组成，每一层后面都接有-ReLU（修正线性单元）激活函数。\" class=\"headerlink\" title=\"该 Actor 网络由两个全连接（FC）层组成，每一层后面都接有 ReLU（修正线性单元）激活函数。\"></a>该 Actor 网络由两个<strong>全连接（FC）层</strong>组成，每一层后面都接有 <strong>ReLU（修正线性单元）激活函数</strong>。</h4><h4 id=\"最后一层与输出层相连，输出两个动作参数-a，分别表示机器人的线速度-a-1-和角速度-a-2\"><a href=\"#最后一层与输出层相连，输出两个动作参数-a，分别表示机器人的线速度-a-1-和角速度-a-2\" class=\"headerlink\" title=\"最后一层与输出层相连，输出两个动作参数 a，分别表示机器人的线速度 a_1 和角速度 a_2\"></a>最后一层与输出层相连，输出两个动作参数 a，分别表示机器人的线速度 a_1 和角速度 a_2</h4><h4 id=\"输出层采用-tanh-激活函数，将输出限制在区间-−1-1-内\"><a href=\"#输出层采用-tanh-激活函数，将输出限制在区间-−1-1-内\" class=\"headerlink\" title=\"输出层采用 tanh 激活函数，将输出限制在区间 (−1,1) 内\"></a>输出层采用 <strong>tanh 激活函数</strong>，将输出限制在区间 (−1,1) 内</h4><h4 id=\"在将动作应用于环境之前，它们会按以下方式缩放为实际速度值：\"><a href=\"#在将动作应用于环境之前，它们会按以下方式缩放为实际速度值：\" class=\"headerlink\" title=\"在将动作应用于环境之前，它们会按以下方式缩放为实际速度值：\"></a>在将动作应用于环境之前，它们会按以下方式缩放为实际速度值：</h4><p>$$<br>a &#x3D; \\left[ v_{\\max} \\left( \\frac{a_1 + 1}{2} \\right), \\omega_{\\max} a_2 \\right],<br>\\tag{5}<br>$$</p>\n<h4 id=\"最大线速度-v-max，最大角速度-ω-max\"><a href=\"#最大线速度-v-max，最大角速度-ω-max\" class=\"headerlink\" title=\"最大线速度 v_max，最大角速度 ω_max\"></a>最大线速度 v_max，最大角速度 ω_max</h4><h4 id=\"由于激光雷达只记录机器人前方的数据，因此不考虑向后的运动，并将线速度调整为仅为正值。\"><a href=\"#由于激光雷达只记录机器人前方的数据，因此不考虑向后的运动，并将线速度调整为仅为正值。\" class=\"headerlink\" title=\"由于激光雷达只记录机器人前方的数据，因此不考虑向后的运动，并将线速度调整为仅为正值。\"></a>由于激光雷达只记录机器人前方的数据，因此<strong>不考虑向后的运动</strong>，并将<strong>线速度调整为仅为正值</strong>。</h4><h4 id=\"状态-动作对的-Q-值-Q-s-a-由两个-Critic-网络进行评估。这两个-Critic-网络具有相同的结构，但其参数更新是延迟进行的，从而允许它们在参数上产生差异（避免完全同步）。\"><a href=\"#状态-动作对的-Q-值-Q-s-a-由两个-Critic-网络进行评估。这两个-Critic-网络具有相同的结构，但其参数更新是延迟进行的，从而允许它们在参数上产生差异（避免完全同步）。\" class=\"headerlink\" title=\"状态-动作对的 Q 值 Q(s,a) 由两个 Critic 网络进行评估。这两个 Critic 网络具有相同的结构，但其参数更新是延迟进行的，从而允许它们在参数上产生差异（避免完全同步）。\"></a>状态-动作对的 Q 值 Q(s,a) 由两个 <strong>Critic 网络</strong>进行评估。这两个 Critic 网络具有相同的结构，但其参数更新是<strong>延迟进行</strong>的，从而允许它们在参数上产生差异（避免完全同步）。</h4><h4 id=\"Critic-网络以状态-s-和动作-a-的组合作为输入\"><a href=\"#Critic-网络以状态-s-和动作-a-的组合作为输入\" class=\"headerlink\" title=\"Critic 网络以状态 s 和动作 a 的组合作为输入\"></a>Critic 网络以状态 s 和动作 a 的组合作为输入</h4><h4 id=\"其中，状态-s-首先被送入一个全连接层，并接上一个-ReLU-激活函数，输出为-L-s\"><a href=\"#其中，状态-s-首先被送入一个全连接层，并接上一个-ReLU-激活函数，输出为-L-s\" class=\"headerlink\" title=\"其中，状态 s 首先被送入一个全连接层，并接上一个 ReLU 激活函数，输出为 L_s\"></a>其中，状态 s 首先被送入一个<strong>全连接层</strong>，并接上一个 ReLU 激活函数，输出为 L_s</h4><h4 id=\"该层的输出-L-s-以及动作-a，将分别送入两个大小相同的变换全连接层（TFC）中，分别对应变换结果为-τ1-和-τ2，随后，这两个结果按如下方式进行组合：\"><a href=\"#该层的输出-L-s-以及动作-a，将分别送入两个大小相同的变换全连接层（TFC）中，分别对应变换结果为-τ1-和-τ2，随后，这两个结果按如下方式进行组合：\" class=\"headerlink\" title=\"该层的输出 L_s 以及动作 a，将分别送入两个大小相同的变换全连接层（TFC）中，分别对应变换结果为 τ1 和 τ2，随后，这两个结果按如下方式进行组合：\"></a>该层的输出 L_s 以及动作 a，将分别送入两个大小相同的变换全连接层（TFC）中，分别对应变换结果为 τ1 和 τ2，随后，这两个结果按如下方式进行组合：</h4><p>$$<br>L_c &#x3D; L_sW_{\\tau_1} + aW_{\\tau_2} + b_{\\tau_2},<br>\\tag{6}<br>$$</p>\n<h4 id=\"其中，L-c-是组合全连接层（CFC）的输出，W-τ-1-和-W-τ-2-分别是-τ1-和-τ2-的权重，b-τ-2-是-τ-2-的偏执项，然后在这个组合层上使用ReLU激活函数，接着，该输出连接到一个最终输出节点，该节点包含一个参数，表示对应状态-动作对的-Q-值。最终，从两个-Critic-网络中选择较小的-Q-值，作为最后的-Critic-输出，以此来限制对状态-动作值的过高估计。\"><a href=\"#其中，L-c-是组合全连接层（CFC）的输出，W-τ-1-和-W-τ-2-分别是-τ1-和-τ2-的权重，b-τ-2-是-τ-2-的偏执项，然后在这个组合层上使用ReLU激活函数，接着，该输出连接到一个最终输出节点，该节点包含一个参数，表示对应状态-动作对的-Q-值。最终，从两个-Critic-网络中选择较小的-Q-值，作为最后的-Critic-输出，以此来限制对状态-动作值的过高估计。\" class=\"headerlink\" title=\"其中，L_c 是组合全连接层（CFC）的输出，W_τ_1 和 W_τ_2 分别是 τ1 和 τ2 的权重，b_τ_2 是 τ_2 的偏执项，然后在这个组合层上使用ReLU激活函数，接着，该输出连接到一个最终输出节点，该节点包含一个参数，表示对应状态-动作对的 Q 值。最终，从两个 Critic 网络中选择较小的 Q 值，作为最后的 Critic 输出，以此来限制对状态-动作值的过高估计。\"></a>其中，L_c 是组合全连接层（CFC）的输出，W_τ_1 和 W_τ_2 分别是 τ1 和 τ2 的权重，b_τ_2 是 τ_2 的偏执项，然后在这个组合层上使用ReLU激活函数，接着，该输出连接到一个最终输出节点，该节点包含<strong>一个参数</strong>，表示对应状态-动作对的 <strong>Q 值</strong>。最终，从两个 Critic 网络中<strong>选择较小的 Q 值</strong>，作为最后的 Critic 输出，以此来<strong>限制对状态-动作值的过高估计</strong>。</h4><h4 id=\"完整的网络架构如图\"><a href=\"#完整的网络架构如图\" class=\"headerlink\" title=\"完整的网络架构如图\"></a>完整的网络架构如图</h4><p><img src=\"http://picbed.yanzu.tech/img/paper_read/1/3.jpg\"></p>\n<h4 id=\"TD3-网络结构包括-actor-和-critic-两个部分。每一层的类型以及其对应的参数数量在层中都有描述。TFC-层指的是变换全连接层-τ，CFC-层指的是组合全连接层-Lc。\"><a href=\"#TD3-网络结构包括-actor-和-critic-两个部分。每一层的类型以及其对应的参数数量在层中都有描述。TFC-层指的是变换全连接层-τ，CFC-层指的是组合全连接层-Lc。\" class=\"headerlink\" title=\"TD3 网络结构包括 actor 和 critic 两个部分。每一层的类型以及其对应的参数数量在层中都有描述。TFC 层指的是变换全连接层 τ，CFC 层指的是组合全连接层 Lc。\"></a>TD3 网络结构包括 actor 和 critic 两个部分。每一层的类型以及其对应的参数数量在层中都有描述。TFC 层指的是变换全连接层 τ，CFC 层指的是组合全连接层 Lc。</h4><h4 id=\"策略的奖励依据以下函数进行评估-奖励函数\"><a href=\"#策略的奖励依据以下函数进行评估-奖励函数\" class=\"headerlink\" title=\"策略的奖励依据以下函数进行评估(奖励函数)\"></a>策略的奖励依据以下函数进行评估(奖励函数)</h4><p>$$<br>r(s_t, a_t) &#x3D;<br>\\begin{cases}<br>r_g &amp; \\text{if } D_t &lt; \\eta D \\<br>r_c &amp; \\text{if collision} \\<br>v - |\\omega| &amp; \\text{otherwise},<br>\\end{cases},<br>\\tag{7}<br>$$</p>\n<h4 id=\"在时间步-t时，状态-动作对-s-t-a-t-的奖励-r-取决于以下三种情况：\"><a href=\"#在时间步-t时，状态-动作对-s-t-a-t-的奖励-r-取决于以下三种情况：\" class=\"headerlink\" title=\"在时间步 t时，状态-动作对 (s_t, a_t) 的奖励 r 取决于以下三种情况：\"></a>在时间步 t时，状态-动作对 (s_t, a_t) 的奖励 r 取决于以下三种情况：</h4><ul>\n<li><h4 id=\"如果当前时间步与目标点的距离-D-t-小于阈值-η-D，则给予一个正的目标奖励-r-g（也就是鼓励）\"><a href=\"#如果当前时间步与目标点的距离-D-t-小于阈值-η-D，则给予一个正的目标奖励-r-g（也就是鼓励）\" class=\"headerlink\" title=\"如果当前时间步与目标点的距离 D_t 小于阈值 η_D，则给予一个正的目标奖励 r_g（也就是鼓励）\"></a>如果当前时间步与目标点的距离 D_t 小于阈值 η_D，则给予一个正的目标奖励 r_g（也就是鼓励）</h4></li>\n<li><h4 id=\"如果检测到碰撞，则给予一个负的碰撞惩罚-r-c（惩罚）\"><a href=\"#如果检测到碰撞，则给予一个负的碰撞惩罚-r-c（惩罚）\" class=\"headerlink\" title=\"如果检测到碰撞，则给予一个负的碰撞惩罚 r_c（惩罚）\"></a>如果检测到碰撞，则给予一个负的碰撞惩罚 r_c（惩罚）</h4></li>\n<li><h4 id=\"如果以上两种情况均未发生，则根据当前的线速度-v-和角速度-ω-给予即时奖励。\"><a href=\"#如果以上两种情况均未发生，则根据当前的线速度-v-和角速度-ω-给予即时奖励。\" class=\"headerlink\" title=\"如果以上两种情况均未发生，则根据当前的线速度 v 和角速度 ω 给予即时奖励。\"></a>如果以上两种情况均未发生，则根据当前的线速度 v 和角速度 ω 给予即时奖励。</h4></li>\n</ul>\n<h4 id=\"为了引导导航策略朝向给定目标，一个延迟归因奖励方法被采用，其计算如下：\"><a href=\"#为了引导导航策略朝向给定目标，一个延迟归因奖励方法被采用，其计算如下：\" class=\"headerlink\" title=\"为了引导导航策略朝向给定目标，一个延迟归因奖励方法被采用，其计算如下：\"></a>为了引导导航策略朝向给定目标，一个延迟归因奖励方法被采用，其计算如下：</h4><p>$$<br>r_{t-i} &#x3D; r(s_{t-i}, a_{t-i}) + \\frac{r_g}{i}, \\quad \\forall i \\in {1, 2, 3, \\ldots, n},<br>\\tag{8}<br>$$</p>\n<h4 id=\"其中，n-表示前-n-个时间步中需要更新奖励的步数。这意味着，正的目标奖励不仅被分配给达到目标的那个状态-动作对，还以递减的方式分配给其之前的-n-个时间步。通过这种方式，网络学习到了一种局部导航策略，该策略能够仅依赖激光输入，避开障碍物的同时到达局部目标。\"><a href=\"#其中，n-表示前-n-个时间步中需要更新奖励的步数。这意味着，正的目标奖励不仅被分配给达到目标的那个状态-动作对，还以递减的方式分配给其之前的-n-个时间步。通过这种方式，网络学习到了一种局部导航策略，该策略能够仅依赖激光输入，避开障碍物的同时到达局部目标。\" class=\"headerlink\" title=\"其中，n 表示前 n 个时间步中需要更新奖励的步数。这意味着，正的目标奖励不仅被分配给达到目标的那个状态-动作对，还以递减的方式分配给其之前的 n 个时间步。通过这种方式，网络学习到了一种局部导航策略，该策略能够仅依赖激光输入，避开障碍物的同时到达局部目标。\"></a>其中，n 表示前 n 个时间步中需要更新奖励的步数。这意味着，正的目标奖励不仅被分配给达到目标的那个状态-动作对，还以递减的方式分配给其之前的 n 个时间步。通过这种方式，网络学习到了一种局部导航策略，该策略能够仅依赖激光输入，避开障碍物的同时到达局部目标。</h4></blockquote>\n<h3 id=\"C-探索与建图\"><a href=\"#C-探索与建图\" class=\"headerlink\" title=\"C.探索与建图\"></a>C.探索与建图</h3><blockquote>\n<h4 id=\"机器人沿着路径点被引导向全局目标前进。一旦靠近全局目标，机器人将自主导航至该目标。在此过程中，环境被逐步探索和建图。建图依赖于激光雷达和机器人里程计传感器数据，生成环境的占据栅格地图。完整的自主探索与建图算法的伪代码在算法1中进行了描述。\"><a href=\"#机器人沿着路径点被引导向全局目标前进。一旦靠近全局目标，机器人将自主导航至该目标。在此过程中，环境被逐步探索和建图。建图依赖于激光雷达和机器人里程计传感器数据，生成环境的占据栅格地图。完整的自主探索与建图算法的伪代码在算法1中进行了描述。\" class=\"headerlink\" title=\"机器人沿着路径点被引导向全局目标前进。一旦靠近全局目标，机器人将自主导航至该目标。在此过程中，环境被逐步探索和建图。建图依赖于激光雷达和机器人里程计传感器数据，生成环境的占据栅格地图。完整的自主探索与建图算法的伪代码在算法1中进行了描述。\"></a>机器人沿着路径点被引导向全局目标前进。一旦靠近全局目标，机器人将自主导航至该目标。在此过程中，环境被逐步探索和建图。建图依赖于激光雷达和机器人里程计传感器数据，生成环境的占据栅格地图。完整的自主探索与建图算法的伪代码在算法1中进行了描述。</h4></blockquote>\n</blockquote>\n<h3 id=\"实验部分\"><a href=\"#实验部分\" class=\"headerlink\" title=\"实验部分\"></a>实验部分</h3><blockquote>\n<h4 id=\"算法过程：\"><a href=\"#算法过程：\" class=\"headerlink\" title=\"算法过程：\"></a>算法过程：</h4><blockquote>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 输入参数</span><br><span class=\"line\">global_goal\t\t# 全局目标点，即最终导航的目标位置</span><br><span class=\"line\">δ\t\t\t# 导航至全局目标的距离阈值,用于判断机器人是否“足够接近”目标点</span><br><span class=\"line\"></span><br><span class=\"line\"># 主循环 直到达到全局目标</span><br><span class=\"line\">while(reached_global_goal != True)&#123;\t# 判断是否已达到全局目标</span><br><span class=\"line\"></span><br><span class=\"line\">\tread sensor data # 读取传感器数据：如激光雷达、相机、里程计等</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tupdate map from sensor data # 根据传感器数据更新地图：构建或完善占据栅格地图</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tObtain new POI # 获取新的兴趣点 POI,可能是探索边界或未知区域的候选目标点</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t# 判断当前是否已接近目标区域</span><br><span class=\"line\">\tif (D_t &lt; δ_D)&#123;\t# 如果 agent与目标的距离处于接近目标区域</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t\tif(waypoint = global_goal)\t# 若当前导航的子目标 waypoint 已经是 global_goal</span><br><span class=\"line\">\t\t\treachedGlobalGoal = True\t# 那么任务完成</span><br><span class=\"line\">\t\t\t</span><br><span class=\"line\">\t\telse&#123;\t# 否则，进一步判断当前是否靠近全局目标</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\t\tif(d(p_t, g) &lt; δ)\t# 如果当前位置p_t与目标g的距离d(p_t,g) 小于 δ</span><br><span class=\"line\">\t\t\t\twaypoint &lt;-- global_goal\t# 那么就把当前的 waypoint 设置为 global_goal</span><br><span class=\"line\">\t\t\t\t</span><br><span class=\"line\">\t\t\telse\t# 否则，从所有兴趣点中选择下一个最优子目标点</span><br><span class=\"line\">\t\t\t\tfor i in POI\t# 遍历POI中所有的兴趣点</span><br><span class=\"line\">\t\t\t\t\tcaculate h(i) from (1) # 根据式子(1)计算每个兴趣点的启发值h(i)</span><br><span class=\"line\">\t\t\t\twaypoint &lt;-- POI_min(h(i))\t# 将h(i)值最小对应的兴趣点作为新的 waypoint</span><br><span class=\"line\">\t\t\tend if</span><br><span class=\"line\">\t\tend if</span><br><span class=\"line\">\tend if</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tObtain an action from TD3\t# 从 TD3 策略网络中获取当前动作,利用强化学习模型TD3预测最优动作</span><br><span class=\"line\">\tPerform action\t# 执行该动作</span><br><span class=\"line\">end while</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4></blockquote>\n<h3 id=\"A-系统设置\"><a href=\"#A-系统设置\" class=\"headerlink\" title=\"A.系统设置\"></a>A.系统设置</h3><blockquote>\n<h4 id=\"原作者系统配置：\"><a href=\"#原作者系统配置：\" class=\"headerlink\" title=\"原作者系统配置：\"></a>原作者系统配置：</h4><blockquote>\n<h4 id=\"显卡：NVIDIA-GTX-1080\"><a href=\"#显卡：NVIDIA-GTX-1080\" class=\"headerlink\" title=\"显卡：NVIDIA GTX 1080\"></a>显卡：NVIDIA GTX 1080</h4><h4 id=\"运行内存：32G\"><a href=\"#运行内存：32G\" class=\"headerlink\" title=\"运行内存：32G\"></a>运行内存：32G</h4><h4 id=\"CPU：-Intel-Core-i7-6800K\"><a href=\"#CPU：-Intel-Core-i7-6800K\" class=\"headerlink\" title=\"CPU： Intel Core i7-6800K\"></a>CPU： Intel Core i7-6800K</h4></blockquote>\n<h4 id=\"训练参数设置\"><a href=\"#训练参数设置\" class=\"headerlink\" title=\"训练参数设置\"></a>训练参数设置</h4><blockquote>\n<h4 id=\"TD3网络在Gazebo上进行训练，并通过ROS进行控制，训练了800回合，约8h\"><a href=\"#TD3网络在Gazebo上进行训练，并通过ROS进行控制，训练了800回合，约8h\" class=\"headerlink\" title=\"TD3网络在Gazebo上进行训练，并通过ROS进行控制，训练了800回合，约8h\"></a>TD3网络在Gazebo上进行训练，并通过ROS进行控制，训练了800回合，约8h</h4><h4 id=\"每个训练回合在机器人到达目标、发生碰撞或执行了-500-步动作后结束\"><a href=\"#每个训练回合在机器人到达目标、发生碰撞或执行了-500-步动作后结束\" class=\"headerlink\" title=\"每个训练回合在机器人到达目标、发生碰撞或执行了 500 步动作后结束\"></a>每个训练回合在机器人到达目标、发生碰撞或执行了 500 步动作后结束</h4><h4 id=\"最大线速度-v-max-和最大角速度-ω-max-分别设置为-0-5-m-s-和-1rad-s\"><a href=\"#最大线速度-v-max-和最大角速度-ω-max-分别设置为-0-5-m-s-和-1rad-s\" class=\"headerlink\" title=\"最大线速度 v_max 和最大角速度 ω_max 分别设置为 0.5 m&#x2F;s 和 1rad&#x2F;s\"></a>最大线速度 v_max 和最大角速度 ω_max 分别设置为 0.5 m&#x2F;s 和 1rad&#x2F;s</h4><h4 id=\"延迟奖励在最后-n-10-步中更新，参数更新延迟设置为每-2-个回合\"><a href=\"#延迟奖励在最后-n-10-步中更新，参数更新延迟设置为每-2-个回合\" class=\"headerlink\" title=\"延迟奖励在最后 n&#x3D;10 步中更新，参数更新延迟设置为每 2 个回合\"></a>延迟奖励在最后 n&#x3D;10 步中更新，参数更新延迟设置为每 2 个回合</h4></blockquote>\n<h4 id=\"训练在一个-10x10-米的模拟环境中进行，如图所示\"><a href=\"#训练在一个-10x10-米的模拟环境中进行，如图所示\" class=\"headerlink\" title=\"训练在一个 10x10 米的模拟环境中进行，如图所示\"></a>训练在一个 10x10 米的模拟环境中进行，如图所示</h4><p><img src=\"http://picbed.yanzu.tech/img/paper_read/1/4.jpg\"></p>\n<h4 id=\"训练环境示例。蓝色区域表示输入的激光雷达读数和测距。四个箱形障碍物在每一轮训练中都会改变位置，如图-a-、-b-和-c-所示，以实现训练数据的随机化。\"><a href=\"#训练环境示例。蓝色区域表示输入的激光雷达读数和测距。四个箱形障碍物在每一轮训练中都会改变位置，如图-a-、-b-和-c-所示，以实现训练数据的随机化。\" class=\"headerlink\" title=\"训练环境示例。蓝色区域表示输入的激光雷达读数和测距。四个箱形障碍物在每一轮训练中都会改变位置，如图(a)、(b)和(c)所示，以实现训练数据的随机化。\"></a>训练环境示例。蓝色区域表示输入的激光雷达读数和测距。四个箱形障碍物在每一轮训练中都会改变位置，如图(a)、(b)和(c)所示，以实现训练数据的随机化。</h4><h4 id=\"为促进策略的泛化与探索，向传感器读数和动作值中添加了高斯噪声。为了构建多样化的训练环境，在每个回合开始时，箱形障碍物的位置会被随机化。其位置变化的示例如图-a-、-b-、-c-所示。同时，机器人的起始位置与目标位置也在每一回合中被随机设置。\"><a href=\"#为促进策略的泛化与探索，向传感器读数和动作值中添加了高斯噪声。为了构建多样化的训练环境，在每个回合开始时，箱形障碍物的位置会被随机化。其位置变化的示例如图-a-、-b-、-c-所示。同时，机器人的起始位置与目标位置也在每一回合中被随机设置。\" class=\"headerlink\" title=\"为促进策略的泛化与探索，向传感器读数和动作值中添加了高斯噪声。为了构建多样化的训练环境，在每个回合开始时，箱形障碍物的位置会被随机化。其位置变化的示例如图 (a)、(b)、(c) 所示。同时，机器人的起始位置与目标位置也在每一回合中被随机设置。\"></a>为促进策略的泛化与探索，向传感器读数和动作值中添加了高斯噪声。为了构建多样化的训练环境，在每个回合开始时，箱形障碍物的位置会被随机化。其位置变化的示例如图 (a)、(b)、(c) 所示。同时，机器人的起始位置与目标位置也在每一回合中被随机设置。</h4><h4 id=\"ROS-中的-SLAM-Toolbox-软件包用于生成和更新环境的全局地图，并用于机器人在地图中的定位\"><a href=\"#ROS-中的-SLAM-Toolbox-软件包用于生成和更新环境的全局地图，并用于机器人在地图中的定位\" class=\"headerlink\" title=\"ROS 中的 SLAM Toolbox 软件包用于生成和更新环境的全局地图，并用于机器人在地图中的定位\"></a>ROS 中的 SLAM Toolbox 软件包用于生成和更新环境的全局地图，并用于机器人在地图中的定位</h4><h4 id=\"ROS的本地规划器包（TrajectoryPlanner）代替了神经网络\"><a href=\"#ROS的本地规划器包（TrajectoryPlanner）代替了神经网络\" class=\"headerlink\" title=\"ROS的本地规划器包（TrajectoryPlanner）代替了神经网络\"></a>ROS的本地规划器包（TrajectoryPlanner）代替了神经网络</h4><h4 id=\"目标驱动自主探索（GDAE-Goal-Driven-Autonomous-Exploration）\"><a href=\"#目标驱动自主探索（GDAE-Goal-Driven-Autonomous-Exploration）\" class=\"headerlink\" title=\"目标驱动自主探索（GDAE, Goal-Driven Autonomous Exploration）\"></a>目标驱动自主探索（GDAE, Goal-Driven Autonomous Exploration）</h4><h4 id=\"最近前沿探索策略（Nearest-Frontier-NF）\"><a href=\"#最近前沿探索策略（Nearest-Frontier-NF）\" class=\"headerlink\" title=\"最近前沿探索策略（Nearest Frontier, NF）\"></a>最近前沿探索策略（Nearest Frontier, NF）</h4><h4 id=\"目标驱动强化学习（GD-RL-Goal-Driven-Reinforcement-Learning）\"><a href=\"#目标驱动强化学习（GD-RL-Goal-Driven-Reinforcement-Learning）\" class=\"headerlink\" title=\"目标驱动强化学习（GD-RL, Goal-Driven Reinforcement Learning）\"></a>目标驱动强化学习（GD-RL, Goal-Driven Reinforcement Learning）</h4><h4 id=\"本地规划器自主探索（LP-AE，Local-Planner-Autonomous-Exploration）\"><a href=\"#本地规划器自主探索（LP-AE，Local-Planner-Autonomous-Exploration）\" class=\"headerlink\" title=\"本地规划器自主探索（LP-AE，Local Planner Autonomous Exploration）\"></a>本地规划器自主探索（LP-AE，Local Planner Autonomous Exploration）</h4><h4 id=\"路径规划器（PP，Path-Planner）它是基于-Dijkstra-的生成路径的方法\"><a href=\"#路径规划器（PP，Path-Planner）它是基于-Dijkstra-的生成路径的方法\" class=\"headerlink\" title=\"路径规划器（PP，Path Planner）它是基于 Dijkstra 的生成路径的方法\"></a>路径规划器（PP，Path Planner）它是基于 Dijkstra 的生成路径的方法</h4></blockquote>\n<h3 id=\"B-定量实验\"><a href=\"#B-定量实验\" class=\"headerlink\" title=\"B.定量实验\"></a>B.定量实验</h3><h3 id=\"C-定性实验\"><a href=\"#C-定性实验\" class=\"headerlink\" title=\"C.定性实验\"></a>C.定性实验</h3></blockquote>\n<h3 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h3><blockquote>\n<h4 id=\"基于深度强化学习（DRL）的目标驱动型全自主探索系统（GDAE）\"><a href=\"#基于深度强化学习（DRL）的目标驱动型全自主探索系统（GDAE）\" class=\"headerlink\" title=\"基于深度强化学习（DRL）的目标驱动型全自主探索系统（GDAE）\"></a>基于深度强化学习（DRL）的目标驱动型全自主探索系统（GDAE）</h4><h4 id=\"无需人工干预，即可导航至指定目标并记录环境信息（导航去目标点的过程中建图）\"><a href=\"#无需人工干预，即可导航至指定目标并记录环境信息（导航去目标点的过程中建图）\" class=\"headerlink\" title=\"无需人工干预，即可导航至指定目标并记录环境信息（导航去目标点的过程中建图）\"></a>无需人工干预，即可导航至指定目标并记录环境信息（导航去目标点的过程中建图）</h4><h4 id=\"系统有效结合了反应式的本地导航策略和全局导航策略\"><a href=\"#系统有效结合了反应式的本地导航策略和全局导航策略\" class=\"headerlink\" title=\"系统有效结合了反应式的本地导航策略和全局导航策略\"></a>系统有效结合了反应式的本地导航策略和全局导航策略</h4><h4 id=\"将神经网络模块引入端到端系统：机器人无需生成显式路径即可移动，而这一机制的不足则通过引入全局导航策略得到了弥补\"><a href=\"#将神经网络模块引入端到端系统：机器人无需生成显式路径即可移动，而这一机制的不足则通过引入全局导航策略得到了弥补\" class=\"headerlink\" title=\"将神经网络模块引入端到端系统：机器人无需生成显式路径即可移动，而这一机制的不足则通过引入全局导航策略得到了弥补\"></a>将神经网络模块引入端到端系统：机器人无需生成显式路径即可移动，而这一机制的不足则通过引入全局导航策略得到了弥补</h4><h4 id=\"系统的导航性能接近于基于已知地图路径规划器所得的最优解\"><a href=\"#系统的导航性能接近于基于已知地图路径规划器所得的最优解\" class=\"headerlink\" title=\"系统的导航性能接近于基于已知地图路径规划器所得的最优解\"></a>系统的导航性能接近于基于已知地图路径规划器所得的最优解</h4><h4 id=\"GDAE-系统依赖直接的传感器输入而非从不确定地图生成路径，因此在可靠性方面表现更佳\"><a href=\"#GDAE-系统依赖直接的传感器输入而非从不确定地图生成路径，因此在可靠性方面表现更佳\" class=\"headerlink\" title=\"GDAE 系统依赖直接的传感器输入而非从不确定地图生成路径，因此在可靠性方面表现更佳\"></a>GDAE 系统依赖<strong>直接的传感器输入</strong>而非从不确定地图生成路径，因此在可靠性方面表现更佳</h4><h4 id=\"若希望进一步泛化至不同类型的机器人，可以将机器人动力学作为神经网络的一个输入状态，并据此开展训练。只需提供机器人动力学信息，即可在不同平台上实现最优的本地导航。\"><a href=\"#若希望进一步泛化至不同类型的机器人，可以将机器人动力学作为神经网络的一个输入状态，并据此开展训练。只需提供机器人动力学信息，即可在不同平台上实现最优的本地导航。\" class=\"headerlink\" title=\"若希望进一步泛化至不同类型的机器人，可以将机器人动力学作为神经网络的一个输入状态，并据此开展训练。只需提供机器人动力学信息，即可在不同平台上实现最优的本地导航。\"></a>若希望进一步<strong>泛化至不同类型的机器人</strong>，可以将<strong>机器人动力学作为神经网络的一个输入状态</strong>，并据此开展训练。只需提供机器人动力学信息，即可在不同平台上实现最优的本地导航。</h4><h4 id=\"接下来的研究：\"><a href=\"#接下来的研究：\" class=\"headerlink\" title=\"接下来的研究：\"></a>接下来的研究：</h4><h4 id=\"引入长短时记忆（LSTM）结构也可能有助于缓解局部最优问题，并辅助规避当前传感器视野之外的障碍物\"><a href=\"#引入长短时记忆（LSTM）结构也可能有助于缓解局部最优问题，并辅助规避当前传感器视野之外的障碍物\" class=\"headerlink\" title=\"引入长短时记忆（LSTM）结构也可能有助于缓解局部最优问题，并辅助规避当前传感器视野之外的障碍物\"></a>引入<strong>长短时记忆（LSTM）结构</strong>也可能有助于缓解局部最优问题，并辅助规避当前传感器视野之外的障碍物</h4></blockquote>\n<h3 id=\"代码部分\"><a href=\"#代码部分\" class=\"headerlink\" title=\"代码部分\"></a>代码部分</h3><blockquote>\n<h4 id=\"两个量的定义\"><a href=\"#两个量的定义\" class=\"headerlink\" title=\"两个量的定义\"></a>两个量的定义</h4><h4 id=\"Episode：\"><a href=\"#Episode：\" class=\"headerlink\" title=\"Episode：\"></a>Episode：</h4><blockquote>\n<h4 id=\"它是后续步骤的集合，直到达到其中一个终止条件（终止条件：达到目标、与障碍物碰撞或达到最大步数max-ep）\"><a href=\"#它是后续步骤的集合，直到达到其中一个终止条件（终止条件：达到目标、与障碍物碰撞或达到最大步数max-ep）\" class=\"headerlink\" title=\"它是后续步骤的集合，直到达到其中一个终止条件（终止条件：达到目标、与障碍物碰撞或达到最大步数max_ep）\"></a>它是后续步骤的集合，直到达到其中一个终止条件（终止条件：达到目标、与障碍物碰撞或达到最大步数max_ep）</h4><h4 id=\"它可以理解是一个训练过程，如果达到终止条件就开始新的一个训练过程，并且伴随着环境的改变，包括区域内的墙体位置改变、障碍物的位置改变、agent的初始位置改变和目标点位置的改变\"><a href=\"#它可以理解是一个训练过程，如果达到终止条件就开始新的一个训练过程，并且伴随着环境的改变，包括区域内的墙体位置改变、障碍物的位置改变、agent的初始位置改变和目标点位置的改变\" class=\"headerlink\" title=\"它可以理解是一个训练过程，如果达到终止条件就开始新的一个训练过程，并且伴随着环境的改变，包括区域内的墙体位置改变、障碍物的位置改变、agent的初始位置改变和目标点位置的改变\"></a>它可以理解是一个训练过程，如果达到终止条件就开始新的一个训练过程，并且伴随着环境的改变，包括区域内的墙体位置改变、障碍物的位置改变、agent的初始位置改变和目标点位置的改变</h4></blockquote>\n<h4 id=\"Epoch：\"><a href=\"#Epoch：\" class=\"headerlink\" title=\"Epoch：\"></a>Epoch：</h4><blockquote>\n<h4 id=\"执行评估之间的后续事件数（episode）或者时间步长（timesteps）\"><a href=\"#执行评估之间的后续事件数（episode）或者时间步长（timesteps）\" class=\"headerlink\" title=\"执行评估之间的后续事件数（episode）或者时间步长（timesteps）\"></a>执行评估之间的后续事件数（episode）或者时间步长（timesteps）</h4><h4 id=\"一个epoch运行5000个步骤step（对应代码中的-eval-freq-这个参数），步长为0-1秒，也就是说一个epoch大概要运行8分多钟\"><a href=\"#一个epoch运行5000个步骤step（对应代码中的-eval-freq-这个参数），步长为0-1秒，也就是说一个epoch大概要运行8分多钟\" class=\"headerlink\" title=\"一个epoch运行5000个步骤step（对应代码中的 eval_freq 这个参数），步长为0.1秒，也就是说一个epoch大概要运行8分多钟\"></a>一个epoch运行5000个步骤step（对应代码中的 eval_freq 这个参数），步长为0.1秒，也就是说一个epoch大概要运行8分多钟</h4></blockquote>\n</blockquote>\n","cover_type":"img","excerpt":"","more":"<h1 id=\"Goal-Driven-Autonomous-Exploration-Through-Deep-Reinforcement-Learning—通过深度强化学习实现目标驱动的自主探索\"><a href=\"#Goal-Driven-Autonomous-Exploration-Through-Deep-Reinforcement-Learning—通过深度强化学习实现目标驱动的自主探索\" class=\"headerlink\" title=\"Goal-Driven Autonomous Exploration Through Deep Reinforcement  Learning—通过深度强化学习实现目标驱动的自主探索\"></a>Goal-Driven Autonomous Exploration Through Deep Reinforcement  Learning—通过深度强化学习实现目标驱动的自主探索</h1><h3 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h3><blockquote>\n<blockquote>\n<h4 id=\"本文提出了一种基于深度强化学习（DRL）的自主导航系统，用于在未知环境中进行目标驱动的探索。系统从环境中获取可能的导航方向的兴趣点（POI），并基于可用数据选择一个最优的航路点。机器人根据这些航路点被引导向全局目标，缓解了反应式导航中的局部最优问题。随后，在仿真环境中通过DRL框架学习用于局部导航的运动策略。我们开发了一个导航系统，将该学习到的策略集成到运动规划系统中，作为局部导航层，用于驱动机器人在航路点之间移动，最终到达全局目标。整个自主导航过程中无需任何先验知识，同时在机器人移动过程中会记录环境地图。实验表明，该方法在无需地图或先验信息的情况下，在复杂的静态和动态环境中相较于其他探索方法具有优势。\"><a href=\"#本文提出了一种基于深度强化学习（DRL）的自主导航系统，用于在未知环境中进行目标驱动的探索。系统从环境中获取可能的导航方向的兴趣点（POI），并基于可用数据选择一个最优的航路点。机器人根据这些航路点被引导向全局目标，缓解了反应式导航中的局部最优问题。随后，在仿真环境中通过DRL框架学习用于局部导航的运动策略。我们开发了一个导航系统，将该学习到的策略集成到运动规划系统中，作为局部导航层，用于驱动机器人在航路点之间移动，最终到达全局目标。整个自主导航过程中无需任何先验知识，同时在机器人移动过程中会记录环境地图。实验表明，该方法在无需地图或先验信息的情况下，在复杂的静态和动态环境中相较于其他探索方法具有优势。\" class=\"headerlink\" title=\"本文提出了一种基于深度强化学习（DRL）的自主导航系统，用于在未知环境中进行目标驱动的探索。系统从环境中获取可能的导航方向的兴趣点（POI），并基于可用数据选择一个最优的航路点。机器人根据这些航路点被引导向全局目标，缓解了反应式导航中的局部最优问题。随后，在仿真环境中通过DRL框架学习用于局部导航的运动策略。我们开发了一个导航系统，将该学习到的策略集成到运动规划系统中，作为局部导航层，用于驱动机器人在航路点之间移动，最终到达全局目标。整个自主导航过程中无需任何先验知识，同时在机器人移动过程中会记录环境地图。实验表明，该方法在无需地图或先验信息的情况下，在复杂的静态和动态环境中相较于其他探索方法具有优势。\"></a>本文提出了一种基于深度强化学习（DRL）的自主导航系统，用于在未知环境中进行目标驱动的探索。系统从环境中获取可能的导航方向的兴趣点（POI），并基于可用数据选择一个最优的航路点。机器人根据这些航路点被引导向全局目标，缓解了反应式导航中的局部最优问题。随后，在仿真环境中通过DRL框架学习用于局部导航的运动策略。我们开发了一个导航系统，将该学习到的策略集成到运动规划系统中，作为局部导航层，用于驱动机器人在航路点之间移动，最终到达全局目标。整个自主导航过程中无需任何先验知识，同时在机器人移动过程中会记录环境地图。实验表明，该方法在无需地图或先验信息的情况下，在复杂的静态和动态环境中相较于其他探索方法具有优势。</h4></blockquote>\n<h4 id=\"自主导航系统\"><a href=\"#自主导航系统\" class=\"headerlink\" title=\"自主导航系统\"></a>自主导航系统</h4><h4 id=\"用于通过DRL对未知环境进行目标驱动的探索\"><a href=\"#用于通过DRL对未知环境进行目标驱动的探索\" class=\"headerlink\" title=\"用于通过DRL对未知环境进行目标驱动的探索\"></a>用于通过DRL对未知环境进行目标驱动的探索</h4><h4 id=\"获取可能导航方向的兴趣点-POI\"><a href=\"#获取可能导航方向的兴趣点-POI\" class=\"headerlink\" title=\"获取可能导航方向的兴趣点(POI)\"></a>获取可能导航方向的兴趣点(POI)</h4><h4 id=\"根据可用数据选择最佳航路点\"><a href=\"#根据可用数据选择最佳航路点\" class=\"headerlink\" title=\"根据可用数据选择最佳航路点\"></a>根据可用数据选择最佳航路点</h4><h4 id=\"缓解反应式导航中的局部最优问题\"><a href=\"#缓解反应式导航中的局部最优问题\" class=\"headerlink\" title=\"缓解反应式导航中的局部最优问题\"></a>缓解反应式导航中的局部最优问题</h4><h4 id=\"采用的是TD3算法（双延迟深度确定性策略梯度）\"><a href=\"#采用的是TD3算法（双延迟深度确定性策略梯度）\" class=\"headerlink\" title=\"采用的是TD3算法（双延迟深度确定性策略梯度）\"></a>采用的是TD3算法（双延迟深度确定性策略梯度）</h4><blockquote>\n<h4 id=\"它是在DDPG算法的基础上进行的扩展\"><a href=\"#它是在DDPG算法的基础上进行的扩展\" class=\"headerlink\" title=\"它是在DDPG算法的基础上进行的扩展\"></a>它是在DDPG算法的基础上进行的扩展</h4><h4 id=\"DDPG（Deep-Deterministic-Policy-Gradient）深度确定性策略梯度：它是一种同时学习Q函数和策略的方法，它使用非策略数据和贝尔曼方程来学习Q函数，并使用Q函数来学习策略\"><a href=\"#DDPG（Deep-Deterministic-Policy-Gradient）深度确定性策略梯度：它是一种同时学习Q函数和策略的方法，它使用非策略数据和贝尔曼方程来学习Q函数，并使用Q函数来学习策略\" class=\"headerlink\" title=\"DDPG（Deep Deterministic Policy Gradient）深度确定性策略梯度：它是一种同时学习Q函数和策略的方法，它使用非策略数据和贝尔曼方程来学习Q函数，并使用Q函数来学习策略\"></a>DDPG（Deep Deterministic Policy Gradient）深度确定性策略梯度：它是一种同时学习Q函数和策略的方法，它使用非策略数据和贝尔曼方程来学习Q函数，并使用Q函数来学习策略</h4><h4 id=\"详细介绍：https-spinningup-openai-com-en-latest-algorithms-ddpg-html-background\"><a href=\"#详细介绍：https-spinningup-openai-com-en-latest-algorithms-ddpg-html-background\" class=\"headerlink\" title=\"详细介绍：https://spinningup.openai.com/en/latest/algorithms/ddpg.html#background\"></a>详细介绍：<a href=\"https://spinningup.openai.com/en/latest/algorithms/ddpg.html#background\">https://spinningup.openai.com/en/latest/algorithms/ddpg.html#background</a></h4><h4 id=\"DDPG-对超参数和其他调优手段非常敏感，表现常常不稳定，它的一个常见失败模式是：所学习的-Q-函数会严重高估-Q-值，进而导致策略崩溃，因为策略会利用-Q-函数中的误差\"><a href=\"#DDPG-对超参数和其他调优手段非常敏感，表现常常不稳定，它的一个常见失败模式是：所学习的-Q-函数会严重高估-Q-值，进而导致策略崩溃，因为策略会利用-Q-函数中的误差\" class=\"headerlink\" title=\"DDPG 对超参数和其他调优手段非常敏感，表现常常不稳定，它的一个常见失败模式是：所学习的 Q 函数会严重高估 Q 值，进而导致策略崩溃，因为策略会利用 Q 函数中的误差\"></a>DDPG 对超参数和其他调优手段<strong>非常敏感</strong>，表现常常不稳定，它的一个常见失败模式是：所学习的 Q 函数会<strong>严重高估 Q 值</strong>，进而导致策略崩溃，因为策略会利用 Q 函数中的误差</h4><h4 id=\"而双延迟深度确定性策略梯度算法（TD3）引入三个关键技巧来解决DDPG的问题：\"><a href=\"#而双延迟深度确定性策略梯度算法（TD3）引入三个关键技巧来解决DDPG的问题：\" class=\"headerlink\" title=\"而双延迟深度确定性策略梯度算法（TD3）引入三个关键技巧来解决DDPG的问题：\"></a>而双延迟深度确定性策略梯度算法（TD3）引入三个关键技巧来解决DDPG的问题：</h4><blockquote>\n<ol>\n<li><h4 id=\"截断的双Q学习：TD3-同时学习两个-Q-函数（因此称为“双”Q），在计算贝尔曼误差损失函数中的目标值时，取两个Q值中较小的一个，以降低过估计的风险\"><a href=\"#截断的双Q学习：TD3-同时学习两个-Q-函数（因此称为“双”Q），在计算贝尔曼误差损失函数中的目标值时，取两个Q值中较小的一个，以降低过估计的风险\" class=\"headerlink\" title=\"截断的双Q学习：TD3 同时学习两个 Q 函数（因此称为“双”Q），在计算贝尔曼误差损失函数中的目标值时，取两个Q值中较小的一个，以降低过估计的风险\"></a>截断的双Q学习：TD3 同时学习两个 Q 函数（因此称为“双”Q），在计算贝尔曼误差损失函数中的目标值时，取两个Q值中较小的一个，以降低过估计的风险</h4></li>\n<li><h4 id=\"延迟的策略更新：TD3-中，策略网络（Actor）和目标网络的更新频率低于-Q-函数的更新频率，建议是，更新两次-Q-网络，只更新-一次策略网络，以提高训练稳定性\"><a href=\"#延迟的策略更新：TD3-中，策略网络（Actor）和目标网络的更新频率低于-Q-函数的更新频率，建议是，更新两次-Q-网络，只更新-一次策略网络，以提高训练稳定性\" class=\"headerlink\" title=\"延迟的策略更新：TD3 中，策略网络（Actor）和目标网络的更新频率低于 Q 函数的更新频率，建议是，更新两次 Q 网络，只更新 一次策略网络，以提高训练稳定性\"></a>延迟的策略更新：TD3 中，策略网络（Actor）和目标网络的更新频率<strong>低于 Q 函数</strong>的更新频率，建议是，更新两次 Q 网络，只更新 <strong>一次策略网络</strong>，以提高训练稳定性</h4></li>\n<li><h4 id=\"目标策略平滑：TD3-在计算目标-Q-值时，会在目标动作上添加噪声，一般是高斯噪声，使得动作空间内的-Q-值更平滑，从而防止策略去“钻空子”利用-Q-函数中的误差\"><a href=\"#目标策略平滑：TD3-在计算目标-Q-值时，会在目标动作上添加噪声，一般是高斯噪声，使得动作空间内的-Q-值更平滑，从而防止策略去“钻空子”利用-Q-函数中的误差\" class=\"headerlink\" title=\"目标策略平滑：TD3 在计算目标 Q 值时，会在目标动作上添加噪声，一般是高斯噪声，使得动作空间内的 Q 值更平滑，从而防止策略去“钻空子”利用 Q 函数中的误差\"></a>目标策略平滑：TD3 在计算目标 Q 值时，会在目标动作上添加噪声，一般是高斯噪声，使得动作空间内的 Q 值更平滑，从而防止策略去“钻空子”利用 Q 函数中的误差</h4></li>\n</ol>\n</blockquote>\n<h4 id=\"详细介绍：https-spinningup-openai-com-en-latest-algorithms-td3-html\"><a href=\"#详细介绍：https-spinningup-openai-com-en-latest-algorithms-td3-html\" class=\"headerlink\" title=\"详细介绍：https://spinningup.openai.com/en/latest/algorithms/td3.html\"></a>详细介绍：<a href=\"https://spinningup.openai.com/en/latest/algorithms/td3.html\">https://spinningup.openai.com/en/latest/algorithms/td3.html</a></h4></blockquote>\n</blockquote>\n<h3 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h3><blockquote>\n<h4 id=\"完全自主的目标驱动探索是一个包含两个方面的问题\"><a href=\"#完全自主的目标驱动探索是一个包含两个方面的问题\" class=\"headerlink\" title=\"完全自主的目标驱动探索是一个包含两个方面的问题\"></a><strong>完全自主的目标驱动探索</strong>是一个包含两个方面的问题</h4><blockquote>\n<h4 id=\"首先，探索机器人需要决策去哪一个方向，以最大可能性到达全局目标。在没有先验信息或无法观测到全局目标的情况下，系统需要直接从传感器数据中指示可能的导航方向。从这些兴趣点（POI）中，需要选择一个最优点作为航路点，以最优的方式引导机器人接近全局目标。\"><a href=\"#首先，探索机器人需要决策去哪一个方向，以最大可能性到达全局目标。在没有先验信息或无法观测到全局目标的情况下，系统需要直接从传感器数据中指示可能的导航方向。从这些兴趣点（POI）中，需要选择一个最优点作为航路点，以最优的方式引导机器人接近全局目标。\" class=\"headerlink\" title=\"首先，探索机器人需要决策去哪一个方向，以最大可能性到达全局目标。在没有先验信息或无法观测到全局目标的情况下，系统需要直接从传感器数据中指示可能的导航方向。从这些兴趣点（POI）中，需要选择一个最优点作为航路点，以最优的方式引导机器人接近全局目标。\"></a>首先，探索机器人需要决策去哪一个方向，以最大可能性到达全局目标。在没有先验信息或无法观测到全局目标的情况下，系统需要直接从传感器数据中指示可能的导航方向。从这些兴趣点（POI）中，需要选择一个最优点作为航路点，以最优的方式引导机器人接近全局目标。</h4><h4 id=\"其次，在不依赖地图数据的前提下，还需要获得一种适用于不确定环境的机器人运动策略。\"><a href=\"#其次，在不依赖地图数据的前提下，还需要获得一种适用于不确定环境的机器人运动策略。\" class=\"headerlink\" title=\"其次，在不依赖地图数据的前提下，还需要获得一种适用于不确定环境的机器人运动策略。\"></a>其次，在不依赖地图数据的前提下，还需要获得一种适用于不确定环境的机器人运动策略。</h4><h4 id=\"这就引入了DRL\"><a href=\"#这就引入了DRL\" class=\"headerlink\" title=\"这就引入了DRL\"></a>这就引入了DRL</h4><h4 id=\"但是DRL存在一个问题：DRL具有反应性特征且缺乏全局信息，其容易陷入局部最优问题尤其是在大规模导航任务中表现尤为明显。\"><a href=\"#但是DRL存在一个问题：DRL具有反应性特征且缺乏全局信息，其容易陷入局部最优问题尤其是在大规模导航任务中表现尤为明显。\" class=\"headerlink\" title=\"但是DRL存在一个问题：DRL具有反应性特征且缺乏全局信息，其容易陷入局部最优问题尤其是在大规模导航任务中表现尤为明显。\"></a>但是DRL存在一个问题：DRL具有反应性特征且缺乏全局信息，其容易陷入局部最优问题尤其是在大规模导航任务中表现尤为明显。</h4><blockquote>\n<h4 id=\"DRL的反应性特征\"><a href=\"#DRL的反应性特征\" class=\"headerlink\" title=\"DRL的反应性特征\"></a>DRL的反应性特征</h4><blockquote>\n<h4 id=\"DRL中的大多策略都是基于当前状态做出决策的，也即-reactive-policy（反应式策略），也就是具有马尔可夫性\"><a href=\"#DRL中的大多策略都是基于当前状态做出决策的，也即-reactive-policy（反应式策略），也就是具有马尔可夫性\" class=\"headerlink\" title=\"DRL中的大多策略都是基于当前状态做出决策的，也即 reactive policy（反应式策略），也就是具有马尔可夫性\"></a>DRL中的大多策略都是基于当前状态做出决策的，也即 reactive policy（反应式策略），也就是具有马尔可夫性</h4><h4 id=\"这会导致：\"><a href=\"#这会导致：\" class=\"headerlink\" title=\"这会导致：\"></a>这会导致：</h4><ul>\n<li><h4 id=\"agent只依据当前观测（摄像头图像、雷达扫描）来决定或判断下一动作\"><a href=\"#agent只依据当前观测（摄像头图像、雷达扫描）来决定或判断下一动作\" class=\"headerlink\" title=\"agent只依据当前观测（摄像头图像、雷达扫描）来决定或判断下一动作\"></a>agent只依据当前观测（摄像头图像、雷达扫描）来决定或判断下一动作</h4></li>\n<li><h4 id=\"没有全局地图或任务结果的长期记忆，缺少更高层次的计划或决策能力\"><a href=\"#没有全局地图或任务结果的长期记忆，缺少更高层次的计划或决策能力\" class=\"headerlink\" title=\"没有全局地图或任务结果的长期记忆，缺少更高层次的计划或决策能力\"></a>没有全局地图或任务结果的长期记忆，缺少更高层次的计划或决策能力</h4></li>\n<li><h4 id=\"动作的选择往往基于短期奖励最大化，而非长期全局最优\"><a href=\"#动作的选择往往基于短期奖励最大化，而非长期全局最优\" class=\"headerlink\" title=\"动作的选择往往基于短期奖励最大化，而非长期全局最优\"></a>动作的选择往往基于短期奖励最大化，而非长期全局最优</h4></li>\n</ul>\n<h4 id=\"这种策略适用于快速反应、实时避障等场景\"><a href=\"#这种策略适用于快速反应、实时避障等场景\" class=\"headerlink\" title=\"这种策略适用于快速反应、实时避障等场景\"></a>这种策略适用于快速反应、实时避障等场景</h4></blockquote>\n<h4 id=\"全局信息的缺乏\"><a href=\"#全局信息的缺乏\" class=\"headerlink\" title=\"全局信息的缺乏\"></a>全局信息的缺乏</h4><blockquote>\n<h4 id=\"大多数DRL系统的输入都是局部传感器数据（当前位置的摄像头图像、雷达扫描），因此DRL也只能感知局部环境。另外，强化学习RL本身就是一种局部策略的学习方法，它更适合解决的是马尔可夫决策过程MDP问题。还有就是，DRL的训练目标就是最大化累计奖励，而非构建认知地图。\"><a href=\"#大多数DRL系统的输入都是局部传感器数据（当前位置的摄像头图像、雷达扫描），因此DRL也只能感知局部环境。另外，强化学习RL本身就是一种局部策略的学习方法，它更适合解决的是马尔可夫决策过程MDP问题。还有就是，DRL的训练目标就是最大化累计奖励，而非构建认知地图。\" class=\"headerlink\" title=\"大多数DRL系统的输入都是局部传感器数据（当前位置的摄像头图像、雷达扫描），因此DRL也只能感知局部环境。另外，强化学习RL本身就是一种局部策略的学习方法，它更适合解决的是马尔可夫决策过程MDP问题。还有就是，DRL的训练目标就是最大化累计奖励，而非构建认知地图。\"></a>大多数DRL系统的输入都是局部传感器数据（当前位置的摄像头图像、雷达扫描），因此DRL也只能感知局部环境。另外，强化学习RL本身就是一种局部策略的学习方法，它更适合解决的是马尔可夫决策过程MDP问题。还有就是，DRL的训练目标就是最大化累计奖励，而非构建认知地图。</h4></blockquote>\n<h4 id=\"局部最优问题\"><a href=\"#局部最优问题\" class=\"headerlink\" title=\"局部最优问题\"></a>局部最优问题</h4><blockquote>\n<h4 id=\"系统在当前状态下选择了一个看似收益最大（或最短路径）的行为，但这个选择却可能阻碍了其通向真正全局最优解的路径。\"><a href=\"#系统在当前状态下选择了一个看似收益最大（或最短路径）的行为，但这个选择却可能阻碍了其通向真正全局最优解的路径。\" class=\"headerlink\" title=\"系统在当前状态下选择了一个看似收益最大（或最短路径）的行为，但这个选择却可能阻碍了其通向真正全局最优解的路径。\"></a>系统在当前状态下选择了一个看似收益最大（或最短路径）的行为，但这个选择却可能阻碍了其通向真正全局最优解的路径。</h4></blockquote>\n</blockquote>\n</blockquote>\n<h4 id=\"本文提出一种完全自主的探索系统，用于在无需人工控制或环境先验信息的情况下实现导航至全局目标。\"><a href=\"#本文提出一种完全自主的探索系统，用于在无需人工控制或环境先验信息的情况下实现导航至全局目标。\" class=\"headerlink\" title=\"本文提出一种完全自主的探索系统，用于在无需人工控制或环境先验信息的情况下实现导航至全局目标。\"></a>本文提出一种完全自主的探索系统，用于在无需人工控制或环境先验信息的情况下实现导航至全局目标。</h4><h4 id=\"该系统从机器人周围的局部环境中提取兴趣点（POI），对其进行评估，并从中选取一个作为航路点（waypoint）。\"><a href=\"#该系统从机器人周围的局部环境中提取兴趣点（POI），对其进行评估，并从中选取一个作为航路点（waypoint）。\" class=\"headerlink\" title=\"该系统从机器人周围的局部环境中提取兴趣点（POI），对其进行评估，并从中选取一个作为航路点（waypoint）。\"></a>该系统从机器人周围的局部环境中提取<strong>兴趣点（POI）</strong>，对其进行评估，并从中选取一个作为航路点（waypoint）。</h4><h4 id=\"这些航路点用于引导基于深度强化学习（DRL）的运动策略朝向全局目标，从而缓解局部最优问题。\"><a href=\"#这些航路点用于引导基于深度强化学习（DRL）的运动策略朝向全局目标，从而缓解局部最优问题。\" class=\"headerlink\" title=\"这些航路点用于引导基于深度强化学习（DRL）的运动策略朝向全局目标，从而缓解局部最优问题。\"></a>这些航路点用于引导基于深度强化学习（DRL）的运动策略朝向全局目标，从而缓解局部最优问题。</h4><h4 id=\"机器人依据该策略进行运动，无需对周围环境进行完整建图。\"><a href=\"#机器人依据该策略进行运动，无需对周围环境进行完整建图。\" class=\"headerlink\" title=\"机器人依据该策略进行运动，无需对周围环境进行完整建图。\"></a>机器人依据该策略进行运动，无需对周围环境进行完整建图。</h4><h4 id=\"POI\"><a href=\"#POI\" class=\"headerlink\" title=\"POI\"></a>POI</h4><blockquote>\n<h4 id=\"这里所说的-POI-兴趣点，是指机器人环境中那些具有导航价值的特定点或位置，这些点通常是机器人决策“去哪儿”的候选目标。它们帮助机器人在未知环境里选择下一步行动方向，特别是在没有完整地图信息时。POI-可以作为导航路径的候选点，也即中间航路点waypoint。\"><a href=\"#这里所说的-POI-兴趣点，是指机器人环境中那些具有导航价值的特定点或位置，这些点通常是机器人决策“去哪儿”的候选目标。它们帮助机器人在未知环境里选择下一步行动方向，特别是在没有完整地图信息时。POI-可以作为导航路径的候选点，也即中间航路点waypoint。\" class=\"headerlink\" title=\"这里所说的 POI 兴趣点，是指机器人环境中那些具有导航价值的特定点或位置，这些点通常是机器人决策“去哪儿”的候选目标。它们帮助机器人在未知环境里选择下一步行动方向，特别是在没有完整地图信息时。POI 可以作为导航路径的候选点，也即中间航路点waypoint。\"></a>这里所说的 POI 兴趣点，是指机器人环境中那些具有导航价值的特定点或位置，这些点通常是机器人决策“去哪儿”的候选目标。它们帮助机器人在未知环境里选择下一步行动方向，特别是在没有完整地图信息时。POI 可以作为导航路径的候选点，也即中间航路点waypoint。</h4><h4 id=\"POI的确定\"><a href=\"#POI的确定\" class=\"headerlink\" title=\"POI的确定\"></a>POI的确定</h4><blockquote>\n<h4 id=\"激光读数间隙产生的POI：当连续两束激光雷达测距数值差距较大时，说明可能有一个开口或通道，这个位置就被视作一个-POI。\"><a href=\"#激光读数间隙产生的POI：当连续两束激光雷达测距数值差距较大时，说明可能有一个开口或通道，这个位置就被视作一个-POI。\" class=\"headerlink\" title=\"激光读数间隙产生的POI：当连续两束激光雷达测距数值差距较大时，说明可能有一个开口或通道，这个位置就被视作一个 POI。\"></a>激光读数间隙产生的POI：当连续两束激光雷达测距数值差距较大时，说明可能有一个开口或通道，这个位置就被视作一个 POI。</h4><h4 id=\"非数值激光读数产生的POI：激光雷达超出测量范围时返回非数值，代表远处有自由空间，这些边缘位置也被标记为兴趣点。\"><a href=\"#非数值激光读数产生的POI：激光雷达超出测量范围时返回非数值，代表远处有自由空间，这些边缘位置也被标记为兴趣点。\" class=\"headerlink\" title=\"非数值激光读数产生的POI：激光雷达超出测量范围时返回非数值，代表远处有自由空间，这些边缘位置也被标记为兴趣点。\"></a>非数值激光读数产生的POI：激光雷达超出测量范围时返回非数值，代表远处有自由空间，这些边缘位置也被标记为兴趣点。</h4></blockquote>\n</blockquote>\n<h4 id=\"导航系统的实现：左侧展示了机器人的设置，中间和右侧分别可视化了全局导航与局部导航的组成部分及其数据流。\"><a href=\"#导航系统的实现：左侧展示了机器人的设置，中间和右侧分别可视化了全局导航与局部导航的组成部分及其数据流。\" class=\"headerlink\" title=\"导航系统的实现：左侧展示了机器人的设置，中间和右侧分别可视化了全局导航与局部导航的组成部分及其数据流。\"></a>导航系统的实现：左侧展示了机器人的设置，中间和右侧分别可视化了全局导航与局部导航的组成部分及其数据流。</h4><p><img src=\"http://picbed.yanzu.tech/img/paper_read/1/1.jpg\"></p>\n</blockquote>\n<h3 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h3><blockquote>\n<h4 id=\"导航结构分两部分：\"><a href=\"#导航结构分两部分：\" class=\"headerlink\" title=\"导航结构分两部分：\"></a>导航结构分两部分：</h4><blockquote>\n<h4 id=\"具有最优航路点选择机制的全局导航与建图模块\"><a href=\"#具有最优航路点选择机制的全局导航与建图模块\" class=\"headerlink\" title=\"具有最优航路点选择机制的全局导航与建图模块\"></a>具有<strong>最优航路点选择机制</strong>的<strong>全局导航与建图</strong>模块</h4><h4 id=\"基于深度强化学习的局部导航模块\"><a href=\"#基于深度强化学习的局部导航模块\" class=\"headerlink\" title=\"基于深度强化学习的局部导航模块\"></a>基于<strong>深度强化学习</strong>的<strong>局部导航</strong>模块</h4></blockquote>\n<h4 id=\"系统首先从环境中提取兴趣点（POI），并依据设定的评估标准选择一个最优航路点。\"><a href=\"#系统首先从环境中提取兴趣点（POI），并依据设定的评估标准选择一个最优航路点。\" class=\"headerlink\" title=\"系统首先从环境中提取兴趣点（POI），并依据设定的评估标准选择一个最优航路点。\"></a>系统首先从环境中提取<strong>兴趣点（POI）</strong>，并依据设定的评估标准选择一个<strong>最优航路点</strong>。</h4><h4 id=\"在每一个导航步骤中，系统会将该航路点以相对于机器人当前位置与朝向的极坐标形式输入神经网络。\"><a href=\"#在每一个导航步骤中，系统会将该航路点以相对于机器人当前位置与朝向的极坐标形式输入神经网络。\" class=\"headerlink\" title=\"在每一个导航步骤中，系统会将该航路点以相对于机器人当前位置与朝向的极坐标形式输入神经网络。\"></a>在每一个导航步骤中，系统会将该航路点以<strong>相对于机器人当前位置与朝向的极坐标形式</strong>输入神经网络。</h4><h4 id=\"随后，网络基于当前的传感器数据计算并执行一个动作，以朝向该航路点运动。\"><a href=\"#随后，网络基于当前的传感器数据计算并执行一个动作，以朝向该航路点运动。\" class=\"headerlink\" title=\"随后，网络基于当前的传感器数据计算并执行一个动作，以朝向该航路点运动。\"></a>随后，网络基于当前的传感器数据计算并执行一个动作，以朝向该航路点运动。</h4><h4 id=\"在机器人沿着多个航路点逐步向全局目标移动的过程中，同时完成对环境的地图构建（建图）。\"><a href=\"#在机器人沿着多个航路点逐步向全局目标移动的过程中，同时完成对环境的地图构建（建图）。\" class=\"headerlink\" title=\"在机器人沿着多个航路点逐步向全局目标移动的过程中，同时完成对环境的地图构建（建图）。\"></a>在机器人沿着多个航路点逐步向全局目标移动的过程中，同时完成对环境的<strong>地图构建（建图）</strong>。</h4><h3 id=\"A-全局导航\"><a href=\"#A-全局导航\" class=\"headerlink\" title=\"A.全局导航\"></a>A.全局导航</h3><blockquote>\n<h4 id=\"为了使机器人能够朝向全局目标导航，必须从当前可用的兴趣点（POI）中选择用于局部导航的中间航路点。\"><a href=\"#为了使机器人能够朝向全局目标导航，必须从当前可用的兴趣点（POI）中选择用于局部导航的中间航路点。\" class=\"headerlink\" title=\"为了使机器人能够朝向全局目标导航，必须从当前可用的兴趣点（POI）中选择用于局部导航的中间航路点。\"></a>为了使机器人能够朝向全局目标导航，必须从当前可用的兴趣点（POI）中选择<strong>用于局部导航的中间航路点</strong>。</h4><h4 id=\"机器人不仅需要被引导前往目标，还必须在行进过程中探索周围环境，以便在遇到死路时能够识别出可能的替代路径。\"><a href=\"#机器人不仅需要被引导前往目标，还必须在行进过程中探索周围环境，以便在遇到死路时能够识别出可能的替代路径。\" class=\"headerlink\" title=\"机器人不仅需要被引导前往目标，还必须在行进过程中探索周围环境，以便在遇到死路时能够识别出可能的替代路径。\"></a>机器人不仅需要被引导前往目标，还必须在行进过程中<strong>探索周围环境</strong>，以便在遇到死路时能够识别出可能的替代路径。</h4><h4 id=\"鉴于没有预先提供的环境信息，所有可能的-POI-必须从机器人当前的周边环境中提取，并存储在内存中以供后续使用。\"><a href=\"#鉴于没有预先提供的环境信息，所有可能的-POI-必须从机器人当前的周边环境中提取，并存储在内存中以供后续使用。\" class=\"headerlink\" title=\"鉴于没有预先提供的环境信息，所有可能的 POI 必须从机器人当前的周边环境中提取，并存储在内存中以供后续使用。\"></a>鉴于没有预先提供的环境信息，所有可能的 POI 必须从<strong>机器人当前的周边环境中提取</strong>，并<strong>存储在内存中</strong>以供后续使用。</h4><h4 id=\"如果在后续步骤中发现某个兴趣点（POI）位于障碍物附近，则该点会从内存中删除。\"><a href=\"#如果在后续步骤中发现某个兴趣点（POI）位于障碍物附近，则该点会从内存中删除。\" class=\"headerlink\" title=\"如果在后续步骤中发现某个兴趣点（POI）位于障碍物附近，则该点会从内存中删除。\"></a>如果在后续步骤中发现某个兴趣点（POI）位于障碍物附近，则该点会从内存中删除。</h4><h4 id=\"在机器人已经访问过的位置，激光雷达不会再提取新的-POI。\"><a href=\"#在机器人已经访问过的位置，激光雷达不会再提取新的-POI。\" class=\"headerlink\" title=\"在机器人已经访问过的位置，激光雷达不会再提取新的 POI。\"></a>在机器人已经访问过的位置，激光雷达不会再提取新的 POI。</h4><h4 id=\"此外，如果某个-POI-被选为航路点但在若干步内始终无法到达，它也会被删除，并重新选择一个新的航路点。\"><a href=\"#此外，如果某个-POI-被选为航路点但在若干步内始终无法到达，它也会被删除，并重新选择一个新的航路点。\" class=\"headerlink\" title=\"此外，如果某个 POI 被选为航路点但在若干步内始终无法到达，它也会被删除，并重新选择一个新的航路点。\"></a>此外，如果某个 POI 被选为航路点但在若干步内始终无法到达，它也会被删除，并重新选择一个新的航路点。</h4><h4 id=\"获取新的POI的方法：就是上面提到的POI的确定\"><a href=\"#获取新的POI的方法：就是上面提到的POI的确定\" class=\"headerlink\" title=\"获取新的POI的方法：就是上面提到的POI的确定\"></a>获取新的POI的方法：就是上面提到的POI的确定</h4><h4 id=\"从当前环境中提取POI，蓝色圆圈表示通过相应方法提取的兴趣点，绿色圆圈表示当前的航路点。\"><a href=\"#从当前环境中提取POI，蓝色圆圈表示通过相应方法提取的兴趣点，绿色圆圈表示当前的航路点。\" class=\"headerlink\" title=\"从当前环境中提取POI，蓝色圆圈表示通过相应方法提取的兴趣点，绿色圆圈表示当前的航路点。\"></a>从当前环境中提取POI，蓝色圆圈表示通过相应方法提取的兴趣点，绿色圆圈表示当前的航路点。</h4><h4 id=\"a-蓝色-POI-是通过激光雷达测距数据之间的间隙提取的；\"><a href=\"#a-蓝色-POI-是通过激光雷达测距数据之间的间隙提取的；\" class=\"headerlink\" title=\"(a) 蓝色 POI 是通过激光雷达测距数据之间的间隙提取的；\"></a>(a) 蓝色 POI 是通过激光雷达测距数据之间的间隙提取的；</h4><h4 id=\"b-蓝色-POI-是从非数值型的激光读数中提取的。\"><a href=\"#b-蓝色-POI-是从非数值型的激光读数中提取的。\" class=\"headerlink\" title=\"(b)蓝色 POI 是从非数值型的激光读数中提取的。\"></a>(b)蓝色 POI 是从非数值型的激光读数中提取的。</h4><p><img src=\"http://picbed.yanzu.tech/img/paper_read/1/2.jpg\"></p>\n<h4 id=\"在时刻-t，从当前可用的兴趣点（POI）中，使用基于信息的距离受限探索方法（Information-based-Distance-Limited-Exploration，简称-IDLE）来选择最优的航路点。IDLE-方法通过以下方式评估每个候选-POI-的适应度：\"><a href=\"#在时刻-t，从当前可用的兴趣点（POI）中，使用基于信息的距离受限探索方法（Information-based-Distance-Limited-Exploration，简称-IDLE）来选择最优的航路点。IDLE-方法通过以下方式评估每个候选-POI-的适应度：\" class=\"headerlink\" title=\"在时刻 t，从当前可用的兴趣点（POI）中，使用基于信息的距离受限探索方法（Information-based Distance Limited Exploration，简称 IDLE）来选择最优的航路点。IDLE 方法通过以下方式评估每个候选 POI 的适应度：\"></a>在时刻 <em>t</em>，从当前可用的兴趣点（POI）中，使用<strong>基于信息的距离受限探索方法</strong>（Information-based Distance Limited Exploration，简称 <strong>IDLE</strong>）来选择最优的航路点。IDLE 方法通过以下方式评估每个候选 POI 的适应度：</h4><p>$$<br>h(c_i) &#x3D; \\tanh\\left(\\frac{e^{\\left(\\frac{d(p_t, c_i)}{l_2 - l_1}\\right)^2}}{e^{\\left(\\frac{l_2}{l_2 - l_1}\\right)^2}}\\right) l_2 + d(c_i, g) + e^{I_{i,t}}<br>\\tag{1}<br>$$</p>\n<h4 id=\"每个候选兴趣点-c-索引为-i-的得分-h-由三部分组成\"><a href=\"#每个候选兴趣点-c-索引为-i-的得分-h-由三部分组成\" class=\"headerlink\" title=\"每个候选兴趣点 c (索引为 i) 的得分 h 由三部分组成\"></a>每个候选兴趣点 c (索引为 i) 的得分 h 由三部分组成</h4><h4 id=\"其中，机器人在时刻-t-的位置-p-t-与候选兴趣点-c-i-之间的欧几里得距离分量-d-p-t-c-i-被表示为一个双曲正切函数（tanh）形式：\"><a href=\"#其中，机器人在时刻-t-的位置-p-t-与候选兴趣点-c-i-之间的欧几里得距离分量-d-p-t-c-i-被表示为一个双曲正切函数（tanh）形式：\" class=\"headerlink\" title=\"其中，机器人在时刻 t 的位置 p_t 与候选兴趣点 c_i 之间的欧几里得距离分量 d(p_t, c_i) 被表示为一个双曲正切函数（tanh）形式：\"></a>其中，机器人在时刻 t 的位置 p_t 与候选兴趣点 <em>c_i</em> 之间的欧几里得距离分量 d(p_t, c_i) 被表示为一个双曲正切函数（tanh）形式：</h4><p>$$<br>\\tanh\\left(\\frac{e^{\\left(\\frac{d(p_t, c_i)}{l_2 - l_1}\\right)^2}}{e^{\\left(\\frac{l_2}{l_2 - l_1}\\right)^2}}\\right) l_2<br>\\tag{2}<br>$$</p>\n<h4 id=\"其中，e-是自然对数的底（欧拉数），l-1-和-l-2-是两个距离阈值，用于对得分进行衰减。这两个距离阈值根据深度强化学习（DRL）训练环境的区域大小设定。-这部分是一个距离惩罚项\"><a href=\"#其中，e-是自然对数的底（欧拉数），l-1-和-l-2-是两个距离阈值，用于对得分进行衰减。这两个距离阈值根据深度强化学习（DRL）训练环境的区域大小设定。-这部分是一个距离惩罚项\" class=\"headerlink\" title=\"其中，e 是自然对数的底（欧拉数），l_1 和 l_2 是两个距离阈值，用于对得分进行衰减。这两个距离阈值根据深度强化学习（DRL）训练环境的区域大小设定。 这部分是一个距离惩罚项\"></a>其中，<em>e</em> 是自然对数的底（欧拉数），<em>l_1</em> 和 <em>l_2</em> 是两个距离阈值，用于对得分进行衰减。这两个距离阈值根据深度强化学习（DRL）训练环境的区域大小设定。 这部分是一个距离惩罚项</h4><h4 id=\"注：\"><a href=\"#注：\" class=\"headerlink\" title=\"注：\"></a>注：</h4><blockquote>\n<h4 id=\"分子中指数部分的-d-p-t-c-i-l-2-l-1-这是在对距离-d-做归一化处理，把距离尺度转化为无单位的比值，l-2-l-1-是距离窗口，也就是期望-d-落在这个区间里，这样做的目的是让距离-d-的大小相对于“允许的距离范围”-（l-2-l-1）-来表达，而不是绝对距离大小，方便在指数和平方计算中保持数值的合理范围。\"><a href=\"#分子中指数部分的-d-p-t-c-i-l-2-l-1-这是在对距离-d-做归一化处理，把距离尺度转化为无单位的比值，l-2-l-1-是距离窗口，也就是期望-d-落在这个区间里，这样做的目的是让距离-d-的大小相对于“允许的距离范围”-（l-2-l-1）-来表达，而不是绝对距离大小，方便在指数和平方计算中保持数值的合理范围。\" class=\"headerlink\" title=\"分子中指数部分的 d(p_t,c_i)&#x2F;(l_2 - l_1) 这是在对距离 d 做归一化处理，把距离尺度转化为无单位的比值，l_2 - l_1 是距离窗口，也就是期望 d 落在这个区间里，这样做的目的是让距离 d 的大小相对于“允许的距离范围” （l_2 - l_1） 来表达，而不是绝对距离大小，方便在指数和平方计算中保持数值的合理范围。\"></a>分子中指数部分的 d(p_t,c_i)&#x2F;(l_2 - l_1) 这是在对距离 d 做归一化处理，把距离尺度转化为无单位的比值，l_2 - l_1 是距离窗口，也就是期望 d 落在这个区间里，这样做的目的是让距离 d 的大小相对于“允许的距离范围” （l_2 - l_1） 来表达，而不是绝对距离大小，方便在指数和平方计算中保持数值的合理范围。</h4><h4 id=\"l-2-l-2-l-1-是对-l-2-进行归一化处理，这个值确定了指数表达式的“最大值”或者“参考值”，它让指数中的分子和分母在相同的尺度下进行比较，实现对距离得分的合理缩放和归一化。本质上也是实现了e-x-e-y-这个分式的归一化处理。\"><a href=\"#l-2-l-2-l-1-是对-l-2-进行归一化处理，这个值确定了指数表达式的“最大值”或者“参考值”，它让指数中的分子和分母在相同的尺度下进行比较，实现对距离得分的合理缩放和归一化。本质上也是实现了e-x-e-y-这个分式的归一化处理。\" class=\"headerlink\" title=\"l_2 &#x2F; (l_2 - l_1) 是对 l_2 进行归一化处理，这个值确定了指数表达式的“最大值”或者“参考值”，它让指数中的分子和分母在相同的尺度下进行比较，实现对距离得分的合理缩放和归一化。本质上也是实现了e^x &#x2F; e^y 这个分式的归一化处理。\"></a>l_2 &#x2F; (l_2 - l_1) 是对 l_2 进行归一化处理，这个值确定了指数表达式的“最大值”或者“参考值”，它让指数中的分子和分母在相同的尺度下进行比较，实现对距离得分的合理缩放和归一化。本质上也是实现了e^x &#x2F; e^y 这个分式的归一化处理。</h4><h4 id=\"通过平方，可以让距离差异更敏感，远离阈值的点权重变化更大。\"><a href=\"#通过平方，可以让距离差异更敏感，远离阈值的点权重变化更大。\" class=\"headerlink\" title=\"通过平方，可以让距离差异更敏感，远离阈值的点权重变化更大。\"></a>通过平方，可以让距离差异更敏感，远离阈值的点权重变化更大。</h4><h4 id=\"通过指数放大这个变化，距离较远或较近的点的得分迅速减小或增大，达到对距离分量的平滑衰减效果。\"><a href=\"#通过指数放大这个变化，距离较远或较近的点的得分迅速减小或增大，达到对距离分量的平滑衰减效果。\" class=\"headerlink\" title=\"通过指数放大这个变化，距离较远或较近的点的得分迅速减小或增大，达到对距离分量的平滑衰减效果。\"></a>通过指数放大这个变化，距离较远或较近的点的得分迅速减小或增大，达到对距离分量的<strong>平滑衰减</strong>效果。</h4><h4 id=\"这种平滑衰减避免了距离直接线性权重导致的极端值影响，使得分分布更合理和稳定。\"><a href=\"#这种平滑衰减避免了距离直接线性权重导致的极端值影响，使得分分布更合理和稳定。\" class=\"headerlink\" title=\"这种平滑衰减避免了距离直接线性权重导致的极端值影响，使得分分布更合理和稳定。\"></a>这种平滑衰减避免了距离直接线性权重导致的极端值影响，使得分分布更合理和稳定。</h4><h4 id=\"使用双曲正切函数-tanh-将指数结果映射到-0-1-之间，起到非线性缩放和平滑归一化的作用。\"><a href=\"#使用双曲正切函数-tanh-将指数结果映射到-0-1-之间，起到非线性缩放和平滑归一化的作用。\" class=\"headerlink\" title=\"使用双曲正切函数 tanh() 将指数结果映射到 (0, 1) 之间，起到非线性缩放和平滑归一化的作用。\"></a>使用双曲正切函数 tanh() 将指数结果映射到 (0, 1) 之间，起到非线性缩放和平滑归一化的作用。</h4><h4 id=\"最后乘以-l-2-对距离分量进行尺度调整，映射到-0-l-2-之间\"><a href=\"#最后乘以-l-2-对距离分量进行尺度调整，映射到-0-l-2-之间\" class=\"headerlink\" title=\"最后乘以 l_2 对距离分量进行尺度调整，映射到 (0, l_2)之间\"></a>最后乘以 l_2 对距离分量进行尺度调整，映射到 (0, l_2)之间</h4><h4 id=\"这一部分得到的值，是一个非线性压缩过的距离得分项，目的就是使距离处于-l-1-l-2-之间的候选兴趣点的得分平滑上升，有利于选点策略更稳定\"><a href=\"#这一部分得到的值，是一个非线性压缩过的距离得分项，目的就是使距离处于-l-1-l-2-之间的候选兴趣点的得分平滑上升，有利于选点策略更稳定\" class=\"headerlink\" title=\"这一部分得到的值，是一个非线性压缩过的距离得分项，目的就是使距离处于 l_1~l_2 之间的候选兴趣点的得分平滑上升，有利于选点策略更稳定\"></a>这一部分得到的值，是一个非线性压缩过的距离得分项，目的就是使距离处于 l_1~l_2 之间的候选兴趣点的得分平滑上升，有利于选点策略更稳定</h4></blockquote>\n<h4 id=\"第二个分量-d-c-i-g-表示候选兴趣点-c-i-与全局目标点-g-之间的欧几里得距离（全局目标距离）。\"><a href=\"#第二个分量-d-c-i-g-表示候选兴趣点-c-i-与全局目标点-g-之间的欧几里得距离（全局目标距离）。\" class=\"headerlink\" title=\"第二个分量 d(c_i, g) 表示候选兴趣点 c_i 与全局目标点 g 之间的欧几里得距离（全局目标距离）。\"></a>第二个分量 d(c_i, g) 表示候选兴趣点 c_i 与全局目标点 g 之间的欧几里得距离（全局目标距离）。</h4><h4 id=\"最后，时刻-t-的地图信息得分（信息增益激励项）表示为：\"><a href=\"#最后，时刻-t-的地图信息得分（信息增益激励项）表示为：\" class=\"headerlink\" title=\"最后，时刻 t 的地图信息得分（信息增益激励项）表示为：\"></a>最后，时刻 t 的地图信息得分（信息增益激励项）表示为：</h4><p>$$<br>e^{I_{i,t}}<br>\\tag{3}<br>$$</p>\n<h4 id=\"其中，I-i-t-的计算方式如下：\"><a href=\"#其中，I-i-t-的计算方式如下：\" class=\"headerlink\" title=\"其中，I_{i,t} 的计算方式如下：\"></a>其中，<em>I_{i,t}</em> 的计算方式如下：</h4><p>$$<br>I_{i,t} &#x3D; \\frac{\\sum\\limits_{w&#x3D;-\\frac{k}{2}}^{\\frac{k}{2}} \\sum\\limits_{h&#x3D;-\\frac{k}{2}}^{\\frac{k}{2}} C(x+w)(y+h)}{k^2}<br>\\tag{4}<br>$$</p>\n<h4 id=\"其中，k-表示用于计算候选点周围信息的卷积核大小，候选点的坐标为-x-和-y-，而-w-和-h-分别表示卷积核的宽度和高度。\"><a href=\"#其中，k-表示用于计算候选点周围信息的卷积核大小，候选点的坐标为-x-和-y-，而-w-和-h-分别表示卷积核的宽度和高度。\" class=\"headerlink\" title=\"其中，k 表示用于计算候选点周围信息的卷积核大小，候选点的坐标为 x 和 y ，而 w 和 h 分别表示卷积核的宽度和高度。\"></a>其中，k 表示用于计算候选点周围信息的卷积核大小，候选点的坐标为 x 和 y ，而 w 和 h 分别表示卷积核的宽度和高度。</h4><h4 id=\"在公式-1-中，具有最小-IDLE-得分的兴趣点（POI）被选为用于局部导航的最优航路点。\"><a href=\"#在公式-1-中，具有最小-IDLE-得分的兴趣点（POI）被选为用于局部导航的最优航路点。\" class=\"headerlink\" title=\"在公式 (1) 中，具有最小 IDLE 得分的兴趣点（POI）被选为用于局部导航的最优航路点。\"></a>在公式 (1) 中，具有最小 IDLE 得分的兴趣点（POI）被选为用于局部导航的最优航路点。</h4><h4 id=\"注：-1\"><a href=\"#注：-1\" class=\"headerlink\" title=\"注：\"></a>注：</h4><blockquote>\n<h4 id=\"I-i-t-表示候选兴趣点（POI）c-i-在时间-t-的信息得分，它用于衡量此点周围环境的“信息丰富度”或“探索潜力”，得分越高，说明此-POI-附近还有未探索、未知或值得探索的区域。\"><a href=\"#I-i-t-表示候选兴趣点（POI）c-i-在时间-t-的信息得分，它用于衡量此点周围环境的“信息丰富度”或“探索潜力”，得分越高，说明此-POI-附近还有未探索、未知或值得探索的区域。\" class=\"headerlink\" title=\"I_{i,t} 表示候选兴趣点（POI）c_i 在时间 t 的信息得分，它用于衡量此点周围环境的“信息丰富度”或“探索潜力”，得分越高，说明此 POI 附近还有未探索、未知或值得探索的区域。\"></a>I_{i,t} 表示候选兴趣点（POI）c_i 在时间 t 的<strong>信息得分</strong>，它用于衡量此点周围环境的“信息丰富度”或“探索潜力”，得分越高，说明此 POI 附近还有<strong>未探索、未知或值得探索的区域</strong>。</h4><h4 id=\"C-x-y-是地图上-x-y-点的置信值或不确定度值，可能来源于占据栅格地图中的熵值、不确定度、未知区域标记、置信概率等，通常，这个值越高，表示该位置的信息越少、越值得探索。\"><a href=\"#C-x-y-是地图上-x-y-点的置信值或不确定度值，可能来源于占据栅格地图中的熵值、不确定度、未知区域标记、置信概率等，通常，这个值越高，表示该位置的信息越少、越值得探索。\" class=\"headerlink\" title=\"C(x, y) 是地图上 (x, y) 点的置信值或不确定度值，可能来源于占据栅格地图中的熵值、不确定度、未知区域标记、置信概率等，通常，这个值越高，表示该位置的信息越少、越值得探索。\"></a>C(x, y) 是地图上 (x, y) 点的<strong>置信值或不确定度值</strong>，可能来源于占据栅格地图中的熵值、不确定度、未知区域标记、置信概率等，通常，这个值越高，表示该位置的信息越少、越值得探索。</h4><h4 id=\"双重求和：它是在点-x-y-的周围取了一个大小为-k-k-的滑动窗口，然后对该窗口内所有位置的-C-值进行求和，也就是在以候选点为中心的邻域中，统计该邻域“信息量”。\"><a href=\"#双重求和：它是在点-x-y-的周围取了一个大小为-k-k-的滑动窗口，然后对该窗口内所有位置的-C-值进行求和，也就是在以候选点为中心的邻域中，统计该邻域“信息量”。\" class=\"headerlink\" title=\"双重求和：它是在点 (x, y) 的周围取了一个大小为 k * k 的滑动窗口，然后对该窗口内所有位置的 C 值进行求和，也就是在以候选点为中心的邻域中，统计该邻域“信息量”。\"></a>双重求和：它是在点 (x, y) 的周围取了一个大小为 k * k 的滑动窗口，然后对该窗口内所有位置的 C 值进行求和，也就是在以候选点为中心的邻域中，统计该邻域“信息量”。</h4><h4 id=\"除以-k-2-是做了一个均值操作，即把窗口内的信息总量归一化为平均信息值，得到的值落在-0-1-内，便于指数函数处理。\"><a href=\"#除以-k-2-是做了一个均值操作，即把窗口内的信息总量归一化为平均信息值，得到的值落在-0-1-内，便于指数函数处理。\" class=\"headerlink\" title=\"除以 k^2 是做了一个均值操作，即把窗口内的信息总量归一化为平均信息值，得到的值落在 [0, 1]内，便于指数函数处理。\"></a>除以 k^2 是做了一个<strong>均值操作</strong>，即把窗口内的信息总量归一化为平均信息值，得到的值落在 [0, 1]内，便于指数函数处理。</h4><h4 id=\"其目的就是鼓励探索未知区域\"><a href=\"#其目的就是鼓励探索未知区域\" class=\"headerlink\" title=\"其目的就是鼓励探索未知区域\"></a>其目的就是鼓励探索未知区域</h4></blockquote>\n<h4 id=\"注意到，h-的三个组成部分好像不能直接相加，第一部分得到的是一个得分值，第二部分是一个距离值，第三部分是一个信息增益激励值，貌似是不能直接相加的，但第一部分进行了一个-l-2-的操作，是一个伪距离，最后一部分也进行了指数化操作，所以是可以相加的\"><a href=\"#注意到，h-的三个组成部分好像不能直接相加，第一部分得到的是一个得分值，第二部分是一个距离值，第三部分是一个信息增益激励值，貌似是不能直接相加的，但第一部分进行了一个-l-2-的操作，是一个伪距离，最后一部分也进行了指数化操作，所以是可以相加的\" class=\"headerlink\" title=\"注意到，h 的三个组成部分好像不能直接相加，第一部分得到的是一个得分值，第二部分是一个距离值，第三部分是一个信息增益激励值，貌似是不能直接相加的，但第一部分进行了一个 *l_2 的操作，是一个伪距离，最后一部分也进行了指数化操作，所以是可以相加的\"></a>注意到，h 的三个组成部分好像不能直接相加，第一部分得到的是一个得分值，第二部分是一个距离值，第三部分是一个信息增益激励值，貌似是不能直接相加的，但第一部分进行了一个 *l_2 的操作，是一个伪距离，最后一部分也进行了指数化操作，所以是可以相加的</h4></blockquote>\n<h3 id=\"B-局部导航\"><a href=\"#B-局部导航\" class=\"headerlink\" title=\"B.局部导航\"></a>B.局部导航</h3><blockquote>\n<h4 id=\"使用基于-双延迟深度确定性策略梯度-TD3，一种-Actor-Critic-架构-的神经网络架构来训练运动策略\"><a href=\"#使用基于-双延迟深度确定性策略梯度-TD3，一种-Actor-Critic-架构-的神经网络架构来训练运动策略\" class=\"headerlink\" title=\"使用基于 双延迟深度确定性策略梯度(TD3，一种 Actor-Critic 架构)的神经网络架构来训练运动策略\"></a>使用基于 双延迟深度确定性策略梯度(TD3，一种 Actor-Critic 架构)的神经网络架构来训练运动策略</h4><h4 id=\"局部环境信息和目标航点相对于-agent-位置的极坐标一起，作为状态输入-s-传入-TD3-的Actor-网络中\"><a href=\"#局部环境信息和目标航点相对于-agent-位置的极坐标一起，作为状态输入-s-传入-TD3-的Actor-网络中\" class=\"headerlink\" title=\"局部环境信息和目标航点相对于 agent 位置的极坐标一起，作为状态输入 s 传入 TD3 的Actor 网络中\"></a>局部环境信息和目标航点相对于 agent 位置的极坐标一起，作为状态输入 s 传入 TD3 的Actor 网络中</h4><h4 id=\"该-Actor-网络由两个全连接（FC）层组成，每一层后面都接有-ReLU（修正线性单元）激活函数。\"><a href=\"#该-Actor-网络由两个全连接（FC）层组成，每一层后面都接有-ReLU（修正线性单元）激活函数。\" class=\"headerlink\" title=\"该 Actor 网络由两个全连接（FC）层组成，每一层后面都接有 ReLU（修正线性单元）激活函数。\"></a>该 Actor 网络由两个<strong>全连接（FC）层</strong>组成，每一层后面都接有 <strong>ReLU（修正线性单元）激活函数</strong>。</h4><h4 id=\"最后一层与输出层相连，输出两个动作参数-a，分别表示机器人的线速度-a-1-和角速度-a-2\"><a href=\"#最后一层与输出层相连，输出两个动作参数-a，分别表示机器人的线速度-a-1-和角速度-a-2\" class=\"headerlink\" title=\"最后一层与输出层相连，输出两个动作参数 a，分别表示机器人的线速度 a_1 和角速度 a_2\"></a>最后一层与输出层相连，输出两个动作参数 a，分别表示机器人的线速度 a_1 和角速度 a_2</h4><h4 id=\"输出层采用-tanh-激活函数，将输出限制在区间-−1-1-内\"><a href=\"#输出层采用-tanh-激活函数，将输出限制在区间-−1-1-内\" class=\"headerlink\" title=\"输出层采用 tanh 激活函数，将输出限制在区间 (−1,1) 内\"></a>输出层采用 <strong>tanh 激活函数</strong>，将输出限制在区间 (−1,1) 内</h4><h4 id=\"在将动作应用于环境之前，它们会按以下方式缩放为实际速度值：\"><a href=\"#在将动作应用于环境之前，它们会按以下方式缩放为实际速度值：\" class=\"headerlink\" title=\"在将动作应用于环境之前，它们会按以下方式缩放为实际速度值：\"></a>在将动作应用于环境之前，它们会按以下方式缩放为实际速度值：</h4><p>$$<br>a &#x3D; \\left[ v_{\\max} \\left( \\frac{a_1 + 1}{2} \\right), \\omega_{\\max} a_2 \\right],<br>\\tag{5}<br>$$</p>\n<h4 id=\"最大线速度-v-max，最大角速度-ω-max\"><a href=\"#最大线速度-v-max，最大角速度-ω-max\" class=\"headerlink\" title=\"最大线速度 v_max，最大角速度 ω_max\"></a>最大线速度 v_max，最大角速度 ω_max</h4><h4 id=\"由于激光雷达只记录机器人前方的数据，因此不考虑向后的运动，并将线速度调整为仅为正值。\"><a href=\"#由于激光雷达只记录机器人前方的数据，因此不考虑向后的运动，并将线速度调整为仅为正值。\" class=\"headerlink\" title=\"由于激光雷达只记录机器人前方的数据，因此不考虑向后的运动，并将线速度调整为仅为正值。\"></a>由于激光雷达只记录机器人前方的数据，因此<strong>不考虑向后的运动</strong>，并将<strong>线速度调整为仅为正值</strong>。</h4><h4 id=\"状态-动作对的-Q-值-Q-s-a-由两个-Critic-网络进行评估。这两个-Critic-网络具有相同的结构，但其参数更新是延迟进行的，从而允许它们在参数上产生差异（避免完全同步）。\"><a href=\"#状态-动作对的-Q-值-Q-s-a-由两个-Critic-网络进行评估。这两个-Critic-网络具有相同的结构，但其参数更新是延迟进行的，从而允许它们在参数上产生差异（避免完全同步）。\" class=\"headerlink\" title=\"状态-动作对的 Q 值 Q(s,a) 由两个 Critic 网络进行评估。这两个 Critic 网络具有相同的结构，但其参数更新是延迟进行的，从而允许它们在参数上产生差异（避免完全同步）。\"></a>状态-动作对的 Q 值 Q(s,a) 由两个 <strong>Critic 网络</strong>进行评估。这两个 Critic 网络具有相同的结构，但其参数更新是<strong>延迟进行</strong>的，从而允许它们在参数上产生差异（避免完全同步）。</h4><h4 id=\"Critic-网络以状态-s-和动作-a-的组合作为输入\"><a href=\"#Critic-网络以状态-s-和动作-a-的组合作为输入\" class=\"headerlink\" title=\"Critic 网络以状态 s 和动作 a 的组合作为输入\"></a>Critic 网络以状态 s 和动作 a 的组合作为输入</h4><h4 id=\"其中，状态-s-首先被送入一个全连接层，并接上一个-ReLU-激活函数，输出为-L-s\"><a href=\"#其中，状态-s-首先被送入一个全连接层，并接上一个-ReLU-激活函数，输出为-L-s\" class=\"headerlink\" title=\"其中，状态 s 首先被送入一个全连接层，并接上一个 ReLU 激活函数，输出为 L_s\"></a>其中，状态 s 首先被送入一个<strong>全连接层</strong>，并接上一个 ReLU 激活函数，输出为 L_s</h4><h4 id=\"该层的输出-L-s-以及动作-a，将分别送入两个大小相同的变换全连接层（TFC）中，分别对应变换结果为-τ1-和-τ2，随后，这两个结果按如下方式进行组合：\"><a href=\"#该层的输出-L-s-以及动作-a，将分别送入两个大小相同的变换全连接层（TFC）中，分别对应变换结果为-τ1-和-τ2，随后，这两个结果按如下方式进行组合：\" class=\"headerlink\" title=\"该层的输出 L_s 以及动作 a，将分别送入两个大小相同的变换全连接层（TFC）中，分别对应变换结果为 τ1 和 τ2，随后，这两个结果按如下方式进行组合：\"></a>该层的输出 L_s 以及动作 a，将分别送入两个大小相同的变换全连接层（TFC）中，分别对应变换结果为 τ1 和 τ2，随后，这两个结果按如下方式进行组合：</h4><p>$$<br>L_c &#x3D; L_sW_{\\tau_1} + aW_{\\tau_2} + b_{\\tau_2},<br>\\tag{6}<br>$$</p>\n<h4 id=\"其中，L-c-是组合全连接层（CFC）的输出，W-τ-1-和-W-τ-2-分别是-τ1-和-τ2-的权重，b-τ-2-是-τ-2-的偏执项，然后在这个组合层上使用ReLU激活函数，接着，该输出连接到一个最终输出节点，该节点包含一个参数，表示对应状态-动作对的-Q-值。最终，从两个-Critic-网络中选择较小的-Q-值，作为最后的-Critic-输出，以此来限制对状态-动作值的过高估计。\"><a href=\"#其中，L-c-是组合全连接层（CFC）的输出，W-τ-1-和-W-τ-2-分别是-τ1-和-τ2-的权重，b-τ-2-是-τ-2-的偏执项，然后在这个组合层上使用ReLU激活函数，接着，该输出连接到一个最终输出节点，该节点包含一个参数，表示对应状态-动作对的-Q-值。最终，从两个-Critic-网络中选择较小的-Q-值，作为最后的-Critic-输出，以此来限制对状态-动作值的过高估计。\" class=\"headerlink\" title=\"其中，L_c 是组合全连接层（CFC）的输出，W_τ_1 和 W_τ_2 分别是 τ1 和 τ2 的权重，b_τ_2 是 τ_2 的偏执项，然后在这个组合层上使用ReLU激活函数，接着，该输出连接到一个最终输出节点，该节点包含一个参数，表示对应状态-动作对的 Q 值。最终，从两个 Critic 网络中选择较小的 Q 值，作为最后的 Critic 输出，以此来限制对状态-动作值的过高估计。\"></a>其中，L_c 是组合全连接层（CFC）的输出，W_τ_1 和 W_τ_2 分别是 τ1 和 τ2 的权重，b_τ_2 是 τ_2 的偏执项，然后在这个组合层上使用ReLU激活函数，接着，该输出连接到一个最终输出节点，该节点包含<strong>一个参数</strong>，表示对应状态-动作对的 <strong>Q 值</strong>。最终，从两个 Critic 网络中<strong>选择较小的 Q 值</strong>，作为最后的 Critic 输出，以此来<strong>限制对状态-动作值的过高估计</strong>。</h4><h4 id=\"完整的网络架构如图\"><a href=\"#完整的网络架构如图\" class=\"headerlink\" title=\"完整的网络架构如图\"></a>完整的网络架构如图</h4><p><img src=\"http://picbed.yanzu.tech/img/paper_read/1/3.jpg\"></p>\n<h4 id=\"TD3-网络结构包括-actor-和-critic-两个部分。每一层的类型以及其对应的参数数量在层中都有描述。TFC-层指的是变换全连接层-τ，CFC-层指的是组合全连接层-Lc。\"><a href=\"#TD3-网络结构包括-actor-和-critic-两个部分。每一层的类型以及其对应的参数数量在层中都有描述。TFC-层指的是变换全连接层-τ，CFC-层指的是组合全连接层-Lc。\" class=\"headerlink\" title=\"TD3 网络结构包括 actor 和 critic 两个部分。每一层的类型以及其对应的参数数量在层中都有描述。TFC 层指的是变换全连接层 τ，CFC 层指的是组合全连接层 Lc。\"></a>TD3 网络结构包括 actor 和 critic 两个部分。每一层的类型以及其对应的参数数量在层中都有描述。TFC 层指的是变换全连接层 τ，CFC 层指的是组合全连接层 Lc。</h4><h4 id=\"策略的奖励依据以下函数进行评估-奖励函数\"><a href=\"#策略的奖励依据以下函数进行评估-奖励函数\" class=\"headerlink\" title=\"策略的奖励依据以下函数进行评估(奖励函数)\"></a>策略的奖励依据以下函数进行评估(奖励函数)</h4><p>$$<br>r(s_t, a_t) &#x3D;<br>\\begin{cases}<br>r_g &amp; \\text{if } D_t &lt; \\eta D \\<br>r_c &amp; \\text{if collision} \\<br>v - |\\omega| &amp; \\text{otherwise},<br>\\end{cases},<br>\\tag{7}<br>$$</p>\n<h4 id=\"在时间步-t时，状态-动作对-s-t-a-t-的奖励-r-取决于以下三种情况：\"><a href=\"#在时间步-t时，状态-动作对-s-t-a-t-的奖励-r-取决于以下三种情况：\" class=\"headerlink\" title=\"在时间步 t时，状态-动作对 (s_t, a_t) 的奖励 r 取决于以下三种情况：\"></a>在时间步 t时，状态-动作对 (s_t, a_t) 的奖励 r 取决于以下三种情况：</h4><ul>\n<li><h4 id=\"如果当前时间步与目标点的距离-D-t-小于阈值-η-D，则给予一个正的目标奖励-r-g（也就是鼓励）\"><a href=\"#如果当前时间步与目标点的距离-D-t-小于阈值-η-D，则给予一个正的目标奖励-r-g（也就是鼓励）\" class=\"headerlink\" title=\"如果当前时间步与目标点的距离 D_t 小于阈值 η_D，则给予一个正的目标奖励 r_g（也就是鼓励）\"></a>如果当前时间步与目标点的距离 D_t 小于阈值 η_D，则给予一个正的目标奖励 r_g（也就是鼓励）</h4></li>\n<li><h4 id=\"如果检测到碰撞，则给予一个负的碰撞惩罚-r-c（惩罚）\"><a href=\"#如果检测到碰撞，则给予一个负的碰撞惩罚-r-c（惩罚）\" class=\"headerlink\" title=\"如果检测到碰撞，则给予一个负的碰撞惩罚 r_c（惩罚）\"></a>如果检测到碰撞，则给予一个负的碰撞惩罚 r_c（惩罚）</h4></li>\n<li><h4 id=\"如果以上两种情况均未发生，则根据当前的线速度-v-和角速度-ω-给予即时奖励。\"><a href=\"#如果以上两种情况均未发生，则根据当前的线速度-v-和角速度-ω-给予即时奖励。\" class=\"headerlink\" title=\"如果以上两种情况均未发生，则根据当前的线速度 v 和角速度 ω 给予即时奖励。\"></a>如果以上两种情况均未发生，则根据当前的线速度 v 和角速度 ω 给予即时奖励。</h4></li>\n</ul>\n<h4 id=\"为了引导导航策略朝向给定目标，一个延迟归因奖励方法被采用，其计算如下：\"><a href=\"#为了引导导航策略朝向给定目标，一个延迟归因奖励方法被采用，其计算如下：\" class=\"headerlink\" title=\"为了引导导航策略朝向给定目标，一个延迟归因奖励方法被采用，其计算如下：\"></a>为了引导导航策略朝向给定目标，一个延迟归因奖励方法被采用，其计算如下：</h4><p>$$<br>r_{t-i} &#x3D; r(s_{t-i}, a_{t-i}) + \\frac{r_g}{i}, \\quad \\forall i \\in {1, 2, 3, \\ldots, n},<br>\\tag{8}<br>$$</p>\n<h4 id=\"其中，n-表示前-n-个时间步中需要更新奖励的步数。这意味着，正的目标奖励不仅被分配给达到目标的那个状态-动作对，还以递减的方式分配给其之前的-n-个时间步。通过这种方式，网络学习到了一种局部导航策略，该策略能够仅依赖激光输入，避开障碍物的同时到达局部目标。\"><a href=\"#其中，n-表示前-n-个时间步中需要更新奖励的步数。这意味着，正的目标奖励不仅被分配给达到目标的那个状态-动作对，还以递减的方式分配给其之前的-n-个时间步。通过这种方式，网络学习到了一种局部导航策略，该策略能够仅依赖激光输入，避开障碍物的同时到达局部目标。\" class=\"headerlink\" title=\"其中，n 表示前 n 个时间步中需要更新奖励的步数。这意味着，正的目标奖励不仅被分配给达到目标的那个状态-动作对，还以递减的方式分配给其之前的 n 个时间步。通过这种方式，网络学习到了一种局部导航策略，该策略能够仅依赖激光输入，避开障碍物的同时到达局部目标。\"></a>其中，n 表示前 n 个时间步中需要更新奖励的步数。这意味着，正的目标奖励不仅被分配给达到目标的那个状态-动作对，还以递减的方式分配给其之前的 n 个时间步。通过这种方式，网络学习到了一种局部导航策略，该策略能够仅依赖激光输入，避开障碍物的同时到达局部目标。</h4></blockquote>\n<h3 id=\"C-探索与建图\"><a href=\"#C-探索与建图\" class=\"headerlink\" title=\"C.探索与建图\"></a>C.探索与建图</h3><blockquote>\n<h4 id=\"机器人沿着路径点被引导向全局目标前进。一旦靠近全局目标，机器人将自主导航至该目标。在此过程中，环境被逐步探索和建图。建图依赖于激光雷达和机器人里程计传感器数据，生成环境的占据栅格地图。完整的自主探索与建图算法的伪代码在算法1中进行了描述。\"><a href=\"#机器人沿着路径点被引导向全局目标前进。一旦靠近全局目标，机器人将自主导航至该目标。在此过程中，环境被逐步探索和建图。建图依赖于激光雷达和机器人里程计传感器数据，生成环境的占据栅格地图。完整的自主探索与建图算法的伪代码在算法1中进行了描述。\" class=\"headerlink\" title=\"机器人沿着路径点被引导向全局目标前进。一旦靠近全局目标，机器人将自主导航至该目标。在此过程中，环境被逐步探索和建图。建图依赖于激光雷达和机器人里程计传感器数据，生成环境的占据栅格地图。完整的自主探索与建图算法的伪代码在算法1中进行了描述。\"></a>机器人沿着路径点被引导向全局目标前进。一旦靠近全局目标，机器人将自主导航至该目标。在此过程中，环境被逐步探索和建图。建图依赖于激光雷达和机器人里程计传感器数据，生成环境的占据栅格地图。完整的自主探索与建图算法的伪代码在算法1中进行了描述。</h4></blockquote>\n</blockquote>\n<h3 id=\"实验部分\"><a href=\"#实验部分\" class=\"headerlink\" title=\"实验部分\"></a>实验部分</h3><blockquote>\n<h4 id=\"算法过程：\"><a href=\"#算法过程：\" class=\"headerlink\" title=\"算法过程：\"></a>算法过程：</h4><blockquote>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 输入参数</span><br><span class=\"line\">global_goal\t\t# 全局目标点，即最终导航的目标位置</span><br><span class=\"line\">δ\t\t\t# 导航至全局目标的距离阈值,用于判断机器人是否“足够接近”目标点</span><br><span class=\"line\"></span><br><span class=\"line\"># 主循环 直到达到全局目标</span><br><span class=\"line\">while(reached_global_goal != True)&#123;\t# 判断是否已达到全局目标</span><br><span class=\"line\"></span><br><span class=\"line\">\tread sensor data # 读取传感器数据：如激光雷达、相机、里程计等</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tupdate map from sensor data # 根据传感器数据更新地图：构建或完善占据栅格地图</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tObtain new POI # 获取新的兴趣点 POI,可能是探索边界或未知区域的候选目标点</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t# 判断当前是否已接近目标区域</span><br><span class=\"line\">\tif (D_t &lt; δ_D)&#123;\t# 如果 agent与目标的距离处于接近目标区域</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t\tif(waypoint = global_goal)\t# 若当前导航的子目标 waypoint 已经是 global_goal</span><br><span class=\"line\">\t\t\treachedGlobalGoal = True\t# 那么任务完成</span><br><span class=\"line\">\t\t\t</span><br><span class=\"line\">\t\telse&#123;\t# 否则，进一步判断当前是否靠近全局目标</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\t\tif(d(p_t, g) &lt; δ)\t# 如果当前位置p_t与目标g的距离d(p_t,g) 小于 δ</span><br><span class=\"line\">\t\t\t\twaypoint &lt;-- global_goal\t# 那么就把当前的 waypoint 设置为 global_goal</span><br><span class=\"line\">\t\t\t\t</span><br><span class=\"line\">\t\t\telse\t# 否则，从所有兴趣点中选择下一个最优子目标点</span><br><span class=\"line\">\t\t\t\tfor i in POI\t# 遍历POI中所有的兴趣点</span><br><span class=\"line\">\t\t\t\t\tcaculate h(i) from (1) # 根据式子(1)计算每个兴趣点的启发值h(i)</span><br><span class=\"line\">\t\t\t\twaypoint &lt;-- POI_min(h(i))\t# 将h(i)值最小对应的兴趣点作为新的 waypoint</span><br><span class=\"line\">\t\t\tend if</span><br><span class=\"line\">\t\tend if</span><br><span class=\"line\">\tend if</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tObtain an action from TD3\t# 从 TD3 策略网络中获取当前动作,利用强化学习模型TD3预测最优动作</span><br><span class=\"line\">\tPerform action\t# 执行该动作</span><br><span class=\"line\">end while</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4></blockquote>\n<h3 id=\"A-系统设置\"><a href=\"#A-系统设置\" class=\"headerlink\" title=\"A.系统设置\"></a>A.系统设置</h3><blockquote>\n<h4 id=\"原作者系统配置：\"><a href=\"#原作者系统配置：\" class=\"headerlink\" title=\"原作者系统配置：\"></a>原作者系统配置：</h4><blockquote>\n<h4 id=\"显卡：NVIDIA-GTX-1080\"><a href=\"#显卡：NVIDIA-GTX-1080\" class=\"headerlink\" title=\"显卡：NVIDIA GTX 1080\"></a>显卡：NVIDIA GTX 1080</h4><h4 id=\"运行内存：32G\"><a href=\"#运行内存：32G\" class=\"headerlink\" title=\"运行内存：32G\"></a>运行内存：32G</h4><h4 id=\"CPU：-Intel-Core-i7-6800K\"><a href=\"#CPU：-Intel-Core-i7-6800K\" class=\"headerlink\" title=\"CPU： Intel Core i7-6800K\"></a>CPU： Intel Core i7-6800K</h4></blockquote>\n<h4 id=\"训练参数设置\"><a href=\"#训练参数设置\" class=\"headerlink\" title=\"训练参数设置\"></a>训练参数设置</h4><blockquote>\n<h4 id=\"TD3网络在Gazebo上进行训练，并通过ROS进行控制，训练了800回合，约8h\"><a href=\"#TD3网络在Gazebo上进行训练，并通过ROS进行控制，训练了800回合，约8h\" class=\"headerlink\" title=\"TD3网络在Gazebo上进行训练，并通过ROS进行控制，训练了800回合，约8h\"></a>TD3网络在Gazebo上进行训练，并通过ROS进行控制，训练了800回合，约8h</h4><h4 id=\"每个训练回合在机器人到达目标、发生碰撞或执行了-500-步动作后结束\"><a href=\"#每个训练回合在机器人到达目标、发生碰撞或执行了-500-步动作后结束\" class=\"headerlink\" title=\"每个训练回合在机器人到达目标、发生碰撞或执行了 500 步动作后结束\"></a>每个训练回合在机器人到达目标、发生碰撞或执行了 500 步动作后结束</h4><h4 id=\"最大线速度-v-max-和最大角速度-ω-max-分别设置为-0-5-m-s-和-1rad-s\"><a href=\"#最大线速度-v-max-和最大角速度-ω-max-分别设置为-0-5-m-s-和-1rad-s\" class=\"headerlink\" title=\"最大线速度 v_max 和最大角速度 ω_max 分别设置为 0.5 m&#x2F;s 和 1rad&#x2F;s\"></a>最大线速度 v_max 和最大角速度 ω_max 分别设置为 0.5 m&#x2F;s 和 1rad&#x2F;s</h4><h4 id=\"延迟奖励在最后-n-10-步中更新，参数更新延迟设置为每-2-个回合\"><a href=\"#延迟奖励在最后-n-10-步中更新，参数更新延迟设置为每-2-个回合\" class=\"headerlink\" title=\"延迟奖励在最后 n&#x3D;10 步中更新，参数更新延迟设置为每 2 个回合\"></a>延迟奖励在最后 n&#x3D;10 步中更新，参数更新延迟设置为每 2 个回合</h4></blockquote>\n<h4 id=\"训练在一个-10x10-米的模拟环境中进行，如图所示\"><a href=\"#训练在一个-10x10-米的模拟环境中进行，如图所示\" class=\"headerlink\" title=\"训练在一个 10x10 米的模拟环境中进行，如图所示\"></a>训练在一个 10x10 米的模拟环境中进行，如图所示</h4><p><img src=\"http://picbed.yanzu.tech/img/paper_read/1/4.jpg\"></p>\n<h4 id=\"训练环境示例。蓝色区域表示输入的激光雷达读数和测距。四个箱形障碍物在每一轮训练中都会改变位置，如图-a-、-b-和-c-所示，以实现训练数据的随机化。\"><a href=\"#训练环境示例。蓝色区域表示输入的激光雷达读数和测距。四个箱形障碍物在每一轮训练中都会改变位置，如图-a-、-b-和-c-所示，以实现训练数据的随机化。\" class=\"headerlink\" title=\"训练环境示例。蓝色区域表示输入的激光雷达读数和测距。四个箱形障碍物在每一轮训练中都会改变位置，如图(a)、(b)和(c)所示，以实现训练数据的随机化。\"></a>训练环境示例。蓝色区域表示输入的激光雷达读数和测距。四个箱形障碍物在每一轮训练中都会改变位置，如图(a)、(b)和(c)所示，以实现训练数据的随机化。</h4><h4 id=\"为促进策略的泛化与探索，向传感器读数和动作值中添加了高斯噪声。为了构建多样化的训练环境，在每个回合开始时，箱形障碍物的位置会被随机化。其位置变化的示例如图-a-、-b-、-c-所示。同时，机器人的起始位置与目标位置也在每一回合中被随机设置。\"><a href=\"#为促进策略的泛化与探索，向传感器读数和动作值中添加了高斯噪声。为了构建多样化的训练环境，在每个回合开始时，箱形障碍物的位置会被随机化。其位置变化的示例如图-a-、-b-、-c-所示。同时，机器人的起始位置与目标位置也在每一回合中被随机设置。\" class=\"headerlink\" title=\"为促进策略的泛化与探索，向传感器读数和动作值中添加了高斯噪声。为了构建多样化的训练环境，在每个回合开始时，箱形障碍物的位置会被随机化。其位置变化的示例如图 (a)、(b)、(c) 所示。同时，机器人的起始位置与目标位置也在每一回合中被随机设置。\"></a>为促进策略的泛化与探索，向传感器读数和动作值中添加了高斯噪声。为了构建多样化的训练环境，在每个回合开始时，箱形障碍物的位置会被随机化。其位置变化的示例如图 (a)、(b)、(c) 所示。同时，机器人的起始位置与目标位置也在每一回合中被随机设置。</h4><h4 id=\"ROS-中的-SLAM-Toolbox-软件包用于生成和更新环境的全局地图，并用于机器人在地图中的定位\"><a href=\"#ROS-中的-SLAM-Toolbox-软件包用于生成和更新环境的全局地图，并用于机器人在地图中的定位\" class=\"headerlink\" title=\"ROS 中的 SLAM Toolbox 软件包用于生成和更新环境的全局地图，并用于机器人在地图中的定位\"></a>ROS 中的 SLAM Toolbox 软件包用于生成和更新环境的全局地图，并用于机器人在地图中的定位</h4><h4 id=\"ROS的本地规划器包（TrajectoryPlanner）代替了神经网络\"><a href=\"#ROS的本地规划器包（TrajectoryPlanner）代替了神经网络\" class=\"headerlink\" title=\"ROS的本地规划器包（TrajectoryPlanner）代替了神经网络\"></a>ROS的本地规划器包（TrajectoryPlanner）代替了神经网络</h4><h4 id=\"目标驱动自主探索（GDAE-Goal-Driven-Autonomous-Exploration）\"><a href=\"#目标驱动自主探索（GDAE-Goal-Driven-Autonomous-Exploration）\" class=\"headerlink\" title=\"目标驱动自主探索（GDAE, Goal-Driven Autonomous Exploration）\"></a>目标驱动自主探索（GDAE, Goal-Driven Autonomous Exploration）</h4><h4 id=\"最近前沿探索策略（Nearest-Frontier-NF）\"><a href=\"#最近前沿探索策略（Nearest-Frontier-NF）\" class=\"headerlink\" title=\"最近前沿探索策略（Nearest Frontier, NF）\"></a>最近前沿探索策略（Nearest Frontier, NF）</h4><h4 id=\"目标驱动强化学习（GD-RL-Goal-Driven-Reinforcement-Learning）\"><a href=\"#目标驱动强化学习（GD-RL-Goal-Driven-Reinforcement-Learning）\" class=\"headerlink\" title=\"目标驱动强化学习（GD-RL, Goal-Driven Reinforcement Learning）\"></a>目标驱动强化学习（GD-RL, Goal-Driven Reinforcement Learning）</h4><h4 id=\"本地规划器自主探索（LP-AE，Local-Planner-Autonomous-Exploration）\"><a href=\"#本地规划器自主探索（LP-AE，Local-Planner-Autonomous-Exploration）\" class=\"headerlink\" title=\"本地规划器自主探索（LP-AE，Local Planner Autonomous Exploration）\"></a>本地规划器自主探索（LP-AE，Local Planner Autonomous Exploration）</h4><h4 id=\"路径规划器（PP，Path-Planner）它是基于-Dijkstra-的生成路径的方法\"><a href=\"#路径规划器（PP，Path-Planner）它是基于-Dijkstra-的生成路径的方法\" class=\"headerlink\" title=\"路径规划器（PP，Path Planner）它是基于 Dijkstra 的生成路径的方法\"></a>路径规划器（PP，Path Planner）它是基于 Dijkstra 的生成路径的方法</h4></blockquote>\n<h3 id=\"B-定量实验\"><a href=\"#B-定量实验\" class=\"headerlink\" title=\"B.定量实验\"></a>B.定量实验</h3><h3 id=\"C-定性实验\"><a href=\"#C-定性实验\" class=\"headerlink\" title=\"C.定性实验\"></a>C.定性实验</h3></blockquote>\n<h3 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h3><blockquote>\n<h4 id=\"基于深度强化学习（DRL）的目标驱动型全自主探索系统（GDAE）\"><a href=\"#基于深度强化学习（DRL）的目标驱动型全自主探索系统（GDAE）\" class=\"headerlink\" title=\"基于深度强化学习（DRL）的目标驱动型全自主探索系统（GDAE）\"></a>基于深度强化学习（DRL）的目标驱动型全自主探索系统（GDAE）</h4><h4 id=\"无需人工干预，即可导航至指定目标并记录环境信息（导航去目标点的过程中建图）\"><a href=\"#无需人工干预，即可导航至指定目标并记录环境信息（导航去目标点的过程中建图）\" class=\"headerlink\" title=\"无需人工干预，即可导航至指定目标并记录环境信息（导航去目标点的过程中建图）\"></a>无需人工干预，即可导航至指定目标并记录环境信息（导航去目标点的过程中建图）</h4><h4 id=\"系统有效结合了反应式的本地导航策略和全局导航策略\"><a href=\"#系统有效结合了反应式的本地导航策略和全局导航策略\" class=\"headerlink\" title=\"系统有效结合了反应式的本地导航策略和全局导航策略\"></a>系统有效结合了反应式的本地导航策略和全局导航策略</h4><h4 id=\"将神经网络模块引入端到端系统：机器人无需生成显式路径即可移动，而这一机制的不足则通过引入全局导航策略得到了弥补\"><a href=\"#将神经网络模块引入端到端系统：机器人无需生成显式路径即可移动，而这一机制的不足则通过引入全局导航策略得到了弥补\" class=\"headerlink\" title=\"将神经网络模块引入端到端系统：机器人无需生成显式路径即可移动，而这一机制的不足则通过引入全局导航策略得到了弥补\"></a>将神经网络模块引入端到端系统：机器人无需生成显式路径即可移动，而这一机制的不足则通过引入全局导航策略得到了弥补</h4><h4 id=\"系统的导航性能接近于基于已知地图路径规划器所得的最优解\"><a href=\"#系统的导航性能接近于基于已知地图路径规划器所得的最优解\" class=\"headerlink\" title=\"系统的导航性能接近于基于已知地图路径规划器所得的最优解\"></a>系统的导航性能接近于基于已知地图路径规划器所得的最优解</h4><h4 id=\"GDAE-系统依赖直接的传感器输入而非从不确定地图生成路径，因此在可靠性方面表现更佳\"><a href=\"#GDAE-系统依赖直接的传感器输入而非从不确定地图生成路径，因此在可靠性方面表现更佳\" class=\"headerlink\" title=\"GDAE 系统依赖直接的传感器输入而非从不确定地图生成路径，因此在可靠性方面表现更佳\"></a>GDAE 系统依赖<strong>直接的传感器输入</strong>而非从不确定地图生成路径，因此在可靠性方面表现更佳</h4><h4 id=\"若希望进一步泛化至不同类型的机器人，可以将机器人动力学作为神经网络的一个输入状态，并据此开展训练。只需提供机器人动力学信息，即可在不同平台上实现最优的本地导航。\"><a href=\"#若希望进一步泛化至不同类型的机器人，可以将机器人动力学作为神经网络的一个输入状态，并据此开展训练。只需提供机器人动力学信息，即可在不同平台上实现最优的本地导航。\" class=\"headerlink\" title=\"若希望进一步泛化至不同类型的机器人，可以将机器人动力学作为神经网络的一个输入状态，并据此开展训练。只需提供机器人动力学信息，即可在不同平台上实现最优的本地导航。\"></a>若希望进一步<strong>泛化至不同类型的机器人</strong>，可以将<strong>机器人动力学作为神经网络的一个输入状态</strong>，并据此开展训练。只需提供机器人动力学信息，即可在不同平台上实现最优的本地导航。</h4><h4 id=\"接下来的研究：\"><a href=\"#接下来的研究：\" class=\"headerlink\" title=\"接下来的研究：\"></a>接下来的研究：</h4><h4 id=\"引入长短时记忆（LSTM）结构也可能有助于缓解局部最优问题，并辅助规避当前传感器视野之外的障碍物\"><a href=\"#引入长短时记忆（LSTM）结构也可能有助于缓解局部最优问题，并辅助规避当前传感器视野之外的障碍物\" class=\"headerlink\" title=\"引入长短时记忆（LSTM）结构也可能有助于缓解局部最优问题，并辅助规避当前传感器视野之外的障碍物\"></a>引入<strong>长短时记忆（LSTM）结构</strong>也可能有助于缓解局部最优问题，并辅助规避当前传感器视野之外的障碍物</h4></blockquote>\n<h3 id=\"代码部分\"><a href=\"#代码部分\" class=\"headerlink\" title=\"代码部分\"></a>代码部分</h3><blockquote>\n<h4 id=\"两个量的定义\"><a href=\"#两个量的定义\" class=\"headerlink\" title=\"两个量的定义\"></a>两个量的定义</h4><h4 id=\"Episode：\"><a href=\"#Episode：\" class=\"headerlink\" title=\"Episode：\"></a>Episode：</h4><blockquote>\n<h4 id=\"它是后续步骤的集合，直到达到其中一个终止条件（终止条件：达到目标、与障碍物碰撞或达到最大步数max-ep）\"><a href=\"#它是后续步骤的集合，直到达到其中一个终止条件（终止条件：达到目标、与障碍物碰撞或达到最大步数max-ep）\" class=\"headerlink\" title=\"它是后续步骤的集合，直到达到其中一个终止条件（终止条件：达到目标、与障碍物碰撞或达到最大步数max_ep）\"></a>它是后续步骤的集合，直到达到其中一个终止条件（终止条件：达到目标、与障碍物碰撞或达到最大步数max_ep）</h4><h4 id=\"它可以理解是一个训练过程，如果达到终止条件就开始新的一个训练过程，并且伴随着环境的改变，包括区域内的墙体位置改变、障碍物的位置改变、agent的初始位置改变和目标点位置的改变\"><a href=\"#它可以理解是一个训练过程，如果达到终止条件就开始新的一个训练过程，并且伴随着环境的改变，包括区域内的墙体位置改变、障碍物的位置改变、agent的初始位置改变和目标点位置的改变\" class=\"headerlink\" title=\"它可以理解是一个训练过程，如果达到终止条件就开始新的一个训练过程，并且伴随着环境的改变，包括区域内的墙体位置改变、障碍物的位置改变、agent的初始位置改变和目标点位置的改变\"></a>它可以理解是一个训练过程，如果达到终止条件就开始新的一个训练过程，并且伴随着环境的改变，包括区域内的墙体位置改变、障碍物的位置改变、agent的初始位置改变和目标点位置的改变</h4></blockquote>\n<h4 id=\"Epoch：\"><a href=\"#Epoch：\" class=\"headerlink\" title=\"Epoch：\"></a>Epoch：</h4><blockquote>\n<h4 id=\"执行评估之间的后续事件数（episode）或者时间步长（timesteps）\"><a href=\"#执行评估之间的后续事件数（episode）或者时间步长（timesteps）\" class=\"headerlink\" title=\"执行评估之间的后续事件数（episode）或者时间步长（timesteps）\"></a>执行评估之间的后续事件数（episode）或者时间步长（timesteps）</h4><h4 id=\"一个epoch运行5000个步骤step（对应代码中的-eval-freq-这个参数），步长为0-1秒，也就是说一个epoch大概要运行8分多钟\"><a href=\"#一个epoch运行5000个步骤step（对应代码中的-eval-freq-这个参数），步长为0-1秒，也就是说一个epoch大概要运行8分多钟\" class=\"headerlink\" title=\"一个epoch运行5000个步骤step（对应代码中的 eval_freq 这个参数），步长为0.1秒，也就是说一个epoch大概要运行8分多钟\"></a>一个epoch运行5000个步骤step（对应代码中的 eval_freq 这个参数），步长为0.1秒，也就是说一个epoch大概要运行8分多钟</h4></blockquote>\n</blockquote>\n"},{"title":"DL之路---啃鱼书（4）","data":"2025-06-16T13:38:00.000Z","updated":"2025-06-16T13:38:00.000Z","type":"DL","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p23.png","_content":"\n\n# 误差反向传播法\n\n\n\n### 1.计算图\n\n> #### 无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题\n>\n> #### 另外，利用计算图可以将中间的计算结果全部保存起来 \n>\n> #### 其最大的优势在于，可以通过反向传播高效计算导数\n>\n> #### 反向传播传递的是局部导数\n>\n> #### 下面是，支付金额关于苹果的的价格的导数\n>\n> ![](http://picbed.yanzu.tech/img/DL/4/1.png)\n>\n> #### 就是总的金额，220/220 = 1，220/200 = 1.1，220/100=2.2\n>\n> #### 计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数\n>\n> #### 计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值\n\n\n\n### 2.链式法则\n\n> #### 如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示，这就是链式法则\n>\n> #### 反向传播是基于链式法则的\n>\n> ![](http://picbed.yanzu.tech/img/DL/4/2.png)\n\n\n\n### 3.反向传播\n\n> #### 加法节点的反向传播，只是将输入信号输出到下一个节点，输入的值会原封不动地流向下一个节点\n>\n> ![](http://picbed.yanzu.tech/img/DL/4/3.png)\n>\n> #### 乘法节点的反向传播，会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游，翻转值表示一种翻转关系\n>\n> ![](http://picbed.yanzu.tech/img/DL/4/4.png)\n>\n> #### 乘法的反向传播需要正向传播时的输入信号值，实现乘法节点的反向传播时，要保存正向传播的输入信号\n>\n> \n\n\n\n### 4.简单层的实现\n\n> #### 层，是神经网络中功能的单位，如负责sigmoid函数的sigmoid，是以层为单位进行实现的，通常，用一个类来实现一个层\n\n\n\n### 5.激活函数层的实现\n\n> #### ReLU层\n>\n> > #### 表达式如下\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/5.png)\n> >\n> > #### 求导之后\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/6.png)\n> >\n> > #### 如果正向传播时的输入x大于0，则反向传播会将上游的值原封不动地传给下游，如果正向传播时的x小于等于0，则反向传播中传给下游的信号将停在此处\n>\n> \n>\n> #### Sigmoid层\n>\n> > #### 表达式如下\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/7.png)\n> >\n> > #### 计算图表示表达式如下(正向传播)\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/8.png)\n> >\n> > #### 反向传播\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/9.png)\n> >\n> > #### 观察到反向传播的输出，可以根据正向传播的输入x和输出y计算出来\n> >\n> > #### 另外， $\\frac {\\partial L}{\\partial y} y^2 exp(-x)$ 可以进一步整理\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/10.png)\n> >\n> > #### 此时，反向传播的输出可直接根据正向传播的输出y就能计算出来\n> >\n> > \n\n\n\n### 6.Affine层/Softmax层的实现\n\n> #### Affine层（全连接层）\n>\n> > #### 神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”，将进行仿射变换的处理实现为“Affine层”，本质就是 $y = xW + b$ ，表示一个线性变换\n> >\n> > #### 几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算\n> >\n> > #### 在神经网络中使用的是 dot 进行矩阵乘积运算，Affine层的计算图如下(正向传播)\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/11.png)\n> >\n> > #### 反向传播\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/12.png)\n> >\n> > #### 注意上面反向传播的两个输出，W 和 X 的位置是不同的\n> >\n> > #### 另外，这里输入的 X 是以单个数据为对象的\n>\n> \n>\n> #### 批版本的Affine层\n>\n> > #### 考虑N个数据一起进行正向传播，就要使用 batch，batch版的Affine层计算图如下\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/13.png)\n>\n> \n>\n> #### softmax-with-loss层\n>\n> > #### 神经网络中未被正规化的输出结果有时被称为**得分**\n> >\n> > #### 当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要 Softmax层\n> >\n> > #### 因为是包含损失函数的交叉熵误差，所以叫做Softmax-with-Loss层，计算图如下\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/14.png)\n> >\n> > \n\n\n\n### 7.误差反向传播的实现\n\n> #### 误差反向传播法的梯度确认\n>\n> > #### 数值微分虽然计算狠很耗费时间，但它的优点是实现简单，因此，一般情况下不太容易出错；而误差反向传播法虽然计算很快，但它的实现很复杂，容易出错。所以，经常会比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的实现是否正确\n> >\n> > #### 确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地讲，是非常相近）的操作称为梯度确认（gradient check）\n\n\n\n\n\n\n### code\n\n> #### 简单层的实现\n>\n> ```python\n> # coding: utf-8\n> \n> \"\"\"\n> 实现了神经网络中的两个基础层：\n> 1. MulLayer: 乘法层，用于实现两个数的乘法运算\n> 2. AddLayer: 加法层，用于实现两个数的加法运算\n> 这些层都实现了前向传播(forward)和反向传播(backward)方法\n> \"\"\"\n> \n> class MulLayer:\n>     \"\"\"\n>     乘法层的实现\n>     用于计算两个数的乘积，并支持反向传播\n>     \"\"\"\n>     def __init__(self):\n>         # 初始化存储输入值的变量\n>         self.x = None  # 存储第一个输入值\n>         self.y = None  # 存储第二个输入值\n> \n>     def forward(self, x, y):\n>         \"\"\"\n>         前向传播：计算两个数的乘积\n>         \n>         参数:\n>             x: 第一个输入值\n>             y: 第二个输入值\n>         返回:\n>             out: x和y的乘积\n>         \"\"\"\n>         self.x = x  # 保存输入值，用于反向传播\n>         self.y = y                \n>         out = x * y  # 计算乘积\n> \n>         return out\n> \n>     def backward(self, dout):\n>         \"\"\"\n>         反向传播：计算梯度\n>         \n>         参数:\n>             dout: 上游传来的梯度\n>         返回:\n>             dx: 对x的梯度\n>             dy: 对y的梯度\n>         \"\"\"\n>         dx = dout * self.y  # 对x的梯度 = 上游梯度 * y\n>         dy = dout * self.x  # 对y的梯度 = 上游梯度 * x\n> \n>         return dx, dy\n> \n> \n> class AddLayer:\n>     \"\"\"\n>     加法层的实现\n>     用于计算两个数的和，并支持反向传播\n>     \"\"\"\n>     def __init__(self):\n>         # 加法层不需要存储中间值，所以pass\n>         pass\n> \n>     def forward(self, x, y):\n>         \"\"\"\n>         前向传播：计算两个数的和\n>         \n>         参数:\n>             x: 第一个输入值\n>             y: 第二个输入值\n>         返回:\n>             out: x和y的和\n>         \"\"\"\n>         out = x + y  # 计算和\n> \n>         return out\n> \n>     def backward(self, dout):\n>         \"\"\"\n>         反向传播：计算梯度\n>         \n>         参数:\n>             dout: 上游传来的梯度\n>         返回:\n>             dx: 对x的梯度\n>             dy: 对y的梯度\n>         \"\"\"\n>         dx = dout * 1  # 对x的梯度 = 上游梯度 * 1\n>         dy = dout * 1  # 对y的梯度 = 上游梯度 * 1\n> \n>         return dx, dy\n> \n>     \n>     \n> # 购买2个苹果3个橘子的例子\n> apple = 100\n> apple_num = 2\n> orange = 150\n> orange_num = 3\n> tax = 1.1\n> \n> mul_apple_layer = MulLayer()\n> mul_orange_layer = MulLayer()\n> add_apple_orange_layer = AddLayer()\n> mul_tax_layer = MulLayer()\n> \n> # forward\n> apple_price = mul_apple_layer.forward(apple, apple_num)\n> orange_price = mul_orange_layer.forward(orange, orange_num)\n> all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n> price = mul_tax_layer.forward(all_price, tax)\n> \n> # backward\n> d_price = 1\n> d_all_price, d_tax = mul_tax_layer.backward(d_price)\n> d_apple_peice, d_orange_price = add_apple_orange_layer.backward(d_all_price)\n> d_orange, d_orange_num = mul_orange_layer.backward(d_orange_price)\n> d_apple, d_apple_num = mul_apple_layer.backward(d_apple_peice)\n> \n> print(price)\n> print(d_apple_num)\n> print(d_apple)\n> print(d_orange_num)\n> print(d_orange)\n> print(d_tax)\n> ```\n>\n> \n>\n> #### 激活函数层的实现\n>\n> > #### ReLU层\n> >\n> > ```python\n> > class ReLU:\n> >  def __init__(self):\n> >      self.mask = None\n> > \n> >  def forward(self, x):\n> >      # x <= 0的为true，x>0的为false\n> >      self.mask = (x <= 0)\t# 保存结果，用于反向传播\n> >      # out是x的复制\n> >      out = x.copy()\n> >      # 这一步很关键，mask是一个bool数组，当它作为索引时，会选中True的位置，让后将其置为0，而False的位置不会被选中，就会保持原来的值\n> >      # 这就实现了 x <= 0置为0，x > 0保持原值\n> >      out[self.mask] = 0\n> > \n> >      return out\n> > \n> >  def backward(self, dout):\n> >      dout[self.mask] = 0\n> >      dx = dout\n> > \n> >      return dx\n> > \n> > \n> > x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n> > print(x)\n> > mask = (x <= 0 )\n> > print(mask)\n> > ```\n> >\n> > #### Sigmoid层\n> >\n> > ```python\n> > class Sigmoid:\n> >  def __init__(self):\n> >      self.out = None\n> > \n> >  def forward(self, x):\n> >      out = 1 / (1 + np.exp(-x))\n> >      self.out = out\t# 保存输出，用于反向传播\n> > \n> >      return out\n> > \n> >  def backward(self, dout):\n> >      # 对应的就是 (\\partial L/ partial y) * y * (1 - y)\n> >      dx = dout * (1.0 - self.out) * self.out\n> >      return dx\n> > ```\n> >\n> > #### Affine层\n> >\n> > ```python\n> > class Affine:\n> >  def __init__(self, W, b):\n> >      self.W = W\n> >      self.b = b\n> >      self.dW = None\n> >      self.db = None\n> > \n> >  def forward(self, x):\n> >      self.x = x\t# 保存输入，供反向传播用\n> >      out = np.dot(x, self.W) + self.b\t# b使用到了广播机制\n> > \n> >      return out\n> > \n> >  def backward(self, dout):\n> >      dx = np.dot(dout, self.W.T)\n> >      self.dW = np.dot(self.x.T, dout)\n> >      self.db = np.sum(dout, axis=0)\n> > \n> >      return dx\n> > ```\n> >\n> > #### 需要说明的是，backward中，分别表示了 $\\frac {\\partial L} {\\partial X} = \\frac {\\partial L}{\\partial Y} W^T$ ，$\\frac {\\partial L} {\\partial W} = X^T \\frac {\\partial L}{\\partial Y}$ ，$\\frac{\\partial L}{\\partial B} = \\frac{\\partial L}{\\partial Y}$ \n> >\n> > #### Softmax-with-Loss层的实现\n> >\n> > ```python\n> > class SoftmaxWithLoss:\n> >     def __init__(self):\n> >         self.loss = None # 损失\n> >         self.y = None # softmax的输出\n> >         self.t = None # 监督数据(one-hot vector)\n> > \n> >     def forward(self, x, t):\n> >         self.t = t\n> >         self.y = softmax(x)\n> >         self.loss = cross_entropy_error(self.y, self.t)\n> > \n> >         return self.loss\n> > \n> >     def backward(self, dout=1):\n> >         batch_size = self.t.shape[0]\n> >         dx = (self.y - self.t) / batch_size\n> > \n> >         return dx\n> > ```\n>\n> \n>\n> #### 误差反向传播的实现\n>\n> > #### 误差反向传播的神经网络的实现\n> >\n> > ```python\n> > import os, sys\n> > sys.path.append('../../py_pro/DL/')\n> > import numpy as np\n> > from common.layers import *\n> > from common.gradient import numerical_gradient\n> > from collections import OrderedDict\n> > from dataset.mnist import load_mnist\n> > from ch04.two_layer_net import TwoLayerNet\n> > \n> > \n> > class TwoLayerNet:\n> >  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n> >      # 初始化权重\n> >      self.params = {}\n> >      self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n> >      self.params['b1'] = np.zeros(hidden_size)\n> >      self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n> >      self.params['b2'] = np.zeros(output_size)\n> > \n> >      # 生成层\n> >      # OrderedDict是有序字典，会记录往字典中添加元素的顺序\n> >      self.layers = OrderedDict()\n> >      self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n> >      self.layers['ReLU1'] = Relu()\n> >      self.params['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n> >      \n> >      self.lastLayer = SoftmaxWithLoss()\n> > \n> >  def predict(self, x):\n> >      for layer in self.layers.values():\n> >          x = layer.forward(x)\n> >      return x\n> > \n> >  def loss(self, x, t):\n> >      y = self.predict(x)\n> >      y = np.argmax(y, axis=1)\n> >      if t.ndim != 1 : t = np.argmax(t, axis=1)\n> >      accuracy = np.sum(y == t) / float(x.shape[0])\n> >      return accuracy\n> > \n> >  def numerical_gradient(self, x, t):\n> >      loss_W = lambda W : self.loss(x, t)\n> >      grads = {}\n> > \n> >      grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n> >      grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n> >      grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n> >      grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n> > \n> >      return grads\n> > \n> >  def gradient(self, x, t):\n> >      # forward\n> >      self.loss(x, t)\n> > \n> >      # backward\n> >      dout = 1\n> >      dout = self.lastLayer.backward(dout)\n> > \n> >      layers = list(self.layers.values())\n> >      layers.reverse()\n> > \n> >      for layer in layers:\n> >          dout = layer.backward(dout)\n> > \n> >      grads = {}\n> >      grads['W1'] = self.layers['Affine1'].dW\n> >      grads['b1'] = self.layers['Affine1'].db\n> >      grads['W2'] = self.layers['Affine2'].dW\n> >      grads['b2'] = self.layers['Affine2'].db\n> > \n> >      return grads\n> >      \n> > \n> > \n> > # 读入数据\n> > (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label= True)\n> > network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n> > \n> > x_batch = x_train[:3]\n> > t_batch = t_train[:3]\n> > \n> > grad_numerical = network.numerical_gradient(x_batch, t_batch)\n> > grad_backprop = network.gradient(x_batch, t_batch)\n> > \n> > # 求各个权重的绝对误差的平均值\n> > for key in grad_numerical.keys():\n> >  diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n> >  print(key + \": \" + str(diff))\n> > ```\n> >\n> > \n> >\n> > #### 使用误差反向传播的学习\n> >\n> > ```python\n> > import sys, os\n> > sys.path.append(os.pardir)\n> > import numpy as np\n> > from dataset.mnist import load_mnist\n> > from ch05.two_layer_net import TwoLayerNet\n> > \n> > # 读入数据\n> > (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n> > \n> > network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n> > \n> > iters_num = 10000\n> > train_size = x_train.shape[0]\n> > batch_size = 100\n> > learning_rate = 0.1\n> > train_loss_list = []\n> > train_acc_list = []\n> > test_acc_list = []\n> > \n> > iter_per_epoch = max(train_size / batch_size, 1)\n> > \n> > for i in range(iters_num):\n> >     batch_mask = np.random.choice(train_size, batch_size)\n> >     x_batch = x_train[batch_mask]\n> >     t_batch = t_train[batch_mask]\n> >     \n> >     # 通过误差反向传播法求梯度\n> >     grad = network.gradient(x_batch, t_batch)\n> > \n> >     # 更新\n> >     for key in ('W1', 'b1', 'W2', 'b2'):\n> >         network.params[key] -= learning_rate * grad[key]\n> > \n> >     loss = network.loss(x_batch, t_batch)\n> >     train_loss_list.append(loss)\n> > \n> >     if i % iter_per_epoch == 0:\n> >         train_acc = network.accuracy(x_train, t_train)\n> >         test_acc = network.accuracy(x_test, t_test)\n> >         train_acc_list.append(train_acc)\n> >         test_acc_list.append(test_acc)\n> >         print(train_acc, test_acc)\n> > ```\n> >\n> > \n\n\n\n### 裂开，还得再回头多看一哈这个笔记，已经变石头人了","source":"_posts/23.md","raw":"---\ntitle: DL之路---啃鱼书（4）\ndata: 2025-06-16 21:38:00\nupdated: 2025-06-16 21:38:00\ntype: DL\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p23.png\ntags:\n  - DL\n  - Learning\n  - gnaw_book\n---\n\n\n# 误差反向传播法\n\n\n\n### 1.计算图\n\n> #### 无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题\n>\n> #### 另外，利用计算图可以将中间的计算结果全部保存起来 \n>\n> #### 其最大的优势在于，可以通过反向传播高效计算导数\n>\n> #### 反向传播传递的是局部导数\n>\n> #### 下面是，支付金额关于苹果的的价格的导数\n>\n> ![](http://picbed.yanzu.tech/img/DL/4/1.png)\n>\n> #### 就是总的金额，220/220 = 1，220/200 = 1.1，220/100=2.2\n>\n> #### 计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数\n>\n> #### 计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值\n\n\n\n### 2.链式法则\n\n> #### 如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示，这就是链式法则\n>\n> #### 反向传播是基于链式法则的\n>\n> ![](http://picbed.yanzu.tech/img/DL/4/2.png)\n\n\n\n### 3.反向传播\n\n> #### 加法节点的反向传播，只是将输入信号输出到下一个节点，输入的值会原封不动地流向下一个节点\n>\n> ![](http://picbed.yanzu.tech/img/DL/4/3.png)\n>\n> #### 乘法节点的反向传播，会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游，翻转值表示一种翻转关系\n>\n> ![](http://picbed.yanzu.tech/img/DL/4/4.png)\n>\n> #### 乘法的反向传播需要正向传播时的输入信号值，实现乘法节点的反向传播时，要保存正向传播的输入信号\n>\n> \n\n\n\n### 4.简单层的实现\n\n> #### 层，是神经网络中功能的单位，如负责sigmoid函数的sigmoid，是以层为单位进行实现的，通常，用一个类来实现一个层\n\n\n\n### 5.激活函数层的实现\n\n> #### ReLU层\n>\n> > #### 表达式如下\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/5.png)\n> >\n> > #### 求导之后\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/6.png)\n> >\n> > #### 如果正向传播时的输入x大于0，则反向传播会将上游的值原封不动地传给下游，如果正向传播时的x小于等于0，则反向传播中传给下游的信号将停在此处\n>\n> \n>\n> #### Sigmoid层\n>\n> > #### 表达式如下\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/7.png)\n> >\n> > #### 计算图表示表达式如下(正向传播)\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/8.png)\n> >\n> > #### 反向传播\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/9.png)\n> >\n> > #### 观察到反向传播的输出，可以根据正向传播的输入x和输出y计算出来\n> >\n> > #### 另外， $\\frac {\\partial L}{\\partial y} y^2 exp(-x)$ 可以进一步整理\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/10.png)\n> >\n> > #### 此时，反向传播的输出可直接根据正向传播的输出y就能计算出来\n> >\n> > \n\n\n\n### 6.Affine层/Softmax层的实现\n\n> #### Affine层（全连接层）\n>\n> > #### 神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”，将进行仿射变换的处理实现为“Affine层”，本质就是 $y = xW + b$ ，表示一个线性变换\n> >\n> > #### 几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算\n> >\n> > #### 在神经网络中使用的是 dot 进行矩阵乘积运算，Affine层的计算图如下(正向传播)\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/11.png)\n> >\n> > #### 反向传播\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/12.png)\n> >\n> > #### 注意上面反向传播的两个输出，W 和 X 的位置是不同的\n> >\n> > #### 另外，这里输入的 X 是以单个数据为对象的\n>\n> \n>\n> #### 批版本的Affine层\n>\n> > #### 考虑N个数据一起进行正向传播，就要使用 batch，batch版的Affine层计算图如下\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/13.png)\n>\n> \n>\n> #### softmax-with-loss层\n>\n> > #### 神经网络中未被正规化的输出结果有时被称为**得分**\n> >\n> > #### 当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要 Softmax层\n> >\n> > #### 因为是包含损失函数的交叉熵误差，所以叫做Softmax-with-Loss层，计算图如下\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/4/14.png)\n> >\n> > \n\n\n\n### 7.误差反向传播的实现\n\n> #### 误差反向传播法的梯度确认\n>\n> > #### 数值微分虽然计算狠很耗费时间，但它的优点是实现简单，因此，一般情况下不太容易出错；而误差反向传播法虽然计算很快，但它的实现很复杂，容易出错。所以，经常会比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的实现是否正确\n> >\n> > #### 确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地讲，是非常相近）的操作称为梯度确认（gradient check）\n\n\n\n\n\n\n### code\n\n> #### 简单层的实现\n>\n> ```python\n> # coding: utf-8\n> \n> \"\"\"\n> 实现了神经网络中的两个基础层：\n> 1. MulLayer: 乘法层，用于实现两个数的乘法运算\n> 2. AddLayer: 加法层，用于实现两个数的加法运算\n> 这些层都实现了前向传播(forward)和反向传播(backward)方法\n> \"\"\"\n> \n> class MulLayer:\n>     \"\"\"\n>     乘法层的实现\n>     用于计算两个数的乘积，并支持反向传播\n>     \"\"\"\n>     def __init__(self):\n>         # 初始化存储输入值的变量\n>         self.x = None  # 存储第一个输入值\n>         self.y = None  # 存储第二个输入值\n> \n>     def forward(self, x, y):\n>         \"\"\"\n>         前向传播：计算两个数的乘积\n>         \n>         参数:\n>             x: 第一个输入值\n>             y: 第二个输入值\n>         返回:\n>             out: x和y的乘积\n>         \"\"\"\n>         self.x = x  # 保存输入值，用于反向传播\n>         self.y = y                \n>         out = x * y  # 计算乘积\n> \n>         return out\n> \n>     def backward(self, dout):\n>         \"\"\"\n>         反向传播：计算梯度\n>         \n>         参数:\n>             dout: 上游传来的梯度\n>         返回:\n>             dx: 对x的梯度\n>             dy: 对y的梯度\n>         \"\"\"\n>         dx = dout * self.y  # 对x的梯度 = 上游梯度 * y\n>         dy = dout * self.x  # 对y的梯度 = 上游梯度 * x\n> \n>         return dx, dy\n> \n> \n> class AddLayer:\n>     \"\"\"\n>     加法层的实现\n>     用于计算两个数的和，并支持反向传播\n>     \"\"\"\n>     def __init__(self):\n>         # 加法层不需要存储中间值，所以pass\n>         pass\n> \n>     def forward(self, x, y):\n>         \"\"\"\n>         前向传播：计算两个数的和\n>         \n>         参数:\n>             x: 第一个输入值\n>             y: 第二个输入值\n>         返回:\n>             out: x和y的和\n>         \"\"\"\n>         out = x + y  # 计算和\n> \n>         return out\n> \n>     def backward(self, dout):\n>         \"\"\"\n>         反向传播：计算梯度\n>         \n>         参数:\n>             dout: 上游传来的梯度\n>         返回:\n>             dx: 对x的梯度\n>             dy: 对y的梯度\n>         \"\"\"\n>         dx = dout * 1  # 对x的梯度 = 上游梯度 * 1\n>         dy = dout * 1  # 对y的梯度 = 上游梯度 * 1\n> \n>         return dx, dy\n> \n>     \n>     \n> # 购买2个苹果3个橘子的例子\n> apple = 100\n> apple_num = 2\n> orange = 150\n> orange_num = 3\n> tax = 1.1\n> \n> mul_apple_layer = MulLayer()\n> mul_orange_layer = MulLayer()\n> add_apple_orange_layer = AddLayer()\n> mul_tax_layer = MulLayer()\n> \n> # forward\n> apple_price = mul_apple_layer.forward(apple, apple_num)\n> orange_price = mul_orange_layer.forward(orange, orange_num)\n> all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n> price = mul_tax_layer.forward(all_price, tax)\n> \n> # backward\n> d_price = 1\n> d_all_price, d_tax = mul_tax_layer.backward(d_price)\n> d_apple_peice, d_orange_price = add_apple_orange_layer.backward(d_all_price)\n> d_orange, d_orange_num = mul_orange_layer.backward(d_orange_price)\n> d_apple, d_apple_num = mul_apple_layer.backward(d_apple_peice)\n> \n> print(price)\n> print(d_apple_num)\n> print(d_apple)\n> print(d_orange_num)\n> print(d_orange)\n> print(d_tax)\n> ```\n>\n> \n>\n> #### 激活函数层的实现\n>\n> > #### ReLU层\n> >\n> > ```python\n> > class ReLU:\n> >  def __init__(self):\n> >      self.mask = None\n> > \n> >  def forward(self, x):\n> >      # x <= 0的为true，x>0的为false\n> >      self.mask = (x <= 0)\t# 保存结果，用于反向传播\n> >      # out是x的复制\n> >      out = x.copy()\n> >      # 这一步很关键，mask是一个bool数组，当它作为索引时，会选中True的位置，让后将其置为0，而False的位置不会被选中，就会保持原来的值\n> >      # 这就实现了 x <= 0置为0，x > 0保持原值\n> >      out[self.mask] = 0\n> > \n> >      return out\n> > \n> >  def backward(self, dout):\n> >      dout[self.mask] = 0\n> >      dx = dout\n> > \n> >      return dx\n> > \n> > \n> > x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n> > print(x)\n> > mask = (x <= 0 )\n> > print(mask)\n> > ```\n> >\n> > #### Sigmoid层\n> >\n> > ```python\n> > class Sigmoid:\n> >  def __init__(self):\n> >      self.out = None\n> > \n> >  def forward(self, x):\n> >      out = 1 / (1 + np.exp(-x))\n> >      self.out = out\t# 保存输出，用于反向传播\n> > \n> >      return out\n> > \n> >  def backward(self, dout):\n> >      # 对应的就是 (\\partial L/ partial y) * y * (1 - y)\n> >      dx = dout * (1.0 - self.out) * self.out\n> >      return dx\n> > ```\n> >\n> > #### Affine层\n> >\n> > ```python\n> > class Affine:\n> >  def __init__(self, W, b):\n> >      self.W = W\n> >      self.b = b\n> >      self.dW = None\n> >      self.db = None\n> > \n> >  def forward(self, x):\n> >      self.x = x\t# 保存输入，供反向传播用\n> >      out = np.dot(x, self.W) + self.b\t# b使用到了广播机制\n> > \n> >      return out\n> > \n> >  def backward(self, dout):\n> >      dx = np.dot(dout, self.W.T)\n> >      self.dW = np.dot(self.x.T, dout)\n> >      self.db = np.sum(dout, axis=0)\n> > \n> >      return dx\n> > ```\n> >\n> > #### 需要说明的是，backward中，分别表示了 $\\frac {\\partial L} {\\partial X} = \\frac {\\partial L}{\\partial Y} W^T$ ，$\\frac {\\partial L} {\\partial W} = X^T \\frac {\\partial L}{\\partial Y}$ ，$\\frac{\\partial L}{\\partial B} = \\frac{\\partial L}{\\partial Y}$ \n> >\n> > #### Softmax-with-Loss层的实现\n> >\n> > ```python\n> > class SoftmaxWithLoss:\n> >     def __init__(self):\n> >         self.loss = None # 损失\n> >         self.y = None # softmax的输出\n> >         self.t = None # 监督数据(one-hot vector)\n> > \n> >     def forward(self, x, t):\n> >         self.t = t\n> >         self.y = softmax(x)\n> >         self.loss = cross_entropy_error(self.y, self.t)\n> > \n> >         return self.loss\n> > \n> >     def backward(self, dout=1):\n> >         batch_size = self.t.shape[0]\n> >         dx = (self.y - self.t) / batch_size\n> > \n> >         return dx\n> > ```\n>\n> \n>\n> #### 误差反向传播的实现\n>\n> > #### 误差反向传播的神经网络的实现\n> >\n> > ```python\n> > import os, sys\n> > sys.path.append('../../py_pro/DL/')\n> > import numpy as np\n> > from common.layers import *\n> > from common.gradient import numerical_gradient\n> > from collections import OrderedDict\n> > from dataset.mnist import load_mnist\n> > from ch04.two_layer_net import TwoLayerNet\n> > \n> > \n> > class TwoLayerNet:\n> >  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n> >      # 初始化权重\n> >      self.params = {}\n> >      self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n> >      self.params['b1'] = np.zeros(hidden_size)\n> >      self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n> >      self.params['b2'] = np.zeros(output_size)\n> > \n> >      # 生成层\n> >      # OrderedDict是有序字典，会记录往字典中添加元素的顺序\n> >      self.layers = OrderedDict()\n> >      self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n> >      self.layers['ReLU1'] = Relu()\n> >      self.params['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n> >      \n> >      self.lastLayer = SoftmaxWithLoss()\n> > \n> >  def predict(self, x):\n> >      for layer in self.layers.values():\n> >          x = layer.forward(x)\n> >      return x\n> > \n> >  def loss(self, x, t):\n> >      y = self.predict(x)\n> >      y = np.argmax(y, axis=1)\n> >      if t.ndim != 1 : t = np.argmax(t, axis=1)\n> >      accuracy = np.sum(y == t) / float(x.shape[0])\n> >      return accuracy\n> > \n> >  def numerical_gradient(self, x, t):\n> >      loss_W = lambda W : self.loss(x, t)\n> >      grads = {}\n> > \n> >      grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n> >      grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n> >      grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n> >      grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n> > \n> >      return grads\n> > \n> >  def gradient(self, x, t):\n> >      # forward\n> >      self.loss(x, t)\n> > \n> >      # backward\n> >      dout = 1\n> >      dout = self.lastLayer.backward(dout)\n> > \n> >      layers = list(self.layers.values())\n> >      layers.reverse()\n> > \n> >      for layer in layers:\n> >          dout = layer.backward(dout)\n> > \n> >      grads = {}\n> >      grads['W1'] = self.layers['Affine1'].dW\n> >      grads['b1'] = self.layers['Affine1'].db\n> >      grads['W2'] = self.layers['Affine2'].dW\n> >      grads['b2'] = self.layers['Affine2'].db\n> > \n> >      return grads\n> >      \n> > \n> > \n> > # 读入数据\n> > (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label= True)\n> > network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n> > \n> > x_batch = x_train[:3]\n> > t_batch = t_train[:3]\n> > \n> > grad_numerical = network.numerical_gradient(x_batch, t_batch)\n> > grad_backprop = network.gradient(x_batch, t_batch)\n> > \n> > # 求各个权重的绝对误差的平均值\n> > for key in grad_numerical.keys():\n> >  diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n> >  print(key + \": \" + str(diff))\n> > ```\n> >\n> > \n> >\n> > #### 使用误差反向传播的学习\n> >\n> > ```python\n> > import sys, os\n> > sys.path.append(os.pardir)\n> > import numpy as np\n> > from dataset.mnist import load_mnist\n> > from ch05.two_layer_net import TwoLayerNet\n> > \n> > # 读入数据\n> > (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n> > \n> > network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n> > \n> > iters_num = 10000\n> > train_size = x_train.shape[0]\n> > batch_size = 100\n> > learning_rate = 0.1\n> > train_loss_list = []\n> > train_acc_list = []\n> > test_acc_list = []\n> > \n> > iter_per_epoch = max(train_size / batch_size, 1)\n> > \n> > for i in range(iters_num):\n> >     batch_mask = np.random.choice(train_size, batch_size)\n> >     x_batch = x_train[batch_mask]\n> >     t_batch = t_train[batch_mask]\n> >     \n> >     # 通过误差反向传播法求梯度\n> >     grad = network.gradient(x_batch, t_batch)\n> > \n> >     # 更新\n> >     for key in ('W1', 'b1', 'W2', 'b2'):\n> >         network.params[key] -= learning_rate * grad[key]\n> > \n> >     loss = network.loss(x_batch, t_batch)\n> >     train_loss_list.append(loss)\n> > \n> >     if i % iter_per_epoch == 0:\n> >         train_acc = network.accuracy(x_train, t_train)\n> >         test_acc = network.accuracy(x_test, t_test)\n> >         train_acc_list.append(train_acc)\n> >         test_acc_list.append(test_acc)\n> >         print(train_acc, test_acc)\n> > ```\n> >\n> > \n\n\n\n### 裂开，还得再回头多看一哈这个笔记，已经变石头人了","slug":"23","published":1,"date":"2025-06-16T13:38:14.067Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7crm0038iku43cwuebp0","content":"<h1 id=\"误差反向传播法\"><a href=\"#误差反向传播法\" class=\"headerlink\" title=\"误差反向传播法\"></a>误差反向传播法</h1><h3 id=\"1-计算图\"><a href=\"#1-计算图\" class=\"headerlink\" title=\"1.计算图\"></a>1.计算图</h3><blockquote>\n<h4 id=\"无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题\"><a href=\"#无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题\" class=\"headerlink\" title=\"无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题\"></a>无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题</h4><h4 id=\"另外，利用计算图可以将中间的计算结果全部保存起来\"><a href=\"#另外，利用计算图可以将中间的计算结果全部保存起来\" class=\"headerlink\" title=\"另外，利用计算图可以将中间的计算结果全部保存起来\"></a>另外，利用计算图可以将中间的计算结果全部保存起来</h4><h4 id=\"其最大的优势在于，可以通过反向传播高效计算导数\"><a href=\"#其最大的优势在于，可以通过反向传播高效计算导数\" class=\"headerlink\" title=\"其最大的优势在于，可以通过反向传播高效计算导数\"></a>其最大的优势在于，可以通过反向传播高效计算导数</h4><h4 id=\"反向传播传递的是局部导数\"><a href=\"#反向传播传递的是局部导数\" class=\"headerlink\" title=\"反向传播传递的是局部导数\"></a>反向传播传递的是局部导数</h4><h4 id=\"下面是，支付金额关于苹果的的价格的导数\"><a href=\"#下面是，支付金额关于苹果的的价格的导数\" class=\"headerlink\" title=\"下面是，支付金额关于苹果的的价格的导数\"></a>下面是，支付金额关于苹果的的价格的导数</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/1.png\"></p>\n<h4 id=\"就是总的金额，220-220-1，220-200-1-1，220-100-2-2\"><a href=\"#就是总的金额，220-220-1，220-200-1-1，220-100-2-2\" class=\"headerlink\" title=\"就是总的金额，220&#x2F;220 &#x3D; 1，220&#x2F;200 &#x3D; 1.1，220&#x2F;100&#x3D;2.2\"></a>就是总的金额，220&#x2F;220 &#x3D; 1，220&#x2F;200 &#x3D; 1.1，220&#x2F;100&#x3D;2.2</h4><h4 id=\"计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数\"><a href=\"#计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数\" class=\"headerlink\" title=\"计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数\"></a>计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数</h4><h4 id=\"计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值\"><a href=\"#计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值\" class=\"headerlink\" title=\"计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值\"></a>计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值</h4></blockquote>\n<h3 id=\"2-链式法则\"><a href=\"#2-链式法则\" class=\"headerlink\" title=\"2.链式法则\"></a>2.链式法则</h3><blockquote>\n<h4 id=\"如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示，这就是链式法则\"><a href=\"#如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示，这就是链式法则\" class=\"headerlink\" title=\"如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示，这就是链式法则\"></a>如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示，这就是链式法则</h4><h4 id=\"反向传播是基于链式法则的\"><a href=\"#反向传播是基于链式法则的\" class=\"headerlink\" title=\"反向传播是基于链式法则的\"></a>反向传播是基于链式法则的</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/2.png\"></p>\n</blockquote>\n<h3 id=\"3-反向传播\"><a href=\"#3-反向传播\" class=\"headerlink\" title=\"3.反向传播\"></a>3.反向传播</h3><blockquote>\n<h4 id=\"加法节点的反向传播，只是将输入信号输出到下一个节点，输入的值会原封不动地流向下一个节点\"><a href=\"#加法节点的反向传播，只是将输入信号输出到下一个节点，输入的值会原封不动地流向下一个节点\" class=\"headerlink\" title=\"加法节点的反向传播，只是将输入信号输出到下一个节点，输入的值会原封不动地流向下一个节点\"></a>加法节点的反向传播，只是将输入信号输出到下一个节点，输入的值会原封不动地流向下一个节点</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/3.png\"></p>\n<h4 id=\"乘法节点的反向传播，会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游，翻转值表示一种翻转关系\"><a href=\"#乘法节点的反向传播，会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游，翻转值表示一种翻转关系\" class=\"headerlink\" title=\"乘法节点的反向传播，会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游，翻转值表示一种翻转关系\"></a>乘法节点的反向传播，会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游，翻转值表示一种翻转关系</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/4.png\"></p>\n<h4 id=\"乘法的反向传播需要正向传播时的输入信号值，实现乘法节点的反向传播时，要保存正向传播的输入信号\"><a href=\"#乘法的反向传播需要正向传播时的输入信号值，实现乘法节点的反向传播时，要保存正向传播的输入信号\" class=\"headerlink\" title=\"乘法的反向传播需要正向传播时的输入信号值，实现乘法节点的反向传播时，要保存正向传播的输入信号\"></a>乘法的反向传播需要正向传播时的输入信号值，实现乘法节点的反向传播时，要保存正向传播的输入信号</h4></blockquote>\n<h3 id=\"4-简单层的实现\"><a href=\"#4-简单层的实现\" class=\"headerlink\" title=\"4.简单层的实现\"></a>4.简单层的实现</h3><blockquote>\n<h4 id=\"层，是神经网络中功能的单位，如负责sigmoid函数的sigmoid，是以层为单位进行实现的，通常，用一个类来实现一个层\"><a href=\"#层，是神经网络中功能的单位，如负责sigmoid函数的sigmoid，是以层为单位进行实现的，通常，用一个类来实现一个层\" class=\"headerlink\" title=\"层，是神经网络中功能的单位，如负责sigmoid函数的sigmoid，是以层为单位进行实现的，通常，用一个类来实现一个层\"></a>层，是神经网络中功能的单位，如负责sigmoid函数的sigmoid，是以层为单位进行实现的，通常，用一个类来实现一个层</h4></blockquote>\n<h3 id=\"5-激活函数层的实现\"><a href=\"#5-激活函数层的实现\" class=\"headerlink\" title=\"5.激活函数层的实现\"></a>5.激活函数层的实现</h3><blockquote>\n<h4 id=\"ReLU层\"><a href=\"#ReLU层\" class=\"headerlink\" title=\"ReLU层\"></a>ReLU层</h4><blockquote>\n<h4 id=\"表达式如下\"><a href=\"#表达式如下\" class=\"headerlink\" title=\"表达式如下\"></a>表达式如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/5.png\"></p>\n<h4 id=\"求导之后\"><a href=\"#求导之后\" class=\"headerlink\" title=\"求导之后\"></a>求导之后</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/6.png\"></p>\n<h4 id=\"如果正向传播时的输入x大于0，则反向传播会将上游的值原封不动地传给下游，如果正向传播时的x小于等于0，则反向传播中传给下游的信号将停在此处\"><a href=\"#如果正向传播时的输入x大于0，则反向传播会将上游的值原封不动地传给下游，如果正向传播时的x小于等于0，则反向传播中传给下游的信号将停在此处\" class=\"headerlink\" title=\"如果正向传播时的输入x大于0，则反向传播会将上游的值原封不动地传给下游，如果正向传播时的x小于等于0，则反向传播中传给下游的信号将停在此处\"></a>如果正向传播时的输入x大于0，则反向传播会将上游的值原封不动地传给下游，如果正向传播时的x小于等于0，则反向传播中传给下游的信号将停在此处</h4></blockquote>\n<h4 id=\"Sigmoid层\"><a href=\"#Sigmoid层\" class=\"headerlink\" title=\"Sigmoid层\"></a>Sigmoid层</h4><blockquote>\n<h4 id=\"表达式如下-1\"><a href=\"#表达式如下-1\" class=\"headerlink\" title=\"表达式如下\"></a>表达式如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/7.png\"></p>\n<h4 id=\"计算图表示表达式如下-正向传播\"><a href=\"#计算图表示表达式如下-正向传播\" class=\"headerlink\" title=\"计算图表示表达式如下(正向传播)\"></a>计算图表示表达式如下(正向传播)</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/8.png\"></p>\n<h4 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/9.png\"></p>\n<h4 id=\"观察到反向传播的输出，可以根据正向传播的输入x和输出y计算出来\"><a href=\"#观察到反向传播的输出，可以根据正向传播的输入x和输出y计算出来\" class=\"headerlink\" title=\"观察到反向传播的输出，可以根据正向传播的输入x和输出y计算出来\"></a>观察到反向传播的输出，可以根据正向传播的输入x和输出y计算出来</h4><h4 id=\"另外，-frac-partial-L-partial-y-y-2-exp-x-可以进一步整理\"><a href=\"#另外，-frac-partial-L-partial-y-y-2-exp-x-可以进一步整理\" class=\"headerlink\" title=\"另外， $\\frac {\\partial L}{\\partial y} y^2 exp(-x)$ 可以进一步整理\"></a>另外， $\\frac {\\partial L}{\\partial y} y^2 exp(-x)$ 可以进一步整理</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/10.png\"></p>\n<h4 id=\"此时，反向传播的输出可直接根据正向传播的输出y就能计算出来\"><a href=\"#此时，反向传播的输出可直接根据正向传播的输出y就能计算出来\" class=\"headerlink\" title=\"此时，反向传播的输出可直接根据正向传播的输出y就能计算出来\"></a>此时，反向传播的输出可直接根据正向传播的输出y就能计算出来</h4></blockquote>\n</blockquote>\n<h3 id=\"6-Affine层-Softmax层的实现\"><a href=\"#6-Affine层-Softmax层的实现\" class=\"headerlink\" title=\"6.Affine层&#x2F;Softmax层的实现\"></a>6.Affine层&#x2F;Softmax层的实现</h3><blockquote>\n<h4 id=\"Affine层（全连接层）\"><a href=\"#Affine层（全连接层）\" class=\"headerlink\" title=\"Affine层（全连接层）\"></a>Affine层（全连接层）</h4><blockquote>\n<h4 id=\"神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”，将进行仿射变换的处理实现为“Affine层”，本质就是-y-xW-b-，表示一个线性变换\"><a href=\"#神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”，将进行仿射变换的处理实现为“Affine层”，本质就是-y-xW-b-，表示一个线性变换\" class=\"headerlink\" title=\"神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”，将进行仿射变换的处理实现为“Affine层”，本质就是 $y &#x3D; xW + b$ ，表示一个线性变换\"></a>神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”，将进行仿射变换的处理实现为“Affine层”，本质就是 $y &#x3D; xW + b$ ，表示一个线性变换</h4><h4 id=\"几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算\"><a href=\"#几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算\" class=\"headerlink\" title=\"几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算\"></a>几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算</h4><h4 id=\"在神经网络中使用的是-dot-进行矩阵乘积运算，Affine层的计算图如下-正向传播\"><a href=\"#在神经网络中使用的是-dot-进行矩阵乘积运算，Affine层的计算图如下-正向传播\" class=\"headerlink\" title=\"在神经网络中使用的是 dot 进行矩阵乘积运算，Affine层的计算图如下(正向传播)\"></a>在神经网络中使用的是 dot 进行矩阵乘积运算，Affine层的计算图如下(正向传播)</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/11.png\"></p>\n<h4 id=\"反向传播-1\"><a href=\"#反向传播-1\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/12.png\"></p>\n<h4 id=\"注意上面反向传播的两个输出，W-和-X-的位置是不同的\"><a href=\"#注意上面反向传播的两个输出，W-和-X-的位置是不同的\" class=\"headerlink\" title=\"注意上面反向传播的两个输出，W 和 X 的位置是不同的\"></a>注意上面反向传播的两个输出，W 和 X 的位置是不同的</h4><h4 id=\"另外，这里输入的-X-是以单个数据为对象的\"><a href=\"#另外，这里输入的-X-是以单个数据为对象的\" class=\"headerlink\" title=\"另外，这里输入的 X 是以单个数据为对象的\"></a>另外，这里输入的 X 是以单个数据为对象的</h4></blockquote>\n<h4 id=\"批版本的Affine层\"><a href=\"#批版本的Affine层\" class=\"headerlink\" title=\"批版本的Affine层\"></a>批版本的Affine层</h4><blockquote>\n<h4 id=\"考虑N个数据一起进行正向传播，就要使用-batch，batch版的Affine层计算图如下\"><a href=\"#考虑N个数据一起进行正向传播，就要使用-batch，batch版的Affine层计算图如下\" class=\"headerlink\" title=\"考虑N个数据一起进行正向传播，就要使用 batch，batch版的Affine层计算图如下\"></a>考虑N个数据一起进行正向传播，就要使用 batch，batch版的Affine层计算图如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/13.png\"></p>\n</blockquote>\n<h4 id=\"softmax-with-loss层\"><a href=\"#softmax-with-loss层\" class=\"headerlink\" title=\"softmax-with-loss层\"></a>softmax-with-loss层</h4><blockquote>\n<h4 id=\"神经网络中未被正规化的输出结果有时被称为得分\"><a href=\"#神经网络中未被正规化的输出结果有时被称为得分\" class=\"headerlink\" title=\"神经网络中未被正规化的输出结果有时被称为得分\"></a>神经网络中未被正规化的输出结果有时被称为<strong>得分</strong></h4><h4 id=\"当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要-Softmax层\"><a href=\"#当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要-Softmax层\" class=\"headerlink\" title=\"当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要 Softmax层\"></a>当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要 Softmax层</h4><h4 id=\"因为是包含损失函数的交叉熵误差，所以叫做Softmax-with-Loss层，计算图如下\"><a href=\"#因为是包含损失函数的交叉熵误差，所以叫做Softmax-with-Loss层，计算图如下\" class=\"headerlink\" title=\"因为是包含损失函数的交叉熵误差，所以叫做Softmax-with-Loss层，计算图如下\"></a>因为是包含损失函数的交叉熵误差，所以叫做Softmax-with-Loss层，计算图如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/14.png\"></p>\n</blockquote>\n</blockquote>\n<h3 id=\"7-误差反向传播的实现\"><a href=\"#7-误差反向传播的实现\" class=\"headerlink\" title=\"7.误差反向传播的实现\"></a>7.误差反向传播的实现</h3><blockquote>\n<h4 id=\"误差反向传播法的梯度确认\"><a href=\"#误差反向传播法的梯度确认\" class=\"headerlink\" title=\"误差反向传播法的梯度确认\"></a>误差反向传播法的梯度确认</h4><blockquote>\n<h4 id=\"数值微分虽然计算狠很耗费时间，但它的优点是实现简单，因此，一般情况下不太容易出错；而误差反向传播法虽然计算很快，但它的实现很复杂，容易出错。所以，经常会比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的实现是否正确\"><a href=\"#数值微分虽然计算狠很耗费时间，但它的优点是实现简单，因此，一般情况下不太容易出错；而误差反向传播法虽然计算很快，但它的实现很复杂，容易出错。所以，经常会比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的实现是否正确\" class=\"headerlink\" title=\"数值微分虽然计算狠很耗费时间，但它的优点是实现简单，因此，一般情况下不太容易出错；而误差反向传播法虽然计算很快，但它的实现很复杂，容易出错。所以，经常会比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的实现是否正确\"></a>数值微分虽然计算狠很耗费时间，但它的优点是实现简单，因此，一般情况下不太容易出错；而误差反向传播法虽然计算很快，但它的实现很复杂，容易出错。所以，经常会比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的实现是否正确</h4><h4 id=\"确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地讲，是非常相近）的操作称为梯度确认（gradient-check）\"><a href=\"#确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地讲，是非常相近）的操作称为梯度确认（gradient-check）\" class=\"headerlink\" title=\"确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地讲，是非常相近）的操作称为梯度确认（gradient check）\"></a>确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地讲，是非常相近）的操作称为梯度确认（gradient check）</h4></blockquote>\n</blockquote>\n<h3 id=\"code\"><a href=\"#code\" class=\"headerlink\" title=\"code\"></a>code</h3><blockquote>\n<h4 id=\"简单层的实现\"><a href=\"#简单层的实现\" class=\"headerlink\" title=\"简单层的实现\"></a>简单层的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">实现了神经网络中的两个基础层：</span></span><br><span class=\"line\"><span class=\"string\">1. MulLayer: 乘法层，用于实现两个数的乘法运算</span></span><br><span class=\"line\"><span class=\"string\">2. AddLayer: 加法层，用于实现两个数的加法运算</span></span><br><span class=\"line\"><span class=\"string\">这些层都实现了前向传播(forward)和反向传播(backward)方法</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MulLayer</span>:</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    乘法层的实现</span></span><br><span class=\"line\"><span class=\"string\">    用于计算两个数的乘积，并支持反向传播</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 初始化存储输入值的变量</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.x = <span class=\"literal\">None</span>  <span class=\"comment\"># 存储第一个输入值</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.y = <span class=\"literal\">None</span>  <span class=\"comment\"># 存储第二个输入值</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, y</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        前向传播：计算两个数的乘积</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x: 第一个输入值</span></span><br><span class=\"line\"><span class=\"string\">            y: 第二个输入值</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            out: x和y的乘积</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.x = x  <span class=\"comment\"># 保存输入值，用于反向传播</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.y = y                </span><br><span class=\"line\">        out = x * y  <span class=\"comment\"># 计算乘积</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">backward</span>(<span class=\"params\">self, dout</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        反向传播：计算梯度</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            dout: 上游传来的梯度</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            dx: 对x的梯度</span></span><br><span class=\"line\"><span class=\"string\">            dy: 对y的梯度</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        dx = dout * <span class=\"variable language_\">self</span>.y  <span class=\"comment\"># 对x的梯度 = 上游梯度 * y</span></span><br><span class=\"line\">        dy = dout * <span class=\"variable language_\">self</span>.x  <span class=\"comment\"># 对y的梯度 = 上游梯度 * x</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> dx, dy</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">AddLayer</span>:</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    加法层的实现</span></span><br><span class=\"line\"><span class=\"string\">    用于计算两个数的和，并支持反向传播</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 加法层不需要存储中间值，所以pass</span></span><br><span class=\"line\">        <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, y</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        前向传播：计算两个数的和</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x: 第一个输入值</span></span><br><span class=\"line\"><span class=\"string\">            y: 第二个输入值</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            out: x和y的和</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        out = x + y  <span class=\"comment\"># 计算和</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">backward</span>(<span class=\"params\">self, dout</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        反向传播：计算梯度</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            dout: 上游传来的梯度</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            dx: 对x的梯度</span></span><br><span class=\"line\"><span class=\"string\">            dy: 对y的梯度</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        dx = dout * <span class=\"number\">1</span>  <span class=\"comment\"># 对x的梯度 = 上游梯度 * 1</span></span><br><span class=\"line\">        dy = dout * <span class=\"number\">1</span>  <span class=\"comment\"># 对y的梯度 = 上游梯度 * 1</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> dx, dy</span><br><span class=\"line\"></span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\"># 购买2个苹果3个橘子的例子</span></span><br><span class=\"line\">apple = <span class=\"number\">100</span></span><br><span class=\"line\">apple_num = <span class=\"number\">2</span></span><br><span class=\"line\">orange = <span class=\"number\">150</span></span><br><span class=\"line\">orange_num = <span class=\"number\">3</span></span><br><span class=\"line\">tax = <span class=\"number\">1.1</span></span><br><span class=\"line\"></span><br><span class=\"line\">mul_apple_layer = MulLayer()</span><br><span class=\"line\">mul_orange_layer = MulLayer()</span><br><span class=\"line\">add_apple_orange_layer = AddLayer()</span><br><span class=\"line\">mul_tax_layer = MulLayer()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># forward</span></span><br><span class=\"line\">apple_price = mul_apple_layer.forward(apple, apple_num)</span><br><span class=\"line\">orange_price = mul_orange_layer.forward(orange, orange_num)</span><br><span class=\"line\">all_price = add_apple_orange_layer.forward(apple_price, orange_price)</span><br><span class=\"line\">price = mul_tax_layer.forward(all_price, tax)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># backward</span></span><br><span class=\"line\">d_price = <span class=\"number\">1</span></span><br><span class=\"line\">d_all_price, d_tax = mul_tax_layer.backward(d_price)</span><br><span class=\"line\">d_apple_peice, d_orange_price = add_apple_orange_layer.backward(d_all_price)</span><br><span class=\"line\">d_orange, d_orange_num = mul_orange_layer.backward(d_orange_price)</span><br><span class=\"line\">d_apple, d_apple_num = mul_apple_layer.backward(d_apple_peice)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(price)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(d_apple_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(d_apple)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(d_orange_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(d_orange)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(d_tax)</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"激活函数层的实现\"><a href=\"#激活函数层的实现\" class=\"headerlink\" title=\"激活函数层的实现\"></a>激活函数层的实现</h4><blockquote>\n<h4 id=\"ReLU层-1\"><a href=\"#ReLU层-1\" class=\"headerlink\" title=\"ReLU层\"></a>ReLU层</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ReLU</span>:</span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.mask = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">     <span class=\"comment\"># x &lt;= 0的为true，x&gt;0的为false</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.mask = (x &lt;= <span class=\"number\">0</span>)\t<span class=\"comment\"># 保存结果，用于反向传播</span></span><br><span class=\"line\">     <span class=\"comment\"># out是x的复制</span></span><br><span class=\"line\">     out = x.copy()</span><br><span class=\"line\">     <span class=\"comment\"># 这一步很关键，mask是一个bool数组，当它作为索引时，会选中True的位置，让后将其置为0，而False的位置不会被选中，就会保持原来的值</span></span><br><span class=\"line\">     <span class=\"comment\"># 这就实现了 x &lt;= 0置为0，x &gt; 0保持原值</span></span><br><span class=\"line\">     out[<span class=\"variable language_\">self</span>.mask] = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">backward</span>(<span class=\"params\">self, dout</span>):</span><br><span class=\"line\">     dout[<span class=\"variable language_\">self</span>.mask] = <span class=\"number\">0</span></span><br><span class=\"line\">     dx = dout</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"keyword\">return</span> dx</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">x = np.array([[<span class=\"number\">1.0</span>, -<span class=\"number\">0.5</span>], [-<span class=\"number\">2.0</span>, <span class=\"number\">3.0</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)</span><br><span class=\"line\">mask = (x &lt;= <span class=\"number\">0</span> )</span><br><span class=\"line\"><span class=\"built_in\">print</span>(mask)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Sigmoid层-1\"><a href=\"#Sigmoid层-1\" class=\"headerlink\" title=\"Sigmoid层\"></a>Sigmoid层</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Sigmoid</span>:</span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.out = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">     out = <span class=\"number\">1</span> / (<span class=\"number\">1</span> + np.exp(-x))</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.out = out\t<span class=\"comment\"># 保存输出，用于反向传播</span></span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">backward</span>(<span class=\"params\">self, dout</span>):</span><br><span class=\"line\">     <span class=\"comment\"># 对应的就是 (\\partial L/ partial y) * y * (1 - y)</span></span><br><span class=\"line\">     dx = dout * (<span class=\"number\">1.0</span> - <span class=\"variable language_\">self</span>.out) * <span class=\"variable language_\">self</span>.out</span><br><span class=\"line\">     <span class=\"keyword\">return</span> dx</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Affine层\"><a href=\"#Affine层\" class=\"headerlink\" title=\"Affine层\"></a>Affine层</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Affine</span>:</span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, W, b</span>):</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.W = W</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.b = b</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.dW = <span class=\"literal\">None</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.db = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.x = x\t<span class=\"comment\"># 保存输入，供反向传播用</span></span><br><span class=\"line\">     out = np.dot(x, <span class=\"variable language_\">self</span>.W) + <span class=\"variable language_\">self</span>.b\t<span class=\"comment\"># b使用到了广播机制</span></span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">backward</span>(<span class=\"params\">self, dout</span>):</span><br><span class=\"line\">     dx = np.dot(dout, <span class=\"variable language_\">self</span>.W.T)</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.dW = np.dot(<span class=\"variable language_\">self</span>.x.T, dout)</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.db = np.<span class=\"built_in\">sum</span>(dout, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"keyword\">return</span> dx</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"需要说明的是，backward中，分别表示了-frac-partial-L-partial-X-frac-partial-L-partial-Y-W-T-，-frac-partial-L-partial-W-X-T-frac-partial-L-partial-Y-，-frac-partial-L-partial-B-frac-partial-L-partial-Y\"><a href=\"#需要说明的是，backward中，分别表示了-frac-partial-L-partial-X-frac-partial-L-partial-Y-W-T-，-frac-partial-L-partial-W-X-T-frac-partial-L-partial-Y-，-frac-partial-L-partial-B-frac-partial-L-partial-Y\" class=\"headerlink\" title=\"需要说明的是，backward中，分别表示了 $\\frac {\\partial L} {\\partial X} &#x3D; \\frac {\\partial L}{\\partial Y} W^T$ ，$\\frac {\\partial L} {\\partial W} &#x3D; X^T \\frac {\\partial L}{\\partial Y}$ ，$\\frac{\\partial L}{\\partial B} &#x3D; \\frac{\\partial L}{\\partial Y}$\"></a>需要说明的是，backward中，分别表示了 $\\frac {\\partial L} {\\partial X} &#x3D; \\frac {\\partial L}{\\partial Y} W^T$ ，$\\frac {\\partial L} {\\partial W} &#x3D; X^T \\frac {\\partial L}{\\partial Y}$ ，$\\frac{\\partial L}{\\partial B} &#x3D; \\frac{\\partial L}{\\partial Y}$</h4><h4 id=\"Softmax-with-Loss层的实现\"><a href=\"#Softmax-with-Loss层的实现\" class=\"headerlink\" title=\"Softmax-with-Loss层的实现\"></a>Softmax-with-Loss层的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SoftmaxWithLoss</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.loss = <span class=\"literal\">None</span> <span class=\"comment\"># 损失</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.y = <span class=\"literal\">None</span> <span class=\"comment\"># softmax的输出</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.t = <span class=\"literal\">None</span> <span class=\"comment\"># 监督数据(one-hot vector)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.t = t</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.y = softmax(x)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.loss = cross_entropy_error(<span class=\"variable language_\">self</span>.y, <span class=\"variable language_\">self</span>.t)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.loss</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">backward</span>(<span class=\"params\">self, dout=<span class=\"number\">1</span></span>):</span><br><span class=\"line\">        batch_size = <span class=\"variable language_\">self</span>.t.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">        dx = (<span class=\"variable language_\">self</span>.y - <span class=\"variable language_\">self</span>.t) / batch_size</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> dx</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h4 id=\"误差反向传播的实现\"><a href=\"#误差反向传播的实现\" class=\"headerlink\" title=\"误差反向传播的实现\"></a>误差反向传播的实现</h4><blockquote>\n<h4 id=\"误差反向传播的神经网络的实现\"><a href=\"#误差反向传播的神经网络的实现\" class=\"headerlink\" title=\"误差反向传播的神经网络的实现\"></a>误差反向传播的神经网络的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os, sys</span><br><span class=\"line\">sys.path.append(<span class=\"string\">&#x27;../../py_pro/DL/&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.layers <span class=\"keyword\">import</span> *</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.gradient <span class=\"keyword\">import</span> numerical_gradient</span><br><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> OrderedDict</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist</span><br><span class=\"line\"><span class=\"keyword\">from</span> ch04.two_layer_net <span class=\"keyword\">import</span> TwoLayerNet</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">TwoLayerNet</span>:</span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, input_size, hidden_size, output_size, weight_init_std=<span class=\"number\">0.01</span></span>):</span><br><span class=\"line\">     <span class=\"comment\"># 初始化权重</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.params = &#123;&#125;</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W1&#x27;</span>] = weight_init_std * np.random.randn(input_size, hidden_size)</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b1&#x27;</span>] = np.zeros(hidden_size)</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W2&#x27;</span>] = weight_init_std * np.random.randn(hidden_size, output_size)</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b2&#x27;</span>] = np.zeros(output_size)</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"comment\"># 生成层</span></span><br><span class=\"line\">     <span class=\"comment\"># OrderedDict是有序字典，会记录往字典中添加元素的顺序</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.layers = OrderedDict()</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine1&#x27;</span>] = Affine(<span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W1&#x27;</span>], <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b1&#x27;</span>])</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;ReLU1&#x27;</span>] = Relu()</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;Affine2&#x27;</span>] = Affine(<span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W2&#x27;</span>], <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b2&#x27;</span>])</span><br><span class=\"line\">     </span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.lastLayer = SoftmaxWithLoss()</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">     <span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>.layers.values():</span><br><span class=\"line\">         x = layer.forward(x)</span><br><span class=\"line\">     <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">loss</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">     y = <span class=\"variable language_\">self</span>.predict(x)</span><br><span class=\"line\">     y = np.argmax(y, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">     <span class=\"keyword\">if</span> t.ndim != <span class=\"number\">1</span> : t = np.argmax(t, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">     accuracy = np.<span class=\"built_in\">sum</span>(y == t) / <span class=\"built_in\">float</span>(x.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">     <span class=\"keyword\">return</span> accuracy</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">numerical_gradient</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">     loss_W = <span class=\"keyword\">lambda</span> W : <span class=\"variable language_\">self</span>.loss(x, t)</span><br><span class=\"line\">     grads = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">     grads[<span class=\"string\">&#x27;W1&#x27;</span>] = numerical_gradient(loss_W, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W1&#x27;</span>])</span><br><span class=\"line\">     grads[<span class=\"string\">&#x27;b1&#x27;</span>] = numerical_gradient(loss_W, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b1&#x27;</span>])</span><br><span class=\"line\">     grads[<span class=\"string\">&#x27;W2&#x27;</span>] = numerical_gradient(loss_W, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W2&#x27;</span>])</span><br><span class=\"line\">     grads[<span class=\"string\">&#x27;b2&#x27;</span>] = numerical_gradient(loss_W, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b2&#x27;</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"keyword\">return</span> grads</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">gradient</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">     <span class=\"comment\"># forward</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.loss(x, t)</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"comment\"># backward</span></span><br><span class=\"line\">     dout = <span class=\"number\">1</span></span><br><span class=\"line\">     dout = <span class=\"variable language_\">self</span>.lastLayer.backward(dout)</span><br><span class=\"line\"></span><br><span class=\"line\">     layers = <span class=\"built_in\">list</span>(<span class=\"variable language_\">self</span>.layers.values())</span><br><span class=\"line\">     layers.reverse()</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> layers:</span><br><span class=\"line\">         dout = layer.backward(dout)</span><br><span class=\"line\"></span><br><span class=\"line\">     grads = &#123;&#125;</span><br><span class=\"line\">     grads[<span class=\"string\">&#x27;W1&#x27;</span>] = <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine1&#x27;</span>].dW</span><br><span class=\"line\">     grads[<span class=\"string\">&#x27;b1&#x27;</span>] = <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine1&#x27;</span>].db</span><br><span class=\"line\">     grads[<span class=\"string\">&#x27;W2&#x27;</span>] = <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine2&#x27;</span>].dW</span><br><span class=\"line\">     grads[<span class=\"string\">&#x27;b2&#x27;</span>] = <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine2&#x27;</span>].db</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"keyword\">return</span> grads</span><br><span class=\"line\">     </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 读入数据</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>, one_hot_label= <span class=\"literal\">True</span>)</span><br><span class=\"line\">network = TwoLayerNet(input_size=<span class=\"number\">784</span>, hidden_size=<span class=\"number\">50</span>, output_size=<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">x_batch = x_train[:<span class=\"number\">3</span>]</span><br><span class=\"line\">t_batch = t_train[:<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">grad_numerical = network.numerical_gradient(x_batch, t_batch)</span><br><span class=\"line\">grad_backprop = network.gradient(x_batch, t_batch)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 求各个权重的绝对误差的平均值</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> grad_numerical.keys():</span><br><span class=\"line\"> diff = np.average( np.<span class=\"built_in\">abs</span>(grad_backprop[key] - grad_numerical[key]) )</span><br><span class=\"line\"> <span class=\"built_in\">print</span>(key + <span class=\"string\">&quot;: &quot;</span> + <span class=\"built_in\">str</span>(diff))</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"使用误差反向传播的学习\"><a href=\"#使用误差反向传播的学习\" class=\"headerlink\" title=\"使用误差反向传播的学习\"></a>使用误差反向传播的学习</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys, os</span><br><span class=\"line\">sys.path.append(os.pardir)</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist</span><br><span class=\"line\"><span class=\"keyword\">from</span> ch05.two_layer_net <span class=\"keyword\">import</span> TwoLayerNet</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 读入数据</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>, one_hot_label=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">network = TwoLayerNet(input_size=<span class=\"number\">784</span>, hidden_size=<span class=\"number\">50</span>, output_size=<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">iters_num = <span class=\"number\">10000</span></span><br><span class=\"line\">train_size = x_train.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">batch_size = <span class=\"number\">100</span></span><br><span class=\"line\">learning_rate = <span class=\"number\">0.1</span></span><br><span class=\"line\">train_loss_list = []</span><br><span class=\"line\">train_acc_list = []</span><br><span class=\"line\">test_acc_list = []</span><br><span class=\"line\"></span><br><span class=\"line\">iter_per_epoch = <span class=\"built_in\">max</span>(train_size / batch_size, <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(iters_num):</span><br><span class=\"line\">    batch_mask = np.random.choice(train_size, batch_size)</span><br><span class=\"line\">    x_batch = x_train[batch_mask]</span><br><span class=\"line\">    t_batch = t_train[batch_mask]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 通过误差反向传播法求梯度</span></span><br><span class=\"line\">    grad = network.gradient(x_batch, t_batch)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 更新</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> (<span class=\"string\">&#x27;W1&#x27;</span>, <span class=\"string\">&#x27;b1&#x27;</span>, <span class=\"string\">&#x27;W2&#x27;</span>, <span class=\"string\">&#x27;b2&#x27;</span>):</span><br><span class=\"line\">        network.params[key] -= learning_rate * grad[key]</span><br><span class=\"line\"></span><br><span class=\"line\">    loss = network.loss(x_batch, t_batch)</span><br><span class=\"line\">    train_loss_list.append(loss)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> i % iter_per_epoch == <span class=\"number\">0</span>:</span><br><span class=\"line\">        train_acc = network.accuracy(x_train, t_train)</span><br><span class=\"line\">        test_acc = network.accuracy(x_test, t_test)</span><br><span class=\"line\">        train_acc_list.append(train_acc)</span><br><span class=\"line\">        test_acc_list.append(test_acc)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(train_acc, test_acc)</span><br></pre></td></tr></table></figure>\n\n\n</blockquote>\n</blockquote>\n<h3 id=\"裂开，还得再回头多看一哈这个笔记，已经变石头人了\"><a href=\"#裂开，还得再回头多看一哈这个笔记，已经变石头人了\" class=\"headerlink\" title=\"裂开，还得再回头多看一哈这个笔记，已经变石头人了\"></a>裂开，还得再回头多看一哈这个笔记，已经变石头人了</h3>","cover_type":"img","excerpt":"","more":"<h1 id=\"误差反向传播法\"><a href=\"#误差反向传播法\" class=\"headerlink\" title=\"误差反向传播法\"></a>误差反向传播法</h1><h3 id=\"1-计算图\"><a href=\"#1-计算图\" class=\"headerlink\" title=\"1.计算图\"></a>1.计算图</h3><blockquote>\n<h4 id=\"无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题\"><a href=\"#无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题\" class=\"headerlink\" title=\"无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题\"></a>无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题</h4><h4 id=\"另外，利用计算图可以将中间的计算结果全部保存起来\"><a href=\"#另外，利用计算图可以将中间的计算结果全部保存起来\" class=\"headerlink\" title=\"另外，利用计算图可以将中间的计算结果全部保存起来\"></a>另外，利用计算图可以将中间的计算结果全部保存起来</h4><h4 id=\"其最大的优势在于，可以通过反向传播高效计算导数\"><a href=\"#其最大的优势在于，可以通过反向传播高效计算导数\" class=\"headerlink\" title=\"其最大的优势在于，可以通过反向传播高效计算导数\"></a>其最大的优势在于，可以通过反向传播高效计算导数</h4><h4 id=\"反向传播传递的是局部导数\"><a href=\"#反向传播传递的是局部导数\" class=\"headerlink\" title=\"反向传播传递的是局部导数\"></a>反向传播传递的是局部导数</h4><h4 id=\"下面是，支付金额关于苹果的的价格的导数\"><a href=\"#下面是，支付金额关于苹果的的价格的导数\" class=\"headerlink\" title=\"下面是，支付金额关于苹果的的价格的导数\"></a>下面是，支付金额关于苹果的的价格的导数</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/1.png\"></p>\n<h4 id=\"就是总的金额，220-220-1，220-200-1-1，220-100-2-2\"><a href=\"#就是总的金额，220-220-1，220-200-1-1，220-100-2-2\" class=\"headerlink\" title=\"就是总的金额，220&#x2F;220 &#x3D; 1，220&#x2F;200 &#x3D; 1.1，220&#x2F;100&#x3D;2.2\"></a>就是总的金额，220&#x2F;220 &#x3D; 1，220&#x2F;200 &#x3D; 1.1，220&#x2F;100&#x3D;2.2</h4><h4 id=\"计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数\"><a href=\"#计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数\" class=\"headerlink\" title=\"计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数\"></a>计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数</h4><h4 id=\"计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值\"><a href=\"#计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值\" class=\"headerlink\" title=\"计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值\"></a>计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值</h4></blockquote>\n<h3 id=\"2-链式法则\"><a href=\"#2-链式法则\" class=\"headerlink\" title=\"2.链式法则\"></a>2.链式法则</h3><blockquote>\n<h4 id=\"如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示，这就是链式法则\"><a href=\"#如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示，这就是链式法则\" class=\"headerlink\" title=\"如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示，这就是链式法则\"></a>如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示，这就是链式法则</h4><h4 id=\"反向传播是基于链式法则的\"><a href=\"#反向传播是基于链式法则的\" class=\"headerlink\" title=\"反向传播是基于链式法则的\"></a>反向传播是基于链式法则的</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/2.png\"></p>\n</blockquote>\n<h3 id=\"3-反向传播\"><a href=\"#3-反向传播\" class=\"headerlink\" title=\"3.反向传播\"></a>3.反向传播</h3><blockquote>\n<h4 id=\"加法节点的反向传播，只是将输入信号输出到下一个节点，输入的值会原封不动地流向下一个节点\"><a href=\"#加法节点的反向传播，只是将输入信号输出到下一个节点，输入的值会原封不动地流向下一个节点\" class=\"headerlink\" title=\"加法节点的反向传播，只是将输入信号输出到下一个节点，输入的值会原封不动地流向下一个节点\"></a>加法节点的反向传播，只是将输入信号输出到下一个节点，输入的值会原封不动地流向下一个节点</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/3.png\"></p>\n<h4 id=\"乘法节点的反向传播，会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游，翻转值表示一种翻转关系\"><a href=\"#乘法节点的反向传播，会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游，翻转值表示一种翻转关系\" class=\"headerlink\" title=\"乘法节点的反向传播，会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游，翻转值表示一种翻转关系\"></a>乘法节点的反向传播，会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游，翻转值表示一种翻转关系</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/4.png\"></p>\n<h4 id=\"乘法的反向传播需要正向传播时的输入信号值，实现乘法节点的反向传播时，要保存正向传播的输入信号\"><a href=\"#乘法的反向传播需要正向传播时的输入信号值，实现乘法节点的反向传播时，要保存正向传播的输入信号\" class=\"headerlink\" title=\"乘法的反向传播需要正向传播时的输入信号值，实现乘法节点的反向传播时，要保存正向传播的输入信号\"></a>乘法的反向传播需要正向传播时的输入信号值，实现乘法节点的反向传播时，要保存正向传播的输入信号</h4></blockquote>\n<h3 id=\"4-简单层的实现\"><a href=\"#4-简单层的实现\" class=\"headerlink\" title=\"4.简单层的实现\"></a>4.简单层的实现</h3><blockquote>\n<h4 id=\"层，是神经网络中功能的单位，如负责sigmoid函数的sigmoid，是以层为单位进行实现的，通常，用一个类来实现一个层\"><a href=\"#层，是神经网络中功能的单位，如负责sigmoid函数的sigmoid，是以层为单位进行实现的，通常，用一个类来实现一个层\" class=\"headerlink\" title=\"层，是神经网络中功能的单位，如负责sigmoid函数的sigmoid，是以层为单位进行实现的，通常，用一个类来实现一个层\"></a>层，是神经网络中功能的单位，如负责sigmoid函数的sigmoid，是以层为单位进行实现的，通常，用一个类来实现一个层</h4></blockquote>\n<h3 id=\"5-激活函数层的实现\"><a href=\"#5-激活函数层的实现\" class=\"headerlink\" title=\"5.激活函数层的实现\"></a>5.激活函数层的实现</h3><blockquote>\n<h4 id=\"ReLU层\"><a href=\"#ReLU层\" class=\"headerlink\" title=\"ReLU层\"></a>ReLU层</h4><blockquote>\n<h4 id=\"表达式如下\"><a href=\"#表达式如下\" class=\"headerlink\" title=\"表达式如下\"></a>表达式如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/5.png\"></p>\n<h4 id=\"求导之后\"><a href=\"#求导之后\" class=\"headerlink\" title=\"求导之后\"></a>求导之后</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/6.png\"></p>\n<h4 id=\"如果正向传播时的输入x大于0，则反向传播会将上游的值原封不动地传给下游，如果正向传播时的x小于等于0，则反向传播中传给下游的信号将停在此处\"><a href=\"#如果正向传播时的输入x大于0，则反向传播会将上游的值原封不动地传给下游，如果正向传播时的x小于等于0，则反向传播中传给下游的信号将停在此处\" class=\"headerlink\" title=\"如果正向传播时的输入x大于0，则反向传播会将上游的值原封不动地传给下游，如果正向传播时的x小于等于0，则反向传播中传给下游的信号将停在此处\"></a>如果正向传播时的输入x大于0，则反向传播会将上游的值原封不动地传给下游，如果正向传播时的x小于等于0，则反向传播中传给下游的信号将停在此处</h4></blockquote>\n<h4 id=\"Sigmoid层\"><a href=\"#Sigmoid层\" class=\"headerlink\" title=\"Sigmoid层\"></a>Sigmoid层</h4><blockquote>\n<h4 id=\"表达式如下-1\"><a href=\"#表达式如下-1\" class=\"headerlink\" title=\"表达式如下\"></a>表达式如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/7.png\"></p>\n<h4 id=\"计算图表示表达式如下-正向传播\"><a href=\"#计算图表示表达式如下-正向传播\" class=\"headerlink\" title=\"计算图表示表达式如下(正向传播)\"></a>计算图表示表达式如下(正向传播)</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/8.png\"></p>\n<h4 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/9.png\"></p>\n<h4 id=\"观察到反向传播的输出，可以根据正向传播的输入x和输出y计算出来\"><a href=\"#观察到反向传播的输出，可以根据正向传播的输入x和输出y计算出来\" class=\"headerlink\" title=\"观察到反向传播的输出，可以根据正向传播的输入x和输出y计算出来\"></a>观察到反向传播的输出，可以根据正向传播的输入x和输出y计算出来</h4><h4 id=\"另外，-frac-partial-L-partial-y-y-2-exp-x-可以进一步整理\"><a href=\"#另外，-frac-partial-L-partial-y-y-2-exp-x-可以进一步整理\" class=\"headerlink\" title=\"另外， $\\frac {\\partial L}{\\partial y} y^2 exp(-x)$ 可以进一步整理\"></a>另外， $\\frac {\\partial L}{\\partial y} y^2 exp(-x)$ 可以进一步整理</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/10.png\"></p>\n<h4 id=\"此时，反向传播的输出可直接根据正向传播的输出y就能计算出来\"><a href=\"#此时，反向传播的输出可直接根据正向传播的输出y就能计算出来\" class=\"headerlink\" title=\"此时，反向传播的输出可直接根据正向传播的输出y就能计算出来\"></a>此时，反向传播的输出可直接根据正向传播的输出y就能计算出来</h4></blockquote>\n</blockquote>\n<h3 id=\"6-Affine层-Softmax层的实现\"><a href=\"#6-Affine层-Softmax层的实现\" class=\"headerlink\" title=\"6.Affine层&#x2F;Softmax层的实现\"></a>6.Affine层&#x2F;Softmax层的实现</h3><blockquote>\n<h4 id=\"Affine层（全连接层）\"><a href=\"#Affine层（全连接层）\" class=\"headerlink\" title=\"Affine层（全连接层）\"></a>Affine层（全连接层）</h4><blockquote>\n<h4 id=\"神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”，将进行仿射变换的处理实现为“Affine层”，本质就是-y-xW-b-，表示一个线性变换\"><a href=\"#神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”，将进行仿射变换的处理实现为“Affine层”，本质就是-y-xW-b-，表示一个线性变换\" class=\"headerlink\" title=\"神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”，将进行仿射变换的处理实现为“Affine层”，本质就是 $y &#x3D; xW + b$ ，表示一个线性变换\"></a>神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”，将进行仿射变换的处理实现为“Affine层”，本质就是 $y &#x3D; xW + b$ ，表示一个线性变换</h4><h4 id=\"几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算\"><a href=\"#几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算\" class=\"headerlink\" title=\"几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算\"></a>几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算</h4><h4 id=\"在神经网络中使用的是-dot-进行矩阵乘积运算，Affine层的计算图如下-正向传播\"><a href=\"#在神经网络中使用的是-dot-进行矩阵乘积运算，Affine层的计算图如下-正向传播\" class=\"headerlink\" title=\"在神经网络中使用的是 dot 进行矩阵乘积运算，Affine层的计算图如下(正向传播)\"></a>在神经网络中使用的是 dot 进行矩阵乘积运算，Affine层的计算图如下(正向传播)</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/11.png\"></p>\n<h4 id=\"反向传播-1\"><a href=\"#反向传播-1\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/12.png\"></p>\n<h4 id=\"注意上面反向传播的两个输出，W-和-X-的位置是不同的\"><a href=\"#注意上面反向传播的两个输出，W-和-X-的位置是不同的\" class=\"headerlink\" title=\"注意上面反向传播的两个输出，W 和 X 的位置是不同的\"></a>注意上面反向传播的两个输出，W 和 X 的位置是不同的</h4><h4 id=\"另外，这里输入的-X-是以单个数据为对象的\"><a href=\"#另外，这里输入的-X-是以单个数据为对象的\" class=\"headerlink\" title=\"另外，这里输入的 X 是以单个数据为对象的\"></a>另外，这里输入的 X 是以单个数据为对象的</h4></blockquote>\n<h4 id=\"批版本的Affine层\"><a href=\"#批版本的Affine层\" class=\"headerlink\" title=\"批版本的Affine层\"></a>批版本的Affine层</h4><blockquote>\n<h4 id=\"考虑N个数据一起进行正向传播，就要使用-batch，batch版的Affine层计算图如下\"><a href=\"#考虑N个数据一起进行正向传播，就要使用-batch，batch版的Affine层计算图如下\" class=\"headerlink\" title=\"考虑N个数据一起进行正向传播，就要使用 batch，batch版的Affine层计算图如下\"></a>考虑N个数据一起进行正向传播，就要使用 batch，batch版的Affine层计算图如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/13.png\"></p>\n</blockquote>\n<h4 id=\"softmax-with-loss层\"><a href=\"#softmax-with-loss层\" class=\"headerlink\" title=\"softmax-with-loss层\"></a>softmax-with-loss层</h4><blockquote>\n<h4 id=\"神经网络中未被正规化的输出结果有时被称为得分\"><a href=\"#神经网络中未被正规化的输出结果有时被称为得分\" class=\"headerlink\" title=\"神经网络中未被正规化的输出结果有时被称为得分\"></a>神经网络中未被正规化的输出结果有时被称为<strong>得分</strong></h4><h4 id=\"当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要-Softmax层\"><a href=\"#当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要-Softmax层\" class=\"headerlink\" title=\"当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要 Softmax层\"></a>当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要 Softmax层</h4><h4 id=\"因为是包含损失函数的交叉熵误差，所以叫做Softmax-with-Loss层，计算图如下\"><a href=\"#因为是包含损失函数的交叉熵误差，所以叫做Softmax-with-Loss层，计算图如下\" class=\"headerlink\" title=\"因为是包含损失函数的交叉熵误差，所以叫做Softmax-with-Loss层，计算图如下\"></a>因为是包含损失函数的交叉熵误差，所以叫做Softmax-with-Loss层，计算图如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/4/14.png\"></p>\n</blockquote>\n</blockquote>\n<h3 id=\"7-误差反向传播的实现\"><a href=\"#7-误差反向传播的实现\" class=\"headerlink\" title=\"7.误差反向传播的实现\"></a>7.误差反向传播的实现</h3><blockquote>\n<h4 id=\"误差反向传播法的梯度确认\"><a href=\"#误差反向传播法的梯度确认\" class=\"headerlink\" title=\"误差反向传播法的梯度确认\"></a>误差反向传播法的梯度确认</h4><blockquote>\n<h4 id=\"数值微分虽然计算狠很耗费时间，但它的优点是实现简单，因此，一般情况下不太容易出错；而误差反向传播法虽然计算很快，但它的实现很复杂，容易出错。所以，经常会比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的实现是否正确\"><a href=\"#数值微分虽然计算狠很耗费时间，但它的优点是实现简单，因此，一般情况下不太容易出错；而误差反向传播法虽然计算很快，但它的实现很复杂，容易出错。所以，经常会比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的实现是否正确\" class=\"headerlink\" title=\"数值微分虽然计算狠很耗费时间，但它的优点是实现简单，因此，一般情况下不太容易出错；而误差反向传播法虽然计算很快，但它的实现很复杂，容易出错。所以，经常会比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的实现是否正确\"></a>数值微分虽然计算狠很耗费时间，但它的优点是实现简单，因此，一般情况下不太容易出错；而误差反向传播法虽然计算很快，但它的实现很复杂，容易出错。所以，经常会比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的实现是否正确</h4><h4 id=\"确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地讲，是非常相近）的操作称为梯度确认（gradient-check）\"><a href=\"#确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地讲，是非常相近）的操作称为梯度确认（gradient-check）\" class=\"headerlink\" title=\"确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地讲，是非常相近）的操作称为梯度确认（gradient check）\"></a>确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地讲，是非常相近）的操作称为梯度确认（gradient check）</h4></blockquote>\n</blockquote>\n<h3 id=\"code\"><a href=\"#code\" class=\"headerlink\" title=\"code\"></a>code</h3><blockquote>\n<h4 id=\"简单层的实现\"><a href=\"#简单层的实现\" class=\"headerlink\" title=\"简单层的实现\"></a>简单层的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">实现了神经网络中的两个基础层：</span></span><br><span class=\"line\"><span class=\"string\">1. MulLayer: 乘法层，用于实现两个数的乘法运算</span></span><br><span class=\"line\"><span class=\"string\">2. AddLayer: 加法层，用于实现两个数的加法运算</span></span><br><span class=\"line\"><span class=\"string\">这些层都实现了前向传播(forward)和反向传播(backward)方法</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">MulLayer</span>:</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    乘法层的实现</span></span><br><span class=\"line\"><span class=\"string\">    用于计算两个数的乘积，并支持反向传播</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 初始化存储输入值的变量</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.x = <span class=\"literal\">None</span>  <span class=\"comment\"># 存储第一个输入值</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.y = <span class=\"literal\">None</span>  <span class=\"comment\"># 存储第二个输入值</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, y</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        前向传播：计算两个数的乘积</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x: 第一个输入值</span></span><br><span class=\"line\"><span class=\"string\">            y: 第二个输入值</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            out: x和y的乘积</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.x = x  <span class=\"comment\"># 保存输入值，用于反向传播</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.y = y                </span><br><span class=\"line\">        out = x * y  <span class=\"comment\"># 计算乘积</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">backward</span>(<span class=\"params\">self, dout</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        反向传播：计算梯度</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            dout: 上游传来的梯度</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            dx: 对x的梯度</span></span><br><span class=\"line\"><span class=\"string\">            dy: 对y的梯度</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        dx = dout * <span class=\"variable language_\">self</span>.y  <span class=\"comment\"># 对x的梯度 = 上游梯度 * y</span></span><br><span class=\"line\">        dy = dout * <span class=\"variable language_\">self</span>.x  <span class=\"comment\"># 对y的梯度 = 上游梯度 * x</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> dx, dy</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">AddLayer</span>:</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    加法层的实现</span></span><br><span class=\"line\"><span class=\"string\">    用于计算两个数的和，并支持反向传播</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 加法层不需要存储中间值，所以pass</span></span><br><span class=\"line\">        <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, y</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        前向传播：计算两个数的和</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x: 第一个输入值</span></span><br><span class=\"line\"><span class=\"string\">            y: 第二个输入值</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            out: x和y的和</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        out = x + y  <span class=\"comment\"># 计算和</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">backward</span>(<span class=\"params\">self, dout</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        反向传播：计算梯度</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            dout: 上游传来的梯度</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            dx: 对x的梯度</span></span><br><span class=\"line\"><span class=\"string\">            dy: 对y的梯度</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        dx = dout * <span class=\"number\">1</span>  <span class=\"comment\"># 对x的梯度 = 上游梯度 * 1</span></span><br><span class=\"line\">        dy = dout * <span class=\"number\">1</span>  <span class=\"comment\"># 对y的梯度 = 上游梯度 * 1</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> dx, dy</span><br><span class=\"line\"></span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\"># 购买2个苹果3个橘子的例子</span></span><br><span class=\"line\">apple = <span class=\"number\">100</span></span><br><span class=\"line\">apple_num = <span class=\"number\">2</span></span><br><span class=\"line\">orange = <span class=\"number\">150</span></span><br><span class=\"line\">orange_num = <span class=\"number\">3</span></span><br><span class=\"line\">tax = <span class=\"number\">1.1</span></span><br><span class=\"line\"></span><br><span class=\"line\">mul_apple_layer = MulLayer()</span><br><span class=\"line\">mul_orange_layer = MulLayer()</span><br><span class=\"line\">add_apple_orange_layer = AddLayer()</span><br><span class=\"line\">mul_tax_layer = MulLayer()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># forward</span></span><br><span class=\"line\">apple_price = mul_apple_layer.forward(apple, apple_num)</span><br><span class=\"line\">orange_price = mul_orange_layer.forward(orange, orange_num)</span><br><span class=\"line\">all_price = add_apple_orange_layer.forward(apple_price, orange_price)</span><br><span class=\"line\">price = mul_tax_layer.forward(all_price, tax)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># backward</span></span><br><span class=\"line\">d_price = <span class=\"number\">1</span></span><br><span class=\"line\">d_all_price, d_tax = mul_tax_layer.backward(d_price)</span><br><span class=\"line\">d_apple_peice, d_orange_price = add_apple_orange_layer.backward(d_all_price)</span><br><span class=\"line\">d_orange, d_orange_num = mul_orange_layer.backward(d_orange_price)</span><br><span class=\"line\">d_apple, d_apple_num = mul_apple_layer.backward(d_apple_peice)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(price)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(d_apple_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(d_apple)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(d_orange_num)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(d_orange)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(d_tax)</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"激活函数层的实现\"><a href=\"#激活函数层的实现\" class=\"headerlink\" title=\"激活函数层的实现\"></a>激活函数层的实现</h4><blockquote>\n<h4 id=\"ReLU层-1\"><a href=\"#ReLU层-1\" class=\"headerlink\" title=\"ReLU层\"></a>ReLU层</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ReLU</span>:</span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.mask = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">     <span class=\"comment\"># x &lt;= 0的为true，x&gt;0的为false</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.mask = (x &lt;= <span class=\"number\">0</span>)\t<span class=\"comment\"># 保存结果，用于反向传播</span></span><br><span class=\"line\">     <span class=\"comment\"># out是x的复制</span></span><br><span class=\"line\">     out = x.copy()</span><br><span class=\"line\">     <span class=\"comment\"># 这一步很关键，mask是一个bool数组，当它作为索引时，会选中True的位置，让后将其置为0，而False的位置不会被选中，就会保持原来的值</span></span><br><span class=\"line\">     <span class=\"comment\"># 这就实现了 x &lt;= 0置为0，x &gt; 0保持原值</span></span><br><span class=\"line\">     out[<span class=\"variable language_\">self</span>.mask] = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">backward</span>(<span class=\"params\">self, dout</span>):</span><br><span class=\"line\">     dout[<span class=\"variable language_\">self</span>.mask] = <span class=\"number\">0</span></span><br><span class=\"line\">     dx = dout</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"keyword\">return</span> dx</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">x = np.array([[<span class=\"number\">1.0</span>, -<span class=\"number\">0.5</span>], [-<span class=\"number\">2.0</span>, <span class=\"number\">3.0</span>]])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x)</span><br><span class=\"line\">mask = (x &lt;= <span class=\"number\">0</span> )</span><br><span class=\"line\"><span class=\"built_in\">print</span>(mask)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Sigmoid层-1\"><a href=\"#Sigmoid层-1\" class=\"headerlink\" title=\"Sigmoid层\"></a>Sigmoid层</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Sigmoid</span>:</span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.out = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">     out = <span class=\"number\">1</span> / (<span class=\"number\">1</span> + np.exp(-x))</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.out = out\t<span class=\"comment\"># 保存输出，用于反向传播</span></span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">backward</span>(<span class=\"params\">self, dout</span>):</span><br><span class=\"line\">     <span class=\"comment\"># 对应的就是 (\\partial L/ partial y) * y * (1 - y)</span></span><br><span class=\"line\">     dx = dout * (<span class=\"number\">1.0</span> - <span class=\"variable language_\">self</span>.out) * <span class=\"variable language_\">self</span>.out</span><br><span class=\"line\">     <span class=\"keyword\">return</span> dx</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Affine层\"><a href=\"#Affine层\" class=\"headerlink\" title=\"Affine层\"></a>Affine层</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Affine</span>:</span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, W, b</span>):</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.W = W</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.b = b</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.dW = <span class=\"literal\">None</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.db = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.x = x\t<span class=\"comment\"># 保存输入，供反向传播用</span></span><br><span class=\"line\">     out = np.dot(x, <span class=\"variable language_\">self</span>.W) + <span class=\"variable language_\">self</span>.b\t<span class=\"comment\"># b使用到了广播机制</span></span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">backward</span>(<span class=\"params\">self, dout</span>):</span><br><span class=\"line\">     dx = np.dot(dout, <span class=\"variable language_\">self</span>.W.T)</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.dW = np.dot(<span class=\"variable language_\">self</span>.x.T, dout)</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.db = np.<span class=\"built_in\">sum</span>(dout, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"keyword\">return</span> dx</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"需要说明的是，backward中，分别表示了-frac-partial-L-partial-X-frac-partial-L-partial-Y-W-T-，-frac-partial-L-partial-W-X-T-frac-partial-L-partial-Y-，-frac-partial-L-partial-B-frac-partial-L-partial-Y\"><a href=\"#需要说明的是，backward中，分别表示了-frac-partial-L-partial-X-frac-partial-L-partial-Y-W-T-，-frac-partial-L-partial-W-X-T-frac-partial-L-partial-Y-，-frac-partial-L-partial-B-frac-partial-L-partial-Y\" class=\"headerlink\" title=\"需要说明的是，backward中，分别表示了 $\\frac {\\partial L} {\\partial X} &#x3D; \\frac {\\partial L}{\\partial Y} W^T$ ，$\\frac {\\partial L} {\\partial W} &#x3D; X^T \\frac {\\partial L}{\\partial Y}$ ，$\\frac{\\partial L}{\\partial B} &#x3D; \\frac{\\partial L}{\\partial Y}$\"></a>需要说明的是，backward中，分别表示了 $\\frac {\\partial L} {\\partial X} &#x3D; \\frac {\\partial L}{\\partial Y} W^T$ ，$\\frac {\\partial L} {\\partial W} &#x3D; X^T \\frac {\\partial L}{\\partial Y}$ ，$\\frac{\\partial L}{\\partial B} &#x3D; \\frac{\\partial L}{\\partial Y}$</h4><h4 id=\"Softmax-with-Loss层的实现\"><a href=\"#Softmax-with-Loss层的实现\" class=\"headerlink\" title=\"Softmax-with-Loss层的实现\"></a>Softmax-with-Loss层的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SoftmaxWithLoss</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.loss = <span class=\"literal\">None</span> <span class=\"comment\"># 损失</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.y = <span class=\"literal\">None</span> <span class=\"comment\"># softmax的输出</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.t = <span class=\"literal\">None</span> <span class=\"comment\"># 监督数据(one-hot vector)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.t = t</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.y = softmax(x)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.loss = cross_entropy_error(<span class=\"variable language_\">self</span>.y, <span class=\"variable language_\">self</span>.t)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.loss</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">backward</span>(<span class=\"params\">self, dout=<span class=\"number\">1</span></span>):</span><br><span class=\"line\">        batch_size = <span class=\"variable language_\">self</span>.t.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">        dx = (<span class=\"variable language_\">self</span>.y - <span class=\"variable language_\">self</span>.t) / batch_size</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> dx</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h4 id=\"误差反向传播的实现\"><a href=\"#误差反向传播的实现\" class=\"headerlink\" title=\"误差反向传播的实现\"></a>误差反向传播的实现</h4><blockquote>\n<h4 id=\"误差反向传播的神经网络的实现\"><a href=\"#误差反向传播的神经网络的实现\" class=\"headerlink\" title=\"误差反向传播的神经网络的实现\"></a>误差反向传播的神经网络的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os, sys</span><br><span class=\"line\">sys.path.append(<span class=\"string\">&#x27;../../py_pro/DL/&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.layers <span class=\"keyword\">import</span> *</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.gradient <span class=\"keyword\">import</span> numerical_gradient</span><br><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> OrderedDict</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist</span><br><span class=\"line\"><span class=\"keyword\">from</span> ch04.two_layer_net <span class=\"keyword\">import</span> TwoLayerNet</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">TwoLayerNet</span>:</span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, input_size, hidden_size, output_size, weight_init_std=<span class=\"number\">0.01</span></span>):</span><br><span class=\"line\">     <span class=\"comment\"># 初始化权重</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.params = &#123;&#125;</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W1&#x27;</span>] = weight_init_std * np.random.randn(input_size, hidden_size)</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b1&#x27;</span>] = np.zeros(hidden_size)</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W2&#x27;</span>] = weight_init_std * np.random.randn(hidden_size, output_size)</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b2&#x27;</span>] = np.zeros(output_size)</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"comment\"># 生成层</span></span><br><span class=\"line\">     <span class=\"comment\"># OrderedDict是有序字典，会记录往字典中添加元素的顺序</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.layers = OrderedDict()</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine1&#x27;</span>] = Affine(<span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W1&#x27;</span>], <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b1&#x27;</span>])</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;ReLU1&#x27;</span>] = Relu()</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;Affine2&#x27;</span>] = Affine(<span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W2&#x27;</span>], <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b2&#x27;</span>])</span><br><span class=\"line\">     </span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.lastLayer = SoftmaxWithLoss()</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">     <span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>.layers.values():</span><br><span class=\"line\">         x = layer.forward(x)</span><br><span class=\"line\">     <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">loss</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">     y = <span class=\"variable language_\">self</span>.predict(x)</span><br><span class=\"line\">     y = np.argmax(y, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">     <span class=\"keyword\">if</span> t.ndim != <span class=\"number\">1</span> : t = np.argmax(t, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">     accuracy = np.<span class=\"built_in\">sum</span>(y == t) / <span class=\"built_in\">float</span>(x.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">     <span class=\"keyword\">return</span> accuracy</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">numerical_gradient</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">     loss_W = <span class=\"keyword\">lambda</span> W : <span class=\"variable language_\">self</span>.loss(x, t)</span><br><span class=\"line\">     grads = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">     grads[<span class=\"string\">&#x27;W1&#x27;</span>] = numerical_gradient(loss_W, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W1&#x27;</span>])</span><br><span class=\"line\">     grads[<span class=\"string\">&#x27;b1&#x27;</span>] = numerical_gradient(loss_W, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b1&#x27;</span>])</span><br><span class=\"line\">     grads[<span class=\"string\">&#x27;W2&#x27;</span>] = numerical_gradient(loss_W, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W2&#x27;</span>])</span><br><span class=\"line\">     grads[<span class=\"string\">&#x27;b2&#x27;</span>] = numerical_gradient(loss_W, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b2&#x27;</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"keyword\">return</span> grads</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">gradient</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">     <span class=\"comment\"># forward</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.loss(x, t)</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"comment\"># backward</span></span><br><span class=\"line\">     dout = <span class=\"number\">1</span></span><br><span class=\"line\">     dout = <span class=\"variable language_\">self</span>.lastLayer.backward(dout)</span><br><span class=\"line\"></span><br><span class=\"line\">     layers = <span class=\"built_in\">list</span>(<span class=\"variable language_\">self</span>.layers.values())</span><br><span class=\"line\">     layers.reverse()</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> layers:</span><br><span class=\"line\">         dout = layer.backward(dout)</span><br><span class=\"line\"></span><br><span class=\"line\">     grads = &#123;&#125;</span><br><span class=\"line\">     grads[<span class=\"string\">&#x27;W1&#x27;</span>] = <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine1&#x27;</span>].dW</span><br><span class=\"line\">     grads[<span class=\"string\">&#x27;b1&#x27;</span>] = <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine1&#x27;</span>].db</span><br><span class=\"line\">     grads[<span class=\"string\">&#x27;W2&#x27;</span>] = <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine2&#x27;</span>].dW</span><br><span class=\"line\">     grads[<span class=\"string\">&#x27;b2&#x27;</span>] = <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine2&#x27;</span>].db</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"keyword\">return</span> grads</span><br><span class=\"line\">     </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 读入数据</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>, one_hot_label= <span class=\"literal\">True</span>)</span><br><span class=\"line\">network = TwoLayerNet(input_size=<span class=\"number\">784</span>, hidden_size=<span class=\"number\">50</span>, output_size=<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">x_batch = x_train[:<span class=\"number\">3</span>]</span><br><span class=\"line\">t_batch = t_train[:<span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">grad_numerical = network.numerical_gradient(x_batch, t_batch)</span><br><span class=\"line\">grad_backprop = network.gradient(x_batch, t_batch)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 求各个权重的绝对误差的平均值</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> grad_numerical.keys():</span><br><span class=\"line\"> diff = np.average( np.<span class=\"built_in\">abs</span>(grad_backprop[key] - grad_numerical[key]) )</span><br><span class=\"line\"> <span class=\"built_in\">print</span>(key + <span class=\"string\">&quot;: &quot;</span> + <span class=\"built_in\">str</span>(diff))</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"使用误差反向传播的学习\"><a href=\"#使用误差反向传播的学习\" class=\"headerlink\" title=\"使用误差反向传播的学习\"></a>使用误差反向传播的学习</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys, os</span><br><span class=\"line\">sys.path.append(os.pardir)</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist</span><br><span class=\"line\"><span class=\"keyword\">from</span> ch05.two_layer_net <span class=\"keyword\">import</span> TwoLayerNet</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 读入数据</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>, one_hot_label=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">network = TwoLayerNet(input_size=<span class=\"number\">784</span>, hidden_size=<span class=\"number\">50</span>, output_size=<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">iters_num = <span class=\"number\">10000</span></span><br><span class=\"line\">train_size = x_train.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">batch_size = <span class=\"number\">100</span></span><br><span class=\"line\">learning_rate = <span class=\"number\">0.1</span></span><br><span class=\"line\">train_loss_list = []</span><br><span class=\"line\">train_acc_list = []</span><br><span class=\"line\">test_acc_list = []</span><br><span class=\"line\"></span><br><span class=\"line\">iter_per_epoch = <span class=\"built_in\">max</span>(train_size / batch_size, <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(iters_num):</span><br><span class=\"line\">    batch_mask = np.random.choice(train_size, batch_size)</span><br><span class=\"line\">    x_batch = x_train[batch_mask]</span><br><span class=\"line\">    t_batch = t_train[batch_mask]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 通过误差反向传播法求梯度</span></span><br><span class=\"line\">    grad = network.gradient(x_batch, t_batch)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 更新</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> (<span class=\"string\">&#x27;W1&#x27;</span>, <span class=\"string\">&#x27;b1&#x27;</span>, <span class=\"string\">&#x27;W2&#x27;</span>, <span class=\"string\">&#x27;b2&#x27;</span>):</span><br><span class=\"line\">        network.params[key] -= learning_rate * grad[key]</span><br><span class=\"line\"></span><br><span class=\"line\">    loss = network.loss(x_batch, t_batch)</span><br><span class=\"line\">    train_loss_list.append(loss)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> i % iter_per_epoch == <span class=\"number\">0</span>:</span><br><span class=\"line\">        train_acc = network.accuracy(x_train, t_train)</span><br><span class=\"line\">        test_acc = network.accuracy(x_test, t_test)</span><br><span class=\"line\">        train_acc_list.append(train_acc)</span><br><span class=\"line\">        test_acc_list.append(test_acc)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(train_acc, test_acc)</span><br></pre></td></tr></table></figure>\n\n\n</blockquote>\n</blockquote>\n<h3 id=\"裂开，还得再回头多看一哈这个笔记，已经变石头人了\"><a href=\"#裂开，还得再回头多看一哈这个笔记，已经变石头人了\" class=\"headerlink\" title=\"裂开，还得再回头多看一哈这个笔记，已经变石头人了\"></a>裂开，还得再回头多看一哈这个笔记，已经变石头人了</h3>"},{"title":"DL之路---啃鱼书（3）","data":"2025-06-15T12:27:00.000Z","updated":"2025-06-15T12:27:00.000Z","type":"DL","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p22.jpg","_content":"\n\n# 神经网络的学习\n\n\n\n### 从数据中学习\n\n> #### 所谓的学习就是从训练数据中自动获取最优权重参数的过程\n>\n> #### 为了使神经网络能够学习，引入了损失函数这一指标，而学习的目标就是以损失函数为基准，找出使它的值达到最小的权重参数（函数斜率的梯度法）\n>\n> \n>\n> #### 特征量：是指可以从输入数据（输入图像）中准确地提取出本质数据（重要数据）的转换器（图像的特征量通常表示为向量形式），但特征量的选取是人为选择的，还是存在人为因素的介入\n>\n> \n>\n> #### 深度学习也被称为端到端机器学习（end-to-end machine learning），从原始数据（输入）中获得目标结果（输出）\n>\n> #### 神经网络的优点是对所有的问题都可以使用同样的流程来解决，与待处理的问题无关，它可以将数据直接作为原始数据，进行端到端学习\n>\n> \n>\n> #### 在机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验，首先使用训练数据进行学习，寻找最优参数，再使用测试数据评价训练得到的模型的实际能力\n>\n> #### 划分为训练数据和测试数据（也称为监督数据）是为了追求“泛化能力”\n>\n> #### 泛化能力：是指处理未被观察过的数据（不包含在训练数据中的数据）的能力\n>\n> #### 获得泛化能力是机器学习的最终目标\n>\n> #### 过拟合：它是指只对某个数据集过度拟合的状态\n\n\n\n### 损失函数\n\n> #### 神经网络的学习通过某个指标表示现在的状态，再以这个指标为基准，寻找最优权重参数，而这个指标就是损失函数\n>\n> #### 损失函数可以使用任何函数，但一般使用的是均方误差和交叉熵误差\n>\n> #### 损失函数是表示神经网络性能好坏的指标，它反映了当前的神经网络对监督数据的拟合程度\n>\n> \n>\n> #### 均方误差\n>\n> > #### 其表达式如下：\n> >\n> > $$\n> > E = \\frac{1}{2} \\sum_{k} (y_k - t_k)^2\n> > $$\n> >\n> > #### 其中，$y_k$ 是表示神经网络的输出，$t_k$ 表示监督数据（测试数据），$k$ 表示数据的维数\n> >\n> > #### 题外话：均方误差式子的一个特点就是，其名中的4个字与式子中皆有对应，均 对应式子中的 $\\frac{1}{2}$ ，方 对应式子中的 $()^2$ ，误差 对应式子中的 $ y_k - t_k $ \n> >\n> > \n>\n> #### 交叉熵误差\n>\n> > #### 其表达式如下：\n> >\n> > $$\n> > E = - \\sum_{k} t_k \\log y_k\n> > $$\n> >\n> > #### 其中，$y_k$ 是表示神经网络的输出，$t_k$ 是正确解标签，且 $t_k$ 中只有正确解标签的索引为1，其余的都是0(one-hot表示)\n> >\n> > #### 所以说，实际上交叉熵误差式子只计算对应正确解标签的输出的自然对数，交叉熵误差的值是由正确解标签所对应的输出结果决定的\n> >\n> > #### 正确解标签对应的输出越大，交叉熵误差的值就越接近0，反之则越大，输出为1时，交叉熵误差为0，log函数图像如下\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/3/1.png)\n>\n> \n>\n> #### mini-batch学习\n>\n> > #### 机器学习使用训练数据进行学习，其本质就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数，所以计算损失函数时必须将所有的训练数据作为对象\n> >\n> > \n> >\n> > #### 前面提到的损失函数都是针对的单个数据，若要求所有的训练数据的损失函数的总和，以交叉熵误差为例，损失函数可以写为：\n> >\n> > $$\n> > E = - \\frac{1}{N} \\sum_{n} \\sum_{k} t_{nk} \\log y_{nk}\n> > $$\n> >\n> > #### 这里是假设有$N$个数据，$t_{nk}$ 表示第 $n$ 个数据的第 $k$ 个元素的值，$t_{nk}$ 是监督数据，$t_{nk}$ 是神经网络的输出\n> >\n> > #### 通过除以 $N$ （正规化），可以求单个数据的平均损失函数，这样的平均化，可以获得和训练数据的数量无关的统一指标\n> >\n> > \n> >\n> > #### 如果将全部的训练数据作为对象求损失函数的总和，那这个计算过程会是非常漫长的，训练数据量非常庞大时，以全部训练数据作为对象计算损失函数更是不现实的\n> >\n> > #### 因此，从全部训练数据中选择一部分，作为全部训练数据的“近似”\n> >\n> > #### 神经网络的学习也是从训练数据中选出一批数据（称为mini-batch,小批量），然后对每个mini-batch进行学习，用这一批数据的结果来近似全部数据的结果，这种学习方式就是所谓的 **mini-batch学习**\n> >\n> > \n>\n> \n>\n> #### 为什么要引入损失函数\n>\n> > #### 为了找到使损失函数的值尽可能小的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值\n> >\n> > #### 损失函数的导数一般不会有0值，更不会恒为0（阶跃函数除外），而识别精度的导数恒为0或者只有极小部分不为0，这样权重的调整并不会影响其值，这并不是我们想要的\n\n\n\n### 数值微分（数值梯度）\n\n> #### 梯度法使用梯度的信息决定前进的方向\n>\n> #### 利用微小的差分求导数的过程就是所谓的数值微分（numerical differentiation）\n>\n> #### 基于数学式的推到求导数的过程，则称之为 解析性求解或解析性求导\n>\n\n\n\n### 梯度\n\n> #### 由全部变量的偏导数汇总成的向量称为梯度\n>\n> \n>\n> #### 梯度法\n>\n> > #### 寻找的最优参数是指损失函数取最小值时的参数\n> >\n> > #### 使用梯度来寻找损失函数最小值的方法就是所谓的 梯度法，通过不断地沿梯度方向前进，逐渐减小函数值的过程就是梯度法\n> >\n> > #### 梯度表示的各点处的函数值减小最多的方向\n> >\n> > #### 函数最小值、极小值和鞍点处的梯度为 0（梯度法就是要找梯度为0的地方）\n> >\n> > #### 极小值是局部最小值，鞍点是从某个方向上看是极大值，从另一个方向看是极小值的点\n> >\n> > #### 学习高原：它是指，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入所谓的**学习高原**而无法前进的停滞期\n> >\n> > #### 根据寻找目的（最大值或最小值）的不同，梯度法分为了：梯度下降法和梯度上升法\n> >\n> > - #### 梯度下降法：寻找最小值的梯度法\n> >\n> > - #### 梯度上升法：寻找最大值的梯度法\n> >\n> > #### 通过反转损失函数的符号，求最小值和求最大值的问题可以变成相同的问题\n> >\n> > #### 神经网络（深度学习）中，梯度法一般是指的梯度下降法\n> >\n> > #### 学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数\n> >\n> > #### 在神经网络的学习中，一般会一边改变学习率的值，一边确认学习是否正确进行了\n> >\n> > #### 学习率这样的参数也被称为**超参数**，需要人工设定\n>\n> \n>\n> #### 神经网络的梯度\n>\n> > #### 这里的梯度指的是损失函数关于权重参数的梯度\n> >\n> > #### 例子说明，一个神经网络的权重W形状为 2x3，损失函数用L表示，此时梯度可以用 $\\frac{\\partial L}{\\partial W}$ 来表示，数学表示式如下\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/3/2.png)\n> >\n> > #### 求出神经网络的梯度后，接下来只需根据梯度法，更新权重参数即可\n>\n> \n>\n> #### 学习算法的实现\n>\n> > #### 神经网络的学习的步骤\n> >\n> > > #### 前提是：神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”\n> > >\n> > > 1. #### （mini-batch）从训练数据中随机选出一部分数据，这部分数据称为mini-batch，目标是是减小mini-batch的损失函数的值\n> > >\n> > > 2. #### （计算梯度）为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度，梯度表示损失函数的值减小最多的方向\n> > >\n> > > 3. #### （更新参数）将权重参数沿梯度方向进行微小更新\n> > >\n> > > 4. #### （重复）重复前三步\n> > >\n> > > #### 因为更新权重参数采用的梯度法是梯度下降法，又是随机选择的mini-batch数据，所以称之为随机梯度下降法（stochastic gradient descent，SGD）\n> >\n> > \n> >\n> > #### 手写数字识别的神经网络的实现\n> >\n> > #### 基于测试数据的评价\n> >\n> > > #### 神经网络的学习中，必须确认是否能够正确识别训练数据以外的其他数 据，即确认是否会发生过拟合\n> > >\n> > > #### 过拟合是指，虽然训练数据中的数字图像能被正确辨别，但是不在训练数据中的数字图像却无法被识别的现象\n> > >\n> > > #### 要评价神经网络的泛化能力，就必须使用不包含在训练数据中的数据，这就要用到测试数据了\n> > >\n> > > #### 另外又引入了一个新的概念——epoch\n> > >\n> > > > #### epoch是一个单位，一个epoch表示学习中所有训练数据均被使用过一次时的更新次数\n> > > >\n> > > > #### 如，对于10000笔训练数据，用大小为 100 笔数据的mini-batch进行学习时，重复随机梯度下降法 100次，所有的训练数据就都被“看过”了，此时，100次就是一个 epoch\n> > > >\n> > > > #### 遍历一次所有数据，就成为一个epoch\n\n### code\n\n> #### 均方误差的实现\n>\n> ```python\n> import numpy as np\n> \n> def mean_squared_error(y, t):\n> return 0.5 * np.sum((y - t)**2)\n> \n> \n> # 设2为正确解\n> t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n> y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n> result = mean_squared_error(y, t)\n> print(result)\n> \n> y = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])\n> result_ = mean_squared_error(y, t)\n> print(result_)\n> ```\n>\n> \n>\n> #### 交叉熵误差的实现\n>\n> ```python\n> def cross_entropy_error(y, t):\n> # 加上delta是为了避免出现 log(0)，导致出现负无穷\n> delta = 1e-7\n> return -np.sum(t * np.log(y + delta))\n> \n> \n> t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n> y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n> result = cross_entropy_error(y, t)\n> print(result)\n> \n> y = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])\n> result_ = cross_entropy_error(y, t)\n> print(result_)\n> ```\n>\n> \n>\n> #### 使用mini-batch学习的手写数字识别\n>\n> ```python\n> import os, sys\n> sys.path.append(os.pardir)\t# 将父目录添加到系统路径，以便导入自定义模块\n> import numpy as np\n> import pickle\n> from dataset.mnist import load_mnist\n> from common.function import sigmoid, softmax\n> \n> \n> # 加载MNIST数据集\n> def get_data():\n> # normalize=True:将图像的像素值正规化到0.0~1.0\n> # flatten=True:将图像从2维数组转为1维数组\n> # one_hot_label=False:标签为0~9的数字,而不是one-hot编码\n> (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)\n> \n> return x_test, t_test\n> \n> \n> # 初始化神经网络\n> def init_network():\n> # 从文件加载预训练的网络参数（权重和偏置）\n> with open(\"sample_weight.pkl\", rb)as f:\n> network = pickle.load(f)\n> \treturn network\n> \n> \n> # 神经网络的预测函数\n> def predict(network, x):\n> # x 是输入数据\n> # 获取网络的权重和偏置\n> w1, w2, w3 = network['W1'], network['W2'], network['W3']\n> b1, b2, b3 = network['b1'], network['b2'], network['b3']\n> \n> # 向前传播\n> a1 = np.dot(x, w1) + b1\n> z1 = sigmoid(a1)\n> a2 = np.dot(z1, w2) + b2\n> z2 = sigmoid(a2)\n> a3 = np.dot(z2, w3) + b3\n> y = softmax(a3)\n> \n> return y\n> \n> \n> x, t = get_data()\n> network = init_network()\n> \n> batch_size = 100\t# 设置批处理大小\n> accuracy_cnt = 0\t# 初始化准确率计数器\n> \n> # 使用批处理进行预测\n> for i in range(0, len(x), batch_size):\n> x_batch = x[i:i+batch_size]\t# 获取当前批次输入的数据\n> y_batch = predict(network, x_batch)\t# 对当前批次进行预测\n> p = np.argmax(y_batch, axis=1)\t# 获取每个样本预测概率最大的类别\n> accuracy_cnt += np.sum(p == t[i:i+batch_size])\t# 统计预测正确的样本数\n> \n> # 输出最终的识别准确率    \n> print(\"Accuracy: \" + str(float(accuracy_cnt) / len(x)))\n> ```\n>\n> \n>\n> #### 数值微分\n>\n> > #### 导数的实现\n> >\n> > ```python\n> > def numerical_diff(f, x):\n> >  h = 1e-4\n> >  return (f(h + x) - f(x)) / h\n> > ```\n> >\n> > #### 上面这样计算函数f的差分是不准确的，它计算的是(x+h)和(x)之间的斜率，它与实际要计算的导数是有差异的，这一差异是因h不可能无限接近0导致的，这种属于是前向差分\n> > #### 正确的应该是 (x+h)和(x-h)之间的差分，以x为中心，这也称之为中心差分\n> >\n> > ```python\n> > def numerical_diff(f, x):\n> >  h = 1e-4\n> >  return (f(x+h) - f(x-h)) / (2 * h)\n> > ```\n> >\n> > #### 举个数值微分实例，y = 0.01X^2 + 0.1x\n> >\n> > ```python\n> > def fun_1(x):\n> >  return 0.01*x**2 + 0.1 * x\n> > ```\n> >\n> > #### 绘制其图像\n> >\n> > ```python\n> > import numpy as np\n> > import matplotlib.pylab as plt\n> > \n> > x = np.arange(0.0, 20.0, 0.1)\n> > y = fun_1(x)\n> > plt.xlabel(\"x\")\n> > plt.ylabel(\"f(x)\")\n> > plt.plot(x, y)\n> > plt.show()\n> > ```\n> >\n> > #### 计算x=5和x=10处的导数，并画出其图像（含切线）\n> >\n> > ```python\n> > a = numerical_diff(fun_1, 5)\n> > b = numerical_diff(fun_1, 10)\n> > print(a)\n> > print(b)\n> > \n> > def tangent_line(f, x):\n> >  d = numerical_diff(f, x)\n> >  print(d)\n> >  y = f(x) - d*x\n> >  return lambda t: d*t + y\n> > \n> > x = np.arange(0.0, 20.0, 0.1)\n> > y = fun_1(x)\n> > plt.xlabel(\"x\")\n> > plt.ylabel(\"f(x)\")\n> > \n> > # 设置切点坐标\n> > tangent_x = 5\n> > tangent_y = fun_1(tangent_x)\n> > \n> > # 绘制原函数和切线图像\n> > tf = tangent_line(fun_1, tangent_x)\n> > y2 = tf(x)\n> > plt.plot(x, y)\n> > plt.plot(x, y2)\n> > \n> > # 绘制切点处辅助线\n> > # 垂直x轴的\n> > plt.axvline(x=tangent_x, color='gray', linestyle='--', alpha=0.5)\n> > # 垂直y轴的\n> > plt.axhline(y=tangent_y, color='gray', linestyle='--', alpha=0.5)\n> > # 标记切点(红色圆点)\n> > plt.plot(tangent_x, tangent_y, 'ro')\n> > # 添加坐标标注\n> > plt.text(tangent_x, tangent_y, f'({tangent_x:.1f}, {tangent_y:.2f})', \n> >       horizontalalignment='right', verticalalignment='bottom')\n> > plt.show()\n> > \n> > # 设置切点坐标\n> > tangent_x = 10\n> > tangent_y = fun_1(tangent_x)\n> > \n> > tf = tangent_line(fun_1, tangent_x)\n> > y3 = tf(x)\n> > plt.plot(x, y)\n> > plt.plot(x, y3)\n> > \n> > # 绘制切点处辅助线\n> > # 垂直x轴的\n> > plt.axvline(x=tangent_x, color='gray', linestyle='--', alpha=0.5)\n> > # 垂直y轴的\n> > plt.axhline(y=tangent_y, color='gray', linestyle='--', alpha=0.5)\n> > # 标记切点(红色圆点)\n> > plt.plot(tangent_x, tangent_y, 'ro')\n> > # 添加坐标标注\n> > plt.text(tangent_x, tangent_y, f'({tangent_x:.1f}, {tangent_y:.2f})', \n> >       horizontalalignment='right', verticalalignment='bottom')\n> > plt.show()\n> > ```\n> >\n> > \n> >\n> > #### 偏导数\n> >\n> > > #### 求 $f(x_0,x_1)=x_0^2+x_1^2$的偏导，并画出其函数图像\n> > >\n> > > ```python\n> > > import numpy as np\n> > > import matplotlib.pyplot as plt\n> > > \n> > > from mpl_toolkits.mplot3d import Axes3D\n> > > \n> > > # 定义网格\n> > > x0 = np.linspace(-3, 3, 100)\n> > > x1 = np.linspace(-3, 3, 100)\n> > > X0, X1 = np.meshgrid(x0, x1)\n> > > \n> > > # 定义函数\n> > > Z = X0**2 + X1**2\n> > > \n> > > # 创建图形窗口\n> > > fig = plt.figure(figsize=(14, 6))\n> > > \n> > > # 1. 三维图像\n> > > ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n> > > ax1.plot_surface(X0, X1, Z, cmap='viridis', edgecolor='none')\n> > > ax1.set_title(r'$f(x_0, x_1) = x_0^2 + x_1^2$')\n> > > ax1.set_xlabel('$x_0$')\n> > > ax1.set_ylabel('$x_1$')\n> > > ax1.set_zlabel('$f(x_0, x_1)$')\n> > > \n> > > # 2. 等高线图 + 梯度向量\n> > > ax2 = fig.add_subplot(1, 2, 2)\n> > > contour = ax2.contourf(X0, X1, Z, levels=30, cmap='viridis')\n> > > ax2.set_title('Contour and Gradient')\n> > > ax2.set_xlabel('$x_0$')\n> > > ax2.set_ylabel('$x_1$')\n> > > \n> > > # 计算梯度（偏导）\n> > > grad_x0 = 2 * X0\n> > > grad_x1 = 2 * X1\n> > > \n> > > # 绘制梯度向量（用 quiver 画箭头）\n> > > skip = 5  # 降低密度\n> > > ax2.quiver(X0[::skip, ::skip], X1[::skip, ::skip],\n> > >            grad_x0[::skip, ::skip], grad_x1[::skip, ::skip],\n> > >            color='white', alpha=0.8)\n> > > \n> > > plt.colorbar(contour, ax=ax2, label='Function Value')\n> > > plt.tight_layout()\n> > > plt.show()\n> > > ```\n>\n> \n>\n> #### 梯度\n>\n> > #### 简单的梯度的实现\n> >\n> > ```python\n> > # 梯度的实现\n> > def numerical_gradient(f, x):\n> >     h = 1e-4\n> >     # 生成和x形状相同的数组，且所有元素为0\n> >     grad = np.zeros_like(x)\n> > \n> >     for idx in range(x.size):\n> >         tmp_val = x[idx]\n> >         # f(x+h)的计算\n> >         x[idx] = tmp_val + h\n> >         fxh1 = f(x)\n> > \n> >         # f(x-h)的计算\n> >         x[idx] = tmp_val - h\n> >         fxh2 = f(x)\n> > \n> >         grad[idx] = (fxh1 - fxh2) / (2 * h)\n> >         x[idx] = tmp_val # 还原值\n> >     \n> >     return grad\n> > \n> > \n> > print(numerical_gradient(fun_2, np.array([3.0, 4.0])))\n> > print(numerical_gradient(fun_2, np.array([0.0, 2.0])))\n> > print(numerical_gradient(fun_2, np.array([3.0, 0.0])))\n> > ```\n> >\n> > #### 这里给出一个计算任意维数组的数值梯度\n> >\n> > ```python\n> > def numerical_gradient(f, x):\n> >     \"\"\"\n> >     计算任意维数组的数值梯度\n> >     参数:\n> >         f: 目标函数\n> >         x: 任意维度的数组\n> >     返回:\n> >         梯度数组，形状与x相同\n> >     \n> >     实现原理：\n> >     1. 使用中心差分法计算梯度：(f(x+h) - f(x-h)) / (2h)\n> >     2. 对数组中的每个元素分别计算梯度\n> >     3. 使用numpy的迭代器处理任意维度的数组\n> >     \"\"\"\n> >     h = 1e-4  # 0.0001，微小的变化量\n> >     grad = np.zeros_like(x)  # 创建与x形状相同的零数组，用于存储梯度\n> >     \n> >     # 使用numpy的迭代器遍历数组的所有元素\n> >     # flags=['multi_index']: 获取多维索引\n> >     # op_flags=['readwrite']: 允许读写操作\n> >     it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n> >     while not it.finished:\n> >         idx = it.multi_index  # 获取当前元素的多维索引\n> >         tmp_val = x[idx]  # 保存当前值\n> >         \n> >         # 计算f(x+h)\n> >         x[idx] = tmp_val + h\n> >         fxh1 = f(x)\n> >         \n> >         # 计算f(x-h)\n> >         x[idx] = tmp_val - h \n> >         fxh2 = f(x)\n> >         \n> >         # 使用中心差分法计算梯度\n> >         grad[idx] = (fxh1 - fxh2) / (2*h)\n> >         \n> >         x[idx] = tmp_val  # 恢复原始值\n> >         it.iternext()  # 移动到下一个元素\n> >         \n> >     return grad\n> > ```\n> >\n> > \n> >\n> > #### 梯度下降法的实现\n> >\n> > ```python\n> > # f 是要进行最优化的函数，init_x是初始值，lr是学习率，step_num是梯度法的重复次数\n> > def gradient_descent(f, init_x, lr=0.01, step_num=100):\n> >     x = init_x\n> > \n> >     for i in range(step_num):\n> >         grad = numerical_gradient(f, x)\n> >         x -= lr * grad\n> > \n> >     return x\n> > \n> > \n> > init_x = np.array([-3.0, 4.0])\n> > result_ = gradient_descent(fun_2, init_x=init_x, lr=0.1, step_num=100)\n> > print(result_)\n> > ```\n> >\n> > #### 图像展示梯度法的过程\n> >\n> > ```python\n> > def gradient_descent_(f, init_x, lr=0.01, step_num=100):\n> >     x = init_x\n> >     x_history = []\n> > \n> >     for i in range(step_num):\n> >         x_history.append(x.copy())\n> >         \n> >         grad = numerical_gradient(f, x)\n> >         x -= lr * grad\n> > \n> >     return x, np.array(x_history)\n> > \n> > \n> > init_x = np.array([-3.0, 4.0])\n> > \n> > lr = 0.1\n> > step_num = 20\n> > x, x_history = gradient_descent_(fun_2, init_x, lr, step_num)\n> > \n> > plt.plot([-5, 5], [0, 0], '--b')\n> > plt.plot([0, 0], [-5, 5], '--b')\n> > plt.plot(x_history[:, 0], x_history[:, 1], 'o')\n> > \n> > plt.xlim(-3.5, 3.5)\n> > plt.ylim(-4.5, 4.5)\n> > plt.xlabel(\"X0\")\n> > plt.ylabel(\"X1\")\n> > plt.show()\n> > ```\n>\n> \n>\n> #### 神经网络的梯度\n>\n> > #### 简单的神经网络\n> >\n> > ```python\n> > # coding: utf-8\n> > # 导入必要的库\n> > import sys, os\n> > # 将父目录添加到系统路径中，以便能够导入common模块\n> > sys.path.append(os.pardir)\n> > from common.functions import softmax, cross_entropy_error\n> > from common.gradient import numerical_gradient\n> > \n> > \n> > class simpleNet:\n> >     \"\"\"\n> >     一个简单的神经网络类\n> >     实现了一个2输入3输出的单层神经网络\n> >     \"\"\"\n> >     def __init__(self):\n> >         # 初始化权重矩阵，使用随机数生成2x3的权重矩阵\n> >         self.W = np.random.randn(2,3)\n> > \n> >     def predict(self, x):\n> >         \"\"\"\n> >         前向传播函数\n> >         参数:\n> >             x: 输入数据\n> >         返回:\n> >             神经网络的输出\n> >         \"\"\"\n> >         return np.dot(x, self.W)\n> > \n> >     def loss(self, x, t):\n> >         \"\"\"\n> >         计算损失函数\n> >         参数:\n> >             x: 输入数据\n> >             t: 正确解标签\n> >         返回:\n> >             损失函数的值\n> >         \"\"\"\n> >         z = self.predict(x)  # 获取神经网络的输出\n> >         y = softmax(z)       # 使用softmax函数将输出转换为概率\n> >         loss = cross_entropy_error(y, t)  # 计算交叉熵误差\n> > \n> >         return loss\n> > \n> > # 创建测试数据\n> > x = np.array([0.6, 0.9])  # 输入数据\n> > t = np.array([0, 0, 1])   # 正确解标签\n> > \n> > # 创建神经网络实例\n> > net = simpleNet()\n> > \n> > # 定义损失函数，用于计算梯度\n> > f = lambda w: net.loss(x, t)\n> > # 使用数值微分计算梯度\n> > dW = numerical_gradient(f, net.W)\n> > \n> > # 打印梯度结果\n> > print(dW)\n> > ```\n> >\n> > #### 这里需要注意的是，为什么要在类外定义一个损失函数f，因为类中的损失函数loss()是一个实例方法，需要访问实例的权重self.W，而numerical_gradient()函数参数中的f，需要的是一个纯函数，而不是一个类中的实例方法\n>\n> \n>\n> #### 手写数字识别的神经网络的实现\n>\n> ```python\n> # coding: utf-8\n> import sys, os\n> sys.path.append(os.pardir)  # 将父目录添加到系统路径中，以便导入common模块\n> from common.functions import *\n> from common.gradient import numerical_gradient\n> import numpy as np\n> \n> \n> class TwoLayerNet:\n>     \"\"\"\n>     两层神经网络类\n>     实现了一个具有一个隐藏层的神经网络\n>     结构：输入层 -> 隐藏层 -> 输出层\n>     \"\"\"\n> \n>     def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n>         \"\"\"\n>         初始化神经网络\n>         参数:\n>             input_size: 输入层神经元数量\n>             hidden_size: 隐藏层神经元数量\n>             output_size: 输出层神经元数量\n>             weight_init_std: 权重初始化的标准差\n>         \"\"\"\n>         # 初始化权重和偏置\n>         self.params = {}\n>         # 第一层权重矩阵，形状为(input_size, hidden_size)\n>         self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n>         # 第一层偏置，形状为(hidden_size,)\n>         self.params['b1'] = np.zeros(hidden_size)\n>         # 第二层权重矩阵，形状为(hidden_size, output_size)\n>         self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n>         # 第二层偏置，形状为(output_size,)\n>         self.params['b2'] = np.zeros(output_size)\n> \n>     def predict(self, x):\n>         \"\"\"\n>         前向传播，计算神经网络的输出\n>         参数:\n>             x: 输入数据\n>         返回:\n>             神经网络的输出（经过softmax后的概率分布）\n>         \"\"\"\n>         W1, W2 = self.params['W1'], self.params['W2']\n>         b1, b2 = self.params['b1'], self.params['b2']\n>     \n>         # 第一层计算\n>         a1 = np.dot(x, W1) + b1  # 线性变换\n>         z1 = sigmoid(a1)         # 激活函数\n>         \n>         # 第二层计算\n>         a2 = np.dot(z1, W2) + b2  # 线性变换\n>         y = softmax(a2)           # 输出层激活函数\n>         \n>         return y\n>         \n>     def loss(self, x, t):\n>         \"\"\"\n>         计算损失函数\n>         参数:\n>             x: 输入数据\n>             t: 正确解标签\n>         返回:\n>             损失函数的值\n>         \"\"\"\n>         y = self.predict(x)\n>         return cross_entropy_error(y, t)\n>     \n>     def accuracy(self, x, t):\n>         \"\"\"\n>         计算识别精度\n>         参数:\n>             x: 输入数据\n>             t: 正确解标签\n>         返回:\n>             识别精度（0-1之间的值）\n>         \"\"\"\n>         y = self.predict(x)\n>         y = np.argmax(y, axis=1)  # 获取预测结果\n>         t = np.argmax(t, axis=1)  # 获取正确解\n>         \n>         # 计算正确率\n>         accuracy = np.sum(y == t) / float(x.shape[0])\n>         return accuracy\n>         \n>     def numerical_gradient(self, x, t):\n>         \"\"\"\n>         使用数值微分计算梯度\n>         参数:\n>             x: 输入数据\n>             t: 正确解标签\n>         返回:\n>             包含各层权重和偏置梯度的字典\n>         \"\"\"\n>         # 定义损失函数\n>         loss_W = lambda W: self.loss(x, t)\n>         \n>         # 计算各层参数的梯度\n>         grads = {}\n>         grads['W1'] = numerical_gradient(loss_W, self.params['W1'])  # 第一层权重梯度\n>         grads['b1'] = numerical_gradient(loss_W, self.params['b1'])  # 第一层偏置梯度\n>         grads['W2'] = numerical_gradient(loss_W, self.params['W2'])  # 第二层权重梯度\n>         grads['b2'] = numerical_gradient(loss_W, self.params['b2'])  # 第二层偏置梯度\n>         \n>         return grads\n>         \n>     def gradient(self, x, t):\n>         \"\"\"\n>         使用反向传播计算梯度\n>         参数:\n>             x: 输入数据\n>             t: 正确解标签\n>         返回:\n>             包含各层权重和偏置梯度的字典\n>         \"\"\"\n>         W1, W2 = self.params['W1'], self.params['W2']\n>         b1, b2 = self.params['b1'], self.params['b2']\n>         grads = {}\n>         \n>         batch_num = x.shape[0]  # 批次大小\n>         \n>         # 前向传播\n>         a1 = np.dot(x, W1) + b1\n>         z1 = sigmoid(a1)\n>         a2 = np.dot(z1, W2) + b2\n>         y = softmax(a2)\n>         \n>         # 反向传播\n>         # 输出层的误差\n>         dy = (y - t) / batch_num\n>         # 第二层权重的梯度\n>         grads['W2'] = np.dot(z1.T, dy)\n>         # 第二层偏置的梯度\n>         grads['b2'] = np.sum(dy, axis=0)\n>         \n>         # 第一层的误差\n>         dz1 = np.dot(dy, W2.T)\n>         da1 = sigmoid_grad(a1) * dz1\n>         # 第一层权重的梯度\n>         grads['W1'] = np.dot(x.T, da1)\n>         # 第一层偏置的梯度\n>         grads['b1'] = np.sum(da1, axis=0)\n> \n>         return grads\n>         \n>         \n> net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n> print(net.params['W1'].shape)\n> print(net.params['b1'].shape)\n> print(net.params['W2'].shape)\n> print(net.params['b2'].shape)\n> \n> x = np.random.rand(100, 784)\n> t = np.random.rand(100, 10)\n> \n> grads = net.numerical_gradient(x, t)\n> \n> print(grads['W1'].shape)\n> print(grads['b1'].shape)\n> print(grads['W2'].shape)\n> print(grads['b2'].shape)\n> ```\n>\n> #### mini-batch的实现，以及基于测试数据的评价\n>\n> ```python\n> # coding: utf-8\n> import sys, os\n> sys.path.append('../../py_pro/DL/')  # 将父目录添加到系统路径中，以便导入其他模块\n> import numpy as np\n> import matplotlib.pyplot as plt\n> from dataset.mnist import load_mnist  # 导入MNIST数据集加载函数\n> from two_layer_net import TwoLayerNet  # 导入两层神经网络类\n> \n> # 加载MNIST数据集\n> # normalize=True: 将输入图像像素值正规化到0~1之间\n> # one_hot_label=True: 将标签转换为one-hot形式\n> (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n> \n> # 创建两层神经网络实例\n> # 输入层784个神经元（28x28像素）\n> # 隐藏层50个神经元\n> # 输出层10个神经元（0-9十个数字）\n> network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n> \n> # 设置超参数\n> iters_num = 10000  # 训练迭代次数\n> train_size = x_train.shape[0]  # 训练数据的大小\n> batch_size = 100  # 每批次的样本数\n> learning_rate = 0.1  # 学习率\n> \n> # 用于记录训练过程的列表\n> train_loss_list = []  # 记录训练损失\n> train_acc_list = []   # 记录训练准确率\n> test_acc_list = []    # 记录测试准确率\n> \n> # 计算每个epoch的迭代次数\n> iter_per_epoch = max(train_size / batch_size, 1)\n> \n> # 开始训练\n> for i in range(iters_num):\n>     # 随机选择批次数据\n>     batch_mask = np.random.choice(train_size, batch_size)\n>     x_batch = x_train[batch_mask]\n>     t_batch = t_train[batch_mask]\n>     \n>     # 计算梯度\n>     # 使用反向传播计算梯度（比数值微分更快）\n>     #grad = network.numerical_gradient(x_batch, t_batch)  # 数值微分（较慢）\n>     grad = network.gradient(x_batch, t_batch)  # 反向传播（较快）\n>     \n>     # 更新参数\n>     # 对每个参数进行梯度下降更新\n>     for key in ('W1', 'b1', 'W2', 'b2'):\n>         network.params[key] -= learning_rate * grad[key]\n>     \n>     # 记录训练损失\n>     loss = network.loss(x_batch, t_batch)\n>     train_loss_list.append(loss)\n>     \n>     # 每个epoch计算一次训练集和测试集的准确率\n>     if i % iter_per_epoch == 0:\n>         train_acc = network.accuracy(x_train, t_train)\n>         test_acc = network.accuracy(x_test, t_test)\n>         train_acc_list.append(train_acc)\n>         test_acc_list.append(test_acc)\n>         print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n> \n> # 绘制训练结果\n> markers = {'train': 'o', 'test': 's'}  # 设置图例标记\n> x = np.arange(len(train_acc_list))\n> plt.plot(x, train_acc_list, label='train acc')  # 绘制训练准确率曲线\n> plt.plot(x, test_acc_list, label='test acc', linestyle='--')  # 绘制测试准确率曲线\n> plt.xlabel(\"epochs\")  # x轴标签\n> plt.ylabel(\"accuracy\")  # y轴标签\n> plt.ylim(0, 1.0)  # 设置y轴范围\n> plt.legend(loc='lower right')  # 显示图例\n> plt.show()  # 显示图形\n> ```\n>\n> \n\n\n\n### 都说这本鱼书是 最最最 最入门的神经网络和深度学习的书了，但是，从这一章开始，已经开始头脑风暴了，难顶哟(TUT)","source":"_posts/22.md","raw":"---\ntitle: DL之路---啃鱼书（3）\ndata: 2025-06-15 20:27:00\nupdated: 2025-06-15 20:27:00\ntype: DL\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p22.jpg\ntags:\n  - DL\n  - Learning\n  - gnaw_book\n---\n\n\n# 神经网络的学习\n\n\n\n### 从数据中学习\n\n> #### 所谓的学习就是从训练数据中自动获取最优权重参数的过程\n>\n> #### 为了使神经网络能够学习，引入了损失函数这一指标，而学习的目标就是以损失函数为基准，找出使它的值达到最小的权重参数（函数斜率的梯度法）\n>\n> \n>\n> #### 特征量：是指可以从输入数据（输入图像）中准确地提取出本质数据（重要数据）的转换器（图像的特征量通常表示为向量形式），但特征量的选取是人为选择的，还是存在人为因素的介入\n>\n> \n>\n> #### 深度学习也被称为端到端机器学习（end-to-end machine learning），从原始数据（输入）中获得目标结果（输出）\n>\n> #### 神经网络的优点是对所有的问题都可以使用同样的流程来解决，与待处理的问题无关，它可以将数据直接作为原始数据，进行端到端学习\n>\n> \n>\n> #### 在机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验，首先使用训练数据进行学习，寻找最优参数，再使用测试数据评价训练得到的模型的实际能力\n>\n> #### 划分为训练数据和测试数据（也称为监督数据）是为了追求“泛化能力”\n>\n> #### 泛化能力：是指处理未被观察过的数据（不包含在训练数据中的数据）的能力\n>\n> #### 获得泛化能力是机器学习的最终目标\n>\n> #### 过拟合：它是指只对某个数据集过度拟合的状态\n\n\n\n### 损失函数\n\n> #### 神经网络的学习通过某个指标表示现在的状态，再以这个指标为基准，寻找最优权重参数，而这个指标就是损失函数\n>\n> #### 损失函数可以使用任何函数，但一般使用的是均方误差和交叉熵误差\n>\n> #### 损失函数是表示神经网络性能好坏的指标，它反映了当前的神经网络对监督数据的拟合程度\n>\n> \n>\n> #### 均方误差\n>\n> > #### 其表达式如下：\n> >\n> > $$\n> > E = \\frac{1}{2} \\sum_{k} (y_k - t_k)^2\n> > $$\n> >\n> > #### 其中，$y_k$ 是表示神经网络的输出，$t_k$ 表示监督数据（测试数据），$k$ 表示数据的维数\n> >\n> > #### 题外话：均方误差式子的一个特点就是，其名中的4个字与式子中皆有对应，均 对应式子中的 $\\frac{1}{2}$ ，方 对应式子中的 $()^2$ ，误差 对应式子中的 $ y_k - t_k $ \n> >\n> > \n>\n> #### 交叉熵误差\n>\n> > #### 其表达式如下：\n> >\n> > $$\n> > E = - \\sum_{k} t_k \\log y_k\n> > $$\n> >\n> > #### 其中，$y_k$ 是表示神经网络的输出，$t_k$ 是正确解标签，且 $t_k$ 中只有正确解标签的索引为1，其余的都是0(one-hot表示)\n> >\n> > #### 所以说，实际上交叉熵误差式子只计算对应正确解标签的输出的自然对数，交叉熵误差的值是由正确解标签所对应的输出结果决定的\n> >\n> > #### 正确解标签对应的输出越大，交叉熵误差的值就越接近0，反之则越大，输出为1时，交叉熵误差为0，log函数图像如下\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/3/1.png)\n>\n> \n>\n> #### mini-batch学习\n>\n> > #### 机器学习使用训练数据进行学习，其本质就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数，所以计算损失函数时必须将所有的训练数据作为对象\n> >\n> > \n> >\n> > #### 前面提到的损失函数都是针对的单个数据，若要求所有的训练数据的损失函数的总和，以交叉熵误差为例，损失函数可以写为：\n> >\n> > $$\n> > E = - \\frac{1}{N} \\sum_{n} \\sum_{k} t_{nk} \\log y_{nk}\n> > $$\n> >\n> > #### 这里是假设有$N$个数据，$t_{nk}$ 表示第 $n$ 个数据的第 $k$ 个元素的值，$t_{nk}$ 是监督数据，$t_{nk}$ 是神经网络的输出\n> >\n> > #### 通过除以 $N$ （正规化），可以求单个数据的平均损失函数，这样的平均化，可以获得和训练数据的数量无关的统一指标\n> >\n> > \n> >\n> > #### 如果将全部的训练数据作为对象求损失函数的总和，那这个计算过程会是非常漫长的，训练数据量非常庞大时，以全部训练数据作为对象计算损失函数更是不现实的\n> >\n> > #### 因此，从全部训练数据中选择一部分，作为全部训练数据的“近似”\n> >\n> > #### 神经网络的学习也是从训练数据中选出一批数据（称为mini-batch,小批量），然后对每个mini-batch进行学习，用这一批数据的结果来近似全部数据的结果，这种学习方式就是所谓的 **mini-batch学习**\n> >\n> > \n>\n> \n>\n> #### 为什么要引入损失函数\n>\n> > #### 为了找到使损失函数的值尽可能小的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值\n> >\n> > #### 损失函数的导数一般不会有0值，更不会恒为0（阶跃函数除外），而识别精度的导数恒为0或者只有极小部分不为0，这样权重的调整并不会影响其值，这并不是我们想要的\n\n\n\n### 数值微分（数值梯度）\n\n> #### 梯度法使用梯度的信息决定前进的方向\n>\n> #### 利用微小的差分求导数的过程就是所谓的数值微分（numerical differentiation）\n>\n> #### 基于数学式的推到求导数的过程，则称之为 解析性求解或解析性求导\n>\n\n\n\n### 梯度\n\n> #### 由全部变量的偏导数汇总成的向量称为梯度\n>\n> \n>\n> #### 梯度法\n>\n> > #### 寻找的最优参数是指损失函数取最小值时的参数\n> >\n> > #### 使用梯度来寻找损失函数最小值的方法就是所谓的 梯度法，通过不断地沿梯度方向前进，逐渐减小函数值的过程就是梯度法\n> >\n> > #### 梯度表示的各点处的函数值减小最多的方向\n> >\n> > #### 函数最小值、极小值和鞍点处的梯度为 0（梯度法就是要找梯度为0的地方）\n> >\n> > #### 极小值是局部最小值，鞍点是从某个方向上看是极大值，从另一个方向看是极小值的点\n> >\n> > #### 学习高原：它是指，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入所谓的**学习高原**而无法前进的停滞期\n> >\n> > #### 根据寻找目的（最大值或最小值）的不同，梯度法分为了：梯度下降法和梯度上升法\n> >\n> > - #### 梯度下降法：寻找最小值的梯度法\n> >\n> > - #### 梯度上升法：寻找最大值的梯度法\n> >\n> > #### 通过反转损失函数的符号，求最小值和求最大值的问题可以变成相同的问题\n> >\n> > #### 神经网络（深度学习）中，梯度法一般是指的梯度下降法\n> >\n> > #### 学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数\n> >\n> > #### 在神经网络的学习中，一般会一边改变学习率的值，一边确认学习是否正确进行了\n> >\n> > #### 学习率这样的参数也被称为**超参数**，需要人工设定\n>\n> \n>\n> #### 神经网络的梯度\n>\n> > #### 这里的梯度指的是损失函数关于权重参数的梯度\n> >\n> > #### 例子说明，一个神经网络的权重W形状为 2x3，损失函数用L表示，此时梯度可以用 $\\frac{\\partial L}{\\partial W}$ 来表示，数学表示式如下\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/3/2.png)\n> >\n> > #### 求出神经网络的梯度后，接下来只需根据梯度法，更新权重参数即可\n>\n> \n>\n> #### 学习算法的实现\n>\n> > #### 神经网络的学习的步骤\n> >\n> > > #### 前提是：神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”\n> > >\n> > > 1. #### （mini-batch）从训练数据中随机选出一部分数据，这部分数据称为mini-batch，目标是是减小mini-batch的损失函数的值\n> > >\n> > > 2. #### （计算梯度）为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度，梯度表示损失函数的值减小最多的方向\n> > >\n> > > 3. #### （更新参数）将权重参数沿梯度方向进行微小更新\n> > >\n> > > 4. #### （重复）重复前三步\n> > >\n> > > #### 因为更新权重参数采用的梯度法是梯度下降法，又是随机选择的mini-batch数据，所以称之为随机梯度下降法（stochastic gradient descent，SGD）\n> >\n> > \n> >\n> > #### 手写数字识别的神经网络的实现\n> >\n> > #### 基于测试数据的评价\n> >\n> > > #### 神经网络的学习中，必须确认是否能够正确识别训练数据以外的其他数 据，即确认是否会发生过拟合\n> > >\n> > > #### 过拟合是指，虽然训练数据中的数字图像能被正确辨别，但是不在训练数据中的数字图像却无法被识别的现象\n> > >\n> > > #### 要评价神经网络的泛化能力，就必须使用不包含在训练数据中的数据，这就要用到测试数据了\n> > >\n> > > #### 另外又引入了一个新的概念——epoch\n> > >\n> > > > #### epoch是一个单位，一个epoch表示学习中所有训练数据均被使用过一次时的更新次数\n> > > >\n> > > > #### 如，对于10000笔训练数据，用大小为 100 笔数据的mini-batch进行学习时，重复随机梯度下降法 100次，所有的训练数据就都被“看过”了，此时，100次就是一个 epoch\n> > > >\n> > > > #### 遍历一次所有数据，就成为一个epoch\n\n### code\n\n> #### 均方误差的实现\n>\n> ```python\n> import numpy as np\n> \n> def mean_squared_error(y, t):\n> return 0.5 * np.sum((y - t)**2)\n> \n> \n> # 设2为正确解\n> t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n> y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n> result = mean_squared_error(y, t)\n> print(result)\n> \n> y = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])\n> result_ = mean_squared_error(y, t)\n> print(result_)\n> ```\n>\n> \n>\n> #### 交叉熵误差的实现\n>\n> ```python\n> def cross_entropy_error(y, t):\n> # 加上delta是为了避免出现 log(0)，导致出现负无穷\n> delta = 1e-7\n> return -np.sum(t * np.log(y + delta))\n> \n> \n> t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n> y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n> result = cross_entropy_error(y, t)\n> print(result)\n> \n> y = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])\n> result_ = cross_entropy_error(y, t)\n> print(result_)\n> ```\n>\n> \n>\n> #### 使用mini-batch学习的手写数字识别\n>\n> ```python\n> import os, sys\n> sys.path.append(os.pardir)\t# 将父目录添加到系统路径，以便导入自定义模块\n> import numpy as np\n> import pickle\n> from dataset.mnist import load_mnist\n> from common.function import sigmoid, softmax\n> \n> \n> # 加载MNIST数据集\n> def get_data():\n> # normalize=True:将图像的像素值正规化到0.0~1.0\n> # flatten=True:将图像从2维数组转为1维数组\n> # one_hot_label=False:标签为0~9的数字,而不是one-hot编码\n> (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)\n> \n> return x_test, t_test\n> \n> \n> # 初始化神经网络\n> def init_network():\n> # 从文件加载预训练的网络参数（权重和偏置）\n> with open(\"sample_weight.pkl\", rb)as f:\n> network = pickle.load(f)\n> \treturn network\n> \n> \n> # 神经网络的预测函数\n> def predict(network, x):\n> # x 是输入数据\n> # 获取网络的权重和偏置\n> w1, w2, w3 = network['W1'], network['W2'], network['W3']\n> b1, b2, b3 = network['b1'], network['b2'], network['b3']\n> \n> # 向前传播\n> a1 = np.dot(x, w1) + b1\n> z1 = sigmoid(a1)\n> a2 = np.dot(z1, w2) + b2\n> z2 = sigmoid(a2)\n> a3 = np.dot(z2, w3) + b3\n> y = softmax(a3)\n> \n> return y\n> \n> \n> x, t = get_data()\n> network = init_network()\n> \n> batch_size = 100\t# 设置批处理大小\n> accuracy_cnt = 0\t# 初始化准确率计数器\n> \n> # 使用批处理进行预测\n> for i in range(0, len(x), batch_size):\n> x_batch = x[i:i+batch_size]\t# 获取当前批次输入的数据\n> y_batch = predict(network, x_batch)\t# 对当前批次进行预测\n> p = np.argmax(y_batch, axis=1)\t# 获取每个样本预测概率最大的类别\n> accuracy_cnt += np.sum(p == t[i:i+batch_size])\t# 统计预测正确的样本数\n> \n> # 输出最终的识别准确率    \n> print(\"Accuracy: \" + str(float(accuracy_cnt) / len(x)))\n> ```\n>\n> \n>\n> #### 数值微分\n>\n> > #### 导数的实现\n> >\n> > ```python\n> > def numerical_diff(f, x):\n> >  h = 1e-4\n> >  return (f(h + x) - f(x)) / h\n> > ```\n> >\n> > #### 上面这样计算函数f的差分是不准确的，它计算的是(x+h)和(x)之间的斜率，它与实际要计算的导数是有差异的，这一差异是因h不可能无限接近0导致的，这种属于是前向差分\n> > #### 正确的应该是 (x+h)和(x-h)之间的差分，以x为中心，这也称之为中心差分\n> >\n> > ```python\n> > def numerical_diff(f, x):\n> >  h = 1e-4\n> >  return (f(x+h) - f(x-h)) / (2 * h)\n> > ```\n> >\n> > #### 举个数值微分实例，y = 0.01X^2 + 0.1x\n> >\n> > ```python\n> > def fun_1(x):\n> >  return 0.01*x**2 + 0.1 * x\n> > ```\n> >\n> > #### 绘制其图像\n> >\n> > ```python\n> > import numpy as np\n> > import matplotlib.pylab as plt\n> > \n> > x = np.arange(0.0, 20.0, 0.1)\n> > y = fun_1(x)\n> > plt.xlabel(\"x\")\n> > plt.ylabel(\"f(x)\")\n> > plt.plot(x, y)\n> > plt.show()\n> > ```\n> >\n> > #### 计算x=5和x=10处的导数，并画出其图像（含切线）\n> >\n> > ```python\n> > a = numerical_diff(fun_1, 5)\n> > b = numerical_diff(fun_1, 10)\n> > print(a)\n> > print(b)\n> > \n> > def tangent_line(f, x):\n> >  d = numerical_diff(f, x)\n> >  print(d)\n> >  y = f(x) - d*x\n> >  return lambda t: d*t + y\n> > \n> > x = np.arange(0.0, 20.0, 0.1)\n> > y = fun_1(x)\n> > plt.xlabel(\"x\")\n> > plt.ylabel(\"f(x)\")\n> > \n> > # 设置切点坐标\n> > tangent_x = 5\n> > tangent_y = fun_1(tangent_x)\n> > \n> > # 绘制原函数和切线图像\n> > tf = tangent_line(fun_1, tangent_x)\n> > y2 = tf(x)\n> > plt.plot(x, y)\n> > plt.plot(x, y2)\n> > \n> > # 绘制切点处辅助线\n> > # 垂直x轴的\n> > plt.axvline(x=tangent_x, color='gray', linestyle='--', alpha=0.5)\n> > # 垂直y轴的\n> > plt.axhline(y=tangent_y, color='gray', linestyle='--', alpha=0.5)\n> > # 标记切点(红色圆点)\n> > plt.plot(tangent_x, tangent_y, 'ro')\n> > # 添加坐标标注\n> > plt.text(tangent_x, tangent_y, f'({tangent_x:.1f}, {tangent_y:.2f})', \n> >       horizontalalignment='right', verticalalignment='bottom')\n> > plt.show()\n> > \n> > # 设置切点坐标\n> > tangent_x = 10\n> > tangent_y = fun_1(tangent_x)\n> > \n> > tf = tangent_line(fun_1, tangent_x)\n> > y3 = tf(x)\n> > plt.plot(x, y)\n> > plt.plot(x, y3)\n> > \n> > # 绘制切点处辅助线\n> > # 垂直x轴的\n> > plt.axvline(x=tangent_x, color='gray', linestyle='--', alpha=0.5)\n> > # 垂直y轴的\n> > plt.axhline(y=tangent_y, color='gray', linestyle='--', alpha=0.5)\n> > # 标记切点(红色圆点)\n> > plt.plot(tangent_x, tangent_y, 'ro')\n> > # 添加坐标标注\n> > plt.text(tangent_x, tangent_y, f'({tangent_x:.1f}, {tangent_y:.2f})', \n> >       horizontalalignment='right', verticalalignment='bottom')\n> > plt.show()\n> > ```\n> >\n> > \n> >\n> > #### 偏导数\n> >\n> > > #### 求 $f(x_0,x_1)=x_0^2+x_1^2$的偏导，并画出其函数图像\n> > >\n> > > ```python\n> > > import numpy as np\n> > > import matplotlib.pyplot as plt\n> > > \n> > > from mpl_toolkits.mplot3d import Axes3D\n> > > \n> > > # 定义网格\n> > > x0 = np.linspace(-3, 3, 100)\n> > > x1 = np.linspace(-3, 3, 100)\n> > > X0, X1 = np.meshgrid(x0, x1)\n> > > \n> > > # 定义函数\n> > > Z = X0**2 + X1**2\n> > > \n> > > # 创建图形窗口\n> > > fig = plt.figure(figsize=(14, 6))\n> > > \n> > > # 1. 三维图像\n> > > ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n> > > ax1.plot_surface(X0, X1, Z, cmap='viridis', edgecolor='none')\n> > > ax1.set_title(r'$f(x_0, x_1) = x_0^2 + x_1^2$')\n> > > ax1.set_xlabel('$x_0$')\n> > > ax1.set_ylabel('$x_1$')\n> > > ax1.set_zlabel('$f(x_0, x_1)$')\n> > > \n> > > # 2. 等高线图 + 梯度向量\n> > > ax2 = fig.add_subplot(1, 2, 2)\n> > > contour = ax2.contourf(X0, X1, Z, levels=30, cmap='viridis')\n> > > ax2.set_title('Contour and Gradient')\n> > > ax2.set_xlabel('$x_0$')\n> > > ax2.set_ylabel('$x_1$')\n> > > \n> > > # 计算梯度（偏导）\n> > > grad_x0 = 2 * X0\n> > > grad_x1 = 2 * X1\n> > > \n> > > # 绘制梯度向量（用 quiver 画箭头）\n> > > skip = 5  # 降低密度\n> > > ax2.quiver(X0[::skip, ::skip], X1[::skip, ::skip],\n> > >            grad_x0[::skip, ::skip], grad_x1[::skip, ::skip],\n> > >            color='white', alpha=0.8)\n> > > \n> > > plt.colorbar(contour, ax=ax2, label='Function Value')\n> > > plt.tight_layout()\n> > > plt.show()\n> > > ```\n>\n> \n>\n> #### 梯度\n>\n> > #### 简单的梯度的实现\n> >\n> > ```python\n> > # 梯度的实现\n> > def numerical_gradient(f, x):\n> >     h = 1e-4\n> >     # 生成和x形状相同的数组，且所有元素为0\n> >     grad = np.zeros_like(x)\n> > \n> >     for idx in range(x.size):\n> >         tmp_val = x[idx]\n> >         # f(x+h)的计算\n> >         x[idx] = tmp_val + h\n> >         fxh1 = f(x)\n> > \n> >         # f(x-h)的计算\n> >         x[idx] = tmp_val - h\n> >         fxh2 = f(x)\n> > \n> >         grad[idx] = (fxh1 - fxh2) / (2 * h)\n> >         x[idx] = tmp_val # 还原值\n> >     \n> >     return grad\n> > \n> > \n> > print(numerical_gradient(fun_2, np.array([3.0, 4.0])))\n> > print(numerical_gradient(fun_2, np.array([0.0, 2.0])))\n> > print(numerical_gradient(fun_2, np.array([3.0, 0.0])))\n> > ```\n> >\n> > #### 这里给出一个计算任意维数组的数值梯度\n> >\n> > ```python\n> > def numerical_gradient(f, x):\n> >     \"\"\"\n> >     计算任意维数组的数值梯度\n> >     参数:\n> >         f: 目标函数\n> >         x: 任意维度的数组\n> >     返回:\n> >         梯度数组，形状与x相同\n> >     \n> >     实现原理：\n> >     1. 使用中心差分法计算梯度：(f(x+h) - f(x-h)) / (2h)\n> >     2. 对数组中的每个元素分别计算梯度\n> >     3. 使用numpy的迭代器处理任意维度的数组\n> >     \"\"\"\n> >     h = 1e-4  # 0.0001，微小的变化量\n> >     grad = np.zeros_like(x)  # 创建与x形状相同的零数组，用于存储梯度\n> >     \n> >     # 使用numpy的迭代器遍历数组的所有元素\n> >     # flags=['multi_index']: 获取多维索引\n> >     # op_flags=['readwrite']: 允许读写操作\n> >     it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n> >     while not it.finished:\n> >         idx = it.multi_index  # 获取当前元素的多维索引\n> >         tmp_val = x[idx]  # 保存当前值\n> >         \n> >         # 计算f(x+h)\n> >         x[idx] = tmp_val + h\n> >         fxh1 = f(x)\n> >         \n> >         # 计算f(x-h)\n> >         x[idx] = tmp_val - h \n> >         fxh2 = f(x)\n> >         \n> >         # 使用中心差分法计算梯度\n> >         grad[idx] = (fxh1 - fxh2) / (2*h)\n> >         \n> >         x[idx] = tmp_val  # 恢复原始值\n> >         it.iternext()  # 移动到下一个元素\n> >         \n> >     return grad\n> > ```\n> >\n> > \n> >\n> > #### 梯度下降法的实现\n> >\n> > ```python\n> > # f 是要进行最优化的函数，init_x是初始值，lr是学习率，step_num是梯度法的重复次数\n> > def gradient_descent(f, init_x, lr=0.01, step_num=100):\n> >     x = init_x\n> > \n> >     for i in range(step_num):\n> >         grad = numerical_gradient(f, x)\n> >         x -= lr * grad\n> > \n> >     return x\n> > \n> > \n> > init_x = np.array([-3.0, 4.0])\n> > result_ = gradient_descent(fun_2, init_x=init_x, lr=0.1, step_num=100)\n> > print(result_)\n> > ```\n> >\n> > #### 图像展示梯度法的过程\n> >\n> > ```python\n> > def gradient_descent_(f, init_x, lr=0.01, step_num=100):\n> >     x = init_x\n> >     x_history = []\n> > \n> >     for i in range(step_num):\n> >         x_history.append(x.copy())\n> >         \n> >         grad = numerical_gradient(f, x)\n> >         x -= lr * grad\n> > \n> >     return x, np.array(x_history)\n> > \n> > \n> > init_x = np.array([-3.0, 4.0])\n> > \n> > lr = 0.1\n> > step_num = 20\n> > x, x_history = gradient_descent_(fun_2, init_x, lr, step_num)\n> > \n> > plt.plot([-5, 5], [0, 0], '--b')\n> > plt.plot([0, 0], [-5, 5], '--b')\n> > plt.plot(x_history[:, 0], x_history[:, 1], 'o')\n> > \n> > plt.xlim(-3.5, 3.5)\n> > plt.ylim(-4.5, 4.5)\n> > plt.xlabel(\"X0\")\n> > plt.ylabel(\"X1\")\n> > plt.show()\n> > ```\n>\n> \n>\n> #### 神经网络的梯度\n>\n> > #### 简单的神经网络\n> >\n> > ```python\n> > # coding: utf-8\n> > # 导入必要的库\n> > import sys, os\n> > # 将父目录添加到系统路径中，以便能够导入common模块\n> > sys.path.append(os.pardir)\n> > from common.functions import softmax, cross_entropy_error\n> > from common.gradient import numerical_gradient\n> > \n> > \n> > class simpleNet:\n> >     \"\"\"\n> >     一个简单的神经网络类\n> >     实现了一个2输入3输出的单层神经网络\n> >     \"\"\"\n> >     def __init__(self):\n> >         # 初始化权重矩阵，使用随机数生成2x3的权重矩阵\n> >         self.W = np.random.randn(2,3)\n> > \n> >     def predict(self, x):\n> >         \"\"\"\n> >         前向传播函数\n> >         参数:\n> >             x: 输入数据\n> >         返回:\n> >             神经网络的输出\n> >         \"\"\"\n> >         return np.dot(x, self.W)\n> > \n> >     def loss(self, x, t):\n> >         \"\"\"\n> >         计算损失函数\n> >         参数:\n> >             x: 输入数据\n> >             t: 正确解标签\n> >         返回:\n> >             损失函数的值\n> >         \"\"\"\n> >         z = self.predict(x)  # 获取神经网络的输出\n> >         y = softmax(z)       # 使用softmax函数将输出转换为概率\n> >         loss = cross_entropy_error(y, t)  # 计算交叉熵误差\n> > \n> >         return loss\n> > \n> > # 创建测试数据\n> > x = np.array([0.6, 0.9])  # 输入数据\n> > t = np.array([0, 0, 1])   # 正确解标签\n> > \n> > # 创建神经网络实例\n> > net = simpleNet()\n> > \n> > # 定义损失函数，用于计算梯度\n> > f = lambda w: net.loss(x, t)\n> > # 使用数值微分计算梯度\n> > dW = numerical_gradient(f, net.W)\n> > \n> > # 打印梯度结果\n> > print(dW)\n> > ```\n> >\n> > #### 这里需要注意的是，为什么要在类外定义一个损失函数f，因为类中的损失函数loss()是一个实例方法，需要访问实例的权重self.W，而numerical_gradient()函数参数中的f，需要的是一个纯函数，而不是一个类中的实例方法\n>\n> \n>\n> #### 手写数字识别的神经网络的实现\n>\n> ```python\n> # coding: utf-8\n> import sys, os\n> sys.path.append(os.pardir)  # 将父目录添加到系统路径中，以便导入common模块\n> from common.functions import *\n> from common.gradient import numerical_gradient\n> import numpy as np\n> \n> \n> class TwoLayerNet:\n>     \"\"\"\n>     两层神经网络类\n>     实现了一个具有一个隐藏层的神经网络\n>     结构：输入层 -> 隐藏层 -> 输出层\n>     \"\"\"\n> \n>     def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n>         \"\"\"\n>         初始化神经网络\n>         参数:\n>             input_size: 输入层神经元数量\n>             hidden_size: 隐藏层神经元数量\n>             output_size: 输出层神经元数量\n>             weight_init_std: 权重初始化的标准差\n>         \"\"\"\n>         # 初始化权重和偏置\n>         self.params = {}\n>         # 第一层权重矩阵，形状为(input_size, hidden_size)\n>         self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n>         # 第一层偏置，形状为(hidden_size,)\n>         self.params['b1'] = np.zeros(hidden_size)\n>         # 第二层权重矩阵，形状为(hidden_size, output_size)\n>         self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n>         # 第二层偏置，形状为(output_size,)\n>         self.params['b2'] = np.zeros(output_size)\n> \n>     def predict(self, x):\n>         \"\"\"\n>         前向传播，计算神经网络的输出\n>         参数:\n>             x: 输入数据\n>         返回:\n>             神经网络的输出（经过softmax后的概率分布）\n>         \"\"\"\n>         W1, W2 = self.params['W1'], self.params['W2']\n>         b1, b2 = self.params['b1'], self.params['b2']\n>     \n>         # 第一层计算\n>         a1 = np.dot(x, W1) + b1  # 线性变换\n>         z1 = sigmoid(a1)         # 激活函数\n>         \n>         # 第二层计算\n>         a2 = np.dot(z1, W2) + b2  # 线性变换\n>         y = softmax(a2)           # 输出层激活函数\n>         \n>         return y\n>         \n>     def loss(self, x, t):\n>         \"\"\"\n>         计算损失函数\n>         参数:\n>             x: 输入数据\n>             t: 正确解标签\n>         返回:\n>             损失函数的值\n>         \"\"\"\n>         y = self.predict(x)\n>         return cross_entropy_error(y, t)\n>     \n>     def accuracy(self, x, t):\n>         \"\"\"\n>         计算识别精度\n>         参数:\n>             x: 输入数据\n>             t: 正确解标签\n>         返回:\n>             识别精度（0-1之间的值）\n>         \"\"\"\n>         y = self.predict(x)\n>         y = np.argmax(y, axis=1)  # 获取预测结果\n>         t = np.argmax(t, axis=1)  # 获取正确解\n>         \n>         # 计算正确率\n>         accuracy = np.sum(y == t) / float(x.shape[0])\n>         return accuracy\n>         \n>     def numerical_gradient(self, x, t):\n>         \"\"\"\n>         使用数值微分计算梯度\n>         参数:\n>             x: 输入数据\n>             t: 正确解标签\n>         返回:\n>             包含各层权重和偏置梯度的字典\n>         \"\"\"\n>         # 定义损失函数\n>         loss_W = lambda W: self.loss(x, t)\n>         \n>         # 计算各层参数的梯度\n>         grads = {}\n>         grads['W1'] = numerical_gradient(loss_W, self.params['W1'])  # 第一层权重梯度\n>         grads['b1'] = numerical_gradient(loss_W, self.params['b1'])  # 第一层偏置梯度\n>         grads['W2'] = numerical_gradient(loss_W, self.params['W2'])  # 第二层权重梯度\n>         grads['b2'] = numerical_gradient(loss_W, self.params['b2'])  # 第二层偏置梯度\n>         \n>         return grads\n>         \n>     def gradient(self, x, t):\n>         \"\"\"\n>         使用反向传播计算梯度\n>         参数:\n>             x: 输入数据\n>             t: 正确解标签\n>         返回:\n>             包含各层权重和偏置梯度的字典\n>         \"\"\"\n>         W1, W2 = self.params['W1'], self.params['W2']\n>         b1, b2 = self.params['b1'], self.params['b2']\n>         grads = {}\n>         \n>         batch_num = x.shape[0]  # 批次大小\n>         \n>         # 前向传播\n>         a1 = np.dot(x, W1) + b1\n>         z1 = sigmoid(a1)\n>         a2 = np.dot(z1, W2) + b2\n>         y = softmax(a2)\n>         \n>         # 反向传播\n>         # 输出层的误差\n>         dy = (y - t) / batch_num\n>         # 第二层权重的梯度\n>         grads['W2'] = np.dot(z1.T, dy)\n>         # 第二层偏置的梯度\n>         grads['b2'] = np.sum(dy, axis=0)\n>         \n>         # 第一层的误差\n>         dz1 = np.dot(dy, W2.T)\n>         da1 = sigmoid_grad(a1) * dz1\n>         # 第一层权重的梯度\n>         grads['W1'] = np.dot(x.T, da1)\n>         # 第一层偏置的梯度\n>         grads['b1'] = np.sum(da1, axis=0)\n> \n>         return grads\n>         \n>         \n> net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n> print(net.params['W1'].shape)\n> print(net.params['b1'].shape)\n> print(net.params['W2'].shape)\n> print(net.params['b2'].shape)\n> \n> x = np.random.rand(100, 784)\n> t = np.random.rand(100, 10)\n> \n> grads = net.numerical_gradient(x, t)\n> \n> print(grads['W1'].shape)\n> print(grads['b1'].shape)\n> print(grads['W2'].shape)\n> print(grads['b2'].shape)\n> ```\n>\n> #### mini-batch的实现，以及基于测试数据的评价\n>\n> ```python\n> # coding: utf-8\n> import sys, os\n> sys.path.append('../../py_pro/DL/')  # 将父目录添加到系统路径中，以便导入其他模块\n> import numpy as np\n> import matplotlib.pyplot as plt\n> from dataset.mnist import load_mnist  # 导入MNIST数据集加载函数\n> from two_layer_net import TwoLayerNet  # 导入两层神经网络类\n> \n> # 加载MNIST数据集\n> # normalize=True: 将输入图像像素值正规化到0~1之间\n> # one_hot_label=True: 将标签转换为one-hot形式\n> (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n> \n> # 创建两层神经网络实例\n> # 输入层784个神经元（28x28像素）\n> # 隐藏层50个神经元\n> # 输出层10个神经元（0-9十个数字）\n> network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n> \n> # 设置超参数\n> iters_num = 10000  # 训练迭代次数\n> train_size = x_train.shape[0]  # 训练数据的大小\n> batch_size = 100  # 每批次的样本数\n> learning_rate = 0.1  # 学习率\n> \n> # 用于记录训练过程的列表\n> train_loss_list = []  # 记录训练损失\n> train_acc_list = []   # 记录训练准确率\n> test_acc_list = []    # 记录测试准确率\n> \n> # 计算每个epoch的迭代次数\n> iter_per_epoch = max(train_size / batch_size, 1)\n> \n> # 开始训练\n> for i in range(iters_num):\n>     # 随机选择批次数据\n>     batch_mask = np.random.choice(train_size, batch_size)\n>     x_batch = x_train[batch_mask]\n>     t_batch = t_train[batch_mask]\n>     \n>     # 计算梯度\n>     # 使用反向传播计算梯度（比数值微分更快）\n>     #grad = network.numerical_gradient(x_batch, t_batch)  # 数值微分（较慢）\n>     grad = network.gradient(x_batch, t_batch)  # 反向传播（较快）\n>     \n>     # 更新参数\n>     # 对每个参数进行梯度下降更新\n>     for key in ('W1', 'b1', 'W2', 'b2'):\n>         network.params[key] -= learning_rate * grad[key]\n>     \n>     # 记录训练损失\n>     loss = network.loss(x_batch, t_batch)\n>     train_loss_list.append(loss)\n>     \n>     # 每个epoch计算一次训练集和测试集的准确率\n>     if i % iter_per_epoch == 0:\n>         train_acc = network.accuracy(x_train, t_train)\n>         test_acc = network.accuracy(x_test, t_test)\n>         train_acc_list.append(train_acc)\n>         test_acc_list.append(test_acc)\n>         print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n> \n> # 绘制训练结果\n> markers = {'train': 'o', 'test': 's'}  # 设置图例标记\n> x = np.arange(len(train_acc_list))\n> plt.plot(x, train_acc_list, label='train acc')  # 绘制训练准确率曲线\n> plt.plot(x, test_acc_list, label='test acc', linestyle='--')  # 绘制测试准确率曲线\n> plt.xlabel(\"epochs\")  # x轴标签\n> plt.ylabel(\"accuracy\")  # y轴标签\n> plt.ylim(0, 1.0)  # 设置y轴范围\n> plt.legend(loc='lower right')  # 显示图例\n> plt.show()  # 显示图形\n> ```\n>\n> \n\n\n\n### 都说这本鱼书是 最最最 最入门的神经网络和深度学习的书了，但是，从这一章开始，已经开始头脑风暴了，难顶哟(TUT)","slug":"22","published":1,"date":"2025-06-15T12:25:16.686Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7crn0039iku4a06j4efw","content":"<h1 id=\"神经网络的学习\"><a href=\"#神经网络的学习\" class=\"headerlink\" title=\"神经网络的学习\"></a>神经网络的学习</h1><h3 id=\"从数据中学习\"><a href=\"#从数据中学习\" class=\"headerlink\" title=\"从数据中学习\"></a>从数据中学习</h3><blockquote>\n<h4 id=\"所谓的学习就是从训练数据中自动获取最优权重参数的过程\"><a href=\"#所谓的学习就是从训练数据中自动获取最优权重参数的过程\" class=\"headerlink\" title=\"所谓的学习就是从训练数据中自动获取最优权重参数的过程\"></a>所谓的学习就是从训练数据中自动获取最优权重参数的过程</h4><h4 id=\"为了使神经网络能够学习，引入了损失函数这一指标，而学习的目标就是以损失函数为基准，找出使它的值达到最小的权重参数（函数斜率的梯度法）\"><a href=\"#为了使神经网络能够学习，引入了损失函数这一指标，而学习的目标就是以损失函数为基准，找出使它的值达到最小的权重参数（函数斜率的梯度法）\" class=\"headerlink\" title=\"为了使神经网络能够学习，引入了损失函数这一指标，而学习的目标就是以损失函数为基准，找出使它的值达到最小的权重参数（函数斜率的梯度法）\"></a>为了使神经网络能够学习，引入了损失函数这一指标，而学习的目标就是以损失函数为基准，找出使它的值达到最小的权重参数（函数斜率的梯度法）</h4><h4 id=\"特征量：是指可以从输入数据（输入图像）中准确地提取出本质数据（重要数据）的转换器（图像的特征量通常表示为向量形式），但特征量的选取是人为选择的，还是存在人为因素的介入\"><a href=\"#特征量：是指可以从输入数据（输入图像）中准确地提取出本质数据（重要数据）的转换器（图像的特征量通常表示为向量形式），但特征量的选取是人为选择的，还是存在人为因素的介入\" class=\"headerlink\" title=\"特征量：是指可以从输入数据（输入图像）中准确地提取出本质数据（重要数据）的转换器（图像的特征量通常表示为向量形式），但特征量的选取是人为选择的，还是存在人为因素的介入\"></a>特征量：是指可以从输入数据（输入图像）中准确地提取出本质数据（重要数据）的转换器（图像的特征量通常表示为向量形式），但特征量的选取是人为选择的，还是存在人为因素的介入</h4><h4 id=\"深度学习也被称为端到端机器学习（end-to-end-machine-learning），从原始数据（输入）中获得目标结果（输出）\"><a href=\"#深度学习也被称为端到端机器学习（end-to-end-machine-learning），从原始数据（输入）中获得目标结果（输出）\" class=\"headerlink\" title=\"深度学习也被称为端到端机器学习（end-to-end machine learning），从原始数据（输入）中获得目标结果（输出）\"></a>深度学习也被称为端到端机器学习（end-to-end machine learning），从原始数据（输入）中获得目标结果（输出）</h4><h4 id=\"神经网络的优点是对所有的问题都可以使用同样的流程来解决，与待处理的问题无关，它可以将数据直接作为原始数据，进行端到端学习\"><a href=\"#神经网络的优点是对所有的问题都可以使用同样的流程来解决，与待处理的问题无关，它可以将数据直接作为原始数据，进行端到端学习\" class=\"headerlink\" title=\"神经网络的优点是对所有的问题都可以使用同样的流程来解决，与待处理的问题无关，它可以将数据直接作为原始数据，进行端到端学习\"></a>神经网络的优点是对所有的问题都可以使用同样的流程来解决，与待处理的问题无关，它可以将数据直接作为原始数据，进行端到端学习</h4><h4 id=\"在机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验，首先使用训练数据进行学习，寻找最优参数，再使用测试数据评价训练得到的模型的实际能力\"><a href=\"#在机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验，首先使用训练数据进行学习，寻找最优参数，再使用测试数据评价训练得到的模型的实际能力\" class=\"headerlink\" title=\"在机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验，首先使用训练数据进行学习，寻找最优参数，再使用测试数据评价训练得到的模型的实际能力\"></a>在机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验，首先使用训练数据进行学习，寻找最优参数，再使用测试数据评价训练得到的模型的实际能力</h4><h4 id=\"划分为训练数据和测试数据（也称为监督数据）是为了追求“泛化能力”\"><a href=\"#划分为训练数据和测试数据（也称为监督数据）是为了追求“泛化能力”\" class=\"headerlink\" title=\"划分为训练数据和测试数据（也称为监督数据）是为了追求“泛化能力”\"></a>划分为训练数据和测试数据（也称为监督数据）是为了追求“泛化能力”</h4><h4 id=\"泛化能力：是指处理未被观察过的数据（不包含在训练数据中的数据）的能力\"><a href=\"#泛化能力：是指处理未被观察过的数据（不包含在训练数据中的数据）的能力\" class=\"headerlink\" title=\"泛化能力：是指处理未被观察过的数据（不包含在训练数据中的数据）的能力\"></a>泛化能力：是指处理未被观察过的数据（不包含在训练数据中的数据）的能力</h4><h4 id=\"获得泛化能力是机器学习的最终目标\"><a href=\"#获得泛化能力是机器学习的最终目标\" class=\"headerlink\" title=\"获得泛化能力是机器学习的最终目标\"></a>获得泛化能力是机器学习的最终目标</h4><h4 id=\"过拟合：它是指只对某个数据集过度拟合的状态\"><a href=\"#过拟合：它是指只对某个数据集过度拟合的状态\" class=\"headerlink\" title=\"过拟合：它是指只对某个数据集过度拟合的状态\"></a>过拟合：它是指只对某个数据集过度拟合的状态</h4></blockquote>\n<h3 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h3><blockquote>\n<h4 id=\"神经网络的学习通过某个指标表示现在的状态，再以这个指标为基准，寻找最优权重参数，而这个指标就是损失函数\"><a href=\"#神经网络的学习通过某个指标表示现在的状态，再以这个指标为基准，寻找最优权重参数，而这个指标就是损失函数\" class=\"headerlink\" title=\"神经网络的学习通过某个指标表示现在的状态，再以这个指标为基准，寻找最优权重参数，而这个指标就是损失函数\"></a>神经网络的学习通过某个指标表示现在的状态，再以这个指标为基准，寻找最优权重参数，而这个指标就是损失函数</h4><h4 id=\"损失函数可以使用任何函数，但一般使用的是均方误差和交叉熵误差\"><a href=\"#损失函数可以使用任何函数，但一般使用的是均方误差和交叉熵误差\" class=\"headerlink\" title=\"损失函数可以使用任何函数，但一般使用的是均方误差和交叉熵误差\"></a>损失函数可以使用任何函数，但一般使用的是均方误差和交叉熵误差</h4><h4 id=\"损失函数是表示神经网络性能好坏的指标，它反映了当前的神经网络对监督数据的拟合程度\"><a href=\"#损失函数是表示神经网络性能好坏的指标，它反映了当前的神经网络对监督数据的拟合程度\" class=\"headerlink\" title=\"损失函数是表示神经网络性能好坏的指标，它反映了当前的神经网络对监督数据的拟合程度\"></a>损失函数是表示神经网络性能好坏的指标，它反映了当前的神经网络对监督数据的拟合程度</h4><h4 id=\"均方误差\"><a href=\"#均方误差\" class=\"headerlink\" title=\"均方误差\"></a>均方误差</h4><blockquote>\n<h4 id=\"其表达式如下：\"><a href=\"#其表达式如下：\" class=\"headerlink\" title=\"其表达式如下：\"></a>其表达式如下：</h4><p>$$<br>E &#x3D; \\frac{1}{2} \\sum_{k} (y_k - t_k)^2<br>$$</p>\n<h4 id=\"其中，-y-k-是表示神经网络的输出，-t-k-表示监督数据（测试数据），-k-表示数据的维数\"><a href=\"#其中，-y-k-是表示神经网络的输出，-t-k-表示监督数据（测试数据），-k-表示数据的维数\" class=\"headerlink\" title=\"其中，$y_k$ 是表示神经网络的输出，$t_k$ 表示监督数据（测试数据），$k$ 表示数据的维数\"></a>其中，$y_k$ 是表示神经网络的输出，$t_k$ 表示监督数据（测试数据），$k$ 表示数据的维数</h4><h4 id=\"题外话：均方误差式子的一个特点就是，其名中的4个字与式子中皆有对应，均-对应式子中的-frac-1-2-，方-对应式子中的-2-，误差-对应式子中的-y-k-t-k\"><a href=\"#题外话：均方误差式子的一个特点就是，其名中的4个字与式子中皆有对应，均-对应式子中的-frac-1-2-，方-对应式子中的-2-，误差-对应式子中的-y-k-t-k\" class=\"headerlink\" title=\"题外话：均方误差式子的一个特点就是，其名中的4个字与式子中皆有对应，均 对应式子中的 $\\frac{1}{2}$ ，方 对应式子中的 $()^2$ ，误差 对应式子中的 $ y_k - t_k $\"></a>题外话：均方误差式子的一个特点就是，其名中的4个字与式子中皆有对应，均 对应式子中的 $\\frac{1}{2}$ ，方 对应式子中的 $()^2$ ，误差 对应式子中的 $ y_k - t_k $</h4></blockquote>\n<h4 id=\"交叉熵误差\"><a href=\"#交叉熵误差\" class=\"headerlink\" title=\"交叉熵误差\"></a>交叉熵误差</h4><blockquote>\n<h4 id=\"其表达式如下：-1\"><a href=\"#其表达式如下：-1\" class=\"headerlink\" title=\"其表达式如下：\"></a>其表达式如下：</h4><p>$$<br>E &#x3D; - \\sum_{k} t_k \\log y_k<br>$$</p>\n<h4 id=\"其中，-y-k-是表示神经网络的输出，-t-k-是正确解标签，且-t-k-中只有正确解标签的索引为1，其余的都是0-one-hot表示\"><a href=\"#其中，-y-k-是表示神经网络的输出，-t-k-是正确解标签，且-t-k-中只有正确解标签的索引为1，其余的都是0-one-hot表示\" class=\"headerlink\" title=\"其中，$y_k$ 是表示神经网络的输出，$t_k$ 是正确解标签，且 $t_k$ 中只有正确解标签的索引为1，其余的都是0(one-hot表示)\"></a>其中，$y_k$ 是表示神经网络的输出，$t_k$ 是正确解标签，且 $t_k$ 中只有正确解标签的索引为1，其余的都是0(one-hot表示)</h4><h4 id=\"所以说，实际上交叉熵误差式子只计算对应正确解标签的输出的自然对数，交叉熵误差的值是由正确解标签所对应的输出结果决定的\"><a href=\"#所以说，实际上交叉熵误差式子只计算对应正确解标签的输出的自然对数，交叉熵误差的值是由正确解标签所对应的输出结果决定的\" class=\"headerlink\" title=\"所以说，实际上交叉熵误差式子只计算对应正确解标签的输出的自然对数，交叉熵误差的值是由正确解标签所对应的输出结果决定的\"></a>所以说，实际上交叉熵误差式子只计算对应正确解标签的输出的自然对数，交叉熵误差的值是由正确解标签所对应的输出结果决定的</h4><h4 id=\"正确解标签对应的输出越大，交叉熵误差的值就越接近0，反之则越大，输出为1时，交叉熵误差为0，log函数图像如下\"><a href=\"#正确解标签对应的输出越大，交叉熵误差的值就越接近0，反之则越大，输出为1时，交叉熵误差为0，log函数图像如下\" class=\"headerlink\" title=\"正确解标签对应的输出越大，交叉熵误差的值就越接近0，反之则越大，输出为1时，交叉熵误差为0，log函数图像如下\"></a>正确解标签对应的输出越大，交叉熵误差的值就越接近0，反之则越大，输出为1时，交叉熵误差为0，log函数图像如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/3/1.png\"></p>\n</blockquote>\n<h4 id=\"mini-batch学习\"><a href=\"#mini-batch学习\" class=\"headerlink\" title=\"mini-batch学习\"></a>mini-batch学习</h4><blockquote>\n<h4 id=\"机器学习使用训练数据进行学习，其本质就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数，所以计算损失函数时必须将所有的训练数据作为对象\"><a href=\"#机器学习使用训练数据进行学习，其本质就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数，所以计算损失函数时必须将所有的训练数据作为对象\" class=\"headerlink\" title=\"机器学习使用训练数据进行学习，其本质就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数，所以计算损失函数时必须将所有的训练数据作为对象\"></a>机器学习使用训练数据进行学习，其本质就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数，所以计算损失函数时必须将所有的训练数据作为对象</h4><h4 id=\"前面提到的损失函数都是针对的单个数据，若要求所有的训练数据的损失函数的总和，以交叉熵误差为例，损失函数可以写为：\"><a href=\"#前面提到的损失函数都是针对的单个数据，若要求所有的训练数据的损失函数的总和，以交叉熵误差为例，损失函数可以写为：\" class=\"headerlink\" title=\"前面提到的损失函数都是针对的单个数据，若要求所有的训练数据的损失函数的总和，以交叉熵误差为例，损失函数可以写为：\"></a>前面提到的损失函数都是针对的单个数据，若要求所有的训练数据的损失函数的总和，以交叉熵误差为例，损失函数可以写为：</h4><p>$$<br>E &#x3D; - \\frac{1}{N} \\sum_{n} \\sum_{k} t_{nk} \\log y_{nk}<br>$$</p>\n<h4 id=\"这里是假设有-N-个数据，-t-nk-表示第-n-个数据的第-k-个元素的值，-t-nk-是监督数据，-t-nk-是神经网络的输出\"><a href=\"#这里是假设有-N-个数据，-t-nk-表示第-n-个数据的第-k-个元素的值，-t-nk-是监督数据，-t-nk-是神经网络的输出\" class=\"headerlink\" title=\"这里是假设有$N$个数据，$t_{nk}$ 表示第 $n$ 个数据的第 $k$ 个元素的值，$t_{nk}$ 是监督数据，$t_{nk}$ 是神经网络的输出\"></a>这里是假设有$N$个数据，$t_{nk}$ 表示第 $n$ 个数据的第 $k$ 个元素的值，$t_{nk}$ 是监督数据，$t_{nk}$ 是神经网络的输出</h4><h4 id=\"通过除以-N-（正规化），可以求单个数据的平均损失函数，这样的平均化，可以获得和训练数据的数量无关的统一指标\"><a href=\"#通过除以-N-（正规化），可以求单个数据的平均损失函数，这样的平均化，可以获得和训练数据的数量无关的统一指标\" class=\"headerlink\" title=\"通过除以 $N$ （正规化），可以求单个数据的平均损失函数，这样的平均化，可以获得和训练数据的数量无关的统一指标\"></a>通过除以 $N$ （正规化），可以求单个数据的平均损失函数，这样的平均化，可以获得和训练数据的数量无关的统一指标</h4><h4 id=\"如果将全部的训练数据作为对象求损失函数的总和，那这个计算过程会是非常漫长的，训练数据量非常庞大时，以全部训练数据作为对象计算损失函数更是不现实的\"><a href=\"#如果将全部的训练数据作为对象求损失函数的总和，那这个计算过程会是非常漫长的，训练数据量非常庞大时，以全部训练数据作为对象计算损失函数更是不现实的\" class=\"headerlink\" title=\"如果将全部的训练数据作为对象求损失函数的总和，那这个计算过程会是非常漫长的，训练数据量非常庞大时，以全部训练数据作为对象计算损失函数更是不现实的\"></a>如果将全部的训练数据作为对象求损失函数的总和，那这个计算过程会是非常漫长的，训练数据量非常庞大时，以全部训练数据作为对象计算损失函数更是不现实的</h4><h4 id=\"因此，从全部训练数据中选择一部分，作为全部训练数据的“近似”\"><a href=\"#因此，从全部训练数据中选择一部分，作为全部训练数据的“近似”\" class=\"headerlink\" title=\"因此，从全部训练数据中选择一部分，作为全部训练数据的“近似”\"></a>因此，从全部训练数据中选择一部分，作为全部训练数据的“近似”</h4><h4 id=\"神经网络的学习也是从训练数据中选出一批数据（称为mini-batch-小批量），然后对每个mini-batch进行学习，用这一批数据的结果来近似全部数据的结果，这种学习方式就是所谓的-mini-batch学习\"><a href=\"#神经网络的学习也是从训练数据中选出一批数据（称为mini-batch-小批量），然后对每个mini-batch进行学习，用这一批数据的结果来近似全部数据的结果，这种学习方式就是所谓的-mini-batch学习\" class=\"headerlink\" title=\"神经网络的学习也是从训练数据中选出一批数据（称为mini-batch,小批量），然后对每个mini-batch进行学习，用这一批数据的结果来近似全部数据的结果，这种学习方式就是所谓的 mini-batch学习\"></a>神经网络的学习也是从训练数据中选出一批数据（称为mini-batch,小批量），然后对每个mini-batch进行学习，用这一批数据的结果来近似全部数据的结果，这种学习方式就是所谓的 <strong>mini-batch学习</strong></h4></blockquote>\n<h4 id=\"为什么要引入损失函数\"><a href=\"#为什么要引入损失函数\" class=\"headerlink\" title=\"为什么要引入损失函数\"></a>为什么要引入损失函数</h4><blockquote>\n<h4 id=\"为了找到使损失函数的值尽可能小的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值\"><a href=\"#为了找到使损失函数的值尽可能小的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值\" class=\"headerlink\" title=\"为了找到使损失函数的值尽可能小的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值\"></a>为了找到使损失函数的值尽可能小的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值</h4><h4 id=\"损失函数的导数一般不会有0值，更不会恒为0（阶跃函数除外），而识别精度的导数恒为0或者只有极小部分不为0，这样权重的调整并不会影响其值，这并不是我们想要的\"><a href=\"#损失函数的导数一般不会有0值，更不会恒为0（阶跃函数除外），而识别精度的导数恒为0或者只有极小部分不为0，这样权重的调整并不会影响其值，这并不是我们想要的\" class=\"headerlink\" title=\"损失函数的导数一般不会有0值，更不会恒为0（阶跃函数除外），而识别精度的导数恒为0或者只有极小部分不为0，这样权重的调整并不会影响其值，这并不是我们想要的\"></a>损失函数的导数一般不会有0值，更不会恒为0（阶跃函数除外），而识别精度的导数恒为0或者只有极小部分不为0，这样权重的调整并不会影响其值，这并不是我们想要的</h4></blockquote>\n</blockquote>\n<h3 id=\"数值微分（数值梯度）\"><a href=\"#数值微分（数值梯度）\" class=\"headerlink\" title=\"数值微分（数值梯度）\"></a>数值微分（数值梯度）</h3><blockquote>\n<h4 id=\"梯度法使用梯度的信息决定前进的方向\"><a href=\"#梯度法使用梯度的信息决定前进的方向\" class=\"headerlink\" title=\"梯度法使用梯度的信息决定前进的方向\"></a>梯度法使用梯度的信息决定前进的方向</h4><h4 id=\"利用微小的差分求导数的过程就是所谓的数值微分（numerical-differentiation）\"><a href=\"#利用微小的差分求导数的过程就是所谓的数值微分（numerical-differentiation）\" class=\"headerlink\" title=\"利用微小的差分求导数的过程就是所谓的数值微分（numerical differentiation）\"></a>利用微小的差分求导数的过程就是所谓的数值微分（numerical differentiation）</h4><h4 id=\"基于数学式的推到求导数的过程，则称之为-解析性求解或解析性求导\"><a href=\"#基于数学式的推到求导数的过程，则称之为-解析性求解或解析性求导\" class=\"headerlink\" title=\"基于数学式的推到求导数的过程，则称之为 解析性求解或解析性求导\"></a>基于数学式的推到求导数的过程，则称之为 解析性求解或解析性求导</h4></blockquote>\n<h3 id=\"梯度\"><a href=\"#梯度\" class=\"headerlink\" title=\"梯度\"></a>梯度</h3><blockquote>\n<h4 id=\"由全部变量的偏导数汇总成的向量称为梯度\"><a href=\"#由全部变量的偏导数汇总成的向量称为梯度\" class=\"headerlink\" title=\"由全部变量的偏导数汇总成的向量称为梯度\"></a>由全部变量的偏导数汇总成的向量称为梯度</h4><h4 id=\"梯度法\"><a href=\"#梯度法\" class=\"headerlink\" title=\"梯度法\"></a>梯度法</h4><blockquote>\n<h4 id=\"寻找的最优参数是指损失函数取最小值时的参数\"><a href=\"#寻找的最优参数是指损失函数取最小值时的参数\" class=\"headerlink\" title=\"寻找的最优参数是指损失函数取最小值时的参数\"></a>寻找的最优参数是指损失函数取最小值时的参数</h4><h4 id=\"使用梯度来寻找损失函数最小值的方法就是所谓的-梯度法，通过不断地沿梯度方向前进，逐渐减小函数值的过程就是梯度法\"><a href=\"#使用梯度来寻找损失函数最小值的方法就是所谓的-梯度法，通过不断地沿梯度方向前进，逐渐减小函数值的过程就是梯度法\" class=\"headerlink\" title=\"使用梯度来寻找损失函数最小值的方法就是所谓的 梯度法，通过不断地沿梯度方向前进，逐渐减小函数值的过程就是梯度法\"></a>使用梯度来寻找损失函数最小值的方法就是所谓的 梯度法，通过不断地沿梯度方向前进，逐渐减小函数值的过程就是梯度法</h4><h4 id=\"梯度表示的各点处的函数值减小最多的方向\"><a href=\"#梯度表示的各点处的函数值减小最多的方向\" class=\"headerlink\" title=\"梯度表示的各点处的函数值减小最多的方向\"></a>梯度表示的各点处的函数值减小最多的方向</h4><h4 id=\"函数最小值、极小值和鞍点处的梯度为-0（梯度法就是要找梯度为0的地方）\"><a href=\"#函数最小值、极小值和鞍点处的梯度为-0（梯度法就是要找梯度为0的地方）\" class=\"headerlink\" title=\"函数最小值、极小值和鞍点处的梯度为 0（梯度法就是要找梯度为0的地方）\"></a>函数最小值、极小值和鞍点处的梯度为 0（梯度法就是要找梯度为0的地方）</h4><h4 id=\"极小值是局部最小值，鞍点是从某个方向上看是极大值，从另一个方向看是极小值的点\"><a href=\"#极小值是局部最小值，鞍点是从某个方向上看是极大值，从另一个方向看是极小值的点\" class=\"headerlink\" title=\"极小值是局部最小值，鞍点是从某个方向上看是极大值，从另一个方向看是极小值的点\"></a>极小值是局部最小值，鞍点是从某个方向上看是极大值，从另一个方向看是极小值的点</h4><h4 id=\"学习高原：它是指，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入所谓的学习高原而无法前进的停滞期\"><a href=\"#学习高原：它是指，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入所谓的学习高原而无法前进的停滞期\" class=\"headerlink\" title=\"学习高原：它是指，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入所谓的学习高原而无法前进的停滞期\"></a>学习高原：它是指，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入所谓的<strong>学习高原</strong>而无法前进的停滞期</h4><h4 id=\"根据寻找目的（最大值或最小值）的不同，梯度法分为了：梯度下降法和梯度上升法\"><a href=\"#根据寻找目的（最大值或最小值）的不同，梯度法分为了：梯度下降法和梯度上升法\" class=\"headerlink\" title=\"根据寻找目的（最大值或最小值）的不同，梯度法分为了：梯度下降法和梯度上升法\"></a>根据寻找目的（最大值或最小值）的不同，梯度法分为了：梯度下降法和梯度上升法</h4><ul>\n<li><h4 id=\"梯度下降法：寻找最小值的梯度法\"><a href=\"#梯度下降法：寻找最小值的梯度法\" class=\"headerlink\" title=\"梯度下降法：寻找最小值的梯度法\"></a>梯度下降法：寻找最小值的梯度法</h4></li>\n<li><h4 id=\"梯度上升法：寻找最大值的梯度法\"><a href=\"#梯度上升法：寻找最大值的梯度法\" class=\"headerlink\" title=\"梯度上升法：寻找最大值的梯度法\"></a>梯度上升法：寻找最大值的梯度法</h4></li>\n</ul>\n<h4 id=\"通过反转损失函数的符号，求最小值和求最大值的问题可以变成相同的问题\"><a href=\"#通过反转损失函数的符号，求最小值和求最大值的问题可以变成相同的问题\" class=\"headerlink\" title=\"通过反转损失函数的符号，求最小值和求最大值的问题可以变成相同的问题\"></a>通过反转损失函数的符号，求最小值和求最大值的问题可以变成相同的问题</h4><h4 id=\"神经网络（深度学习）中，梯度法一般是指的梯度下降法\"><a href=\"#神经网络（深度学习）中，梯度法一般是指的梯度下降法\" class=\"headerlink\" title=\"神经网络（深度学习）中，梯度法一般是指的梯度下降法\"></a>神经网络（深度学习）中，梯度法一般是指的梯度下降法</h4><h4 id=\"学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数\"><a href=\"#学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数\" class=\"headerlink\" title=\"学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数\"></a>学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数</h4><h4 id=\"在神经网络的学习中，一般会一边改变学习率的值，一边确认学习是否正确进行了\"><a href=\"#在神经网络的学习中，一般会一边改变学习率的值，一边确认学习是否正确进行了\" class=\"headerlink\" title=\"在神经网络的学习中，一般会一边改变学习率的值，一边确认学习是否正确进行了\"></a>在神经网络的学习中，一般会一边改变学习率的值，一边确认学习是否正确进行了</h4><h4 id=\"学习率这样的参数也被称为超参数，需要人工设定\"><a href=\"#学习率这样的参数也被称为超参数，需要人工设定\" class=\"headerlink\" title=\"学习率这样的参数也被称为超参数，需要人工设定\"></a>学习率这样的参数也被称为<strong>超参数</strong>，需要人工设定</h4></blockquote>\n<h4 id=\"神经网络的梯度\"><a href=\"#神经网络的梯度\" class=\"headerlink\" title=\"神经网络的梯度\"></a>神经网络的梯度</h4><blockquote>\n<h4 id=\"这里的梯度指的是损失函数关于权重参数的梯度\"><a href=\"#这里的梯度指的是损失函数关于权重参数的梯度\" class=\"headerlink\" title=\"这里的梯度指的是损失函数关于权重参数的梯度\"></a>这里的梯度指的是损失函数关于权重参数的梯度</h4><h4 id=\"例子说明，一个神经网络的权重W形状为-2x3，损失函数用L表示，此时梯度可以用-frac-partial-L-partial-W-来表示，数学表示式如下\"><a href=\"#例子说明，一个神经网络的权重W形状为-2x3，损失函数用L表示，此时梯度可以用-frac-partial-L-partial-W-来表示，数学表示式如下\" class=\"headerlink\" title=\"例子说明，一个神经网络的权重W形状为 2x3，损失函数用L表示，此时梯度可以用 $\\frac{\\partial L}{\\partial W}$ 来表示，数学表示式如下\"></a>例子说明，一个神经网络的权重W形状为 2x3，损失函数用L表示，此时梯度可以用 $\\frac{\\partial L}{\\partial W}$ 来表示，数学表示式如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/3/2.png\"></p>\n<h4 id=\"求出神经网络的梯度后，接下来只需根据梯度法，更新权重参数即可\"><a href=\"#求出神经网络的梯度后，接下来只需根据梯度法，更新权重参数即可\" class=\"headerlink\" title=\"求出神经网络的梯度后，接下来只需根据梯度法，更新权重参数即可\"></a>求出神经网络的梯度后，接下来只需根据梯度法，更新权重参数即可</h4></blockquote>\n<h4 id=\"学习算法的实现\"><a href=\"#学习算法的实现\" class=\"headerlink\" title=\"学习算法的实现\"></a>学习算法的实现</h4><blockquote>\n<h4 id=\"神经网络的学习的步骤\"><a href=\"#神经网络的学习的步骤\" class=\"headerlink\" title=\"神经网络的学习的步骤\"></a>神经网络的学习的步骤</h4><blockquote>\n<h4 id=\"前提是：神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”\"><a href=\"#前提是：神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”\" class=\"headerlink\" title=\"前提是：神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”\"></a>前提是：神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”</h4><ol>\n<li><h4 id=\"（mini-batch）从训练数据中随机选出一部分数据，这部分数据称为mini-batch，目标是是减小mini-batch的损失函数的值\"><a href=\"#（mini-batch）从训练数据中随机选出一部分数据，这部分数据称为mini-batch，目标是是减小mini-batch的损失函数的值\" class=\"headerlink\" title=\"（mini-batch）从训练数据中随机选出一部分数据，这部分数据称为mini-batch，目标是是减小mini-batch的损失函数的值\"></a>（mini-batch）从训练数据中随机选出一部分数据，这部分数据称为mini-batch，目标是是减小mini-batch的损失函数的值</h4></li>\n<li><h4 id=\"（计算梯度）为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度，梯度表示损失函数的值减小最多的方向\"><a href=\"#（计算梯度）为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度，梯度表示损失函数的值减小最多的方向\" class=\"headerlink\" title=\"（计算梯度）为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度，梯度表示损失函数的值减小最多的方向\"></a>（计算梯度）为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度，梯度表示损失函数的值减小最多的方向</h4></li>\n<li><h4 id=\"（更新参数）将权重参数沿梯度方向进行微小更新\"><a href=\"#（更新参数）将权重参数沿梯度方向进行微小更新\" class=\"headerlink\" title=\"（更新参数）将权重参数沿梯度方向进行微小更新\"></a>（更新参数）将权重参数沿梯度方向进行微小更新</h4></li>\n<li><h4 id=\"（重复）重复前三步\"><a href=\"#（重复）重复前三步\" class=\"headerlink\" title=\"（重复）重复前三步\"></a>（重复）重复前三步</h4></li>\n</ol>\n<h4 id=\"因为更新权重参数采用的梯度法是梯度下降法，又是随机选择的mini-batch数据，所以称之为随机梯度下降法（stochastic-gradient-descent，SGD）\"><a href=\"#因为更新权重参数采用的梯度法是梯度下降法，又是随机选择的mini-batch数据，所以称之为随机梯度下降法（stochastic-gradient-descent，SGD）\" class=\"headerlink\" title=\"因为更新权重参数采用的梯度法是梯度下降法，又是随机选择的mini-batch数据，所以称之为随机梯度下降法（stochastic gradient descent，SGD）\"></a>因为更新权重参数采用的梯度法是梯度下降法，又是随机选择的mini-batch数据，所以称之为随机梯度下降法（stochastic gradient descent，SGD）</h4></blockquote>\n<h4 id=\"手写数字识别的神经网络的实现\"><a href=\"#手写数字识别的神经网络的实现\" class=\"headerlink\" title=\"手写数字识别的神经网络的实现\"></a>手写数字识别的神经网络的实现</h4><h4 id=\"基于测试数据的评价\"><a href=\"#基于测试数据的评价\" class=\"headerlink\" title=\"基于测试数据的评价\"></a>基于测试数据的评价</h4><blockquote>\n<h4 id=\"神经网络的学习中，必须确认是否能够正确识别训练数据以外的其他数-据，即确认是否会发生过拟合\"><a href=\"#神经网络的学习中，必须确认是否能够正确识别训练数据以外的其他数-据，即确认是否会发生过拟合\" class=\"headerlink\" title=\"神经网络的学习中，必须确认是否能够正确识别训练数据以外的其他数 据，即确认是否会发生过拟合\"></a>神经网络的学习中，必须确认是否能够正确识别训练数据以外的其他数 据，即确认是否会发生过拟合</h4><h4 id=\"过拟合是指，虽然训练数据中的数字图像能被正确辨别，但是不在训练数据中的数字图像却无法被识别的现象\"><a href=\"#过拟合是指，虽然训练数据中的数字图像能被正确辨别，但是不在训练数据中的数字图像却无法被识别的现象\" class=\"headerlink\" title=\"过拟合是指，虽然训练数据中的数字图像能被正确辨别，但是不在训练数据中的数字图像却无法被识别的现象\"></a>过拟合是指，虽然训练数据中的数字图像能被正确辨别，但是不在训练数据中的数字图像却无法被识别的现象</h4><h4 id=\"要评价神经网络的泛化能力，就必须使用不包含在训练数据中的数据，这就要用到测试数据了\"><a href=\"#要评价神经网络的泛化能力，就必须使用不包含在训练数据中的数据，这就要用到测试数据了\" class=\"headerlink\" title=\"要评价神经网络的泛化能力，就必须使用不包含在训练数据中的数据，这就要用到测试数据了\"></a>要评价神经网络的泛化能力，就必须使用不包含在训练数据中的数据，这就要用到测试数据了</h4><h4 id=\"另外又引入了一个新的概念——epoch\"><a href=\"#另外又引入了一个新的概念——epoch\" class=\"headerlink\" title=\"另外又引入了一个新的概念——epoch\"></a>另外又引入了一个新的概念——epoch</h4><blockquote>\n<h4 id=\"epoch是一个单位，一个epoch表示学习中所有训练数据均被使用过一次时的更新次数\"><a href=\"#epoch是一个单位，一个epoch表示学习中所有训练数据均被使用过一次时的更新次数\" class=\"headerlink\" title=\"epoch是一个单位，一个epoch表示学习中所有训练数据均被使用过一次时的更新次数\"></a>epoch是一个单位，一个epoch表示学习中所有训练数据均被使用过一次时的更新次数</h4><h4 id=\"如，对于10000笔训练数据，用大小为-100-笔数据的mini-batch进行学习时，重复随机梯度下降法-100次，所有的训练数据就都被“看过”了，此时，100次就是一个-epoch\"><a href=\"#如，对于10000笔训练数据，用大小为-100-笔数据的mini-batch进行学习时，重复随机梯度下降法-100次，所有的训练数据就都被“看过”了，此时，100次就是一个-epoch\" class=\"headerlink\" title=\"如，对于10000笔训练数据，用大小为 100 笔数据的mini-batch进行学习时，重复随机梯度下降法 100次，所有的训练数据就都被“看过”了，此时，100次就是一个 epoch\"></a>如，对于10000笔训练数据，用大小为 100 笔数据的mini-batch进行学习时，重复随机梯度下降法 100次，所有的训练数据就都被“看过”了，此时，100次就是一个 epoch</h4><h4 id=\"遍历一次所有数据，就成为一个epoch\"><a href=\"#遍历一次所有数据，就成为一个epoch\" class=\"headerlink\" title=\"遍历一次所有数据，就成为一个epoch\"></a>遍历一次所有数据，就成为一个epoch</h4></blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n<h3 id=\"code\"><a href=\"#code\" class=\"headerlink\" title=\"code\"></a>code</h3><blockquote>\n<h4 id=\"均方误差的实现\"><a href=\"#均方误差的实现\" class=\"headerlink\" title=\"均方误差的实现\"></a>均方误差的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">mean_squared_error</span>(<span class=\"params\">y, t</span>):</span><br><span class=\"line\"><span class=\"keyword\">return</span> <span class=\"number\">0.5</span> * np.<span class=\"built_in\">sum</span>((y - t)**<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设2为正确解</span></span><br><span class=\"line\">t = np.array([<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>])</span><br><span class=\"line\">y = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.05</span>, <span class=\"number\">0.6</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.05</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.0</span>])</span><br><span class=\"line\">result = mean_squared_error(y, t)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(result)</span><br><span class=\"line\"></span><br><span class=\"line\">y = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.05</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.05</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.6</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.0</span>])</span><br><span class=\"line\">result_ = mean_squared_error(y, t)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(result_)</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"交叉熵误差的实现\"><a href=\"#交叉熵误差的实现\" class=\"headerlink\" title=\"交叉熵误差的实现\"></a>交叉熵误差的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">cross_entropy_error</span>(<span class=\"params\">y, t</span>):</span><br><span class=\"line\"><span class=\"comment\"># 加上delta是为了避免出现 log(0)，导致出现负无穷</span></span><br><span class=\"line\">delta = <span class=\"number\">1e-7</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> -np.<span class=\"built_in\">sum</span>(t * np.log(y + delta))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">t = np.array([<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>])</span><br><span class=\"line\">y = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.05</span>, <span class=\"number\">0.6</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.05</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.0</span>])</span><br><span class=\"line\">result = cross_entropy_error(y, t)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(result)</span><br><span class=\"line\"></span><br><span class=\"line\">y = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.05</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.05</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.6</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.0</span>])</span><br><span class=\"line\">result_ = cross_entropy_error(y, t)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(result_)</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"使用mini-batch学习的手写数字识别\"><a href=\"#使用mini-batch学习的手写数字识别\" class=\"headerlink\" title=\"使用mini-batch学习的手写数字识别\"></a>使用mini-batch学习的手写数字识别</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os, sys</span><br><span class=\"line\">sys.path.append(os.pardir)\t<span class=\"comment\"># 将父目录添加到系统路径，以便导入自定义模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.function <span class=\"keyword\">import</span> sigmoid, softmax</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载MNIST数据集</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_data</span>():</span><br><span class=\"line\"><span class=\"comment\"># normalize=True:将图像的像素值正规化到0.0~1.0</span></span><br><span class=\"line\"><span class=\"comment\"># flatten=True:将图像从2维数组转为1维数组</span></span><br><span class=\"line\"><span class=\"comment\"># one_hot_label=False:标签为0~9的数字,而不是one-hot编码</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>, flatten=<span class=\"literal\">True</span>, one_hot_label=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">return</span> x_test, t_test</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 初始化神经网络</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_network</span>():</span><br><span class=\"line\"><span class=\"comment\"># 从文件加载预训练的网络参数（权重和偏置）</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&quot;sample_weight.pkl&quot;</span>, rb)<span class=\"keyword\">as</span> f:</span><br><span class=\"line\">network = pickle.load(f)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> network</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 神经网络的预测函数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">network, x</span>):</span><br><span class=\"line\"><span class=\"comment\"># x 是输入数据</span></span><br><span class=\"line\"><span class=\"comment\"># 获取网络的权重和偏置</span></span><br><span class=\"line\">w1, w2, w3 = network[<span class=\"string\">&#x27;W1&#x27;</span>], network[<span class=\"string\">&#x27;W2&#x27;</span>], network[<span class=\"string\">&#x27;W3&#x27;</span>]</span><br><span class=\"line\">b1, b2, b3 = network[<span class=\"string\">&#x27;b1&#x27;</span>], network[<span class=\"string\">&#x27;b2&#x27;</span>], network[<span class=\"string\">&#x27;b3&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 向前传播</span></span><br><span class=\"line\">a1 = np.dot(x, w1) + b1</span><br><span class=\"line\">z1 = sigmoid(a1)</span><br><span class=\"line\">a2 = np.dot(z1, w2) + b2</span><br><span class=\"line\">z2 = sigmoid(a2)</span><br><span class=\"line\">a3 = np.dot(z2, w3) + b3</span><br><span class=\"line\">y = softmax(a3)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">x, t = get_data()</span><br><span class=\"line\">network = init_network()</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">100</span>\t<span class=\"comment\"># 设置批处理大小</span></span><br><span class=\"line\">accuracy_cnt = <span class=\"number\">0</span>\t<span class=\"comment\"># 初始化准确率计数器</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用批处理进行预测</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, <span class=\"built_in\">len</span>(x), batch_size):</span><br><span class=\"line\">x_batch = x[i:i+batch_size]\t<span class=\"comment\"># 获取当前批次输入的数据</span></span><br><span class=\"line\">y_batch = predict(network, x_batch)\t<span class=\"comment\"># 对当前批次进行预测</span></span><br><span class=\"line\">p = np.argmax(y_batch, axis=<span class=\"number\">1</span>)\t<span class=\"comment\"># 获取每个样本预测概率最大的类别</span></span><br><span class=\"line\">accuracy_cnt += np.<span class=\"built_in\">sum</span>(p == t[i:i+batch_size])\t<span class=\"comment\"># 统计预测正确的样本数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出最终的识别准确率    </span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Accuracy: &quot;</span> + <span class=\"built_in\">str</span>(<span class=\"built_in\">float</span>(accuracy_cnt) / <span class=\"built_in\">len</span>(x)))</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"数值微分\"><a href=\"#数值微分\" class=\"headerlink\" title=\"数值微分\"></a>数值微分</h4><blockquote>\n<h4 id=\"导数的实现\"><a href=\"#导数的实现\" class=\"headerlink\" title=\"导数的实现\"></a>导数的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">numerical_diff</span>(<span class=\"params\">f, x</span>):</span><br><span class=\"line\"> h = <span class=\"number\">1e-4</span></span><br><span class=\"line\"> <span class=\"keyword\">return</span> (f(h + x) - f(x)) / h</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"上面这样计算函数f的差分是不准确的，它计算的是-x-h-和-x-之间的斜率，它与实际要计算的导数是有差异的，这一差异是因h不可能无限接近0导致的，这种属于是前向差分\"><a href=\"#上面这样计算函数f的差分是不准确的，它计算的是-x-h-和-x-之间的斜率，它与实际要计算的导数是有差异的，这一差异是因h不可能无限接近0导致的，这种属于是前向差分\" class=\"headerlink\" title=\"上面这样计算函数f的差分是不准确的，它计算的是(x+h)和(x)之间的斜率，它与实际要计算的导数是有差异的，这一差异是因h不可能无限接近0导致的，这种属于是前向差分\"></a>上面这样计算函数f的差分是不准确的，它计算的是(x+h)和(x)之间的斜率，它与实际要计算的导数是有差异的，这一差异是因h不可能无限接近0导致的，这种属于是前向差分</h4><h4 id=\"正确的应该是-x-h-和-x-h-之间的差分，以x为中心，这也称之为中心差分\"><a href=\"#正确的应该是-x-h-和-x-h-之间的差分，以x为中心，这也称之为中心差分\" class=\"headerlink\" title=\"正确的应该是 (x+h)和(x-h)之间的差分，以x为中心，这也称之为中心差分\"></a>正确的应该是 (x+h)和(x-h)之间的差分，以x为中心，这也称之为中心差分</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">numerical_diff</span>(<span class=\"params\">f, x</span>):</span><br><span class=\"line\"> h = <span class=\"number\">1e-4</span></span><br><span class=\"line\"> <span class=\"keyword\">return</span> (f(x+h) - f(x-h)) / (<span class=\"number\">2</span> * h)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"举个数值微分实例，y-0-01X-2-0-1x\"><a href=\"#举个数值微分实例，y-0-01X-2-0-1x\" class=\"headerlink\" title=\"举个数值微分实例，y &#x3D; 0.01X^2 + 0.1x\"></a>举个数值微分实例，y &#x3D; 0.01X^2 + 0.1x</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">fun_1</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\"> <span class=\"keyword\">return</span> <span class=\"number\">0.01</span>*x**<span class=\"number\">2</span> + <span class=\"number\">0.1</span> * x</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"绘制其图像\"><a href=\"#绘制其图像\" class=\"headerlink\" title=\"绘制其图像\"></a>绘制其图像</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pylab <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">x = np.arange(<span class=\"number\">0.0</span>, <span class=\"number\">20.0</span>, <span class=\"number\">0.1</span>)</span><br><span class=\"line\">y = fun_1(x)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;x&quot;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;f(x)&quot;</span>)</span><br><span class=\"line\">plt.plot(x, y)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"计算x-5和x-10处的导数，并画出其图像（含切线）\"><a href=\"#计算x-5和x-10处的导数，并画出其图像（含切线）\" class=\"headerlink\" title=\"计算x&#x3D;5和x&#x3D;10处的导数，并画出其图像（含切线）\"></a>计算x&#x3D;5和x&#x3D;10处的导数，并画出其图像（含切线）</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = numerical_diff(fun_1, <span class=\"number\">5</span>)</span><br><span class=\"line\">b = numerical_diff(fun_1, <span class=\"number\">10</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(a)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(b)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">tangent_line</span>(<span class=\"params\">f, x</span>):</span><br><span class=\"line\"> d = numerical_diff(f, x)</span><br><span class=\"line\"> <span class=\"built_in\">print</span>(d)</span><br><span class=\"line\"> y = f(x) - d*x</span><br><span class=\"line\"> <span class=\"keyword\">return</span> <span class=\"keyword\">lambda</span> t: d*t + y</span><br><span class=\"line\"></span><br><span class=\"line\">x = np.arange(<span class=\"number\">0.0</span>, <span class=\"number\">20.0</span>, <span class=\"number\">0.1</span>)</span><br><span class=\"line\">y = fun_1(x)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;x&quot;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;f(x)&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置切点坐标</span></span><br><span class=\"line\">tangent_x = <span class=\"number\">5</span></span><br><span class=\"line\">tangent_y = fun_1(tangent_x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制原函数和切线图像</span></span><br><span class=\"line\">tf = tangent_line(fun_1, tangent_x)</span><br><span class=\"line\">y2 = tf(x)</span><br><span class=\"line\">plt.plot(x, y)</span><br><span class=\"line\">plt.plot(x, y2)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制切点处辅助线</span></span><br><span class=\"line\"><span class=\"comment\"># 垂直x轴的</span></span><br><span class=\"line\">plt.axvline(x=tangent_x, color=<span class=\"string\">&#x27;gray&#x27;</span>, linestyle=<span class=\"string\">&#x27;--&#x27;</span>, alpha=<span class=\"number\">0.5</span>)</span><br><span class=\"line\"><span class=\"comment\"># 垂直y轴的</span></span><br><span class=\"line\">plt.axhline(y=tangent_y, color=<span class=\"string\">&#x27;gray&#x27;</span>, linestyle=<span class=\"string\">&#x27;--&#x27;</span>, alpha=<span class=\"number\">0.5</span>)</span><br><span class=\"line\"><span class=\"comment\"># 标记切点(红色圆点)</span></span><br><span class=\"line\">plt.plot(tangent_x, tangent_y, <span class=\"string\">&#x27;ro&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 添加坐标标注</span></span><br><span class=\"line\">plt.text(tangent_x, tangent_y, <span class=\"string\">f&#x27;(<span class=\"subst\">&#123;tangent_x:<span class=\"number\">.1</span>f&#125;</span>, <span class=\"subst\">&#123;tangent_y:<span class=\"number\">.2</span>f&#125;</span>)&#x27;</span>, </span><br><span class=\"line\">      horizontalalignment=<span class=\"string\">&#x27;right&#x27;</span>, verticalalignment=<span class=\"string\">&#x27;bottom&#x27;</span>)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置切点坐标</span></span><br><span class=\"line\">tangent_x = <span class=\"number\">10</span></span><br><span class=\"line\">tangent_y = fun_1(tangent_x)</span><br><span class=\"line\"></span><br><span class=\"line\">tf = tangent_line(fun_1, tangent_x)</span><br><span class=\"line\">y3 = tf(x)</span><br><span class=\"line\">plt.plot(x, y)</span><br><span class=\"line\">plt.plot(x, y3)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制切点处辅助线</span></span><br><span class=\"line\"><span class=\"comment\"># 垂直x轴的</span></span><br><span class=\"line\">plt.axvline(x=tangent_x, color=<span class=\"string\">&#x27;gray&#x27;</span>, linestyle=<span class=\"string\">&#x27;--&#x27;</span>, alpha=<span class=\"number\">0.5</span>)</span><br><span class=\"line\"><span class=\"comment\"># 垂直y轴的</span></span><br><span class=\"line\">plt.axhline(y=tangent_y, color=<span class=\"string\">&#x27;gray&#x27;</span>, linestyle=<span class=\"string\">&#x27;--&#x27;</span>, alpha=<span class=\"number\">0.5</span>)</span><br><span class=\"line\"><span class=\"comment\"># 标记切点(红色圆点)</span></span><br><span class=\"line\">plt.plot(tangent_x, tangent_y, <span class=\"string\">&#x27;ro&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 添加坐标标注</span></span><br><span class=\"line\">plt.text(tangent_x, tangent_y, <span class=\"string\">f&#x27;(<span class=\"subst\">&#123;tangent_x:<span class=\"number\">.1</span>f&#125;</span>, <span class=\"subst\">&#123;tangent_y:<span class=\"number\">.2</span>f&#125;</span>)&#x27;</span>, </span><br><span class=\"line\">      horizontalalignment=<span class=\"string\">&#x27;right&#x27;</span>, verticalalignment=<span class=\"string\">&#x27;bottom&#x27;</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"偏导数\"><a href=\"#偏导数\" class=\"headerlink\" title=\"偏导数\"></a>偏导数</h4><blockquote>\n<h4 id=\"求-f-x-0-x-1-x-0-2-x-1-2-的偏导，并画出其函数图像\"><a href=\"#求-f-x-0-x-1-x-0-2-x-1-2-的偏导，并画出其函数图像\" class=\"headerlink\" title=\"求 $f(x_0,x_1)&#x3D;x_0^2+x_1^2$的偏导，并画出其函数图像\"></a>求 $f(x_0,x_1)&#x3D;x_0^2+x_1^2$的偏导，并画出其函数图像</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> mpl_toolkits.mplot3d <span class=\"keyword\">import</span> Axes3D</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义网格</span></span><br><span class=\"line\">x0 = np.linspace(-<span class=\"number\">3</span>, <span class=\"number\">3</span>, <span class=\"number\">100</span>)</span><br><span class=\"line\">x1 = np.linspace(-<span class=\"number\">3</span>, <span class=\"number\">3</span>, <span class=\"number\">100</span>)</span><br><span class=\"line\">X0, X1 = np.meshgrid(x0, x1)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义函数</span></span><br><span class=\"line\">Z = X0**<span class=\"number\">2</span> + X1**<span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建图形窗口</span></span><br><span class=\"line\">fig = plt.figure(figsize=(<span class=\"number\">14</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 1. 三维图像</span></span><br><span class=\"line\">ax1 = fig.add_subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>, projection=<span class=\"string\">&#x27;3d&#x27;</span>)</span><br><span class=\"line\">ax1.plot_surface(X0, X1, Z, cmap=<span class=\"string\">&#x27;viridis&#x27;</span>, edgecolor=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">ax1.set_title(<span class=\"string\">r&#x27;$f(x_0, x_1) = x_0^2 + x_1^2$&#x27;</span>)</span><br><span class=\"line\">ax1.set_xlabel(<span class=\"string\">&#x27;$x_0$&#x27;</span>)</span><br><span class=\"line\">ax1.set_ylabel(<span class=\"string\">&#x27;$x_1$&#x27;</span>)</span><br><span class=\"line\">ax1.set_zlabel(<span class=\"string\">&#x27;$f(x_0, x_1)$&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 等高线图 + 梯度向量</span></span><br><span class=\"line\">ax2 = fig.add_subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">contour = ax2.contourf(X0, X1, Z, levels=<span class=\"number\">30</span>, cmap=<span class=\"string\">&#x27;viridis&#x27;</span>)</span><br><span class=\"line\">ax2.set_title(<span class=\"string\">&#x27;Contour and Gradient&#x27;</span>)</span><br><span class=\"line\">ax2.set_xlabel(<span class=\"string\">&#x27;$x_0$&#x27;</span>)</span><br><span class=\"line\">ax2.set_ylabel(<span class=\"string\">&#x27;$x_1$&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算梯度（偏导）</span></span><br><span class=\"line\">grad_x0 = <span class=\"number\">2</span> * X0</span><br><span class=\"line\">grad_x1 = <span class=\"number\">2</span> * X1</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制梯度向量（用 quiver 画箭头）</span></span><br><span class=\"line\">skip = <span class=\"number\">5</span>  <span class=\"comment\"># 降低密度</span></span><br><span class=\"line\">ax2.quiver(X0[::skip, ::skip], X1[::skip, ::skip],</span><br><span class=\"line\">           grad_x0[::skip, ::skip], grad_x1[::skip, ::skip],</span><br><span class=\"line\">           color=<span class=\"string\">&#x27;white&#x27;</span>, alpha=<span class=\"number\">0.8</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.colorbar(contour, ax=ax2, label=<span class=\"string\">&#x27;Function Value&#x27;</span>)</span><br><span class=\"line\">plt.tight_layout()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n</blockquote>\n</blockquote>\n<h4 id=\"梯度-1\"><a href=\"#梯度-1\" class=\"headerlink\" title=\"梯度\"></a>梯度</h4><blockquote>\n<h4 id=\"简单的梯度的实现\"><a href=\"#简单的梯度的实现\" class=\"headerlink\" title=\"简单的梯度的实现\"></a>简单的梯度的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 梯度的实现</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">numerical_gradient</span>(<span class=\"params\">f, x</span>):</span><br><span class=\"line\">    h = <span class=\"number\">1e-4</span></span><br><span class=\"line\">    <span class=\"comment\"># 生成和x形状相同的数组，且所有元素为0</span></span><br><span class=\"line\">    grad = np.zeros_like(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> idx <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(x.size):</span><br><span class=\"line\">        tmp_val = x[idx]</span><br><span class=\"line\">        <span class=\"comment\"># f(x+h)的计算</span></span><br><span class=\"line\">        x[idx] = tmp_val + h</span><br><span class=\"line\">        fxh1 = f(x)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># f(x-h)的计算</span></span><br><span class=\"line\">        x[idx] = tmp_val - h</span><br><span class=\"line\">        fxh2 = f(x)</span><br><span class=\"line\"></span><br><span class=\"line\">        grad[idx] = (fxh1 - fxh2) / (<span class=\"number\">2</span> * h)</span><br><span class=\"line\">        x[idx] = tmp_val <span class=\"comment\"># 还原值</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> grad</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(numerical_gradient(fun_2, np.array([<span class=\"number\">3.0</span>, <span class=\"number\">4.0</span>])))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(numerical_gradient(fun_2, np.array([<span class=\"number\">0.0</span>, <span class=\"number\">2.0</span>])))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(numerical_gradient(fun_2, np.array([<span class=\"number\">3.0</span>, <span class=\"number\">0.0</span>])))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"这里给出一个计算任意维数组的数值梯度\"><a href=\"#这里给出一个计算任意维数组的数值梯度\" class=\"headerlink\" title=\"这里给出一个计算任意维数组的数值梯度\"></a>这里给出一个计算任意维数组的数值梯度</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">numerical_gradient</span>(<span class=\"params\">f, x</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    计算任意维数组的数值梯度</span></span><br><span class=\"line\"><span class=\"string\">    参数:</span></span><br><span class=\"line\"><span class=\"string\">        f: 目标函数</span></span><br><span class=\"line\"><span class=\"string\">        x: 任意维度的数组</span></span><br><span class=\"line\"><span class=\"string\">    返回:</span></span><br><span class=\"line\"><span class=\"string\">        梯度数组，形状与x相同</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    实现原理：</span></span><br><span class=\"line\"><span class=\"string\">    1. 使用中心差分法计算梯度：(f(x+h) - f(x-h)) / (2h)</span></span><br><span class=\"line\"><span class=\"string\">    2. 对数组中的每个元素分别计算梯度</span></span><br><span class=\"line\"><span class=\"string\">    3. 使用numpy的迭代器处理任意维度的数组</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    h = <span class=\"number\">1e-4</span>  <span class=\"comment\"># 0.0001，微小的变化量</span></span><br><span class=\"line\">    grad = np.zeros_like(x)  <span class=\"comment\"># 创建与x形状相同的零数组，用于存储梯度</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 使用numpy的迭代器遍历数组的所有元素</span></span><br><span class=\"line\">    <span class=\"comment\"># flags=[&#x27;multi_index&#x27;]: 获取多维索引</span></span><br><span class=\"line\">    <span class=\"comment\"># op_flags=[&#x27;readwrite&#x27;]: 允许读写操作</span></span><br><span class=\"line\">    it = np.nditer(x, flags=[<span class=\"string\">&#x27;multi_index&#x27;</span>], op_flags=[<span class=\"string\">&#x27;readwrite&#x27;</span>])</span><br><span class=\"line\">    <span class=\"keyword\">while</span> <span class=\"keyword\">not</span> it.finished:</span><br><span class=\"line\">        idx = it.multi_index  <span class=\"comment\"># 获取当前元素的多维索引</span></span><br><span class=\"line\">        tmp_val = x[idx]  <span class=\"comment\"># 保存当前值</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 计算f(x+h)</span></span><br><span class=\"line\">        x[idx] = tmp_val + h</span><br><span class=\"line\">        fxh1 = f(x)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 计算f(x-h)</span></span><br><span class=\"line\">        x[idx] = tmp_val - h </span><br><span class=\"line\">        fxh2 = f(x)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 使用中心差分法计算梯度</span></span><br><span class=\"line\">        grad[idx] = (fxh1 - fxh2) / (<span class=\"number\">2</span>*h)</span><br><span class=\"line\">        </span><br><span class=\"line\">        x[idx] = tmp_val  <span class=\"comment\"># 恢复原始值</span></span><br><span class=\"line\">        it.iternext()  <span class=\"comment\"># 移动到下一个元素</span></span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">return</span> grad</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"梯度下降法的实现\"><a href=\"#梯度下降法的实现\" class=\"headerlink\" title=\"梯度下降法的实现\"></a>梯度下降法的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># f 是要进行最优化的函数，init_x是初始值，lr是学习率，step_num是梯度法的重复次数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">gradient_descent</span>(<span class=\"params\">f, init_x, lr=<span class=\"number\">0.01</span>, step_num=<span class=\"number\">100</span></span>):</span><br><span class=\"line\">    x = init_x</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(step_num):</span><br><span class=\"line\">        grad = numerical_gradient(f, x)</span><br><span class=\"line\">        x -= lr * grad</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">init_x = np.array([-<span class=\"number\">3.0</span>, <span class=\"number\">4.0</span>])</span><br><span class=\"line\">result_ = gradient_descent(fun_2, init_x=init_x, lr=<span class=\"number\">0.1</span>, step_num=<span class=\"number\">100</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(result_)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"图像展示梯度法的过程\"><a href=\"#图像展示梯度法的过程\" class=\"headerlink\" title=\"图像展示梯度法的过程\"></a>图像展示梯度法的过程</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">gradient_descent_</span>(<span class=\"params\">f, init_x, lr=<span class=\"number\">0.01</span>, step_num=<span class=\"number\">100</span></span>):</span><br><span class=\"line\">    x = init_x</span><br><span class=\"line\">    x_history = []</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(step_num):</span><br><span class=\"line\">        x_history.append(x.copy())</span><br><span class=\"line\">        </span><br><span class=\"line\">        grad = numerical_gradient(f, x)</span><br><span class=\"line\">        x -= lr * grad</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x, np.array(x_history)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">init_x = np.array([-<span class=\"number\">3.0</span>, <span class=\"number\">4.0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">lr = <span class=\"number\">0.1</span></span><br><span class=\"line\">step_num = <span class=\"number\">20</span></span><br><span class=\"line\">x, x_history = gradient_descent_(fun_2, init_x, lr, step_num)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.plot([-<span class=\"number\">5</span>, <span class=\"number\">5</span>], [<span class=\"number\">0</span>, <span class=\"number\">0</span>], <span class=\"string\">&#x27;--b&#x27;</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>, <span class=\"number\">0</span>], [-<span class=\"number\">5</span>, <span class=\"number\">5</span>], <span class=\"string\">&#x27;--b&#x27;</span>)</span><br><span class=\"line\">plt.plot(x_history[:, <span class=\"number\">0</span>], x_history[:, <span class=\"number\">1</span>], <span class=\"string\">&#x27;o&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.xlim(-<span class=\"number\">3.5</span>, <span class=\"number\">3.5</span>)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">4.5</span>, <span class=\"number\">4.5</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;X0&quot;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;X1&quot;</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h4 id=\"神经网络的梯度-1\"><a href=\"#神经网络的梯度-1\" class=\"headerlink\" title=\"神经网络的梯度\"></a>神经网络的梯度</h4><blockquote>\n<h4 id=\"简单的神经网络\"><a href=\"#简单的神经网络\" class=\"headerlink\" title=\"简单的神经网络\"></a>简单的神经网络</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"><span class=\"comment\"># 导入必要的库</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys, os</span><br><span class=\"line\"><span class=\"comment\"># 将父目录添加到系统路径中，以便能够导入common模块</span></span><br><span class=\"line\">sys.path.append(os.pardir)</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.functions <span class=\"keyword\">import</span> softmax, cross_entropy_error</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.gradient <span class=\"keyword\">import</span> numerical_gradient</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">simpleNet</span>:</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    一个简单的神经网络类</span></span><br><span class=\"line\"><span class=\"string\">    实现了一个2输入3输出的单层神经网络</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 初始化权重矩阵，使用随机数生成2x3的权重矩阵</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.W = np.random.randn(<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        前向传播函数</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            神经网络的输出</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> np.dot(x, <span class=\"variable language_\">self</span>.W)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">loss</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        计算损失函数</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">            t: 正确解标签</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            损失函数的值</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        z = <span class=\"variable language_\">self</span>.predict(x)  <span class=\"comment\"># 获取神经网络的输出</span></span><br><span class=\"line\">        y = softmax(z)       <span class=\"comment\"># 使用softmax函数将输出转换为概率</span></span><br><span class=\"line\">        loss = cross_entropy_error(y, t)  <span class=\"comment\"># 计算交叉熵误差</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> loss</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建测试数据</span></span><br><span class=\"line\">x = np.array([<span class=\"number\">0.6</span>, <span class=\"number\">0.9</span>])  <span class=\"comment\"># 输入数据</span></span><br><span class=\"line\">t = np.array([<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>])   <span class=\"comment\"># 正确解标签</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建神经网络实例</span></span><br><span class=\"line\">net = simpleNet()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义损失函数，用于计算梯度</span></span><br><span class=\"line\">f = <span class=\"keyword\">lambda</span> w: net.loss(x, t)</span><br><span class=\"line\"><span class=\"comment\"># 使用数值微分计算梯度</span></span><br><span class=\"line\">dW = numerical_gradient(f, net.W)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 打印梯度结果</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(dW)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"这里需要注意的是，为什么要在类外定义一个损失函数f，因为类中的损失函数loss-是一个实例方法，需要访问实例的权重self-W，而numerical-gradient-函数参数中的f，需要的是一个纯函数，而不是一个类中的实例方法\"><a href=\"#这里需要注意的是，为什么要在类外定义一个损失函数f，因为类中的损失函数loss-是一个实例方法，需要访问实例的权重self-W，而numerical-gradient-函数参数中的f，需要的是一个纯函数，而不是一个类中的实例方法\" class=\"headerlink\" title=\"这里需要注意的是，为什么要在类外定义一个损失函数f，因为类中的损失函数loss()是一个实例方法，需要访问实例的权重self.W，而numerical_gradient()函数参数中的f，需要的是一个纯函数，而不是一个类中的实例方法\"></a>这里需要注意的是，为什么要在类外定义一个损失函数f，因为类中的损失函数loss()是一个实例方法，需要访问实例的权重self.W，而numerical_gradient()函数参数中的f，需要的是一个纯函数，而不是一个类中的实例方法</h4></blockquote>\n<h4 id=\"手写数字识别的神经网络的实现-1\"><a href=\"#手写数字识别的神经网络的实现-1\" class=\"headerlink\" title=\"手写数字识别的神经网络的实现\"></a>手写数字识别的神经网络的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys, os</span><br><span class=\"line\">sys.path.append(os.pardir)  <span class=\"comment\"># 将父目录添加到系统路径中，以便导入common模块</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> common.functions <span class=\"keyword\">import</span> *</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.gradient <span class=\"keyword\">import</span> numerical_gradient</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">TwoLayerNet</span>:</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    两层神经网络类</span></span><br><span class=\"line\"><span class=\"string\">    实现了一个具有一个隐藏层的神经网络</span></span><br><span class=\"line\"><span class=\"string\">    结构：输入层 -&gt; 隐藏层 -&gt; 输出层</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, input_size, hidden_size, output_size, weight_init_std=<span class=\"number\">0.01</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        初始化神经网络</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            input_size: 输入层神经元数量</span></span><br><span class=\"line\"><span class=\"string\">            hidden_size: 隐藏层神经元数量</span></span><br><span class=\"line\"><span class=\"string\">            output_size: 输出层神经元数量</span></span><br><span class=\"line\"><span class=\"string\">            weight_init_std: 权重初始化的标准差</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"comment\"># 初始化权重和偏置</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params = &#123;&#125;</span><br><span class=\"line\">        <span class=\"comment\"># 第一层权重矩阵，形状为(input_size, hidden_size)</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W1&#x27;</span>] = weight_init_std * np.random.randn(input_size, hidden_size)</span><br><span class=\"line\">        <span class=\"comment\"># 第一层偏置，形状为(hidden_size,)</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b1&#x27;</span>] = np.zeros(hidden_size)</span><br><span class=\"line\">        <span class=\"comment\"># 第二层权重矩阵，形状为(hidden_size, output_size)</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W2&#x27;</span>] = weight_init_std * np.random.randn(hidden_size, output_size)</span><br><span class=\"line\">        <span class=\"comment\"># 第二层偏置，形状为(output_size,)</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b2&#x27;</span>] = np.zeros(output_size)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        前向传播，计算神经网络的输出</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            神经网络的输出（经过softmax后的概率分布）</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        W1, W2 = <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W1&#x27;</span>], <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W2&#x27;</span>]</span><br><span class=\"line\">        b1, b2 = <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b1&#x27;</span>], <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b2&#x27;</span>]</span><br><span class=\"line\">    </span><br><span class=\"line\">        <span class=\"comment\"># 第一层计算</span></span><br><span class=\"line\">        a1 = np.dot(x, W1) + b1  <span class=\"comment\"># 线性变换</span></span><br><span class=\"line\">        z1 = sigmoid(a1)         <span class=\"comment\"># 激活函数</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 第二层计算</span></span><br><span class=\"line\">        a2 = np.dot(z1, W2) + b2  <span class=\"comment\"># 线性变换</span></span><br><span class=\"line\">        y = softmax(a2)           <span class=\"comment\"># 输出层激活函数</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> y</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">loss</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        计算损失函数</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">            t: 正确解标签</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            损失函数的值</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        y = <span class=\"variable language_\">self</span>.predict(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> cross_entropy_error(y, t)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">accuracy</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        计算识别精度</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">            t: 正确解标签</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            识别精度（0-1之间的值）</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        y = <span class=\"variable language_\">self</span>.predict(x)</span><br><span class=\"line\">        y = np.argmax(y, axis=<span class=\"number\">1</span>)  <span class=\"comment\"># 获取预测结果</span></span><br><span class=\"line\">        t = np.argmax(t, axis=<span class=\"number\">1</span>)  <span class=\"comment\"># 获取正确解</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 计算正确率</span></span><br><span class=\"line\">        accuracy = np.<span class=\"built_in\">sum</span>(y == t) / <span class=\"built_in\">float</span>(x.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> accuracy</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">numerical_gradient</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        使用数值微分计算梯度</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">            t: 正确解标签</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            包含各层权重和偏置梯度的字典</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"comment\"># 定义损失函数</span></span><br><span class=\"line\">        loss_W = <span class=\"keyword\">lambda</span> W: <span class=\"variable language_\">self</span>.loss(x, t)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 计算各层参数的梯度</span></span><br><span class=\"line\">        grads = &#123;&#125;</span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;W1&#x27;</span>] = numerical_gradient(loss_W, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W1&#x27;</span>])  <span class=\"comment\"># 第一层权重梯度</span></span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;b1&#x27;</span>] = numerical_gradient(loss_W, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b1&#x27;</span>])  <span class=\"comment\"># 第一层偏置梯度</span></span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;W2&#x27;</span>] = numerical_gradient(loss_W, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W2&#x27;</span>])  <span class=\"comment\"># 第二层权重梯度</span></span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;b2&#x27;</span>] = numerical_gradient(loss_W, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b2&#x27;</span>])  <span class=\"comment\"># 第二层偏置梯度</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> grads</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">gradient</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        使用反向传播计算梯度</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">            t: 正确解标签</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            包含各层权重和偏置梯度的字典</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        W1, W2 = <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W1&#x27;</span>], <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W2&#x27;</span>]</span><br><span class=\"line\">        b1, b2 = <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b1&#x27;</span>], <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b2&#x27;</span>]</span><br><span class=\"line\">        grads = &#123;&#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        batch_num = x.shape[<span class=\"number\">0</span>]  <span class=\"comment\"># 批次大小</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 前向传播</span></span><br><span class=\"line\">        a1 = np.dot(x, W1) + b1</span><br><span class=\"line\">        z1 = sigmoid(a1)</span><br><span class=\"line\">        a2 = np.dot(z1, W2) + b2</span><br><span class=\"line\">        y = softmax(a2)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 反向传播</span></span><br><span class=\"line\">        <span class=\"comment\"># 输出层的误差</span></span><br><span class=\"line\">        dy = (y - t) / batch_num</span><br><span class=\"line\">        <span class=\"comment\"># 第二层权重的梯度</span></span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;W2&#x27;</span>] = np.dot(z1.T, dy)</span><br><span class=\"line\">        <span class=\"comment\"># 第二层偏置的梯度</span></span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;b2&#x27;</span>] = np.<span class=\"built_in\">sum</span>(dy, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 第一层的误差</span></span><br><span class=\"line\">        dz1 = np.dot(dy, W2.T)</span><br><span class=\"line\">        da1 = sigmoid_grad(a1) * dz1</span><br><span class=\"line\">        <span class=\"comment\"># 第一层权重的梯度</span></span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;W1&#x27;</span>] = np.dot(x.T, da1)</span><br><span class=\"line\">        <span class=\"comment\"># 第一层偏置的梯度</span></span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;b1&#x27;</span>] = np.<span class=\"built_in\">sum</span>(da1, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> grads</span><br><span class=\"line\">        </span><br><span class=\"line\">        </span><br><span class=\"line\">net = TwoLayerNet(input_size=<span class=\"number\">784</span>, hidden_size=<span class=\"number\">100</span>, output_size=<span class=\"number\">10</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net.params[<span class=\"string\">&#x27;W1&#x27;</span>].shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net.params[<span class=\"string\">&#x27;b1&#x27;</span>].shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net.params[<span class=\"string\">&#x27;W2&#x27;</span>].shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net.params[<span class=\"string\">&#x27;b2&#x27;</span>].shape)</span><br><span class=\"line\"></span><br><span class=\"line\">x = np.random.rand(<span class=\"number\">100</span>, <span class=\"number\">784</span>)</span><br><span class=\"line\">t = np.random.rand(<span class=\"number\">100</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">grads = net.numerical_gradient(x, t)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(grads[<span class=\"string\">&#x27;W1&#x27;</span>].shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(grads[<span class=\"string\">&#x27;b1&#x27;</span>].shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(grads[<span class=\"string\">&#x27;W2&#x27;</span>].shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(grads[<span class=\"string\">&#x27;b2&#x27;</span>].shape)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"mini-batch的实现，以及基于测试数据的评价\"><a href=\"#mini-batch的实现，以及基于测试数据的评价\" class=\"headerlink\" title=\"mini-batch的实现，以及基于测试数据的评价\"></a>mini-batch的实现，以及基于测试数据的评价</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys, os</span><br><span class=\"line\">sys.path.append(<span class=\"string\">&#x27;../../py_pro/DL/&#x27;</span>)  <span class=\"comment\"># 将父目录添加到系统路径中，以便导入其他模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist  <span class=\"comment\"># 导入MNIST数据集加载函数</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> two_layer_net <span class=\"keyword\">import</span> TwoLayerNet  <span class=\"comment\"># 导入两层神经网络类</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载MNIST数据集</span></span><br><span class=\"line\"><span class=\"comment\"># normalize=True: 将输入图像像素值正规化到0~1之间</span></span><br><span class=\"line\"><span class=\"comment\"># one_hot_label=True: 将标签转换为one-hot形式</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>, one_hot_label=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建两层神经网络实例</span></span><br><span class=\"line\"><span class=\"comment\"># 输入层784个神经元（28x28像素）</span></span><br><span class=\"line\"><span class=\"comment\"># 隐藏层50个神经元</span></span><br><span class=\"line\"><span class=\"comment\"># 输出层10个神经元（0-9十个数字）</span></span><br><span class=\"line\">network = TwoLayerNet(input_size=<span class=\"number\">784</span>, hidden_size=<span class=\"number\">50</span>, output_size=<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置超参数</span></span><br><span class=\"line\">iters_num = <span class=\"number\">10000</span>  <span class=\"comment\"># 训练迭代次数</span></span><br><span class=\"line\">train_size = x_train.shape[<span class=\"number\">0</span>]  <span class=\"comment\"># 训练数据的大小</span></span><br><span class=\"line\">batch_size = <span class=\"number\">100</span>  <span class=\"comment\"># 每批次的样本数</span></span><br><span class=\"line\">learning_rate = <span class=\"number\">0.1</span>  <span class=\"comment\"># 学习率</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 用于记录训练过程的列表</span></span><br><span class=\"line\">train_loss_list = []  <span class=\"comment\"># 记录训练损失</span></span><br><span class=\"line\">train_acc_list = []   <span class=\"comment\"># 记录训练准确率</span></span><br><span class=\"line\">test_acc_list = []    <span class=\"comment\"># 记录测试准确率</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算每个epoch的迭代次数</span></span><br><span class=\"line\">iter_per_epoch = <span class=\"built_in\">max</span>(train_size / batch_size, <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 开始训练</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(iters_num):</span><br><span class=\"line\">    <span class=\"comment\"># 随机选择批次数据</span></span><br><span class=\"line\">    batch_mask = np.random.choice(train_size, batch_size)</span><br><span class=\"line\">    x_batch = x_train[batch_mask]</span><br><span class=\"line\">    t_batch = t_train[batch_mask]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 计算梯度</span></span><br><span class=\"line\">    <span class=\"comment\"># 使用反向传播计算梯度（比数值微分更快）</span></span><br><span class=\"line\">    <span class=\"comment\">#grad = network.numerical_gradient(x_batch, t_batch)  # 数值微分（较慢）</span></span><br><span class=\"line\">    grad = network.gradient(x_batch, t_batch)  <span class=\"comment\"># 反向传播（较快）</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 更新参数</span></span><br><span class=\"line\">    <span class=\"comment\"># 对每个参数进行梯度下降更新</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> (<span class=\"string\">&#x27;W1&#x27;</span>, <span class=\"string\">&#x27;b1&#x27;</span>, <span class=\"string\">&#x27;W2&#x27;</span>, <span class=\"string\">&#x27;b2&#x27;</span>):</span><br><span class=\"line\">        network.params[key] -= learning_rate * grad[key]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 记录训练损失</span></span><br><span class=\"line\">    loss = network.loss(x_batch, t_batch)</span><br><span class=\"line\">    train_loss_list.append(loss)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 每个epoch计算一次训练集和测试集的准确率</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> i % iter_per_epoch == <span class=\"number\">0</span>:</span><br><span class=\"line\">        train_acc = network.accuracy(x_train, t_train)</span><br><span class=\"line\">        test_acc = network.accuracy(x_test, t_test)</span><br><span class=\"line\">        train_acc_list.append(train_acc)</span><br><span class=\"line\">        test_acc_list.append(test_acc)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;train acc, test acc | &quot;</span> + <span class=\"built_in\">str</span>(train_acc) + <span class=\"string\">&quot;, &quot;</span> + <span class=\"built_in\">str</span>(test_acc))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制训练结果</span></span><br><span class=\"line\">markers = &#123;<span class=\"string\">&#x27;train&#x27;</span>: <span class=\"string\">&#x27;o&#x27;</span>, <span class=\"string\">&#x27;test&#x27;</span>: <span class=\"string\">&#x27;s&#x27;</span>&#125;  <span class=\"comment\"># 设置图例标记</span></span><br><span class=\"line\">x = np.arange(<span class=\"built_in\">len</span>(train_acc_list))</span><br><span class=\"line\">plt.plot(x, train_acc_list, label=<span class=\"string\">&#x27;train acc&#x27;</span>)  <span class=\"comment\"># 绘制训练准确率曲线</span></span><br><span class=\"line\">plt.plot(x, test_acc_list, label=<span class=\"string\">&#x27;test acc&#x27;</span>, linestyle=<span class=\"string\">&#x27;--&#x27;</span>)  <span class=\"comment\"># 绘制测试准确率曲线</span></span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;epochs&quot;</span>)  <span class=\"comment\"># x轴标签</span></span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;accuracy&quot;</span>)  <span class=\"comment\"># y轴标签</span></span><br><span class=\"line\">plt.ylim(<span class=\"number\">0</span>, <span class=\"number\">1.0</span>)  <span class=\"comment\"># 设置y轴范围</span></span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;lower right&#x27;</span>)  <span class=\"comment\"># 显示图例</span></span><br><span class=\"line\">plt.show()  <span class=\"comment\"># 显示图形</span></span><br></pre></td></tr></table></figure>\n\n\n</blockquote>\n<h3 id=\"都说这本鱼书是-最最最-最入门的神经网络和深度学习的书了，但是，从这一章开始，已经开始头脑风暴了，难顶哟-TUT\"><a href=\"#都说这本鱼书是-最最最-最入门的神经网络和深度学习的书了，但是，从这一章开始，已经开始头脑风暴了，难顶哟-TUT\" class=\"headerlink\" title=\"都说这本鱼书是 最最最 最入门的神经网络和深度学习的书了，但是，从这一章开始，已经开始头脑风暴了，难顶哟(TUT)\"></a>都说这本鱼书是 最最最 最入门的神经网络和深度学习的书了，但是，从这一章开始，已经开始头脑风暴了，难顶哟(TUT)</h3>","cover_type":"img","excerpt":"","more":"<h1 id=\"神经网络的学习\"><a href=\"#神经网络的学习\" class=\"headerlink\" title=\"神经网络的学习\"></a>神经网络的学习</h1><h3 id=\"从数据中学习\"><a href=\"#从数据中学习\" class=\"headerlink\" title=\"从数据中学习\"></a>从数据中学习</h3><blockquote>\n<h4 id=\"所谓的学习就是从训练数据中自动获取最优权重参数的过程\"><a href=\"#所谓的学习就是从训练数据中自动获取最优权重参数的过程\" class=\"headerlink\" title=\"所谓的学习就是从训练数据中自动获取最优权重参数的过程\"></a>所谓的学习就是从训练数据中自动获取最优权重参数的过程</h4><h4 id=\"为了使神经网络能够学习，引入了损失函数这一指标，而学习的目标就是以损失函数为基准，找出使它的值达到最小的权重参数（函数斜率的梯度法）\"><a href=\"#为了使神经网络能够学习，引入了损失函数这一指标，而学习的目标就是以损失函数为基准，找出使它的值达到最小的权重参数（函数斜率的梯度法）\" class=\"headerlink\" title=\"为了使神经网络能够学习，引入了损失函数这一指标，而学习的目标就是以损失函数为基准，找出使它的值达到最小的权重参数（函数斜率的梯度法）\"></a>为了使神经网络能够学习，引入了损失函数这一指标，而学习的目标就是以损失函数为基准，找出使它的值达到最小的权重参数（函数斜率的梯度法）</h4><h4 id=\"特征量：是指可以从输入数据（输入图像）中准确地提取出本质数据（重要数据）的转换器（图像的特征量通常表示为向量形式），但特征量的选取是人为选择的，还是存在人为因素的介入\"><a href=\"#特征量：是指可以从输入数据（输入图像）中准确地提取出本质数据（重要数据）的转换器（图像的特征量通常表示为向量形式），但特征量的选取是人为选择的，还是存在人为因素的介入\" class=\"headerlink\" title=\"特征量：是指可以从输入数据（输入图像）中准确地提取出本质数据（重要数据）的转换器（图像的特征量通常表示为向量形式），但特征量的选取是人为选择的，还是存在人为因素的介入\"></a>特征量：是指可以从输入数据（输入图像）中准确地提取出本质数据（重要数据）的转换器（图像的特征量通常表示为向量形式），但特征量的选取是人为选择的，还是存在人为因素的介入</h4><h4 id=\"深度学习也被称为端到端机器学习（end-to-end-machine-learning），从原始数据（输入）中获得目标结果（输出）\"><a href=\"#深度学习也被称为端到端机器学习（end-to-end-machine-learning），从原始数据（输入）中获得目标结果（输出）\" class=\"headerlink\" title=\"深度学习也被称为端到端机器学习（end-to-end machine learning），从原始数据（输入）中获得目标结果（输出）\"></a>深度学习也被称为端到端机器学习（end-to-end machine learning），从原始数据（输入）中获得目标结果（输出）</h4><h4 id=\"神经网络的优点是对所有的问题都可以使用同样的流程来解决，与待处理的问题无关，它可以将数据直接作为原始数据，进行端到端学习\"><a href=\"#神经网络的优点是对所有的问题都可以使用同样的流程来解决，与待处理的问题无关，它可以将数据直接作为原始数据，进行端到端学习\" class=\"headerlink\" title=\"神经网络的优点是对所有的问题都可以使用同样的流程来解决，与待处理的问题无关，它可以将数据直接作为原始数据，进行端到端学习\"></a>神经网络的优点是对所有的问题都可以使用同样的流程来解决，与待处理的问题无关，它可以将数据直接作为原始数据，进行端到端学习</h4><h4 id=\"在机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验，首先使用训练数据进行学习，寻找最优参数，再使用测试数据评价训练得到的模型的实际能力\"><a href=\"#在机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验，首先使用训练数据进行学习，寻找最优参数，再使用测试数据评价训练得到的模型的实际能力\" class=\"headerlink\" title=\"在机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验，首先使用训练数据进行学习，寻找最优参数，再使用测试数据评价训练得到的模型的实际能力\"></a>在机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验，首先使用训练数据进行学习，寻找最优参数，再使用测试数据评价训练得到的模型的实际能力</h4><h4 id=\"划分为训练数据和测试数据（也称为监督数据）是为了追求“泛化能力”\"><a href=\"#划分为训练数据和测试数据（也称为监督数据）是为了追求“泛化能力”\" class=\"headerlink\" title=\"划分为训练数据和测试数据（也称为监督数据）是为了追求“泛化能力”\"></a>划分为训练数据和测试数据（也称为监督数据）是为了追求“泛化能力”</h4><h4 id=\"泛化能力：是指处理未被观察过的数据（不包含在训练数据中的数据）的能力\"><a href=\"#泛化能力：是指处理未被观察过的数据（不包含在训练数据中的数据）的能力\" class=\"headerlink\" title=\"泛化能力：是指处理未被观察过的数据（不包含在训练数据中的数据）的能力\"></a>泛化能力：是指处理未被观察过的数据（不包含在训练数据中的数据）的能力</h4><h4 id=\"获得泛化能力是机器学习的最终目标\"><a href=\"#获得泛化能力是机器学习的最终目标\" class=\"headerlink\" title=\"获得泛化能力是机器学习的最终目标\"></a>获得泛化能力是机器学习的最终目标</h4><h4 id=\"过拟合：它是指只对某个数据集过度拟合的状态\"><a href=\"#过拟合：它是指只对某个数据集过度拟合的状态\" class=\"headerlink\" title=\"过拟合：它是指只对某个数据集过度拟合的状态\"></a>过拟合：它是指只对某个数据集过度拟合的状态</h4></blockquote>\n<h3 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h3><blockquote>\n<h4 id=\"神经网络的学习通过某个指标表示现在的状态，再以这个指标为基准，寻找最优权重参数，而这个指标就是损失函数\"><a href=\"#神经网络的学习通过某个指标表示现在的状态，再以这个指标为基准，寻找最优权重参数，而这个指标就是损失函数\" class=\"headerlink\" title=\"神经网络的学习通过某个指标表示现在的状态，再以这个指标为基准，寻找最优权重参数，而这个指标就是损失函数\"></a>神经网络的学习通过某个指标表示现在的状态，再以这个指标为基准，寻找最优权重参数，而这个指标就是损失函数</h4><h4 id=\"损失函数可以使用任何函数，但一般使用的是均方误差和交叉熵误差\"><a href=\"#损失函数可以使用任何函数，但一般使用的是均方误差和交叉熵误差\" class=\"headerlink\" title=\"损失函数可以使用任何函数，但一般使用的是均方误差和交叉熵误差\"></a>损失函数可以使用任何函数，但一般使用的是均方误差和交叉熵误差</h4><h4 id=\"损失函数是表示神经网络性能好坏的指标，它反映了当前的神经网络对监督数据的拟合程度\"><a href=\"#损失函数是表示神经网络性能好坏的指标，它反映了当前的神经网络对监督数据的拟合程度\" class=\"headerlink\" title=\"损失函数是表示神经网络性能好坏的指标，它反映了当前的神经网络对监督数据的拟合程度\"></a>损失函数是表示神经网络性能好坏的指标，它反映了当前的神经网络对监督数据的拟合程度</h4><h4 id=\"均方误差\"><a href=\"#均方误差\" class=\"headerlink\" title=\"均方误差\"></a>均方误差</h4><blockquote>\n<h4 id=\"其表达式如下：\"><a href=\"#其表达式如下：\" class=\"headerlink\" title=\"其表达式如下：\"></a>其表达式如下：</h4><p>$$<br>E &#x3D; \\frac{1}{2} \\sum_{k} (y_k - t_k)^2<br>$$</p>\n<h4 id=\"其中，-y-k-是表示神经网络的输出，-t-k-表示监督数据（测试数据），-k-表示数据的维数\"><a href=\"#其中，-y-k-是表示神经网络的输出，-t-k-表示监督数据（测试数据），-k-表示数据的维数\" class=\"headerlink\" title=\"其中，$y_k$ 是表示神经网络的输出，$t_k$ 表示监督数据（测试数据），$k$ 表示数据的维数\"></a>其中，$y_k$ 是表示神经网络的输出，$t_k$ 表示监督数据（测试数据），$k$ 表示数据的维数</h4><h4 id=\"题外话：均方误差式子的一个特点就是，其名中的4个字与式子中皆有对应，均-对应式子中的-frac-1-2-，方-对应式子中的-2-，误差-对应式子中的-y-k-t-k\"><a href=\"#题外话：均方误差式子的一个特点就是，其名中的4个字与式子中皆有对应，均-对应式子中的-frac-1-2-，方-对应式子中的-2-，误差-对应式子中的-y-k-t-k\" class=\"headerlink\" title=\"题外话：均方误差式子的一个特点就是，其名中的4个字与式子中皆有对应，均 对应式子中的 $\\frac{1}{2}$ ，方 对应式子中的 $()^2$ ，误差 对应式子中的 $ y_k - t_k $\"></a>题外话：均方误差式子的一个特点就是，其名中的4个字与式子中皆有对应，均 对应式子中的 $\\frac{1}{2}$ ，方 对应式子中的 $()^2$ ，误差 对应式子中的 $ y_k - t_k $</h4></blockquote>\n<h4 id=\"交叉熵误差\"><a href=\"#交叉熵误差\" class=\"headerlink\" title=\"交叉熵误差\"></a>交叉熵误差</h4><blockquote>\n<h4 id=\"其表达式如下：-1\"><a href=\"#其表达式如下：-1\" class=\"headerlink\" title=\"其表达式如下：\"></a>其表达式如下：</h4><p>$$<br>E &#x3D; - \\sum_{k} t_k \\log y_k<br>$$</p>\n<h4 id=\"其中，-y-k-是表示神经网络的输出，-t-k-是正确解标签，且-t-k-中只有正确解标签的索引为1，其余的都是0-one-hot表示\"><a href=\"#其中，-y-k-是表示神经网络的输出，-t-k-是正确解标签，且-t-k-中只有正确解标签的索引为1，其余的都是0-one-hot表示\" class=\"headerlink\" title=\"其中，$y_k$ 是表示神经网络的输出，$t_k$ 是正确解标签，且 $t_k$ 中只有正确解标签的索引为1，其余的都是0(one-hot表示)\"></a>其中，$y_k$ 是表示神经网络的输出，$t_k$ 是正确解标签，且 $t_k$ 中只有正确解标签的索引为1，其余的都是0(one-hot表示)</h4><h4 id=\"所以说，实际上交叉熵误差式子只计算对应正确解标签的输出的自然对数，交叉熵误差的值是由正确解标签所对应的输出结果决定的\"><a href=\"#所以说，实际上交叉熵误差式子只计算对应正确解标签的输出的自然对数，交叉熵误差的值是由正确解标签所对应的输出结果决定的\" class=\"headerlink\" title=\"所以说，实际上交叉熵误差式子只计算对应正确解标签的输出的自然对数，交叉熵误差的值是由正确解标签所对应的输出结果决定的\"></a>所以说，实际上交叉熵误差式子只计算对应正确解标签的输出的自然对数，交叉熵误差的值是由正确解标签所对应的输出结果决定的</h4><h4 id=\"正确解标签对应的输出越大，交叉熵误差的值就越接近0，反之则越大，输出为1时，交叉熵误差为0，log函数图像如下\"><a href=\"#正确解标签对应的输出越大，交叉熵误差的值就越接近0，反之则越大，输出为1时，交叉熵误差为0，log函数图像如下\" class=\"headerlink\" title=\"正确解标签对应的输出越大，交叉熵误差的值就越接近0，反之则越大，输出为1时，交叉熵误差为0，log函数图像如下\"></a>正确解标签对应的输出越大，交叉熵误差的值就越接近0，反之则越大，输出为1时，交叉熵误差为0，log函数图像如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/3/1.png\"></p>\n</blockquote>\n<h4 id=\"mini-batch学习\"><a href=\"#mini-batch学习\" class=\"headerlink\" title=\"mini-batch学习\"></a>mini-batch学习</h4><blockquote>\n<h4 id=\"机器学习使用训练数据进行学习，其本质就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数，所以计算损失函数时必须将所有的训练数据作为对象\"><a href=\"#机器学习使用训练数据进行学习，其本质就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数，所以计算损失函数时必须将所有的训练数据作为对象\" class=\"headerlink\" title=\"机器学习使用训练数据进行学习，其本质就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数，所以计算损失函数时必须将所有的训练数据作为对象\"></a>机器学习使用训练数据进行学习，其本质就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数，所以计算损失函数时必须将所有的训练数据作为对象</h4><h4 id=\"前面提到的损失函数都是针对的单个数据，若要求所有的训练数据的损失函数的总和，以交叉熵误差为例，损失函数可以写为：\"><a href=\"#前面提到的损失函数都是针对的单个数据，若要求所有的训练数据的损失函数的总和，以交叉熵误差为例，损失函数可以写为：\" class=\"headerlink\" title=\"前面提到的损失函数都是针对的单个数据，若要求所有的训练数据的损失函数的总和，以交叉熵误差为例，损失函数可以写为：\"></a>前面提到的损失函数都是针对的单个数据，若要求所有的训练数据的损失函数的总和，以交叉熵误差为例，损失函数可以写为：</h4><p>$$<br>E &#x3D; - \\frac{1}{N} \\sum_{n} \\sum_{k} t_{nk} \\log y_{nk}<br>$$</p>\n<h4 id=\"这里是假设有-N-个数据，-t-nk-表示第-n-个数据的第-k-个元素的值，-t-nk-是监督数据，-t-nk-是神经网络的输出\"><a href=\"#这里是假设有-N-个数据，-t-nk-表示第-n-个数据的第-k-个元素的值，-t-nk-是监督数据，-t-nk-是神经网络的输出\" class=\"headerlink\" title=\"这里是假设有$N$个数据，$t_{nk}$ 表示第 $n$ 个数据的第 $k$ 个元素的值，$t_{nk}$ 是监督数据，$t_{nk}$ 是神经网络的输出\"></a>这里是假设有$N$个数据，$t_{nk}$ 表示第 $n$ 个数据的第 $k$ 个元素的值，$t_{nk}$ 是监督数据，$t_{nk}$ 是神经网络的输出</h4><h4 id=\"通过除以-N-（正规化），可以求单个数据的平均损失函数，这样的平均化，可以获得和训练数据的数量无关的统一指标\"><a href=\"#通过除以-N-（正规化），可以求单个数据的平均损失函数，这样的平均化，可以获得和训练数据的数量无关的统一指标\" class=\"headerlink\" title=\"通过除以 $N$ （正规化），可以求单个数据的平均损失函数，这样的平均化，可以获得和训练数据的数量无关的统一指标\"></a>通过除以 $N$ （正规化），可以求单个数据的平均损失函数，这样的平均化，可以获得和训练数据的数量无关的统一指标</h4><h4 id=\"如果将全部的训练数据作为对象求损失函数的总和，那这个计算过程会是非常漫长的，训练数据量非常庞大时，以全部训练数据作为对象计算损失函数更是不现实的\"><a href=\"#如果将全部的训练数据作为对象求损失函数的总和，那这个计算过程会是非常漫长的，训练数据量非常庞大时，以全部训练数据作为对象计算损失函数更是不现实的\" class=\"headerlink\" title=\"如果将全部的训练数据作为对象求损失函数的总和，那这个计算过程会是非常漫长的，训练数据量非常庞大时，以全部训练数据作为对象计算损失函数更是不现实的\"></a>如果将全部的训练数据作为对象求损失函数的总和，那这个计算过程会是非常漫长的，训练数据量非常庞大时，以全部训练数据作为对象计算损失函数更是不现实的</h4><h4 id=\"因此，从全部训练数据中选择一部分，作为全部训练数据的“近似”\"><a href=\"#因此，从全部训练数据中选择一部分，作为全部训练数据的“近似”\" class=\"headerlink\" title=\"因此，从全部训练数据中选择一部分，作为全部训练数据的“近似”\"></a>因此，从全部训练数据中选择一部分，作为全部训练数据的“近似”</h4><h4 id=\"神经网络的学习也是从训练数据中选出一批数据（称为mini-batch-小批量），然后对每个mini-batch进行学习，用这一批数据的结果来近似全部数据的结果，这种学习方式就是所谓的-mini-batch学习\"><a href=\"#神经网络的学习也是从训练数据中选出一批数据（称为mini-batch-小批量），然后对每个mini-batch进行学习，用这一批数据的结果来近似全部数据的结果，这种学习方式就是所谓的-mini-batch学习\" class=\"headerlink\" title=\"神经网络的学习也是从训练数据中选出一批数据（称为mini-batch,小批量），然后对每个mini-batch进行学习，用这一批数据的结果来近似全部数据的结果，这种学习方式就是所谓的 mini-batch学习\"></a>神经网络的学习也是从训练数据中选出一批数据（称为mini-batch,小批量），然后对每个mini-batch进行学习，用这一批数据的结果来近似全部数据的结果，这种学习方式就是所谓的 <strong>mini-batch学习</strong></h4></blockquote>\n<h4 id=\"为什么要引入损失函数\"><a href=\"#为什么要引入损失函数\" class=\"headerlink\" title=\"为什么要引入损失函数\"></a>为什么要引入损失函数</h4><blockquote>\n<h4 id=\"为了找到使损失函数的值尽可能小的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值\"><a href=\"#为了找到使损失函数的值尽可能小的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值\" class=\"headerlink\" title=\"为了找到使损失函数的值尽可能小的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值\"></a>为了找到使损失函数的值尽可能小的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值</h4><h4 id=\"损失函数的导数一般不会有0值，更不会恒为0（阶跃函数除外），而识别精度的导数恒为0或者只有极小部分不为0，这样权重的调整并不会影响其值，这并不是我们想要的\"><a href=\"#损失函数的导数一般不会有0值，更不会恒为0（阶跃函数除外），而识别精度的导数恒为0或者只有极小部分不为0，这样权重的调整并不会影响其值，这并不是我们想要的\" class=\"headerlink\" title=\"损失函数的导数一般不会有0值，更不会恒为0（阶跃函数除外），而识别精度的导数恒为0或者只有极小部分不为0，这样权重的调整并不会影响其值，这并不是我们想要的\"></a>损失函数的导数一般不会有0值，更不会恒为0（阶跃函数除外），而识别精度的导数恒为0或者只有极小部分不为0，这样权重的调整并不会影响其值，这并不是我们想要的</h4></blockquote>\n</blockquote>\n<h3 id=\"数值微分（数值梯度）\"><a href=\"#数值微分（数值梯度）\" class=\"headerlink\" title=\"数值微分（数值梯度）\"></a>数值微分（数值梯度）</h3><blockquote>\n<h4 id=\"梯度法使用梯度的信息决定前进的方向\"><a href=\"#梯度法使用梯度的信息决定前进的方向\" class=\"headerlink\" title=\"梯度法使用梯度的信息决定前进的方向\"></a>梯度法使用梯度的信息决定前进的方向</h4><h4 id=\"利用微小的差分求导数的过程就是所谓的数值微分（numerical-differentiation）\"><a href=\"#利用微小的差分求导数的过程就是所谓的数值微分（numerical-differentiation）\" class=\"headerlink\" title=\"利用微小的差分求导数的过程就是所谓的数值微分（numerical differentiation）\"></a>利用微小的差分求导数的过程就是所谓的数值微分（numerical differentiation）</h4><h4 id=\"基于数学式的推到求导数的过程，则称之为-解析性求解或解析性求导\"><a href=\"#基于数学式的推到求导数的过程，则称之为-解析性求解或解析性求导\" class=\"headerlink\" title=\"基于数学式的推到求导数的过程，则称之为 解析性求解或解析性求导\"></a>基于数学式的推到求导数的过程，则称之为 解析性求解或解析性求导</h4></blockquote>\n<h3 id=\"梯度\"><a href=\"#梯度\" class=\"headerlink\" title=\"梯度\"></a>梯度</h3><blockquote>\n<h4 id=\"由全部变量的偏导数汇总成的向量称为梯度\"><a href=\"#由全部变量的偏导数汇总成的向量称为梯度\" class=\"headerlink\" title=\"由全部变量的偏导数汇总成的向量称为梯度\"></a>由全部变量的偏导数汇总成的向量称为梯度</h4><h4 id=\"梯度法\"><a href=\"#梯度法\" class=\"headerlink\" title=\"梯度法\"></a>梯度法</h4><blockquote>\n<h4 id=\"寻找的最优参数是指损失函数取最小值时的参数\"><a href=\"#寻找的最优参数是指损失函数取最小值时的参数\" class=\"headerlink\" title=\"寻找的最优参数是指损失函数取最小值时的参数\"></a>寻找的最优参数是指损失函数取最小值时的参数</h4><h4 id=\"使用梯度来寻找损失函数最小值的方法就是所谓的-梯度法，通过不断地沿梯度方向前进，逐渐减小函数值的过程就是梯度法\"><a href=\"#使用梯度来寻找损失函数最小值的方法就是所谓的-梯度法，通过不断地沿梯度方向前进，逐渐减小函数值的过程就是梯度法\" class=\"headerlink\" title=\"使用梯度来寻找损失函数最小值的方法就是所谓的 梯度法，通过不断地沿梯度方向前进，逐渐减小函数值的过程就是梯度法\"></a>使用梯度来寻找损失函数最小值的方法就是所谓的 梯度法，通过不断地沿梯度方向前进，逐渐减小函数值的过程就是梯度法</h4><h4 id=\"梯度表示的各点处的函数值减小最多的方向\"><a href=\"#梯度表示的各点处的函数值减小最多的方向\" class=\"headerlink\" title=\"梯度表示的各点处的函数值减小最多的方向\"></a>梯度表示的各点处的函数值减小最多的方向</h4><h4 id=\"函数最小值、极小值和鞍点处的梯度为-0（梯度法就是要找梯度为0的地方）\"><a href=\"#函数最小值、极小值和鞍点处的梯度为-0（梯度法就是要找梯度为0的地方）\" class=\"headerlink\" title=\"函数最小值、极小值和鞍点处的梯度为 0（梯度法就是要找梯度为0的地方）\"></a>函数最小值、极小值和鞍点处的梯度为 0（梯度法就是要找梯度为0的地方）</h4><h4 id=\"极小值是局部最小值，鞍点是从某个方向上看是极大值，从另一个方向看是极小值的点\"><a href=\"#极小值是局部最小值，鞍点是从某个方向上看是极大值，从另一个方向看是极小值的点\" class=\"headerlink\" title=\"极小值是局部最小值，鞍点是从某个方向上看是极大值，从另一个方向看是极小值的点\"></a>极小值是局部最小值，鞍点是从某个方向上看是极大值，从另一个方向看是极小值的点</h4><h4 id=\"学习高原：它是指，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入所谓的学习高原而无法前进的停滞期\"><a href=\"#学习高原：它是指，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入所谓的学习高原而无法前进的停滞期\" class=\"headerlink\" title=\"学习高原：它是指，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入所谓的学习高原而无法前进的停滞期\"></a>学习高原：它是指，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入所谓的<strong>学习高原</strong>而无法前进的停滞期</h4><h4 id=\"根据寻找目的（最大值或最小值）的不同，梯度法分为了：梯度下降法和梯度上升法\"><a href=\"#根据寻找目的（最大值或最小值）的不同，梯度法分为了：梯度下降法和梯度上升法\" class=\"headerlink\" title=\"根据寻找目的（最大值或最小值）的不同，梯度法分为了：梯度下降法和梯度上升法\"></a>根据寻找目的（最大值或最小值）的不同，梯度法分为了：梯度下降法和梯度上升法</h4><ul>\n<li><h4 id=\"梯度下降法：寻找最小值的梯度法\"><a href=\"#梯度下降法：寻找最小值的梯度法\" class=\"headerlink\" title=\"梯度下降法：寻找最小值的梯度法\"></a>梯度下降法：寻找最小值的梯度法</h4></li>\n<li><h4 id=\"梯度上升法：寻找最大值的梯度法\"><a href=\"#梯度上升法：寻找最大值的梯度法\" class=\"headerlink\" title=\"梯度上升法：寻找最大值的梯度法\"></a>梯度上升法：寻找最大值的梯度法</h4></li>\n</ul>\n<h4 id=\"通过反转损失函数的符号，求最小值和求最大值的问题可以变成相同的问题\"><a href=\"#通过反转损失函数的符号，求最小值和求最大值的问题可以变成相同的问题\" class=\"headerlink\" title=\"通过反转损失函数的符号，求最小值和求最大值的问题可以变成相同的问题\"></a>通过反转损失函数的符号，求最小值和求最大值的问题可以变成相同的问题</h4><h4 id=\"神经网络（深度学习）中，梯度法一般是指的梯度下降法\"><a href=\"#神经网络（深度学习）中，梯度法一般是指的梯度下降法\" class=\"headerlink\" title=\"神经网络（深度学习）中，梯度法一般是指的梯度下降法\"></a>神经网络（深度学习）中，梯度法一般是指的梯度下降法</h4><h4 id=\"学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数\"><a href=\"#学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数\" class=\"headerlink\" title=\"学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数\"></a>学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数</h4><h4 id=\"在神经网络的学习中，一般会一边改变学习率的值，一边确认学习是否正确进行了\"><a href=\"#在神经网络的学习中，一般会一边改变学习率的值，一边确认学习是否正确进行了\" class=\"headerlink\" title=\"在神经网络的学习中，一般会一边改变学习率的值，一边确认学习是否正确进行了\"></a>在神经网络的学习中，一般会一边改变学习率的值，一边确认学习是否正确进行了</h4><h4 id=\"学习率这样的参数也被称为超参数，需要人工设定\"><a href=\"#学习率这样的参数也被称为超参数，需要人工设定\" class=\"headerlink\" title=\"学习率这样的参数也被称为超参数，需要人工设定\"></a>学习率这样的参数也被称为<strong>超参数</strong>，需要人工设定</h4></blockquote>\n<h4 id=\"神经网络的梯度\"><a href=\"#神经网络的梯度\" class=\"headerlink\" title=\"神经网络的梯度\"></a>神经网络的梯度</h4><blockquote>\n<h4 id=\"这里的梯度指的是损失函数关于权重参数的梯度\"><a href=\"#这里的梯度指的是损失函数关于权重参数的梯度\" class=\"headerlink\" title=\"这里的梯度指的是损失函数关于权重参数的梯度\"></a>这里的梯度指的是损失函数关于权重参数的梯度</h4><h4 id=\"例子说明，一个神经网络的权重W形状为-2x3，损失函数用L表示，此时梯度可以用-frac-partial-L-partial-W-来表示，数学表示式如下\"><a href=\"#例子说明，一个神经网络的权重W形状为-2x3，损失函数用L表示，此时梯度可以用-frac-partial-L-partial-W-来表示，数学表示式如下\" class=\"headerlink\" title=\"例子说明，一个神经网络的权重W形状为 2x3，损失函数用L表示，此时梯度可以用 $\\frac{\\partial L}{\\partial W}$ 来表示，数学表示式如下\"></a>例子说明，一个神经网络的权重W形状为 2x3，损失函数用L表示，此时梯度可以用 $\\frac{\\partial L}{\\partial W}$ 来表示，数学表示式如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/3/2.png\"></p>\n<h4 id=\"求出神经网络的梯度后，接下来只需根据梯度法，更新权重参数即可\"><a href=\"#求出神经网络的梯度后，接下来只需根据梯度法，更新权重参数即可\" class=\"headerlink\" title=\"求出神经网络的梯度后，接下来只需根据梯度法，更新权重参数即可\"></a>求出神经网络的梯度后，接下来只需根据梯度法，更新权重参数即可</h4></blockquote>\n<h4 id=\"学习算法的实现\"><a href=\"#学习算法的实现\" class=\"headerlink\" title=\"学习算法的实现\"></a>学习算法的实现</h4><blockquote>\n<h4 id=\"神经网络的学习的步骤\"><a href=\"#神经网络的学习的步骤\" class=\"headerlink\" title=\"神经网络的学习的步骤\"></a>神经网络的学习的步骤</h4><blockquote>\n<h4 id=\"前提是：神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”\"><a href=\"#前提是：神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”\" class=\"headerlink\" title=\"前提是：神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”\"></a>前提是：神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”</h4><ol>\n<li><h4 id=\"（mini-batch）从训练数据中随机选出一部分数据，这部分数据称为mini-batch，目标是是减小mini-batch的损失函数的值\"><a href=\"#（mini-batch）从训练数据中随机选出一部分数据，这部分数据称为mini-batch，目标是是减小mini-batch的损失函数的值\" class=\"headerlink\" title=\"（mini-batch）从训练数据中随机选出一部分数据，这部分数据称为mini-batch，目标是是减小mini-batch的损失函数的值\"></a>（mini-batch）从训练数据中随机选出一部分数据，这部分数据称为mini-batch，目标是是减小mini-batch的损失函数的值</h4></li>\n<li><h4 id=\"（计算梯度）为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度，梯度表示损失函数的值减小最多的方向\"><a href=\"#（计算梯度）为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度，梯度表示损失函数的值减小最多的方向\" class=\"headerlink\" title=\"（计算梯度）为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度，梯度表示损失函数的值减小最多的方向\"></a>（计算梯度）为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度，梯度表示损失函数的值减小最多的方向</h4></li>\n<li><h4 id=\"（更新参数）将权重参数沿梯度方向进行微小更新\"><a href=\"#（更新参数）将权重参数沿梯度方向进行微小更新\" class=\"headerlink\" title=\"（更新参数）将权重参数沿梯度方向进行微小更新\"></a>（更新参数）将权重参数沿梯度方向进行微小更新</h4></li>\n<li><h4 id=\"（重复）重复前三步\"><a href=\"#（重复）重复前三步\" class=\"headerlink\" title=\"（重复）重复前三步\"></a>（重复）重复前三步</h4></li>\n</ol>\n<h4 id=\"因为更新权重参数采用的梯度法是梯度下降法，又是随机选择的mini-batch数据，所以称之为随机梯度下降法（stochastic-gradient-descent，SGD）\"><a href=\"#因为更新权重参数采用的梯度法是梯度下降法，又是随机选择的mini-batch数据，所以称之为随机梯度下降法（stochastic-gradient-descent，SGD）\" class=\"headerlink\" title=\"因为更新权重参数采用的梯度法是梯度下降法，又是随机选择的mini-batch数据，所以称之为随机梯度下降法（stochastic gradient descent，SGD）\"></a>因为更新权重参数采用的梯度法是梯度下降法，又是随机选择的mini-batch数据，所以称之为随机梯度下降法（stochastic gradient descent，SGD）</h4></blockquote>\n<h4 id=\"手写数字识别的神经网络的实现\"><a href=\"#手写数字识别的神经网络的实现\" class=\"headerlink\" title=\"手写数字识别的神经网络的实现\"></a>手写数字识别的神经网络的实现</h4><h4 id=\"基于测试数据的评价\"><a href=\"#基于测试数据的评价\" class=\"headerlink\" title=\"基于测试数据的评价\"></a>基于测试数据的评价</h4><blockquote>\n<h4 id=\"神经网络的学习中，必须确认是否能够正确识别训练数据以外的其他数-据，即确认是否会发生过拟合\"><a href=\"#神经网络的学习中，必须确认是否能够正确识别训练数据以外的其他数-据，即确认是否会发生过拟合\" class=\"headerlink\" title=\"神经网络的学习中，必须确认是否能够正确识别训练数据以外的其他数 据，即确认是否会发生过拟合\"></a>神经网络的学习中，必须确认是否能够正确识别训练数据以外的其他数 据，即确认是否会发生过拟合</h4><h4 id=\"过拟合是指，虽然训练数据中的数字图像能被正确辨别，但是不在训练数据中的数字图像却无法被识别的现象\"><a href=\"#过拟合是指，虽然训练数据中的数字图像能被正确辨别，但是不在训练数据中的数字图像却无法被识别的现象\" class=\"headerlink\" title=\"过拟合是指，虽然训练数据中的数字图像能被正确辨别，但是不在训练数据中的数字图像却无法被识别的现象\"></a>过拟合是指，虽然训练数据中的数字图像能被正确辨别，但是不在训练数据中的数字图像却无法被识别的现象</h4><h4 id=\"要评价神经网络的泛化能力，就必须使用不包含在训练数据中的数据，这就要用到测试数据了\"><a href=\"#要评价神经网络的泛化能力，就必须使用不包含在训练数据中的数据，这就要用到测试数据了\" class=\"headerlink\" title=\"要评价神经网络的泛化能力，就必须使用不包含在训练数据中的数据，这就要用到测试数据了\"></a>要评价神经网络的泛化能力，就必须使用不包含在训练数据中的数据，这就要用到测试数据了</h4><h4 id=\"另外又引入了一个新的概念——epoch\"><a href=\"#另外又引入了一个新的概念——epoch\" class=\"headerlink\" title=\"另外又引入了一个新的概念——epoch\"></a>另外又引入了一个新的概念——epoch</h4><blockquote>\n<h4 id=\"epoch是一个单位，一个epoch表示学习中所有训练数据均被使用过一次时的更新次数\"><a href=\"#epoch是一个单位，一个epoch表示学习中所有训练数据均被使用过一次时的更新次数\" class=\"headerlink\" title=\"epoch是一个单位，一个epoch表示学习中所有训练数据均被使用过一次时的更新次数\"></a>epoch是一个单位，一个epoch表示学习中所有训练数据均被使用过一次时的更新次数</h4><h4 id=\"如，对于10000笔训练数据，用大小为-100-笔数据的mini-batch进行学习时，重复随机梯度下降法-100次，所有的训练数据就都被“看过”了，此时，100次就是一个-epoch\"><a href=\"#如，对于10000笔训练数据，用大小为-100-笔数据的mini-batch进行学习时，重复随机梯度下降法-100次，所有的训练数据就都被“看过”了，此时，100次就是一个-epoch\" class=\"headerlink\" title=\"如，对于10000笔训练数据，用大小为 100 笔数据的mini-batch进行学习时，重复随机梯度下降法 100次，所有的训练数据就都被“看过”了，此时，100次就是一个 epoch\"></a>如，对于10000笔训练数据，用大小为 100 笔数据的mini-batch进行学习时，重复随机梯度下降法 100次，所有的训练数据就都被“看过”了，此时，100次就是一个 epoch</h4><h4 id=\"遍历一次所有数据，就成为一个epoch\"><a href=\"#遍历一次所有数据，就成为一个epoch\" class=\"headerlink\" title=\"遍历一次所有数据，就成为一个epoch\"></a>遍历一次所有数据，就成为一个epoch</h4></blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n<h3 id=\"code\"><a href=\"#code\" class=\"headerlink\" title=\"code\"></a>code</h3><blockquote>\n<h4 id=\"均方误差的实现\"><a href=\"#均方误差的实现\" class=\"headerlink\" title=\"均方误差的实现\"></a>均方误差的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">mean_squared_error</span>(<span class=\"params\">y, t</span>):</span><br><span class=\"line\"><span class=\"keyword\">return</span> <span class=\"number\">0.5</span> * np.<span class=\"built_in\">sum</span>((y - t)**<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设2为正确解</span></span><br><span class=\"line\">t = np.array([<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>])</span><br><span class=\"line\">y = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.05</span>, <span class=\"number\">0.6</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.05</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.0</span>])</span><br><span class=\"line\">result = mean_squared_error(y, t)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(result)</span><br><span class=\"line\"></span><br><span class=\"line\">y = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.05</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.05</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.6</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.0</span>])</span><br><span class=\"line\">result_ = mean_squared_error(y, t)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(result_)</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"交叉熵误差的实现\"><a href=\"#交叉熵误差的实现\" class=\"headerlink\" title=\"交叉熵误差的实现\"></a>交叉熵误差的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">cross_entropy_error</span>(<span class=\"params\">y, t</span>):</span><br><span class=\"line\"><span class=\"comment\"># 加上delta是为了避免出现 log(0)，导致出现负无穷</span></span><br><span class=\"line\">delta = <span class=\"number\">1e-7</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> -np.<span class=\"built_in\">sum</span>(t * np.log(y + delta))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">t = np.array([<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>])</span><br><span class=\"line\">y = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.05</span>, <span class=\"number\">0.6</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.05</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.0</span>])</span><br><span class=\"line\">result = cross_entropy_error(y, t)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(result)</span><br><span class=\"line\"></span><br><span class=\"line\">y = np.array([<span class=\"number\">0.1</span>, <span class=\"number\">0.05</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.05</span>, <span class=\"number\">0.1</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.6</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0.0</span>])</span><br><span class=\"line\">result_ = cross_entropy_error(y, t)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(result_)</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"使用mini-batch学习的手写数字识别\"><a href=\"#使用mini-batch学习的手写数字识别\" class=\"headerlink\" title=\"使用mini-batch学习的手写数字识别\"></a>使用mini-batch学习的手写数字识别</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os, sys</span><br><span class=\"line\">sys.path.append(os.pardir)\t<span class=\"comment\"># 将父目录添加到系统路径，以便导入自定义模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.function <span class=\"keyword\">import</span> sigmoid, softmax</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载MNIST数据集</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_data</span>():</span><br><span class=\"line\"><span class=\"comment\"># normalize=True:将图像的像素值正规化到0.0~1.0</span></span><br><span class=\"line\"><span class=\"comment\"># flatten=True:将图像从2维数组转为1维数组</span></span><br><span class=\"line\"><span class=\"comment\"># one_hot_label=False:标签为0~9的数字,而不是one-hot编码</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>, flatten=<span class=\"literal\">True</span>, one_hot_label=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">return</span> x_test, t_test</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 初始化神经网络</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_network</span>():</span><br><span class=\"line\"><span class=\"comment\"># 从文件加载预训练的网络参数（权重和偏置）</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&quot;sample_weight.pkl&quot;</span>, rb)<span class=\"keyword\">as</span> f:</span><br><span class=\"line\">network = pickle.load(f)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> network</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 神经网络的预测函数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">network, x</span>):</span><br><span class=\"line\"><span class=\"comment\"># x 是输入数据</span></span><br><span class=\"line\"><span class=\"comment\"># 获取网络的权重和偏置</span></span><br><span class=\"line\">w1, w2, w3 = network[<span class=\"string\">&#x27;W1&#x27;</span>], network[<span class=\"string\">&#x27;W2&#x27;</span>], network[<span class=\"string\">&#x27;W3&#x27;</span>]</span><br><span class=\"line\">b1, b2, b3 = network[<span class=\"string\">&#x27;b1&#x27;</span>], network[<span class=\"string\">&#x27;b2&#x27;</span>], network[<span class=\"string\">&#x27;b3&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 向前传播</span></span><br><span class=\"line\">a1 = np.dot(x, w1) + b1</span><br><span class=\"line\">z1 = sigmoid(a1)</span><br><span class=\"line\">a2 = np.dot(z1, w2) + b2</span><br><span class=\"line\">z2 = sigmoid(a2)</span><br><span class=\"line\">a3 = np.dot(z2, w3) + b3</span><br><span class=\"line\">y = softmax(a3)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">return</span> y</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">x, t = get_data()</span><br><span class=\"line\">network = init_network()</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">100</span>\t<span class=\"comment\"># 设置批处理大小</span></span><br><span class=\"line\">accuracy_cnt = <span class=\"number\">0</span>\t<span class=\"comment\"># 初始化准确率计数器</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用批处理进行预测</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, <span class=\"built_in\">len</span>(x), batch_size):</span><br><span class=\"line\">x_batch = x[i:i+batch_size]\t<span class=\"comment\"># 获取当前批次输入的数据</span></span><br><span class=\"line\">y_batch = predict(network, x_batch)\t<span class=\"comment\"># 对当前批次进行预测</span></span><br><span class=\"line\">p = np.argmax(y_batch, axis=<span class=\"number\">1</span>)\t<span class=\"comment\"># 获取每个样本预测概率最大的类别</span></span><br><span class=\"line\">accuracy_cnt += np.<span class=\"built_in\">sum</span>(p == t[i:i+batch_size])\t<span class=\"comment\"># 统计预测正确的样本数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出最终的识别准确率    </span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Accuracy: &quot;</span> + <span class=\"built_in\">str</span>(<span class=\"built_in\">float</span>(accuracy_cnt) / <span class=\"built_in\">len</span>(x)))</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"数值微分\"><a href=\"#数值微分\" class=\"headerlink\" title=\"数值微分\"></a>数值微分</h4><blockquote>\n<h4 id=\"导数的实现\"><a href=\"#导数的实现\" class=\"headerlink\" title=\"导数的实现\"></a>导数的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">numerical_diff</span>(<span class=\"params\">f, x</span>):</span><br><span class=\"line\"> h = <span class=\"number\">1e-4</span></span><br><span class=\"line\"> <span class=\"keyword\">return</span> (f(h + x) - f(x)) / h</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"上面这样计算函数f的差分是不准确的，它计算的是-x-h-和-x-之间的斜率，它与实际要计算的导数是有差异的，这一差异是因h不可能无限接近0导致的，这种属于是前向差分\"><a href=\"#上面这样计算函数f的差分是不准确的，它计算的是-x-h-和-x-之间的斜率，它与实际要计算的导数是有差异的，这一差异是因h不可能无限接近0导致的，这种属于是前向差分\" class=\"headerlink\" title=\"上面这样计算函数f的差分是不准确的，它计算的是(x+h)和(x)之间的斜率，它与实际要计算的导数是有差异的，这一差异是因h不可能无限接近0导致的，这种属于是前向差分\"></a>上面这样计算函数f的差分是不准确的，它计算的是(x+h)和(x)之间的斜率，它与实际要计算的导数是有差异的，这一差异是因h不可能无限接近0导致的，这种属于是前向差分</h4><h4 id=\"正确的应该是-x-h-和-x-h-之间的差分，以x为中心，这也称之为中心差分\"><a href=\"#正确的应该是-x-h-和-x-h-之间的差分，以x为中心，这也称之为中心差分\" class=\"headerlink\" title=\"正确的应该是 (x+h)和(x-h)之间的差分，以x为中心，这也称之为中心差分\"></a>正确的应该是 (x+h)和(x-h)之间的差分，以x为中心，这也称之为中心差分</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">numerical_diff</span>(<span class=\"params\">f, x</span>):</span><br><span class=\"line\"> h = <span class=\"number\">1e-4</span></span><br><span class=\"line\"> <span class=\"keyword\">return</span> (f(x+h) - f(x-h)) / (<span class=\"number\">2</span> * h)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"举个数值微分实例，y-0-01X-2-0-1x\"><a href=\"#举个数值微分实例，y-0-01X-2-0-1x\" class=\"headerlink\" title=\"举个数值微分实例，y &#x3D; 0.01X^2 + 0.1x\"></a>举个数值微分实例，y &#x3D; 0.01X^2 + 0.1x</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">fun_1</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\"> <span class=\"keyword\">return</span> <span class=\"number\">0.01</span>*x**<span class=\"number\">2</span> + <span class=\"number\">0.1</span> * x</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"绘制其图像\"><a href=\"#绘制其图像\" class=\"headerlink\" title=\"绘制其图像\"></a>绘制其图像</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pylab <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">x = np.arange(<span class=\"number\">0.0</span>, <span class=\"number\">20.0</span>, <span class=\"number\">0.1</span>)</span><br><span class=\"line\">y = fun_1(x)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;x&quot;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;f(x)&quot;</span>)</span><br><span class=\"line\">plt.plot(x, y)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"计算x-5和x-10处的导数，并画出其图像（含切线）\"><a href=\"#计算x-5和x-10处的导数，并画出其图像（含切线）\" class=\"headerlink\" title=\"计算x&#x3D;5和x&#x3D;10处的导数，并画出其图像（含切线）\"></a>计算x&#x3D;5和x&#x3D;10处的导数，并画出其图像（含切线）</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = numerical_diff(fun_1, <span class=\"number\">5</span>)</span><br><span class=\"line\">b = numerical_diff(fun_1, <span class=\"number\">10</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(a)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(b)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">tangent_line</span>(<span class=\"params\">f, x</span>):</span><br><span class=\"line\"> d = numerical_diff(f, x)</span><br><span class=\"line\"> <span class=\"built_in\">print</span>(d)</span><br><span class=\"line\"> y = f(x) - d*x</span><br><span class=\"line\"> <span class=\"keyword\">return</span> <span class=\"keyword\">lambda</span> t: d*t + y</span><br><span class=\"line\"></span><br><span class=\"line\">x = np.arange(<span class=\"number\">0.0</span>, <span class=\"number\">20.0</span>, <span class=\"number\">0.1</span>)</span><br><span class=\"line\">y = fun_1(x)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;x&quot;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;f(x)&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置切点坐标</span></span><br><span class=\"line\">tangent_x = <span class=\"number\">5</span></span><br><span class=\"line\">tangent_y = fun_1(tangent_x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制原函数和切线图像</span></span><br><span class=\"line\">tf = tangent_line(fun_1, tangent_x)</span><br><span class=\"line\">y2 = tf(x)</span><br><span class=\"line\">plt.plot(x, y)</span><br><span class=\"line\">plt.plot(x, y2)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制切点处辅助线</span></span><br><span class=\"line\"><span class=\"comment\"># 垂直x轴的</span></span><br><span class=\"line\">plt.axvline(x=tangent_x, color=<span class=\"string\">&#x27;gray&#x27;</span>, linestyle=<span class=\"string\">&#x27;--&#x27;</span>, alpha=<span class=\"number\">0.5</span>)</span><br><span class=\"line\"><span class=\"comment\"># 垂直y轴的</span></span><br><span class=\"line\">plt.axhline(y=tangent_y, color=<span class=\"string\">&#x27;gray&#x27;</span>, linestyle=<span class=\"string\">&#x27;--&#x27;</span>, alpha=<span class=\"number\">0.5</span>)</span><br><span class=\"line\"><span class=\"comment\"># 标记切点(红色圆点)</span></span><br><span class=\"line\">plt.plot(tangent_x, tangent_y, <span class=\"string\">&#x27;ro&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 添加坐标标注</span></span><br><span class=\"line\">plt.text(tangent_x, tangent_y, <span class=\"string\">f&#x27;(<span class=\"subst\">&#123;tangent_x:<span class=\"number\">.1</span>f&#125;</span>, <span class=\"subst\">&#123;tangent_y:<span class=\"number\">.2</span>f&#125;</span>)&#x27;</span>, </span><br><span class=\"line\">      horizontalalignment=<span class=\"string\">&#x27;right&#x27;</span>, verticalalignment=<span class=\"string\">&#x27;bottom&#x27;</span>)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置切点坐标</span></span><br><span class=\"line\">tangent_x = <span class=\"number\">10</span></span><br><span class=\"line\">tangent_y = fun_1(tangent_x)</span><br><span class=\"line\"></span><br><span class=\"line\">tf = tangent_line(fun_1, tangent_x)</span><br><span class=\"line\">y3 = tf(x)</span><br><span class=\"line\">plt.plot(x, y)</span><br><span class=\"line\">plt.plot(x, y3)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制切点处辅助线</span></span><br><span class=\"line\"><span class=\"comment\"># 垂直x轴的</span></span><br><span class=\"line\">plt.axvline(x=tangent_x, color=<span class=\"string\">&#x27;gray&#x27;</span>, linestyle=<span class=\"string\">&#x27;--&#x27;</span>, alpha=<span class=\"number\">0.5</span>)</span><br><span class=\"line\"><span class=\"comment\"># 垂直y轴的</span></span><br><span class=\"line\">plt.axhline(y=tangent_y, color=<span class=\"string\">&#x27;gray&#x27;</span>, linestyle=<span class=\"string\">&#x27;--&#x27;</span>, alpha=<span class=\"number\">0.5</span>)</span><br><span class=\"line\"><span class=\"comment\"># 标记切点(红色圆点)</span></span><br><span class=\"line\">plt.plot(tangent_x, tangent_y, <span class=\"string\">&#x27;ro&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 添加坐标标注</span></span><br><span class=\"line\">plt.text(tangent_x, tangent_y, <span class=\"string\">f&#x27;(<span class=\"subst\">&#123;tangent_x:<span class=\"number\">.1</span>f&#125;</span>, <span class=\"subst\">&#123;tangent_y:<span class=\"number\">.2</span>f&#125;</span>)&#x27;</span>, </span><br><span class=\"line\">      horizontalalignment=<span class=\"string\">&#x27;right&#x27;</span>, verticalalignment=<span class=\"string\">&#x27;bottom&#x27;</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"偏导数\"><a href=\"#偏导数\" class=\"headerlink\" title=\"偏导数\"></a>偏导数</h4><blockquote>\n<h4 id=\"求-f-x-0-x-1-x-0-2-x-1-2-的偏导，并画出其函数图像\"><a href=\"#求-f-x-0-x-1-x-0-2-x-1-2-的偏导，并画出其函数图像\" class=\"headerlink\" title=\"求 $f(x_0,x_1)&#x3D;x_0^2+x_1^2$的偏导，并画出其函数图像\"></a>求 $f(x_0,x_1)&#x3D;x_0^2+x_1^2$的偏导，并画出其函数图像</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> mpl_toolkits.mplot3d <span class=\"keyword\">import</span> Axes3D</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义网格</span></span><br><span class=\"line\">x0 = np.linspace(-<span class=\"number\">3</span>, <span class=\"number\">3</span>, <span class=\"number\">100</span>)</span><br><span class=\"line\">x1 = np.linspace(-<span class=\"number\">3</span>, <span class=\"number\">3</span>, <span class=\"number\">100</span>)</span><br><span class=\"line\">X0, X1 = np.meshgrid(x0, x1)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义函数</span></span><br><span class=\"line\">Z = X0**<span class=\"number\">2</span> + X1**<span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建图形窗口</span></span><br><span class=\"line\">fig = plt.figure(figsize=(<span class=\"number\">14</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 1. 三维图像</span></span><br><span class=\"line\">ax1 = fig.add_subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>, projection=<span class=\"string\">&#x27;3d&#x27;</span>)</span><br><span class=\"line\">ax1.plot_surface(X0, X1, Z, cmap=<span class=\"string\">&#x27;viridis&#x27;</span>, edgecolor=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">ax1.set_title(<span class=\"string\">r&#x27;$f(x_0, x_1) = x_0^2 + x_1^2$&#x27;</span>)</span><br><span class=\"line\">ax1.set_xlabel(<span class=\"string\">&#x27;$x_0$&#x27;</span>)</span><br><span class=\"line\">ax1.set_ylabel(<span class=\"string\">&#x27;$x_1$&#x27;</span>)</span><br><span class=\"line\">ax1.set_zlabel(<span class=\"string\">&#x27;$f(x_0, x_1)$&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 等高线图 + 梯度向量</span></span><br><span class=\"line\">ax2 = fig.add_subplot(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">contour = ax2.contourf(X0, X1, Z, levels=<span class=\"number\">30</span>, cmap=<span class=\"string\">&#x27;viridis&#x27;</span>)</span><br><span class=\"line\">ax2.set_title(<span class=\"string\">&#x27;Contour and Gradient&#x27;</span>)</span><br><span class=\"line\">ax2.set_xlabel(<span class=\"string\">&#x27;$x_0$&#x27;</span>)</span><br><span class=\"line\">ax2.set_ylabel(<span class=\"string\">&#x27;$x_1$&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算梯度（偏导）</span></span><br><span class=\"line\">grad_x0 = <span class=\"number\">2</span> * X0</span><br><span class=\"line\">grad_x1 = <span class=\"number\">2</span> * X1</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制梯度向量（用 quiver 画箭头）</span></span><br><span class=\"line\">skip = <span class=\"number\">5</span>  <span class=\"comment\"># 降低密度</span></span><br><span class=\"line\">ax2.quiver(X0[::skip, ::skip], X1[::skip, ::skip],</span><br><span class=\"line\">           grad_x0[::skip, ::skip], grad_x1[::skip, ::skip],</span><br><span class=\"line\">           color=<span class=\"string\">&#x27;white&#x27;</span>, alpha=<span class=\"number\">0.8</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.colorbar(contour, ax=ax2, label=<span class=\"string\">&#x27;Function Value&#x27;</span>)</span><br><span class=\"line\">plt.tight_layout()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n</blockquote>\n</blockquote>\n<h4 id=\"梯度-1\"><a href=\"#梯度-1\" class=\"headerlink\" title=\"梯度\"></a>梯度</h4><blockquote>\n<h4 id=\"简单的梯度的实现\"><a href=\"#简单的梯度的实现\" class=\"headerlink\" title=\"简单的梯度的实现\"></a>简单的梯度的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 梯度的实现</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">numerical_gradient</span>(<span class=\"params\">f, x</span>):</span><br><span class=\"line\">    h = <span class=\"number\">1e-4</span></span><br><span class=\"line\">    <span class=\"comment\"># 生成和x形状相同的数组，且所有元素为0</span></span><br><span class=\"line\">    grad = np.zeros_like(x)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> idx <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(x.size):</span><br><span class=\"line\">        tmp_val = x[idx]</span><br><span class=\"line\">        <span class=\"comment\"># f(x+h)的计算</span></span><br><span class=\"line\">        x[idx] = tmp_val + h</span><br><span class=\"line\">        fxh1 = f(x)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># f(x-h)的计算</span></span><br><span class=\"line\">        x[idx] = tmp_val - h</span><br><span class=\"line\">        fxh2 = f(x)</span><br><span class=\"line\"></span><br><span class=\"line\">        grad[idx] = (fxh1 - fxh2) / (<span class=\"number\">2</span> * h)</span><br><span class=\"line\">        x[idx] = tmp_val <span class=\"comment\"># 还原值</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> grad</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(numerical_gradient(fun_2, np.array([<span class=\"number\">3.0</span>, <span class=\"number\">4.0</span>])))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(numerical_gradient(fun_2, np.array([<span class=\"number\">0.0</span>, <span class=\"number\">2.0</span>])))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(numerical_gradient(fun_2, np.array([<span class=\"number\">3.0</span>, <span class=\"number\">0.0</span>])))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"这里给出一个计算任意维数组的数值梯度\"><a href=\"#这里给出一个计算任意维数组的数值梯度\" class=\"headerlink\" title=\"这里给出一个计算任意维数组的数值梯度\"></a>这里给出一个计算任意维数组的数值梯度</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">numerical_gradient</span>(<span class=\"params\">f, x</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    计算任意维数组的数值梯度</span></span><br><span class=\"line\"><span class=\"string\">    参数:</span></span><br><span class=\"line\"><span class=\"string\">        f: 目标函数</span></span><br><span class=\"line\"><span class=\"string\">        x: 任意维度的数组</span></span><br><span class=\"line\"><span class=\"string\">    返回:</span></span><br><span class=\"line\"><span class=\"string\">        梯度数组，形状与x相同</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    实现原理：</span></span><br><span class=\"line\"><span class=\"string\">    1. 使用中心差分法计算梯度：(f(x+h) - f(x-h)) / (2h)</span></span><br><span class=\"line\"><span class=\"string\">    2. 对数组中的每个元素分别计算梯度</span></span><br><span class=\"line\"><span class=\"string\">    3. 使用numpy的迭代器处理任意维度的数组</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    h = <span class=\"number\">1e-4</span>  <span class=\"comment\"># 0.0001，微小的变化量</span></span><br><span class=\"line\">    grad = np.zeros_like(x)  <span class=\"comment\"># 创建与x形状相同的零数组，用于存储梯度</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 使用numpy的迭代器遍历数组的所有元素</span></span><br><span class=\"line\">    <span class=\"comment\"># flags=[&#x27;multi_index&#x27;]: 获取多维索引</span></span><br><span class=\"line\">    <span class=\"comment\"># op_flags=[&#x27;readwrite&#x27;]: 允许读写操作</span></span><br><span class=\"line\">    it = np.nditer(x, flags=[<span class=\"string\">&#x27;multi_index&#x27;</span>], op_flags=[<span class=\"string\">&#x27;readwrite&#x27;</span>])</span><br><span class=\"line\">    <span class=\"keyword\">while</span> <span class=\"keyword\">not</span> it.finished:</span><br><span class=\"line\">        idx = it.multi_index  <span class=\"comment\"># 获取当前元素的多维索引</span></span><br><span class=\"line\">        tmp_val = x[idx]  <span class=\"comment\"># 保存当前值</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 计算f(x+h)</span></span><br><span class=\"line\">        x[idx] = tmp_val + h</span><br><span class=\"line\">        fxh1 = f(x)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 计算f(x-h)</span></span><br><span class=\"line\">        x[idx] = tmp_val - h </span><br><span class=\"line\">        fxh2 = f(x)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 使用中心差分法计算梯度</span></span><br><span class=\"line\">        grad[idx] = (fxh1 - fxh2) / (<span class=\"number\">2</span>*h)</span><br><span class=\"line\">        </span><br><span class=\"line\">        x[idx] = tmp_val  <span class=\"comment\"># 恢复原始值</span></span><br><span class=\"line\">        it.iternext()  <span class=\"comment\"># 移动到下一个元素</span></span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">return</span> grad</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"梯度下降法的实现\"><a href=\"#梯度下降法的实现\" class=\"headerlink\" title=\"梯度下降法的实现\"></a>梯度下降法的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># f 是要进行最优化的函数，init_x是初始值，lr是学习率，step_num是梯度法的重复次数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">gradient_descent</span>(<span class=\"params\">f, init_x, lr=<span class=\"number\">0.01</span>, step_num=<span class=\"number\">100</span></span>):</span><br><span class=\"line\">    x = init_x</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(step_num):</span><br><span class=\"line\">        grad = numerical_gradient(f, x)</span><br><span class=\"line\">        x -= lr * grad</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">init_x = np.array([-<span class=\"number\">3.0</span>, <span class=\"number\">4.0</span>])</span><br><span class=\"line\">result_ = gradient_descent(fun_2, init_x=init_x, lr=<span class=\"number\">0.1</span>, step_num=<span class=\"number\">100</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(result_)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"图像展示梯度法的过程\"><a href=\"#图像展示梯度法的过程\" class=\"headerlink\" title=\"图像展示梯度法的过程\"></a>图像展示梯度法的过程</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">gradient_descent_</span>(<span class=\"params\">f, init_x, lr=<span class=\"number\">0.01</span>, step_num=<span class=\"number\">100</span></span>):</span><br><span class=\"line\">    x = init_x</span><br><span class=\"line\">    x_history = []</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(step_num):</span><br><span class=\"line\">        x_history.append(x.copy())</span><br><span class=\"line\">        </span><br><span class=\"line\">        grad = numerical_gradient(f, x)</span><br><span class=\"line\">        x -= lr * grad</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x, np.array(x_history)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">init_x = np.array([-<span class=\"number\">3.0</span>, <span class=\"number\">4.0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">lr = <span class=\"number\">0.1</span></span><br><span class=\"line\">step_num = <span class=\"number\">20</span></span><br><span class=\"line\">x, x_history = gradient_descent_(fun_2, init_x, lr, step_num)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.plot([-<span class=\"number\">5</span>, <span class=\"number\">5</span>], [<span class=\"number\">0</span>, <span class=\"number\">0</span>], <span class=\"string\">&#x27;--b&#x27;</span>)</span><br><span class=\"line\">plt.plot([<span class=\"number\">0</span>, <span class=\"number\">0</span>], [-<span class=\"number\">5</span>, <span class=\"number\">5</span>], <span class=\"string\">&#x27;--b&#x27;</span>)</span><br><span class=\"line\">plt.plot(x_history[:, <span class=\"number\">0</span>], x_history[:, <span class=\"number\">1</span>], <span class=\"string\">&#x27;o&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.xlim(-<span class=\"number\">3.5</span>, <span class=\"number\">3.5</span>)</span><br><span class=\"line\">plt.ylim(-<span class=\"number\">4.5</span>, <span class=\"number\">4.5</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;X0&quot;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;X1&quot;</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n</blockquote>\n<h4 id=\"神经网络的梯度-1\"><a href=\"#神经网络的梯度-1\" class=\"headerlink\" title=\"神经网络的梯度\"></a>神经网络的梯度</h4><blockquote>\n<h4 id=\"简单的神经网络\"><a href=\"#简单的神经网络\" class=\"headerlink\" title=\"简单的神经网络\"></a>简单的神经网络</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"><span class=\"comment\"># 导入必要的库</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys, os</span><br><span class=\"line\"><span class=\"comment\"># 将父目录添加到系统路径中，以便能够导入common模块</span></span><br><span class=\"line\">sys.path.append(os.pardir)</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.functions <span class=\"keyword\">import</span> softmax, cross_entropy_error</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.gradient <span class=\"keyword\">import</span> numerical_gradient</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">simpleNet</span>:</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    一个简单的神经网络类</span></span><br><span class=\"line\"><span class=\"string\">    实现了一个2输入3输出的单层神经网络</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 初始化权重矩阵，使用随机数生成2x3的权重矩阵</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.W = np.random.randn(<span class=\"number\">2</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        前向传播函数</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            神经网络的输出</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> np.dot(x, <span class=\"variable language_\">self</span>.W)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">loss</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        计算损失函数</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">            t: 正确解标签</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            损失函数的值</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        z = <span class=\"variable language_\">self</span>.predict(x)  <span class=\"comment\"># 获取神经网络的输出</span></span><br><span class=\"line\">        y = softmax(z)       <span class=\"comment\"># 使用softmax函数将输出转换为概率</span></span><br><span class=\"line\">        loss = cross_entropy_error(y, t)  <span class=\"comment\"># 计算交叉熵误差</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> loss</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建测试数据</span></span><br><span class=\"line\">x = np.array([<span class=\"number\">0.6</span>, <span class=\"number\">0.9</span>])  <span class=\"comment\"># 输入数据</span></span><br><span class=\"line\">t = np.array([<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>])   <span class=\"comment\"># 正确解标签</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建神经网络实例</span></span><br><span class=\"line\">net = simpleNet()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义损失函数，用于计算梯度</span></span><br><span class=\"line\">f = <span class=\"keyword\">lambda</span> w: net.loss(x, t)</span><br><span class=\"line\"><span class=\"comment\"># 使用数值微分计算梯度</span></span><br><span class=\"line\">dW = numerical_gradient(f, net.W)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 打印梯度结果</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(dW)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"这里需要注意的是，为什么要在类外定义一个损失函数f，因为类中的损失函数loss-是一个实例方法，需要访问实例的权重self-W，而numerical-gradient-函数参数中的f，需要的是一个纯函数，而不是一个类中的实例方法\"><a href=\"#这里需要注意的是，为什么要在类外定义一个损失函数f，因为类中的损失函数loss-是一个实例方法，需要访问实例的权重self-W，而numerical-gradient-函数参数中的f，需要的是一个纯函数，而不是一个类中的实例方法\" class=\"headerlink\" title=\"这里需要注意的是，为什么要在类外定义一个损失函数f，因为类中的损失函数loss()是一个实例方法，需要访问实例的权重self.W，而numerical_gradient()函数参数中的f，需要的是一个纯函数，而不是一个类中的实例方法\"></a>这里需要注意的是，为什么要在类外定义一个损失函数f，因为类中的损失函数loss()是一个实例方法，需要访问实例的权重self.W，而numerical_gradient()函数参数中的f，需要的是一个纯函数，而不是一个类中的实例方法</h4></blockquote>\n<h4 id=\"手写数字识别的神经网络的实现-1\"><a href=\"#手写数字识别的神经网络的实现-1\" class=\"headerlink\" title=\"手写数字识别的神经网络的实现\"></a>手写数字识别的神经网络的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys, os</span><br><span class=\"line\">sys.path.append(os.pardir)  <span class=\"comment\"># 将父目录添加到系统路径中，以便导入common模块</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> common.functions <span class=\"keyword\">import</span> *</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.gradient <span class=\"keyword\">import</span> numerical_gradient</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">TwoLayerNet</span>:</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    两层神经网络类</span></span><br><span class=\"line\"><span class=\"string\">    实现了一个具有一个隐藏层的神经网络</span></span><br><span class=\"line\"><span class=\"string\">    结构：输入层 -&gt; 隐藏层 -&gt; 输出层</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, input_size, hidden_size, output_size, weight_init_std=<span class=\"number\">0.01</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        初始化神经网络</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            input_size: 输入层神经元数量</span></span><br><span class=\"line\"><span class=\"string\">            hidden_size: 隐藏层神经元数量</span></span><br><span class=\"line\"><span class=\"string\">            output_size: 输出层神经元数量</span></span><br><span class=\"line\"><span class=\"string\">            weight_init_std: 权重初始化的标准差</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"comment\"># 初始化权重和偏置</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params = &#123;&#125;</span><br><span class=\"line\">        <span class=\"comment\"># 第一层权重矩阵，形状为(input_size, hidden_size)</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W1&#x27;</span>] = weight_init_std * np.random.randn(input_size, hidden_size)</span><br><span class=\"line\">        <span class=\"comment\"># 第一层偏置，形状为(hidden_size,)</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b1&#x27;</span>] = np.zeros(hidden_size)</span><br><span class=\"line\">        <span class=\"comment\"># 第二层权重矩阵，形状为(hidden_size, output_size)</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W2&#x27;</span>] = weight_init_std * np.random.randn(hidden_size, output_size)</span><br><span class=\"line\">        <span class=\"comment\"># 第二层偏置，形状为(output_size,)</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b2&#x27;</span>] = np.zeros(output_size)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        前向传播，计算神经网络的输出</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            神经网络的输出（经过softmax后的概率分布）</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        W1, W2 = <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W1&#x27;</span>], <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W2&#x27;</span>]</span><br><span class=\"line\">        b1, b2 = <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b1&#x27;</span>], <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b2&#x27;</span>]</span><br><span class=\"line\">    </span><br><span class=\"line\">        <span class=\"comment\"># 第一层计算</span></span><br><span class=\"line\">        a1 = np.dot(x, W1) + b1  <span class=\"comment\"># 线性变换</span></span><br><span class=\"line\">        z1 = sigmoid(a1)         <span class=\"comment\"># 激活函数</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 第二层计算</span></span><br><span class=\"line\">        a2 = np.dot(z1, W2) + b2  <span class=\"comment\"># 线性变换</span></span><br><span class=\"line\">        y = softmax(a2)           <span class=\"comment\"># 输出层激活函数</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> y</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">loss</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        计算损失函数</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">            t: 正确解标签</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            损失函数的值</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        y = <span class=\"variable language_\">self</span>.predict(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> cross_entropy_error(y, t)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">accuracy</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        计算识别精度</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">            t: 正确解标签</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            识别精度（0-1之间的值）</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        y = <span class=\"variable language_\">self</span>.predict(x)</span><br><span class=\"line\">        y = np.argmax(y, axis=<span class=\"number\">1</span>)  <span class=\"comment\"># 获取预测结果</span></span><br><span class=\"line\">        t = np.argmax(t, axis=<span class=\"number\">1</span>)  <span class=\"comment\"># 获取正确解</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 计算正确率</span></span><br><span class=\"line\">        accuracy = np.<span class=\"built_in\">sum</span>(y == t) / <span class=\"built_in\">float</span>(x.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> accuracy</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">numerical_gradient</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        使用数值微分计算梯度</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">            t: 正确解标签</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            包含各层权重和偏置梯度的字典</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"comment\"># 定义损失函数</span></span><br><span class=\"line\">        loss_W = <span class=\"keyword\">lambda</span> W: <span class=\"variable language_\">self</span>.loss(x, t)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 计算各层参数的梯度</span></span><br><span class=\"line\">        grads = &#123;&#125;</span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;W1&#x27;</span>] = numerical_gradient(loss_W, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W1&#x27;</span>])  <span class=\"comment\"># 第一层权重梯度</span></span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;b1&#x27;</span>] = numerical_gradient(loss_W, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b1&#x27;</span>])  <span class=\"comment\"># 第一层偏置梯度</span></span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;W2&#x27;</span>] = numerical_gradient(loss_W, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W2&#x27;</span>])  <span class=\"comment\"># 第二层权重梯度</span></span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;b2&#x27;</span>] = numerical_gradient(loss_W, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b2&#x27;</span>])  <span class=\"comment\"># 第二层偏置梯度</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> grads</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">gradient</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        使用反向传播计算梯度</span></span><br><span class=\"line\"><span class=\"string\">        参数:</span></span><br><span class=\"line\"><span class=\"string\">            x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">            t: 正确解标签</span></span><br><span class=\"line\"><span class=\"string\">        返回:</span></span><br><span class=\"line\"><span class=\"string\">            包含各层权重和偏置梯度的字典</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        W1, W2 = <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W1&#x27;</span>], <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W2&#x27;</span>]</span><br><span class=\"line\">        b1, b2 = <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b1&#x27;</span>], <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b2&#x27;</span>]</span><br><span class=\"line\">        grads = &#123;&#125;</span><br><span class=\"line\">        </span><br><span class=\"line\">        batch_num = x.shape[<span class=\"number\">0</span>]  <span class=\"comment\"># 批次大小</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 前向传播</span></span><br><span class=\"line\">        a1 = np.dot(x, W1) + b1</span><br><span class=\"line\">        z1 = sigmoid(a1)</span><br><span class=\"line\">        a2 = np.dot(z1, W2) + b2</span><br><span class=\"line\">        y = softmax(a2)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 反向传播</span></span><br><span class=\"line\">        <span class=\"comment\"># 输出层的误差</span></span><br><span class=\"line\">        dy = (y - t) / batch_num</span><br><span class=\"line\">        <span class=\"comment\"># 第二层权重的梯度</span></span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;W2&#x27;</span>] = np.dot(z1.T, dy)</span><br><span class=\"line\">        <span class=\"comment\"># 第二层偏置的梯度</span></span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;b2&#x27;</span>] = np.<span class=\"built_in\">sum</span>(dy, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 第一层的误差</span></span><br><span class=\"line\">        dz1 = np.dot(dy, W2.T)</span><br><span class=\"line\">        da1 = sigmoid_grad(a1) * dz1</span><br><span class=\"line\">        <span class=\"comment\"># 第一层权重的梯度</span></span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;W1&#x27;</span>] = np.dot(x.T, da1)</span><br><span class=\"line\">        <span class=\"comment\"># 第一层偏置的梯度</span></span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;b1&#x27;</span>] = np.<span class=\"built_in\">sum</span>(da1, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> grads</span><br><span class=\"line\">        </span><br><span class=\"line\">        </span><br><span class=\"line\">net = TwoLayerNet(input_size=<span class=\"number\">784</span>, hidden_size=<span class=\"number\">100</span>, output_size=<span class=\"number\">10</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net.params[<span class=\"string\">&#x27;W1&#x27;</span>].shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net.params[<span class=\"string\">&#x27;b1&#x27;</span>].shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net.params[<span class=\"string\">&#x27;W2&#x27;</span>].shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(net.params[<span class=\"string\">&#x27;b2&#x27;</span>].shape)</span><br><span class=\"line\"></span><br><span class=\"line\">x = np.random.rand(<span class=\"number\">100</span>, <span class=\"number\">784</span>)</span><br><span class=\"line\">t = np.random.rand(<span class=\"number\">100</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">grads = net.numerical_gradient(x, t)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(grads[<span class=\"string\">&#x27;W1&#x27;</span>].shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(grads[<span class=\"string\">&#x27;b1&#x27;</span>].shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(grads[<span class=\"string\">&#x27;W2&#x27;</span>].shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(grads[<span class=\"string\">&#x27;b2&#x27;</span>].shape)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"mini-batch的实现，以及基于测试数据的评价\"><a href=\"#mini-batch的实现，以及基于测试数据的评价\" class=\"headerlink\" title=\"mini-batch的实现，以及基于测试数据的评价\"></a>mini-batch的实现，以及基于测试数据的评价</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys, os</span><br><span class=\"line\">sys.path.append(<span class=\"string\">&#x27;../../py_pro/DL/&#x27;</span>)  <span class=\"comment\"># 将父目录添加到系统路径中，以便导入其他模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist  <span class=\"comment\"># 导入MNIST数据集加载函数</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> two_layer_net <span class=\"keyword\">import</span> TwoLayerNet  <span class=\"comment\"># 导入两层神经网络类</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载MNIST数据集</span></span><br><span class=\"line\"><span class=\"comment\"># normalize=True: 将输入图像像素值正规化到0~1之间</span></span><br><span class=\"line\"><span class=\"comment\"># one_hot_label=True: 将标签转换为one-hot形式</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>, one_hot_label=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建两层神经网络实例</span></span><br><span class=\"line\"><span class=\"comment\"># 输入层784个神经元（28x28像素）</span></span><br><span class=\"line\"><span class=\"comment\"># 隐藏层50个神经元</span></span><br><span class=\"line\"><span class=\"comment\"># 输出层10个神经元（0-9十个数字）</span></span><br><span class=\"line\">network = TwoLayerNet(input_size=<span class=\"number\">784</span>, hidden_size=<span class=\"number\">50</span>, output_size=<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置超参数</span></span><br><span class=\"line\">iters_num = <span class=\"number\">10000</span>  <span class=\"comment\"># 训练迭代次数</span></span><br><span class=\"line\">train_size = x_train.shape[<span class=\"number\">0</span>]  <span class=\"comment\"># 训练数据的大小</span></span><br><span class=\"line\">batch_size = <span class=\"number\">100</span>  <span class=\"comment\"># 每批次的样本数</span></span><br><span class=\"line\">learning_rate = <span class=\"number\">0.1</span>  <span class=\"comment\"># 学习率</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 用于记录训练过程的列表</span></span><br><span class=\"line\">train_loss_list = []  <span class=\"comment\"># 记录训练损失</span></span><br><span class=\"line\">train_acc_list = []   <span class=\"comment\"># 记录训练准确率</span></span><br><span class=\"line\">test_acc_list = []    <span class=\"comment\"># 记录测试准确率</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算每个epoch的迭代次数</span></span><br><span class=\"line\">iter_per_epoch = <span class=\"built_in\">max</span>(train_size / batch_size, <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 开始训练</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(iters_num):</span><br><span class=\"line\">    <span class=\"comment\"># 随机选择批次数据</span></span><br><span class=\"line\">    batch_mask = np.random.choice(train_size, batch_size)</span><br><span class=\"line\">    x_batch = x_train[batch_mask]</span><br><span class=\"line\">    t_batch = t_train[batch_mask]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 计算梯度</span></span><br><span class=\"line\">    <span class=\"comment\"># 使用反向传播计算梯度（比数值微分更快）</span></span><br><span class=\"line\">    <span class=\"comment\">#grad = network.numerical_gradient(x_batch, t_batch)  # 数值微分（较慢）</span></span><br><span class=\"line\">    grad = network.gradient(x_batch, t_batch)  <span class=\"comment\"># 反向传播（较快）</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 更新参数</span></span><br><span class=\"line\">    <span class=\"comment\"># 对每个参数进行梯度下降更新</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> (<span class=\"string\">&#x27;W1&#x27;</span>, <span class=\"string\">&#x27;b1&#x27;</span>, <span class=\"string\">&#x27;W2&#x27;</span>, <span class=\"string\">&#x27;b2&#x27;</span>):</span><br><span class=\"line\">        network.params[key] -= learning_rate * grad[key]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 记录训练损失</span></span><br><span class=\"line\">    loss = network.loss(x_batch, t_batch)</span><br><span class=\"line\">    train_loss_list.append(loss)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 每个epoch计算一次训练集和测试集的准确率</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> i % iter_per_epoch == <span class=\"number\">0</span>:</span><br><span class=\"line\">        train_acc = network.accuracy(x_train, t_train)</span><br><span class=\"line\">        test_acc = network.accuracy(x_test, t_test)</span><br><span class=\"line\">        train_acc_list.append(train_acc)</span><br><span class=\"line\">        test_acc_list.append(test_acc)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;train acc, test acc | &quot;</span> + <span class=\"built_in\">str</span>(train_acc) + <span class=\"string\">&quot;, &quot;</span> + <span class=\"built_in\">str</span>(test_acc))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制训练结果</span></span><br><span class=\"line\">markers = &#123;<span class=\"string\">&#x27;train&#x27;</span>: <span class=\"string\">&#x27;o&#x27;</span>, <span class=\"string\">&#x27;test&#x27;</span>: <span class=\"string\">&#x27;s&#x27;</span>&#125;  <span class=\"comment\"># 设置图例标记</span></span><br><span class=\"line\">x = np.arange(<span class=\"built_in\">len</span>(train_acc_list))</span><br><span class=\"line\">plt.plot(x, train_acc_list, label=<span class=\"string\">&#x27;train acc&#x27;</span>)  <span class=\"comment\"># 绘制训练准确率曲线</span></span><br><span class=\"line\">plt.plot(x, test_acc_list, label=<span class=\"string\">&#x27;test acc&#x27;</span>, linestyle=<span class=\"string\">&#x27;--&#x27;</span>)  <span class=\"comment\"># 绘制测试准确率曲线</span></span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;epochs&quot;</span>)  <span class=\"comment\"># x轴标签</span></span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;accuracy&quot;</span>)  <span class=\"comment\"># y轴标签</span></span><br><span class=\"line\">plt.ylim(<span class=\"number\">0</span>, <span class=\"number\">1.0</span>)  <span class=\"comment\"># 设置y轴范围</span></span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;lower right&#x27;</span>)  <span class=\"comment\"># 显示图例</span></span><br><span class=\"line\">plt.show()  <span class=\"comment\"># 显示图形</span></span><br></pre></td></tr></table></figure>\n\n\n</blockquote>\n<h3 id=\"都说这本鱼书是-最最最-最入门的神经网络和深度学习的书了，但是，从这一章开始，已经开始头脑风暴了，难顶哟-TUT\"><a href=\"#都说这本鱼书是-最最最-最入门的神经网络和深度学习的书了，但是，从这一章开始，已经开始头脑风暴了，难顶哟-TUT\" class=\"headerlink\" title=\"都说这本鱼书是 最最最 最入门的神经网络和深度学习的书了，但是，从这一章开始，已经开始头脑风暴了，难顶哟(TUT)\"></a>都说这本鱼书是 最最最 最入门的神经网络和深度学习的书了，但是，从这一章开始，已经开始头脑风暴了，难顶哟(TUT)</h3>"},{"title":"DL之路---啃鱼书（6）","data":"2025-07-15T13:26:00.000Z","updated":"2025-07-15T13:26:00.000Z","type":"DL","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p26.png","_content":"\n\n# 卷积神经网络\n\n\n\n> #### 卷积神经网络（Convolutional Neural Network，CNN），用于图像识别、语音识别等场景\n\n\n\n> #### 相邻层的所有神经元之间都有连接，称之为全连接，使用Affine层实现全连接层\n>\n> #### 下面是一个5层的全连接神经网络\n>\n> ![](http://picbed.yanzu.tech/img/DL/6/1.png)\n>\n> #### 可以看到，每个Affine层后面跟着激活函数层\n>\n> #### 而基于CNN的神经网络结构如下\n>\n> ![](http://picbed.yanzu.tech/img/DL/6/2.png)\n>\n> #### 可以看到，CNN的层的连接顺序是 $Convolution - ReLU-Pooling$ ，Pooling层有时会被省略，另外，靠近输出的层中使用了之前使用了 \"Affine-ReLU\"组合\n>\n> \n>\n> #### 卷积层\n>\n> > #### 回顾Affine层：\n> >\n> > > #### 在全连接层中，相邻层的神经元全部连接在一起，输出的数量可以任意决定\n> > >\n> > > #### 它存在的一个问题是，忽视了输入数据的形状，如输入数据是图像（高、长、通道方向上的3维形状）时，输入到Affine层会将3维数据拉成1维的，无法利用与形状相关的信息\n> >\n> > #### 而卷积层Conv可以保持形状不变，当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层\n> >\n> > #### 在CNN中，有时将卷积层Conv的输入输出数据称为特征图（feature map），输入数据就叫输入特征图，输出就叫输出特征图\n> >\n> > \n> >\n> > #### 卷积运算\n> >\n> > > #### 卷积层进行的处理就是卷积运算，结合例子说明（输入数据和滤波器之间的符号表示卷积运算）\n> > >\n> > > ![](http://picbed.yanzu.tech/img/DL/6/3.png)\n> > >\n> > > #### 这里的滤波器（filter）也被称为“卷积核（convolution kernel）”，滤波器是一个结构单位，里面的值就是权重。另外，输入数据和卷积核都是有高宽方向的形状的数据(height, weight)，卷积运算的过程如下\n> > >\n> > > ![](http://picbed.yanzu.tech/img/DL/6/4.png)\n> > >\n> > > #### 对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（乘积累加运算），然后，将这个结果保存到输出的对应位置，所有位置进行一次，就可以得到卷积运算的输出\n> > >\n> > > #### 在全连接的神经网络中，除了权重参数，还存在偏置；在CNN中，滤波器的参数就对应之前的权重，且也存在偏置，包含偏执的卷积运算处理如下\n> > >\n> > > ![](http://picbed.yanzu.tech/img/DL/6/5.png)\n> > >\n> > > #### 上面的操作，本质上就是在进行多个 $ y = W \\cdot x + b$ 运算，所以所有的输出都要 +b \n> >\n> > \n> >\n> > #### 填充\n> >\n> > > #### 在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（如0等），这称之为填充（padding）\n> > >\n> > > #### 为什么要进行填充操作呢？\n> > >\n> > > > #### 通过之前的卷积操作过程，注意到，经过卷积运算的输入数据得到的输出数据，大小（空间）缩小了，而卷积层往往不止一层，多次卷积操作之后，可能会使得输出数据的大小变为1从而导致不能再进行卷积操作了（赘述）\n> > > >\n> > > > #### 所以，使用填充主要是为了调整输出的大小，之所以要进行填充，是因为如果每次进行卷积运算都会缩小空间（如(4,4)的输入，(3,3)的滤波器，输出是(2,2)），那么在某个时刻输出大小就有可能变为 1，导致无法再应用卷积运算\n> > >\n> > > #### 填充如何实现的，例：对大小为（4，4）的输入数据应用了幅度为1的填充\n> > >\n> > > ![](http://picbed.yanzu.tech/img/DL/6/6.png)\n> > >\n> > > #### \"幅度为1的填充\"是指用幅度为1像素的0填充周围，填0的话直接省略不写，填充后，(4,4)的输入数据变成了(6,6)的形状\n> > >\n> >\n> > \n> >\n> > #### 步幅\n> >\n> > > #### 应用滤波器（卷积核）的位置间隔称为步幅（stride）\n> > >\n> > > #### 下面将应用滤波器的窗口的间隔变为2个元素\n> > >\n> > > ![](http://picbed.yanzu.tech/img/DL/6/7.png)\n> > >\n> > > #### 注意，步幅为2，意味着各个方向上（横向纵向）的移动都是2个元素\n> > >\n> > > #### 增大步幅后，输出大小会变小。而增大填充后，输出大小会变大，对于步幅和填充，计算输出大小\n> > >\n> > > > #### 若输入大小为(H,W)，卷积核大小为（FH,FW），输出大小为(OH,OW)，填充为P，步幅为S，此时，输出大小可通过以下式子计算\n> > > >\n> > > > $$\n> > > > OH = \\frac {H + 2P - FH} {S} + 1 \\\\\n> > > > OW = \\frac {W + 2P - FW} {S} + 1\n> > > > $$\n> > > >\n> > > > #### 不过有一个问题是，分式的值可能是小数（除不尽的情况），这时需要采取报错等对策，采取的深度学习框架不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行\n> >\n> > \n> >\n> > #### 3维数据的卷积运算\n> >\n> > > #### 对于3维数据的卷积运算，相比于2维数据的，其在纵深方向（通道方向）上特征图增加了\n> > >\n> > > #### 通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出\n> > >\n> > > ![](http://picbed.yanzu.tech/img/DL/6/8.png)\n> > >\n> > > #### 需要注意的是，输入数据和滤波器的通道数要设为相同的值\n> > >\n> > > \n> > >\n> > > #### 通过立体方块来理解\n> > >\n> > > > #### 以3维数据为例，把3维数据表示为多维数组时，书写顺序是 $(channel, height, weight)$ 或 $(C,H,W)$，滤波器(卷积核)也是写作这样的形式 $(C,FH,FW)$ ，如下\n> > > >\n> > > > ![](http://picbed.yanzu.tech/img/DL/6/16.png)\n> > > >\n> > > > #### 上面的输出数据是一张特征图，也即通道数为 1 的特征图，如果要在通道方向上也有多个卷积核运算的输出，就需要用到多个滤波器（权重），如下\n> > > >\n> > > > ![](http://picbed.yanzu.tech/img/DL/6/17.png)\n> > > >\n> > > > #### 通过使用FN个卷积核，对应的输出特征图也变成了FN个，将这FN个特征图汇集在一起，就得到了一个形状为 $(FN,OH,OW)$ 的方块，再将这个方块传递到下一层，就形成了CNN的处理流\n> > > >\n> > > > #### 那么4维的数据，卷积核的权重数据的书写顺序应该是 $(FC,CC,FH,FW)$ ，即卷积核个数、卷积核通道数、卷积核高、卷积核宽\n> > > >\n> > > > #### 同样，也要加上偏置，如下\n> > > >\n> > > > ![](http://picbed.yanzu.tech/img/DL/6/18.png)\n> > > >\n> > > > #### 注意，偏置的通道数要与卷积核的个数相同\n> > >\n> > > \n> > >\n> > > #### 批处理\n> > >\n> > > > #### 通过批处理，能够实现处理的高效化和学习时对mini-batch的对应。在卷积运算中应用批处理，需要将在各层间传递的数据保存为4维数据，即 $(batch-num,channel,height,weight)$ \n> > > >\n> > > > #### N个数据的批处理如下，\n> > > >\n> > > > ![](http://picbed.yanzu.tech/img/DL/6/19.png)\n> > > >\n> > > > #### 批处理将N次处理汇总成了一次进行\n> >\n>\n\n\n\n### 池化层\n\n> #### 池化是缩小高、长方向上的空间的运算\n>\n> #### 它的主要作用是对输出特征图进行下采样(subsampling or downsampling)\n>\n> #### 它的作用是\n>\n> > #### 降维、减少计算量和内存开销，当原始输出特征图尺寸较大时，池化可以有效减小后续网络层的输入大小，从而减少参数数量和计算量\n> >\n> > #### 提取主要特征、抑制噪声，如Max池化、Average池化、Global池化等\n> >\n> > #### 提高模型泛化能力，池化引入了某种程度的位置不变性(translation invariance)，使模型对小范围的平移、旋转、缩放更具鲁棒性\n> >\n> > #### 防止过拟合，减少特征维度，限制了网络学习过于复杂的模式，有助于抑制过拟合\n>\n> #### 池化的目的是减小特征尺寸、增强抽象能力、提高效率\n>\n> #### 补充：池化处理的就是张量，张量（tensor）就是一个具有维度的数值数组，0维是标量，1维是向量，2维是矩阵，3维及以上就是张量\n>\n> \n>\n> #### 下面是Max池化的处理顺序\n>\n> ![](http://picbed.yanzu.tech/img/DL/6/20.png)\n>\n> #### 这个例子是按照步幅为2进行池化窗口为 2x2 的Max池化操作的，处理的是输出特征图，Max池化是从目标区域（池化窗口）中取出最大值，提取显著特征\n>\n> #### 一般来说，池化的窗口大小会和步幅设定为相同值\n>\n> #### 除了Max池化，还有Average池化和Global池化，Average池化则是计算目标区域的平均值，用于平滑特征图；而Global池化是对整个特征图求平均或最大值，常用于分类前最后一层\n>\n> #### 池化层的特征\n>\n> > #### 没有要学习的参数，池化层和卷积层不同，没有要学习的参数\n> >\n> > #### 通道数不发生变化，经过池化运算，输入数据和输出数据的通道数不会发生变化\n> >\n> > #### 对微小的位置变化具有鲁棒性，输入数据发生微小偏差时，池化仍会返回相同的结果，如下面的例子\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/6/21.png)\n> >\n> > #### 当输入数据在宽度方向上只偏离1个元素时，输出仍为相同的结果，当然，根据数据的不同，有时结果也不相同\n\n\n\n\n\n### 卷积层和池化层的实现\n\n> #### CNN中各层间传递的数据是4维数据，即 $(数据个数,通道数,高,宽)$ \n>\n> #### 基于im2col的展开\n>\n> > #### im2col是一个函数，将输入数据展开以适合滤波器（权重），对于输入数据，将应用卷积核的区域(3维方块)横向展开为1列，im2col会在所有应用卷积核的区域进行这个展开处理，如下所示\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/6/22.png)\n> >\n> > #### 值得注意的是，滤波器的应用区域几乎都是重叠的，在此情况下，使用im2col展开后，展开后的元素个数会多于原方块(输入数据)的元素个数，因此，使用im2col的实现存在比普通的实现消耗更多内存的缺点。但，汇总成一个大的矩阵进行计算，对计算机的计算颇有益处，因为可以有效地利用线性代数库\n> >\n> > #### 另外，im2col 意为 image to column，即图像到矩阵\n> >\n> > #### im2col的处理过程如下\n> >\n> > > 1. #### 从原始输入图像数据中提取所有滑动窗口区域（patch）\n> > >\n> > > 2. #### 将每个patch展开成1行\n> > >\n> > > 3. #### 再将所有的patch行拼接成一个矩阵（im2col矩阵）\n> > >\n> > > 4. #### 将卷积核拉平成一个列向量\n> > >\n> > > 5. #### 进行矩阵乘法（类似于Affine层进行的处理）\n> > >\n> > > 6. #### 再reshape成输出特征图的形状\n> >\n> > #### 使用im2col展开输入数据后，之后就只需将卷积层的滤波器（权重）纵向展开为1列，并计算2个矩阵的乘积即可，这和全连接层的Affi ne层进行的处理基本相同\n> >\n> > #### 使用im2col的卷积运算的卷积核处理过程如下\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/6/13.png)\n> >\n> > #### 因为CNN中数据会保存为4维数组，所以要将2维输出数据转换(reshape)为合适的形状，这就是卷积层的实现流程\n>\n> \n>\n> #### 卷积层的实现\n>\n> > #### 在卷积层的实现过程中，有以下几个点需要注意\n> >\n> > > 1. #### im2col函数得到的是col，这个col表示 $(输出区域数,每个区域展开后元素个数)$ ，这个输出区域数其实就是 H_out * W_out，它就是经过im2col展开后得到的行数。而第2个参数就是 卷积核大小 * 通道数，即 $FH \\* FW \\* C$ \n> > >\n> > > 2. #### 卷积层的实现的类中的forward方法里，im2col展开处理后的数据形状 col.shape=(N\\*out_h\\*out_w, C\\*FH\\*FW)，卷积核展开之后的形状 col_W.shape = (C\\*FH\\*FW, FN)，那么 dot(col, col_W) 得到的输出的形状就是 out.shape = (N\\*out_h\\*out_w, FN)，这表示为：每一张输入图像，每一个输出位置(out_h*out_w)个，对每一个卷积核（共FN个）都得到一个输出数值。再将得到的输出out还原为 4D 卷积输出张量结构。注意：这里的N是batch_size，也是输入图像的个数；FN是卷积核的个数，也是这层卷积层的输出通道数\n> > >\n> > > 3. #### 将out还原为4D卷积输出张量结构，这里就有一个新的问题，为什么不能直接使用 reshape 将out.shape变为 (N, FN, out_h, out_w) ？这是因为 reshape 只是重新排列元素的形状，不能改变维度的排列顺序（轴顺序），要改变轴顺序必须使用 transpose，out.shape = ( N \\* out_h \\* out_w, FN )，axis=0:N\\*out_h\\*out_w，axis=1:FN，reshape不会打乱数据顺序，而transpose是要调整轴的排列的，transpose(0,3,1,2)这里的数值是reshape之后 0N1H2W3C 的下标，不是实际值，最后要调整为NCHW（0,3,1,2），如下图\n> > >\n> > > 4. #### 在进行卷积层的反向传播时，必须进行im2col的逆处理，这里使用一个col2im函数，代码在code里\n> > >\n> > > ![](http://picbed.yanzu.tech/img/DL/6/23.png)\n>\n> \n>\n> #### 池化层的实现\n>\n> > #### 池化层的实现和卷积层相同，都使用im2col展开输入数据，不过，池化的情况下，在通道方向上是独立的，这一点和卷积层不同，也即池化的应用区域按通道单独展开\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/6/14.png)\n> >\n> > #### 然后进行Max池化\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/6/15.png)\n\n\n\n### CNN的实现\n\n> #### 有几点需要注意\n>\n> > 1. #### 初始化方法中，池化输出大小的代码如下\n> >\n> >    ```python\n> >    pool_output_size = int(filter_num * (conv_output_size / 2) * (conv_output_size / 2))\n> >    ```\n> >\n> >    #### 其中 conv_output_size/2 是因为后面定义的池化窗口大小为2x2，且stride=2，所以池化后输出的高宽会变为原来的一半，池化层的设定如下\n> >\n> >    ```python\n> >    self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n> >    ```\n> >\n> > 2. #### 初始化方法中，参数 weight_init_std=0.01 的作用，这是为了控制权重初始化的“数值范围”，防止信号和梯度在网络中“放大或消失”，从而保持网络稳定训练，np.random.randn()是均值为0、标准差为1的正态分布，数值范围在-3到3，这有可能会导致梯度消失或爆炸。而乘以0.01后就变为了均值为0，标准差为0.01的正态分布，则会更稳定一点\n\n\n\n### CNN的可视化\n\n> #### 卷积层的滤波器会提取边缘或斑块等原始信息\n\n\n\n### 具有代表性的CNN\n\n> #### LeNet\n>\n> #### AlexNet\n\n\n\n\n\n### Code\n\n> #### 卷积层的实现\n>\n> ```python\n> class Convolution:\n>  \"\"\"\n>  卷积层，实现前向和反向传播。\n>  \"\"\"\n>  def __init__(self, W, b, stride=1, pad=0):\n>      self.W = W  # 卷积核权重，形状(FN, C, FH, FW)\n>      self.b = b  # 偏置，形状(FN,)\n>      self.stride = stride  # 步幅\n>      self.pad = pad        # 填充\n>      self.x = None   # 输入\n>      self.col = None # im2col展开后的输入\n>      self.col_W = None # 展开后的卷积核\n>      self.dW = None  # 权重梯度\n>      self.db = None  # 偏置梯度\n> \n>  def forward(self, x):\n>      \"\"\"\n>      前向传播：im2col展开输入，矩阵乘法实现卷积。\n>      \"\"\"\n>      FN, C, FH, FW = self.W.shape  # 卷积核参数\n>      N, C, H, W = x.shape          # 输入参数\n>      out_h = 1 + int((H + 2*self.pad - FH) / self.stride)  # 输出高\n>      out_w = 1 + int((W + 2*self.pad - FW) / self.stride)  # 输出宽\n>      col = im2col(x, FH, FW, self.stride, self.pad)  # 输入展开\n>      col_W = self.W.reshape(FN, -1).T  # 卷积核展开\n>      out = np.dot(col, col_W) + self.b  # 矩阵乘法+偏置\n>      out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)  # 还原输出形状\n>      self.x = x\n>      self.col = col\n>      self.col_W = col_W\n>      return out\n> \n>  def backward(self, dout):\n>      \"\"\"\n>      反向传播：计算输入、权重、偏置的梯度。\n>      \"\"\"\n>      FN, C, FH, FW = self.W.shape\n>      dout = dout.transpose(0,2,3,1).reshape(-1, FN)  # 调整dout形状\n>      self.db = np.sum(dout, axis=0)  # 偏置梯度\n>      self.dW = np.dot(self.col.T, dout)  # 权重梯度\n>      self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)  # 还原权重形状\n>      dcol = np.dot(dout, self.col_W.T)  # 输入展开后的梯度\n>      dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)  # 还原输入形状\n>      return dx\n> ```\n>\n> #### col2im\n>\n> ```python\n> # col2im: 将im2col展开的二维矩阵还原为原始多维图像数据\n> def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n>     \"\"\"\n>     将im2col展开的二维矩阵还原为原始的多维图像数据。\n> \n>     Parameters\n>     ----------\n>     col : 展开后的二维矩阵\n>     input_shape : 原始输入数据的形状（如：(10, 1, 28, 28)）\n>     filter_h : 滤波器高度\n>     filter_w : 滤波器宽度\n>     stride : 步幅\n>     pad : 填充\n> \n>     Returns\n>     -------\n>     img : 还原后的多维图像数据\n>     \"\"\"\n>     N, C, H, W = input_shape\n>     out_h = (H + 2*pad - filter_h)//stride + 1  # 输出高度\n>     out_w = (W + 2*pad - filter_w)//stride + 1  # 输出宽度\n>     # 还原为im2col前的形状\n>     col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n> \n>     # 初始化还原后的图像，注意填充和步幅的影响\n>     img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n>     # 将每个patch累加到对应的位置\n>     for y in range(filter_h):\n>         y_max = y + stride*out_h\n>         for x in range(filter_w):\n>             x_max = x + stride*out_w\n>             img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n> \n>     # 去除填充部分，返回原始大小\n>     return img[:, :, pad:H + pad, pad:W + pad]\n> ```\n>\n> \n>\n> #### 池化层的实现\n>\n> ```python\n> class Pooling:\n>  \"\"\"\n>  池化层，实现最大池化的前向和反向传播。\n>  \"\"\"\n>  def __init__(self, pool_h, pool_w, stride=2, pad=0):\n>      self.pool_h = pool_h  # 池化窗口高\n>      self.pool_w = pool_w  # 池化窗口宽\n>      self.stride = stride  # 步幅\n>      self.pad = pad        # 填充\n>      self.x = None         # 输入\n>      self.arg_max = None   # 最大值索引\n> \n>  def forward(self, x):\n>      \"\"\"\n>      前向传播：im2col展开后做最大池化。\n>      \"\"\"\n>      N, C, H, W = x.shape\n>      out_h = int(1 + (H - self.pool_h) / self.stride)  # 输出高\n>      out_w = int(1 + (W - self.pool_w) / self.stride)  # 输出宽\n>      col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)  # 输入展开\n>      col = col.reshape(-1, self.pool_h*self.pool_w)  # 每个池化区域展平成一行\n>      arg_max = np.argmax(col, axis=1)  # 最大值索引\n>      out = np.max(col, axis=1)         # 最大值\n>      out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)  # 还原输出形状\n>      self.x = x\n>      self.arg_max = arg_max\n>      return out\n> \n>  def backward(self, dout):\n>      \"\"\"\n>      反向传播：最大池化的反向传播，将梯度传递到最大值位置。\n>      \"\"\"\n>      dout = dout.transpose(0, 2, 3, 1)  # 调整dout形状\n>      pool_size = self.pool_h * self.pool_w\n>      dmax = np.zeros((dout.size, pool_size))  # 初始化梯度\n>      dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()  # 只在最大值位置赋值\n>      dmax = dmax.reshape(dout.shape + (pool_size,)) \n>      dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)  # 展平成二维\n>      dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)  # 还原输入形状\n>      return dx\n> ```\n>\n> \n>\n> #### 简单的CNN例子\n>\n> ```python\n> # coding: utf-8\n> import sys, os\n> sys.path.append(os.pardir)  # 添加父目录到模块搜索路径，便于导入common模块\n> import pickle\n> import numpy as np\n> from collections import OrderedDict\n> from common.layers import *\n> from common.gradient import numerical_gradient\n> \n> \n> class SimpleConvNet:\n>     \"\"\"\n>     简单卷积神经网络（ConvNet）实现：\n>     网络结构为 conv - relu - pool - affine - relu - affine - softmax。\n>     支持参数保存与加载，支持数值梯度和反向传播梯度。\n>     \"\"\"\n>     def __init__(self, input_dim=(1, 28, 28), \n>                  conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n>                  hidden_size=100, output_size=10, weight_init_std=0.01):\n>         \"\"\"\n>         初始化网络参数和各层。\n>         input_dim: 输入数据的形状 (通道数, 高, 宽)\n>         conv_param: 卷积层参数字典，包括filter_num, filter_size, pad, stride\n>         hidden_size: 隐藏层神经元数\n>         output_size: 输出类别数\n>         weight_init_std: 权重初始化标准差\n>         \"\"\"\n>         filter_num = conv_param['filter_num']\n>         filter_size = conv_param['filter_size']\n>         filter_pad = conv_param['pad']\n>         filter_stride = conv_param['stride']\n>         input_size = input_dim[1]\n>         # 计算卷积层输出尺寸\n>         conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n>         # 计算池化层输出尺寸（池化窗口为2x2，步幅2）\n>         pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n> \n>         # 权重初始化\n>         self.params = {}\n>         self.params['W1'] = weight_init_std * \\\n>                             np.random.randn(filter_num, input_dim[0], filter_size, filter_size)  # 卷积核权重\n>         self.params['b1'] = np.zeros(filter_num)  # 卷积核偏置\n>         self.params['W2'] = weight_init_std * \\\n>                             np.random.randn(pool_output_size, hidden_size)  # 全连接层1权重\n>         self.params['b2'] = np.zeros(hidden_size)  # 全连接层1偏置\n>         self.params['W3'] = weight_init_std * \\\n>                             np.random.randn(hidden_size, output_size)  # 全连接层2权重\n>         self.params['b3'] = np.zeros(output_size)  # 全连接层2偏置\n> \n>         # 构建网络层（有序字典保证前向/反向顺序）\n>         self.layers = OrderedDict()\n>         self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n>                                            conv_param['stride'], conv_param['pad'])  # 卷积层\n>         self.layers['Relu1'] = Relu()  # 激活层\n>         self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)  # 池化层\n>         self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])  # 全连接层1\n>         self.layers['Relu2'] = Relu()  # 激活层\n>         self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])  # 全连接层2\n> \n>         self.last_layer = SoftmaxWithLoss()  # 最后一层为softmax+损失\n> \n>     def predict(self, x):\n>         \"\"\"\n>         前向传播，输出网络预测结果。\n>         x: 输入数据\n>         返回：输出结果\n>         \"\"\"\n>         for layer in self.layers.values():\n>             x = layer.forward(x)\n>         return x\n> \n>     def loss(self, x, t):\n>         \"\"\"\n>         计算损失函数值。\n>         x: 输入数据\n>         t: 标签（监督信号）\n>         返回：损失值\n>         \"\"\"\n>         y = self.predict(x)\n>         return self.last_layer.forward(y, t)\n> \n>     def accuracy(self, x, t, batch_size=100):\n>         \"\"\"\n>         计算预测精度。\n>         x: 输入数据\n>         t: 标签\n>         batch_size: 批大小\n>         返回：精度（0~1）\n>         \"\"\"\n>         if t.ndim != 1 : t = np.argmax(t, axis=1)  # 若为one-hot则转为标签索引\n>         acc = 0.0\n>         for i in range(int(x.shape[0] / batch_size)):\n>             tx = x[i*batch_size:(i+1)*batch_size]\n>             tt = t[i*batch_size:(i+1)*batch_size]\n>             y = self.predict(tx)\n>             y = np.argmax(y, axis=1)\n>             acc += np.sum(y == tt)  # 统计预测正确个数\n>         return acc / x.shape[0]\n> \n>     def numerical_gradient(self, x, t):\n>         \"\"\"\n>         用数值微分法计算各层参数的梯度。\n>         x: 输入数据\n>         t: 标签\n>         返回：包含各层权重和偏置梯度的字典\n>         \"\"\"\n>         loss_w = lambda w: self.loss(x, t)\n>         grads = {}\n>         for idx in (1, 2, 3):\n>             grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])  # 权重梯度\n>             grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])  # 偏置梯度\n>         return grads\n> \n>     def gradient(self, x, t):\n>         \"\"\"\n>         用反向传播法计算各层参数的梯度。\n>         x: 输入数据\n>         t: 标签\n>         返回：包含各层权重和偏置梯度的字典\n>         \"\"\"\n>         # 前向传播，计算损失\n>         self.loss(x, t)\n>         # 反向传播，计算梯度\n>         dout = 1\n>         dout = self.last_layer.backward(dout)\n>         layers = list(self.layers.values())\n>         layers.reverse()  # 反向传播需逆序\n>         for layer in layers:\n>             dout = layer.backward(dout)\n>         # 收集各层的权重和偏置梯度\n>         grads = {}\n>         grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n>         grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n>         grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n>         return grads\n>         \n>     def save_params(self, file_name=\"params.pkl\"):\n>         \"\"\"\n>         保存网络参数到文件。\n>         file_name: 文件名\n>         \"\"\"\n>         params = {}\n>         for key, val in self.params.items():\n>             params[key] = val\n>         with open(file_name, 'wb') as f:\n>             pickle.dump(params, f)\n> \n>     def load_params(self, file_name=\"params.pkl\"):\n>         \"\"\"\n>         从文件加载网络参数。\n>         file_name: 文件名\n>         \"\"\"\n>         with open(file_name, 'rb') as f:\n>             params = pickle.load(f)\n>         for key, val in params.items():\n>             self.params[key] = val\n>         # 更新各层的权重和偏置\n>         for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n>             self.layers[key].W = self.params['W' + str(i+1)]\n>             self.layers[key].b = self.params['b' + str(i+1)]\n> ```\n>\n> \n>\n> #### 手写数字识别的CNN\n>\n> ```python\n> # coding: utf-8\n> import sys, os\n> sys.path.append(os.pardir)  # 将父目录添加到sys.path，便于导入模块\n> import numpy as np\n> import matplotlib.pyplot as plt\n> from dataset.mnist import load_mnist\n> from simple_convnet import SimpleConvNet\n> from common.trainer import Trainer\n> \n> # 加载MNIST数据集（不展开，保持(N, 1, 28, 28)的形状）\n> (x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n> \n> # 如果训练速度较慢，可以通过取消注释下方代码来减少数据量\n> # x_train, t_train = x_train[:5000], t_train[:5000]\n> # x_test, t_test = x_test[:1000], t_test[:1000]\n> \n> max_epochs = 20  # 训练的总轮数\n> \n> # 初始化卷积神经网络\n> network = SimpleConvNet(\n>     input_dim=(1,28,28),  # 输入数据的形状：(通道数，高，宽)\n>     conv_param={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},  # 卷积层参数\n>     hidden_size=100,  # 隐藏（全连接）层的神经元数量\n>     output_size=10,   # 输出类别数（数字0-9）\n>     weight_init_std=0.01  # 权重初始化的标准差\n> )\n> \n> # 设置训练器\n> trainer = Trainer(\n>     network, x_train, t_train, x_test, t_test,\n>     epochs=max_epochs, mini_batch_size=100,  # 训练轮数和每个小批量的样本数\n>     optimizer='Adam', optimizer_param={'lr': 0.001},  # 优化器及其学习率\n>     evaluate_sample_num_per_epoch=1000  # 每轮评估的样本数\n> )\n> \n> # 开始训练\n> trainer.train()\n> \n> # 保存训练好的网络参数到文件\n> network.save_params(\"params.pkl\")\n> print(\"Saved Network Parameters!\")\n> \n> # 绘制训练集和测试集的准确率随轮数变化的曲线\n> markers = {'train': 'o', 'test': 's'}\n> x = np.arange(max_epochs)\n> plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n> plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n> plt.xlabel(\"epochs\")  # 横坐标为训练轮数\n> plt.ylabel(\"accuracy\")  # 纵坐标为准确率\n> plt.ylim(0, 1.0)  # 设置y轴范围\n> plt.legend(loc='lower right')  # 图例位置\n> plt.show()\n> ```\n>\n> \n>\n> #### CNN的可视化\n>\n> ```python\n> import os, sys\n> import numpy as np\n> sys.path.append('../../py_pro/DL/ch07/')\n> import matplotlib.pyplot as plt\n> from simple_convnet import SimpleConvNet\n> \n> \n> # 可视化卷积核（滤波器）权重的函数\n> def filter_show(filters, nx=8, margin=3, scale=10):\n>     \"\"\"\n>     显示卷积层的滤波器（权重），以灰度图形式排列。\n>     参数：\n>         filters: 卷积核权重，形状为 (FN, C, FH, FW)\n>         nx: 每行显示的滤波器数量\n>         margin: 图像之间的间隔（未使用）\n>         scale: 图像缩放比例（未使用）\n>     \"\"\"\n>     FN, C, FH, FW = filters.shape  # FN:滤波器个数, C:通道数, FH:高, FW:宽\n>     ny = int(np.ceil(FN / nx))     # 计算需要的行数\n> \n>     fig = plt.figure()\n>     # 调整子图间距，去除边距和间隔\n>     fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n> \n>     for i in range(FN):\n>         # 在ny行nx列的子图中添加第i+1个子图，不显示坐标轴刻度\n>         ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n>         # 显示第i个滤波器的第一个通道（通常为灰度图）\n>         ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n>     plt.show()\n> \n> # 创建卷积神经网络实例\n> network = SimpleConvNet()\n> # 显示随机初始化后的第一层卷积核权重\n> filter_show(network.params['W1'])\n> \n> # 加载训练后保存的参数\n> network.load_params(\"../../py_pro/DL/ch07/params.pkl\")\n> # 显示训练后第一层卷积核权重\n> filter_show(network.params['W1'])\n> ```\n>\n> \n\n\n\n### 如懂！！！","source":"_posts/26.md","raw":"---\ntitle: DL之路---啃鱼书（6）\ndata: 2025-07-15 21:26:00\nupdated: 2025-07-15 21:26:00\ntype: DL\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p26.png\ntags:\n  - DL\n  - Learning\n  - gnaw_book\n---\n\n\n# 卷积神经网络\n\n\n\n> #### 卷积神经网络（Convolutional Neural Network，CNN），用于图像识别、语音识别等场景\n\n\n\n> #### 相邻层的所有神经元之间都有连接，称之为全连接，使用Affine层实现全连接层\n>\n> #### 下面是一个5层的全连接神经网络\n>\n> ![](http://picbed.yanzu.tech/img/DL/6/1.png)\n>\n> #### 可以看到，每个Affine层后面跟着激活函数层\n>\n> #### 而基于CNN的神经网络结构如下\n>\n> ![](http://picbed.yanzu.tech/img/DL/6/2.png)\n>\n> #### 可以看到，CNN的层的连接顺序是 $Convolution - ReLU-Pooling$ ，Pooling层有时会被省略，另外，靠近输出的层中使用了之前使用了 \"Affine-ReLU\"组合\n>\n> \n>\n> #### 卷积层\n>\n> > #### 回顾Affine层：\n> >\n> > > #### 在全连接层中，相邻层的神经元全部连接在一起，输出的数量可以任意决定\n> > >\n> > > #### 它存在的一个问题是，忽视了输入数据的形状，如输入数据是图像（高、长、通道方向上的3维形状）时，输入到Affine层会将3维数据拉成1维的，无法利用与形状相关的信息\n> >\n> > #### 而卷积层Conv可以保持形状不变，当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层\n> >\n> > #### 在CNN中，有时将卷积层Conv的输入输出数据称为特征图（feature map），输入数据就叫输入特征图，输出就叫输出特征图\n> >\n> > \n> >\n> > #### 卷积运算\n> >\n> > > #### 卷积层进行的处理就是卷积运算，结合例子说明（输入数据和滤波器之间的符号表示卷积运算）\n> > >\n> > > ![](http://picbed.yanzu.tech/img/DL/6/3.png)\n> > >\n> > > #### 这里的滤波器（filter）也被称为“卷积核（convolution kernel）”，滤波器是一个结构单位，里面的值就是权重。另外，输入数据和卷积核都是有高宽方向的形状的数据(height, weight)，卷积运算的过程如下\n> > >\n> > > ![](http://picbed.yanzu.tech/img/DL/6/4.png)\n> > >\n> > > #### 对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（乘积累加运算），然后，将这个结果保存到输出的对应位置，所有位置进行一次，就可以得到卷积运算的输出\n> > >\n> > > #### 在全连接的神经网络中，除了权重参数，还存在偏置；在CNN中，滤波器的参数就对应之前的权重，且也存在偏置，包含偏执的卷积运算处理如下\n> > >\n> > > ![](http://picbed.yanzu.tech/img/DL/6/5.png)\n> > >\n> > > #### 上面的操作，本质上就是在进行多个 $ y = W \\cdot x + b$ 运算，所以所有的输出都要 +b \n> >\n> > \n> >\n> > #### 填充\n> >\n> > > #### 在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（如0等），这称之为填充（padding）\n> > >\n> > > #### 为什么要进行填充操作呢？\n> > >\n> > > > #### 通过之前的卷积操作过程，注意到，经过卷积运算的输入数据得到的输出数据，大小（空间）缩小了，而卷积层往往不止一层，多次卷积操作之后，可能会使得输出数据的大小变为1从而导致不能再进行卷积操作了（赘述）\n> > > >\n> > > > #### 所以，使用填充主要是为了调整输出的大小，之所以要进行填充，是因为如果每次进行卷积运算都会缩小空间（如(4,4)的输入，(3,3)的滤波器，输出是(2,2)），那么在某个时刻输出大小就有可能变为 1，导致无法再应用卷积运算\n> > >\n> > > #### 填充如何实现的，例：对大小为（4，4）的输入数据应用了幅度为1的填充\n> > >\n> > > ![](http://picbed.yanzu.tech/img/DL/6/6.png)\n> > >\n> > > #### \"幅度为1的填充\"是指用幅度为1像素的0填充周围，填0的话直接省略不写，填充后，(4,4)的输入数据变成了(6,6)的形状\n> > >\n> >\n> > \n> >\n> > #### 步幅\n> >\n> > > #### 应用滤波器（卷积核）的位置间隔称为步幅（stride）\n> > >\n> > > #### 下面将应用滤波器的窗口的间隔变为2个元素\n> > >\n> > > ![](http://picbed.yanzu.tech/img/DL/6/7.png)\n> > >\n> > > #### 注意，步幅为2，意味着各个方向上（横向纵向）的移动都是2个元素\n> > >\n> > > #### 增大步幅后，输出大小会变小。而增大填充后，输出大小会变大，对于步幅和填充，计算输出大小\n> > >\n> > > > #### 若输入大小为(H,W)，卷积核大小为（FH,FW），输出大小为(OH,OW)，填充为P，步幅为S，此时，输出大小可通过以下式子计算\n> > > >\n> > > > $$\n> > > > OH = \\frac {H + 2P - FH} {S} + 1 \\\\\n> > > > OW = \\frac {W + 2P - FW} {S} + 1\n> > > > $$\n> > > >\n> > > > #### 不过有一个问题是，分式的值可能是小数（除不尽的情况），这时需要采取报错等对策，采取的深度学习框架不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行\n> >\n> > \n> >\n> > #### 3维数据的卷积运算\n> >\n> > > #### 对于3维数据的卷积运算，相比于2维数据的，其在纵深方向（通道方向）上特征图增加了\n> > >\n> > > #### 通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出\n> > >\n> > > ![](http://picbed.yanzu.tech/img/DL/6/8.png)\n> > >\n> > > #### 需要注意的是，输入数据和滤波器的通道数要设为相同的值\n> > >\n> > > \n> > >\n> > > #### 通过立体方块来理解\n> > >\n> > > > #### 以3维数据为例，把3维数据表示为多维数组时，书写顺序是 $(channel, height, weight)$ 或 $(C,H,W)$，滤波器(卷积核)也是写作这样的形式 $(C,FH,FW)$ ，如下\n> > > >\n> > > > ![](http://picbed.yanzu.tech/img/DL/6/16.png)\n> > > >\n> > > > #### 上面的输出数据是一张特征图，也即通道数为 1 的特征图，如果要在通道方向上也有多个卷积核运算的输出，就需要用到多个滤波器（权重），如下\n> > > >\n> > > > ![](http://picbed.yanzu.tech/img/DL/6/17.png)\n> > > >\n> > > > #### 通过使用FN个卷积核，对应的输出特征图也变成了FN个，将这FN个特征图汇集在一起，就得到了一个形状为 $(FN,OH,OW)$ 的方块，再将这个方块传递到下一层，就形成了CNN的处理流\n> > > >\n> > > > #### 那么4维的数据，卷积核的权重数据的书写顺序应该是 $(FC,CC,FH,FW)$ ，即卷积核个数、卷积核通道数、卷积核高、卷积核宽\n> > > >\n> > > > #### 同样，也要加上偏置，如下\n> > > >\n> > > > ![](http://picbed.yanzu.tech/img/DL/6/18.png)\n> > > >\n> > > > #### 注意，偏置的通道数要与卷积核的个数相同\n> > >\n> > > \n> > >\n> > > #### 批处理\n> > >\n> > > > #### 通过批处理，能够实现处理的高效化和学习时对mini-batch的对应。在卷积运算中应用批处理，需要将在各层间传递的数据保存为4维数据，即 $(batch-num,channel,height,weight)$ \n> > > >\n> > > > #### N个数据的批处理如下，\n> > > >\n> > > > ![](http://picbed.yanzu.tech/img/DL/6/19.png)\n> > > >\n> > > > #### 批处理将N次处理汇总成了一次进行\n> >\n>\n\n\n\n### 池化层\n\n> #### 池化是缩小高、长方向上的空间的运算\n>\n> #### 它的主要作用是对输出特征图进行下采样(subsampling or downsampling)\n>\n> #### 它的作用是\n>\n> > #### 降维、减少计算量和内存开销，当原始输出特征图尺寸较大时，池化可以有效减小后续网络层的输入大小，从而减少参数数量和计算量\n> >\n> > #### 提取主要特征、抑制噪声，如Max池化、Average池化、Global池化等\n> >\n> > #### 提高模型泛化能力，池化引入了某种程度的位置不变性(translation invariance)，使模型对小范围的平移、旋转、缩放更具鲁棒性\n> >\n> > #### 防止过拟合，减少特征维度，限制了网络学习过于复杂的模式，有助于抑制过拟合\n>\n> #### 池化的目的是减小特征尺寸、增强抽象能力、提高效率\n>\n> #### 补充：池化处理的就是张量，张量（tensor）就是一个具有维度的数值数组，0维是标量，1维是向量，2维是矩阵，3维及以上就是张量\n>\n> \n>\n> #### 下面是Max池化的处理顺序\n>\n> ![](http://picbed.yanzu.tech/img/DL/6/20.png)\n>\n> #### 这个例子是按照步幅为2进行池化窗口为 2x2 的Max池化操作的，处理的是输出特征图，Max池化是从目标区域（池化窗口）中取出最大值，提取显著特征\n>\n> #### 一般来说，池化的窗口大小会和步幅设定为相同值\n>\n> #### 除了Max池化，还有Average池化和Global池化，Average池化则是计算目标区域的平均值，用于平滑特征图；而Global池化是对整个特征图求平均或最大值，常用于分类前最后一层\n>\n> #### 池化层的特征\n>\n> > #### 没有要学习的参数，池化层和卷积层不同，没有要学习的参数\n> >\n> > #### 通道数不发生变化，经过池化运算，输入数据和输出数据的通道数不会发生变化\n> >\n> > #### 对微小的位置变化具有鲁棒性，输入数据发生微小偏差时，池化仍会返回相同的结果，如下面的例子\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/6/21.png)\n> >\n> > #### 当输入数据在宽度方向上只偏离1个元素时，输出仍为相同的结果，当然，根据数据的不同，有时结果也不相同\n\n\n\n\n\n### 卷积层和池化层的实现\n\n> #### CNN中各层间传递的数据是4维数据，即 $(数据个数,通道数,高,宽)$ \n>\n> #### 基于im2col的展开\n>\n> > #### im2col是一个函数，将输入数据展开以适合滤波器（权重），对于输入数据，将应用卷积核的区域(3维方块)横向展开为1列，im2col会在所有应用卷积核的区域进行这个展开处理，如下所示\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/6/22.png)\n> >\n> > #### 值得注意的是，滤波器的应用区域几乎都是重叠的，在此情况下，使用im2col展开后，展开后的元素个数会多于原方块(输入数据)的元素个数，因此，使用im2col的实现存在比普通的实现消耗更多内存的缺点。但，汇总成一个大的矩阵进行计算，对计算机的计算颇有益处，因为可以有效地利用线性代数库\n> >\n> > #### 另外，im2col 意为 image to column，即图像到矩阵\n> >\n> > #### im2col的处理过程如下\n> >\n> > > 1. #### 从原始输入图像数据中提取所有滑动窗口区域（patch）\n> > >\n> > > 2. #### 将每个patch展开成1行\n> > >\n> > > 3. #### 再将所有的patch行拼接成一个矩阵（im2col矩阵）\n> > >\n> > > 4. #### 将卷积核拉平成一个列向量\n> > >\n> > > 5. #### 进行矩阵乘法（类似于Affine层进行的处理）\n> > >\n> > > 6. #### 再reshape成输出特征图的形状\n> >\n> > #### 使用im2col展开输入数据后，之后就只需将卷积层的滤波器（权重）纵向展开为1列，并计算2个矩阵的乘积即可，这和全连接层的Affi ne层进行的处理基本相同\n> >\n> > #### 使用im2col的卷积运算的卷积核处理过程如下\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/6/13.png)\n> >\n> > #### 因为CNN中数据会保存为4维数组，所以要将2维输出数据转换(reshape)为合适的形状，这就是卷积层的实现流程\n>\n> \n>\n> #### 卷积层的实现\n>\n> > #### 在卷积层的实现过程中，有以下几个点需要注意\n> >\n> > > 1. #### im2col函数得到的是col，这个col表示 $(输出区域数,每个区域展开后元素个数)$ ，这个输出区域数其实就是 H_out * W_out，它就是经过im2col展开后得到的行数。而第2个参数就是 卷积核大小 * 通道数，即 $FH \\* FW \\* C$ \n> > >\n> > > 2. #### 卷积层的实现的类中的forward方法里，im2col展开处理后的数据形状 col.shape=(N\\*out_h\\*out_w, C\\*FH\\*FW)，卷积核展开之后的形状 col_W.shape = (C\\*FH\\*FW, FN)，那么 dot(col, col_W) 得到的输出的形状就是 out.shape = (N\\*out_h\\*out_w, FN)，这表示为：每一张输入图像，每一个输出位置(out_h*out_w)个，对每一个卷积核（共FN个）都得到一个输出数值。再将得到的输出out还原为 4D 卷积输出张量结构。注意：这里的N是batch_size，也是输入图像的个数；FN是卷积核的个数，也是这层卷积层的输出通道数\n> > >\n> > > 3. #### 将out还原为4D卷积输出张量结构，这里就有一个新的问题，为什么不能直接使用 reshape 将out.shape变为 (N, FN, out_h, out_w) ？这是因为 reshape 只是重新排列元素的形状，不能改变维度的排列顺序（轴顺序），要改变轴顺序必须使用 transpose，out.shape = ( N \\* out_h \\* out_w, FN )，axis=0:N\\*out_h\\*out_w，axis=1:FN，reshape不会打乱数据顺序，而transpose是要调整轴的排列的，transpose(0,3,1,2)这里的数值是reshape之后 0N1H2W3C 的下标，不是实际值，最后要调整为NCHW（0,3,1,2），如下图\n> > >\n> > > 4. #### 在进行卷积层的反向传播时，必须进行im2col的逆处理，这里使用一个col2im函数，代码在code里\n> > >\n> > > ![](http://picbed.yanzu.tech/img/DL/6/23.png)\n>\n> \n>\n> #### 池化层的实现\n>\n> > #### 池化层的实现和卷积层相同，都使用im2col展开输入数据，不过，池化的情况下，在通道方向上是独立的，这一点和卷积层不同，也即池化的应用区域按通道单独展开\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/6/14.png)\n> >\n> > #### 然后进行Max池化\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/6/15.png)\n\n\n\n### CNN的实现\n\n> #### 有几点需要注意\n>\n> > 1. #### 初始化方法中，池化输出大小的代码如下\n> >\n> >    ```python\n> >    pool_output_size = int(filter_num * (conv_output_size / 2) * (conv_output_size / 2))\n> >    ```\n> >\n> >    #### 其中 conv_output_size/2 是因为后面定义的池化窗口大小为2x2，且stride=2，所以池化后输出的高宽会变为原来的一半，池化层的设定如下\n> >\n> >    ```python\n> >    self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n> >    ```\n> >\n> > 2. #### 初始化方法中，参数 weight_init_std=0.01 的作用，这是为了控制权重初始化的“数值范围”，防止信号和梯度在网络中“放大或消失”，从而保持网络稳定训练，np.random.randn()是均值为0、标准差为1的正态分布，数值范围在-3到3，这有可能会导致梯度消失或爆炸。而乘以0.01后就变为了均值为0，标准差为0.01的正态分布，则会更稳定一点\n\n\n\n### CNN的可视化\n\n> #### 卷积层的滤波器会提取边缘或斑块等原始信息\n\n\n\n### 具有代表性的CNN\n\n> #### LeNet\n>\n> #### AlexNet\n\n\n\n\n\n### Code\n\n> #### 卷积层的实现\n>\n> ```python\n> class Convolution:\n>  \"\"\"\n>  卷积层，实现前向和反向传播。\n>  \"\"\"\n>  def __init__(self, W, b, stride=1, pad=0):\n>      self.W = W  # 卷积核权重，形状(FN, C, FH, FW)\n>      self.b = b  # 偏置，形状(FN,)\n>      self.stride = stride  # 步幅\n>      self.pad = pad        # 填充\n>      self.x = None   # 输入\n>      self.col = None # im2col展开后的输入\n>      self.col_W = None # 展开后的卷积核\n>      self.dW = None  # 权重梯度\n>      self.db = None  # 偏置梯度\n> \n>  def forward(self, x):\n>      \"\"\"\n>      前向传播：im2col展开输入，矩阵乘法实现卷积。\n>      \"\"\"\n>      FN, C, FH, FW = self.W.shape  # 卷积核参数\n>      N, C, H, W = x.shape          # 输入参数\n>      out_h = 1 + int((H + 2*self.pad - FH) / self.stride)  # 输出高\n>      out_w = 1 + int((W + 2*self.pad - FW) / self.stride)  # 输出宽\n>      col = im2col(x, FH, FW, self.stride, self.pad)  # 输入展开\n>      col_W = self.W.reshape(FN, -1).T  # 卷积核展开\n>      out = np.dot(col, col_W) + self.b  # 矩阵乘法+偏置\n>      out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)  # 还原输出形状\n>      self.x = x\n>      self.col = col\n>      self.col_W = col_W\n>      return out\n> \n>  def backward(self, dout):\n>      \"\"\"\n>      反向传播：计算输入、权重、偏置的梯度。\n>      \"\"\"\n>      FN, C, FH, FW = self.W.shape\n>      dout = dout.transpose(0,2,3,1).reshape(-1, FN)  # 调整dout形状\n>      self.db = np.sum(dout, axis=0)  # 偏置梯度\n>      self.dW = np.dot(self.col.T, dout)  # 权重梯度\n>      self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)  # 还原权重形状\n>      dcol = np.dot(dout, self.col_W.T)  # 输入展开后的梯度\n>      dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)  # 还原输入形状\n>      return dx\n> ```\n>\n> #### col2im\n>\n> ```python\n> # col2im: 将im2col展开的二维矩阵还原为原始多维图像数据\n> def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n>     \"\"\"\n>     将im2col展开的二维矩阵还原为原始的多维图像数据。\n> \n>     Parameters\n>     ----------\n>     col : 展开后的二维矩阵\n>     input_shape : 原始输入数据的形状（如：(10, 1, 28, 28)）\n>     filter_h : 滤波器高度\n>     filter_w : 滤波器宽度\n>     stride : 步幅\n>     pad : 填充\n> \n>     Returns\n>     -------\n>     img : 还原后的多维图像数据\n>     \"\"\"\n>     N, C, H, W = input_shape\n>     out_h = (H + 2*pad - filter_h)//stride + 1  # 输出高度\n>     out_w = (W + 2*pad - filter_w)//stride + 1  # 输出宽度\n>     # 还原为im2col前的形状\n>     col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n> \n>     # 初始化还原后的图像，注意填充和步幅的影响\n>     img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n>     # 将每个patch累加到对应的位置\n>     for y in range(filter_h):\n>         y_max = y + stride*out_h\n>         for x in range(filter_w):\n>             x_max = x + stride*out_w\n>             img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n> \n>     # 去除填充部分，返回原始大小\n>     return img[:, :, pad:H + pad, pad:W + pad]\n> ```\n>\n> \n>\n> #### 池化层的实现\n>\n> ```python\n> class Pooling:\n>  \"\"\"\n>  池化层，实现最大池化的前向和反向传播。\n>  \"\"\"\n>  def __init__(self, pool_h, pool_w, stride=2, pad=0):\n>      self.pool_h = pool_h  # 池化窗口高\n>      self.pool_w = pool_w  # 池化窗口宽\n>      self.stride = stride  # 步幅\n>      self.pad = pad        # 填充\n>      self.x = None         # 输入\n>      self.arg_max = None   # 最大值索引\n> \n>  def forward(self, x):\n>      \"\"\"\n>      前向传播：im2col展开后做最大池化。\n>      \"\"\"\n>      N, C, H, W = x.shape\n>      out_h = int(1 + (H - self.pool_h) / self.stride)  # 输出高\n>      out_w = int(1 + (W - self.pool_w) / self.stride)  # 输出宽\n>      col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)  # 输入展开\n>      col = col.reshape(-1, self.pool_h*self.pool_w)  # 每个池化区域展平成一行\n>      arg_max = np.argmax(col, axis=1)  # 最大值索引\n>      out = np.max(col, axis=1)         # 最大值\n>      out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)  # 还原输出形状\n>      self.x = x\n>      self.arg_max = arg_max\n>      return out\n> \n>  def backward(self, dout):\n>      \"\"\"\n>      反向传播：最大池化的反向传播，将梯度传递到最大值位置。\n>      \"\"\"\n>      dout = dout.transpose(0, 2, 3, 1)  # 调整dout形状\n>      pool_size = self.pool_h * self.pool_w\n>      dmax = np.zeros((dout.size, pool_size))  # 初始化梯度\n>      dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()  # 只在最大值位置赋值\n>      dmax = dmax.reshape(dout.shape + (pool_size,)) \n>      dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)  # 展平成二维\n>      dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)  # 还原输入形状\n>      return dx\n> ```\n>\n> \n>\n> #### 简单的CNN例子\n>\n> ```python\n> # coding: utf-8\n> import sys, os\n> sys.path.append(os.pardir)  # 添加父目录到模块搜索路径，便于导入common模块\n> import pickle\n> import numpy as np\n> from collections import OrderedDict\n> from common.layers import *\n> from common.gradient import numerical_gradient\n> \n> \n> class SimpleConvNet:\n>     \"\"\"\n>     简单卷积神经网络（ConvNet）实现：\n>     网络结构为 conv - relu - pool - affine - relu - affine - softmax。\n>     支持参数保存与加载，支持数值梯度和反向传播梯度。\n>     \"\"\"\n>     def __init__(self, input_dim=(1, 28, 28), \n>                  conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n>                  hidden_size=100, output_size=10, weight_init_std=0.01):\n>         \"\"\"\n>         初始化网络参数和各层。\n>         input_dim: 输入数据的形状 (通道数, 高, 宽)\n>         conv_param: 卷积层参数字典，包括filter_num, filter_size, pad, stride\n>         hidden_size: 隐藏层神经元数\n>         output_size: 输出类别数\n>         weight_init_std: 权重初始化标准差\n>         \"\"\"\n>         filter_num = conv_param['filter_num']\n>         filter_size = conv_param['filter_size']\n>         filter_pad = conv_param['pad']\n>         filter_stride = conv_param['stride']\n>         input_size = input_dim[1]\n>         # 计算卷积层输出尺寸\n>         conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n>         # 计算池化层输出尺寸（池化窗口为2x2，步幅2）\n>         pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n> \n>         # 权重初始化\n>         self.params = {}\n>         self.params['W1'] = weight_init_std * \\\n>                             np.random.randn(filter_num, input_dim[0], filter_size, filter_size)  # 卷积核权重\n>         self.params['b1'] = np.zeros(filter_num)  # 卷积核偏置\n>         self.params['W2'] = weight_init_std * \\\n>                             np.random.randn(pool_output_size, hidden_size)  # 全连接层1权重\n>         self.params['b2'] = np.zeros(hidden_size)  # 全连接层1偏置\n>         self.params['W3'] = weight_init_std * \\\n>                             np.random.randn(hidden_size, output_size)  # 全连接层2权重\n>         self.params['b3'] = np.zeros(output_size)  # 全连接层2偏置\n> \n>         # 构建网络层（有序字典保证前向/反向顺序）\n>         self.layers = OrderedDict()\n>         self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n>                                            conv_param['stride'], conv_param['pad'])  # 卷积层\n>         self.layers['Relu1'] = Relu()  # 激活层\n>         self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)  # 池化层\n>         self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])  # 全连接层1\n>         self.layers['Relu2'] = Relu()  # 激活层\n>         self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])  # 全连接层2\n> \n>         self.last_layer = SoftmaxWithLoss()  # 最后一层为softmax+损失\n> \n>     def predict(self, x):\n>         \"\"\"\n>         前向传播，输出网络预测结果。\n>         x: 输入数据\n>         返回：输出结果\n>         \"\"\"\n>         for layer in self.layers.values():\n>             x = layer.forward(x)\n>         return x\n> \n>     def loss(self, x, t):\n>         \"\"\"\n>         计算损失函数值。\n>         x: 输入数据\n>         t: 标签（监督信号）\n>         返回：损失值\n>         \"\"\"\n>         y = self.predict(x)\n>         return self.last_layer.forward(y, t)\n> \n>     def accuracy(self, x, t, batch_size=100):\n>         \"\"\"\n>         计算预测精度。\n>         x: 输入数据\n>         t: 标签\n>         batch_size: 批大小\n>         返回：精度（0~1）\n>         \"\"\"\n>         if t.ndim != 1 : t = np.argmax(t, axis=1)  # 若为one-hot则转为标签索引\n>         acc = 0.0\n>         for i in range(int(x.shape[0] / batch_size)):\n>             tx = x[i*batch_size:(i+1)*batch_size]\n>             tt = t[i*batch_size:(i+1)*batch_size]\n>             y = self.predict(tx)\n>             y = np.argmax(y, axis=1)\n>             acc += np.sum(y == tt)  # 统计预测正确个数\n>         return acc / x.shape[0]\n> \n>     def numerical_gradient(self, x, t):\n>         \"\"\"\n>         用数值微分法计算各层参数的梯度。\n>         x: 输入数据\n>         t: 标签\n>         返回：包含各层权重和偏置梯度的字典\n>         \"\"\"\n>         loss_w = lambda w: self.loss(x, t)\n>         grads = {}\n>         for idx in (1, 2, 3):\n>             grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])  # 权重梯度\n>             grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])  # 偏置梯度\n>         return grads\n> \n>     def gradient(self, x, t):\n>         \"\"\"\n>         用反向传播法计算各层参数的梯度。\n>         x: 输入数据\n>         t: 标签\n>         返回：包含各层权重和偏置梯度的字典\n>         \"\"\"\n>         # 前向传播，计算损失\n>         self.loss(x, t)\n>         # 反向传播，计算梯度\n>         dout = 1\n>         dout = self.last_layer.backward(dout)\n>         layers = list(self.layers.values())\n>         layers.reverse()  # 反向传播需逆序\n>         for layer in layers:\n>             dout = layer.backward(dout)\n>         # 收集各层的权重和偏置梯度\n>         grads = {}\n>         grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n>         grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n>         grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n>         return grads\n>         \n>     def save_params(self, file_name=\"params.pkl\"):\n>         \"\"\"\n>         保存网络参数到文件。\n>         file_name: 文件名\n>         \"\"\"\n>         params = {}\n>         for key, val in self.params.items():\n>             params[key] = val\n>         with open(file_name, 'wb') as f:\n>             pickle.dump(params, f)\n> \n>     def load_params(self, file_name=\"params.pkl\"):\n>         \"\"\"\n>         从文件加载网络参数。\n>         file_name: 文件名\n>         \"\"\"\n>         with open(file_name, 'rb') as f:\n>             params = pickle.load(f)\n>         for key, val in params.items():\n>             self.params[key] = val\n>         # 更新各层的权重和偏置\n>         for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n>             self.layers[key].W = self.params['W' + str(i+1)]\n>             self.layers[key].b = self.params['b' + str(i+1)]\n> ```\n>\n> \n>\n> #### 手写数字识别的CNN\n>\n> ```python\n> # coding: utf-8\n> import sys, os\n> sys.path.append(os.pardir)  # 将父目录添加到sys.path，便于导入模块\n> import numpy as np\n> import matplotlib.pyplot as plt\n> from dataset.mnist import load_mnist\n> from simple_convnet import SimpleConvNet\n> from common.trainer import Trainer\n> \n> # 加载MNIST数据集（不展开，保持(N, 1, 28, 28)的形状）\n> (x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n> \n> # 如果训练速度较慢，可以通过取消注释下方代码来减少数据量\n> # x_train, t_train = x_train[:5000], t_train[:5000]\n> # x_test, t_test = x_test[:1000], t_test[:1000]\n> \n> max_epochs = 20  # 训练的总轮数\n> \n> # 初始化卷积神经网络\n> network = SimpleConvNet(\n>     input_dim=(1,28,28),  # 输入数据的形状：(通道数，高，宽)\n>     conv_param={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},  # 卷积层参数\n>     hidden_size=100,  # 隐藏（全连接）层的神经元数量\n>     output_size=10,   # 输出类别数（数字0-9）\n>     weight_init_std=0.01  # 权重初始化的标准差\n> )\n> \n> # 设置训练器\n> trainer = Trainer(\n>     network, x_train, t_train, x_test, t_test,\n>     epochs=max_epochs, mini_batch_size=100,  # 训练轮数和每个小批量的样本数\n>     optimizer='Adam', optimizer_param={'lr': 0.001},  # 优化器及其学习率\n>     evaluate_sample_num_per_epoch=1000  # 每轮评估的样本数\n> )\n> \n> # 开始训练\n> trainer.train()\n> \n> # 保存训练好的网络参数到文件\n> network.save_params(\"params.pkl\")\n> print(\"Saved Network Parameters!\")\n> \n> # 绘制训练集和测试集的准确率随轮数变化的曲线\n> markers = {'train': 'o', 'test': 's'}\n> x = np.arange(max_epochs)\n> plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n> plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n> plt.xlabel(\"epochs\")  # 横坐标为训练轮数\n> plt.ylabel(\"accuracy\")  # 纵坐标为准确率\n> plt.ylim(0, 1.0)  # 设置y轴范围\n> plt.legend(loc='lower right')  # 图例位置\n> plt.show()\n> ```\n>\n> \n>\n> #### CNN的可视化\n>\n> ```python\n> import os, sys\n> import numpy as np\n> sys.path.append('../../py_pro/DL/ch07/')\n> import matplotlib.pyplot as plt\n> from simple_convnet import SimpleConvNet\n> \n> \n> # 可视化卷积核（滤波器）权重的函数\n> def filter_show(filters, nx=8, margin=3, scale=10):\n>     \"\"\"\n>     显示卷积层的滤波器（权重），以灰度图形式排列。\n>     参数：\n>         filters: 卷积核权重，形状为 (FN, C, FH, FW)\n>         nx: 每行显示的滤波器数量\n>         margin: 图像之间的间隔（未使用）\n>         scale: 图像缩放比例（未使用）\n>     \"\"\"\n>     FN, C, FH, FW = filters.shape  # FN:滤波器个数, C:通道数, FH:高, FW:宽\n>     ny = int(np.ceil(FN / nx))     # 计算需要的行数\n> \n>     fig = plt.figure()\n>     # 调整子图间距，去除边距和间隔\n>     fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n> \n>     for i in range(FN):\n>         # 在ny行nx列的子图中添加第i+1个子图，不显示坐标轴刻度\n>         ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n>         # 显示第i个滤波器的第一个通道（通常为灰度图）\n>         ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n>     plt.show()\n> \n> # 创建卷积神经网络实例\n> network = SimpleConvNet()\n> # 显示随机初始化后的第一层卷积核权重\n> filter_show(network.params['W1'])\n> \n> # 加载训练后保存的参数\n> network.load_params(\"../../py_pro/DL/ch07/params.pkl\")\n> # 显示训练后第一层卷积核权重\n> filter_show(network.params['W1'])\n> ```\n>\n> \n\n\n\n### 如懂！！！","slug":"26","published":1,"date":"2025-07-15T13:26:15.949Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7crn003biku4dy86bbkb","content":"<h1 id=\"卷积神经网络\"><a href=\"#卷积神经网络\" class=\"headerlink\" title=\"卷积神经网络\"></a>卷积神经网络</h1><blockquote>\n<h4 id=\"卷积神经网络（Convolutional-Neural-Network，CNN），用于图像识别、语音识别等场景\"><a href=\"#卷积神经网络（Convolutional-Neural-Network，CNN），用于图像识别、语音识别等场景\" class=\"headerlink\" title=\"卷积神经网络（Convolutional Neural Network，CNN），用于图像识别、语音识别等场景\"></a>卷积神经网络（Convolutional Neural Network，CNN），用于图像识别、语音识别等场景</h4></blockquote>\n<blockquote>\n<h4 id=\"相邻层的所有神经元之间都有连接，称之为全连接，使用Affine层实现全连接层\"><a href=\"#相邻层的所有神经元之间都有连接，称之为全连接，使用Affine层实现全连接层\" class=\"headerlink\" title=\"相邻层的所有神经元之间都有连接，称之为全连接，使用Affine层实现全连接层\"></a>相邻层的所有神经元之间都有连接，称之为全连接，使用Affine层实现全连接层</h4><h4 id=\"下面是一个5层的全连接神经网络\"><a href=\"#下面是一个5层的全连接神经网络\" class=\"headerlink\" title=\"下面是一个5层的全连接神经网络\"></a>下面是一个5层的全连接神经网络</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/1.png\"></p>\n<h4 id=\"可以看到，每个Affine层后面跟着激活函数层\"><a href=\"#可以看到，每个Affine层后面跟着激活函数层\" class=\"headerlink\" title=\"可以看到，每个Affine层后面跟着激活函数层\"></a>可以看到，每个Affine层后面跟着激活函数层</h4><h4 id=\"而基于CNN的神经网络结构如下\"><a href=\"#而基于CNN的神经网络结构如下\" class=\"headerlink\" title=\"而基于CNN的神经网络结构如下\"></a>而基于CNN的神经网络结构如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/2.png\"></p>\n<h4 id=\"可以看到，CNN的层的连接顺序是-Convolution-ReLU-Pooling-，Pooling层有时会被省略，另外，靠近输出的层中使用了之前使用了-“Affine-ReLU”组合\"><a href=\"#可以看到，CNN的层的连接顺序是-Convolution-ReLU-Pooling-，Pooling层有时会被省略，另外，靠近输出的层中使用了之前使用了-“Affine-ReLU”组合\" class=\"headerlink\" title=\"可以看到，CNN的层的连接顺序是 $Convolution - ReLU-Pooling$ ，Pooling层有时会被省略，另外，靠近输出的层中使用了之前使用了 “Affine-ReLU”组合\"></a>可以看到，CNN的层的连接顺序是 $Convolution - ReLU-Pooling$ ，Pooling层有时会被省略，另外，靠近输出的层中使用了之前使用了 “Affine-ReLU”组合</h4><h4 id=\"卷积层\"><a href=\"#卷积层\" class=\"headerlink\" title=\"卷积层\"></a>卷积层</h4><blockquote>\n<h4 id=\"回顾Affine层：\"><a href=\"#回顾Affine层：\" class=\"headerlink\" title=\"回顾Affine层：\"></a>回顾Affine层：</h4><blockquote>\n<h4 id=\"在全连接层中，相邻层的神经元全部连接在一起，输出的数量可以任意决定\"><a href=\"#在全连接层中，相邻层的神经元全部连接在一起，输出的数量可以任意决定\" class=\"headerlink\" title=\"在全连接层中，相邻层的神经元全部连接在一起，输出的数量可以任意决定\"></a>在全连接层中，相邻层的神经元全部连接在一起，输出的数量可以任意决定</h4><h4 id=\"它存在的一个问题是，忽视了输入数据的形状，如输入数据是图像（高、长、通道方向上的3维形状）时，输入到Affine层会将3维数据拉成1维的，无法利用与形状相关的信息\"><a href=\"#它存在的一个问题是，忽视了输入数据的形状，如输入数据是图像（高、长、通道方向上的3维形状）时，输入到Affine层会将3维数据拉成1维的，无法利用与形状相关的信息\" class=\"headerlink\" title=\"它存在的一个问题是，忽视了输入数据的形状，如输入数据是图像（高、长、通道方向上的3维形状）时，输入到Affine层会将3维数据拉成1维的，无法利用与形状相关的信息\"></a>它存在的一个问题是，忽视了输入数据的形状，如输入数据是图像（高、长、通道方向上的3维形状）时，输入到Affine层会将3维数据拉成1维的，无法利用与形状相关的信息</h4></blockquote>\n<h4 id=\"而卷积层Conv可以保持形状不变，当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层\"><a href=\"#而卷积层Conv可以保持形状不变，当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层\" class=\"headerlink\" title=\"而卷积层Conv可以保持形状不变，当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层\"></a>而卷积层Conv可以保持形状不变，当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层</h4><h4 id=\"在CNN中，有时将卷积层Conv的输入输出数据称为特征图（feature-map），输入数据就叫输入特征图，输出就叫输出特征图\"><a href=\"#在CNN中，有时将卷积层Conv的输入输出数据称为特征图（feature-map），输入数据就叫输入特征图，输出就叫输出特征图\" class=\"headerlink\" title=\"在CNN中，有时将卷积层Conv的输入输出数据称为特征图（feature map），输入数据就叫输入特征图，输出就叫输出特征图\"></a>在CNN中，有时将卷积层Conv的输入输出数据称为特征图（feature map），输入数据就叫输入特征图，输出就叫输出特征图</h4><h4 id=\"卷积运算\"><a href=\"#卷积运算\" class=\"headerlink\" title=\"卷积运算\"></a>卷积运算</h4><blockquote>\n<h4 id=\"卷积层进行的处理就是卷积运算，结合例子说明（输入数据和滤波器之间的符号表示卷积运算）\"><a href=\"#卷积层进行的处理就是卷积运算，结合例子说明（输入数据和滤波器之间的符号表示卷积运算）\" class=\"headerlink\" title=\"卷积层进行的处理就是卷积运算，结合例子说明（输入数据和滤波器之间的符号表示卷积运算）\"></a>卷积层进行的处理就是卷积运算，结合例子说明（输入数据和滤波器之间的符号表示卷积运算）</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/3.png\"></p>\n<h4 id=\"这里的滤波器（filter）也被称为“卷积核（convolution-kernel）”，滤波器是一个结构单位，里面的值就是权重。另外，输入数据和卷积核都是有高宽方向的形状的数据-height-weight-，卷积运算的过程如下\"><a href=\"#这里的滤波器（filter）也被称为“卷积核（convolution-kernel）”，滤波器是一个结构单位，里面的值就是权重。另外，输入数据和卷积核都是有高宽方向的形状的数据-height-weight-，卷积运算的过程如下\" class=\"headerlink\" title=\"这里的滤波器（filter）也被称为“卷积核（convolution kernel）”，滤波器是一个结构单位，里面的值就是权重。另外，输入数据和卷积核都是有高宽方向的形状的数据(height, weight)，卷积运算的过程如下\"></a>这里的滤波器（filter）也被称为“卷积核（convolution kernel）”，滤波器是一个结构单位，里面的值就是权重。另外，输入数据和卷积核都是有高宽方向的形状的数据(height, weight)，卷积运算的过程如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/4.png\"></p>\n<h4 id=\"对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（乘积累加运算），然后，将这个结果保存到输出的对应位置，所有位置进行一次，就可以得到卷积运算的输出\"><a href=\"#对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（乘积累加运算），然后，将这个结果保存到输出的对应位置，所有位置进行一次，就可以得到卷积运算的输出\" class=\"headerlink\" title=\"对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（乘积累加运算），然后，将这个结果保存到输出的对应位置，所有位置进行一次，就可以得到卷积运算的输出\"></a>对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（乘积累加运算），然后，将这个结果保存到输出的对应位置，所有位置进行一次，就可以得到卷积运算的输出</h4><h4 id=\"在全连接的神经网络中，除了权重参数，还存在偏置；在CNN中，滤波器的参数就对应之前的权重，且也存在偏置，包含偏执的卷积运算处理如下\"><a href=\"#在全连接的神经网络中，除了权重参数，还存在偏置；在CNN中，滤波器的参数就对应之前的权重，且也存在偏置，包含偏执的卷积运算处理如下\" class=\"headerlink\" title=\"在全连接的神经网络中，除了权重参数，还存在偏置；在CNN中，滤波器的参数就对应之前的权重，且也存在偏置，包含偏执的卷积运算处理如下\"></a>在全连接的神经网络中，除了权重参数，还存在偏置；在CNN中，滤波器的参数就对应之前的权重，且也存在偏置，包含偏执的卷积运算处理如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/5.png\"></p>\n<h4 id=\"上面的操作，本质上就是在进行多个-y-W-cdot-x-b-运算，所以所有的输出都要-b\"><a href=\"#上面的操作，本质上就是在进行多个-y-W-cdot-x-b-运算，所以所有的输出都要-b\" class=\"headerlink\" title=\"上面的操作，本质上就是在进行多个 $ y &#x3D; W \\cdot x + b$ 运算，所以所有的输出都要 +b\"></a>上面的操作，本质上就是在进行多个 $ y &#x3D; W \\cdot x + b$ 运算，所以所有的输出都要 +b</h4></blockquote>\n<h4 id=\"填充\"><a href=\"#填充\" class=\"headerlink\" title=\"填充\"></a>填充</h4><blockquote>\n<h4 id=\"在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（如0等），这称之为填充（padding）\"><a href=\"#在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（如0等），这称之为填充（padding）\" class=\"headerlink\" title=\"在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（如0等），这称之为填充（padding）\"></a>在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（如0等），这称之为填充（padding）</h4><h4 id=\"为什么要进行填充操作呢？\"><a href=\"#为什么要进行填充操作呢？\" class=\"headerlink\" title=\"为什么要进行填充操作呢？\"></a>为什么要进行填充操作呢？</h4><blockquote>\n<h4 id=\"通过之前的卷积操作过程，注意到，经过卷积运算的输入数据得到的输出数据，大小（空间）缩小了，而卷积层往往不止一层，多次卷积操作之后，可能会使得输出数据的大小变为1从而导致不能再进行卷积操作了（赘述）\"><a href=\"#通过之前的卷积操作过程，注意到，经过卷积运算的输入数据得到的输出数据，大小（空间）缩小了，而卷积层往往不止一层，多次卷积操作之后，可能会使得输出数据的大小变为1从而导致不能再进行卷积操作了（赘述）\" class=\"headerlink\" title=\"通过之前的卷积操作过程，注意到，经过卷积运算的输入数据得到的输出数据，大小（空间）缩小了，而卷积层往往不止一层，多次卷积操作之后，可能会使得输出数据的大小变为1从而导致不能再进行卷积操作了（赘述）\"></a>通过之前的卷积操作过程，注意到，经过卷积运算的输入数据得到的输出数据，大小（空间）缩小了，而卷积层往往不止一层，多次卷积操作之后，可能会使得输出数据的大小变为1从而导致不能再进行卷积操作了（赘述）</h4><h4 id=\"所以，使用填充主要是为了调整输出的大小，之所以要进行填充，是因为如果每次进行卷积运算都会缩小空间（如-4-4-的输入，-3-3-的滤波器，输出是-2-2-），那么在某个时刻输出大小就有可能变为-1，导致无法再应用卷积运算\"><a href=\"#所以，使用填充主要是为了调整输出的大小，之所以要进行填充，是因为如果每次进行卷积运算都会缩小空间（如-4-4-的输入，-3-3-的滤波器，输出是-2-2-），那么在某个时刻输出大小就有可能变为-1，导致无法再应用卷积运算\" class=\"headerlink\" title=\"所以，使用填充主要是为了调整输出的大小，之所以要进行填充，是因为如果每次进行卷积运算都会缩小空间（如(4,4)的输入，(3,3)的滤波器，输出是(2,2)），那么在某个时刻输出大小就有可能变为 1，导致无法再应用卷积运算\"></a>所以，使用填充主要是为了调整输出的大小，之所以要进行填充，是因为如果每次进行卷积运算都会缩小空间（如(4,4)的输入，(3,3)的滤波器，输出是(2,2)），那么在某个时刻输出大小就有可能变为 1，导致无法再应用卷积运算</h4></blockquote>\n<h4 id=\"填充如何实现的，例：对大小为（4，4）的输入数据应用了幅度为1的填充\"><a href=\"#填充如何实现的，例：对大小为（4，4）的输入数据应用了幅度为1的填充\" class=\"headerlink\" title=\"填充如何实现的，例：对大小为（4，4）的输入数据应用了幅度为1的填充\"></a>填充如何实现的，例：对大小为（4，4）的输入数据应用了幅度为1的填充</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/6.png\"></p>\n<h4 id=\"“幅度为1的填充”是指用幅度为1像素的0填充周围，填0的话直接省略不写，填充后，-4-4-的输入数据变成了-6-6-的形状\"><a href=\"#“幅度为1的填充”是指用幅度为1像素的0填充周围，填0的话直接省略不写，填充后，-4-4-的输入数据变成了-6-6-的形状\" class=\"headerlink\" title=\"“幅度为1的填充”是指用幅度为1像素的0填充周围，填0的话直接省略不写，填充后，(4,4)的输入数据变成了(6,6)的形状\"></a>“幅度为1的填充”是指用幅度为1像素的0填充周围，填0的话直接省略不写，填充后，(4,4)的输入数据变成了(6,6)的形状</h4></blockquote>\n<h4 id=\"步幅\"><a href=\"#步幅\" class=\"headerlink\" title=\"步幅\"></a>步幅</h4><blockquote>\n<h4 id=\"应用滤波器（卷积核）的位置间隔称为步幅（stride）\"><a href=\"#应用滤波器（卷积核）的位置间隔称为步幅（stride）\" class=\"headerlink\" title=\"应用滤波器（卷积核）的位置间隔称为步幅（stride）\"></a>应用滤波器（卷积核）的位置间隔称为步幅（stride）</h4><h4 id=\"下面将应用滤波器的窗口的间隔变为2个元素\"><a href=\"#下面将应用滤波器的窗口的间隔变为2个元素\" class=\"headerlink\" title=\"下面将应用滤波器的窗口的间隔变为2个元素\"></a>下面将应用滤波器的窗口的间隔变为2个元素</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/7.png\"></p>\n<h4 id=\"注意，步幅为2，意味着各个方向上（横向纵向）的移动都是2个元素\"><a href=\"#注意，步幅为2，意味着各个方向上（横向纵向）的移动都是2个元素\" class=\"headerlink\" title=\"注意，步幅为2，意味着各个方向上（横向纵向）的移动都是2个元素\"></a>注意，步幅为2，意味着各个方向上（横向纵向）的移动都是2个元素</h4><h4 id=\"增大步幅后，输出大小会变小。而增大填充后，输出大小会变大，对于步幅和填充，计算输出大小\"><a href=\"#增大步幅后，输出大小会变小。而增大填充后，输出大小会变大，对于步幅和填充，计算输出大小\" class=\"headerlink\" title=\"增大步幅后，输出大小会变小。而增大填充后，输出大小会变大，对于步幅和填充，计算输出大小\"></a>增大步幅后，输出大小会变小。而增大填充后，输出大小会变大，对于步幅和填充，计算输出大小</h4><blockquote>\n<h4 id=\"若输入大小为-H-W-，卷积核大小为（FH-FW），输出大小为-OH-OW-，填充为P，步幅为S，此时，输出大小可通过以下式子计算\"><a href=\"#若输入大小为-H-W-，卷积核大小为（FH-FW），输出大小为-OH-OW-，填充为P，步幅为S，此时，输出大小可通过以下式子计算\" class=\"headerlink\" title=\"若输入大小为(H,W)，卷积核大小为（FH,FW），输出大小为(OH,OW)，填充为P，步幅为S，此时，输出大小可通过以下式子计算\"></a>若输入大小为(H,W)，卷积核大小为（FH,FW），输出大小为(OH,OW)，填充为P，步幅为S，此时，输出大小可通过以下式子计算</h4><p>$$<br>OH &#x3D; \\frac {H + 2P - FH} {S} + 1 \\<br>OW &#x3D; \\frac {W + 2P - FW} {S} + 1<br>$$</p>\n<h4 id=\"不过有一个问题是，分式的值可能是小数（除不尽的情况），这时需要采取报错等对策，采取的深度学习框架不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行\"><a href=\"#不过有一个问题是，分式的值可能是小数（除不尽的情况），这时需要采取报错等对策，采取的深度学习框架不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行\" class=\"headerlink\" title=\"不过有一个问题是，分式的值可能是小数（除不尽的情况），这时需要采取报错等对策，采取的深度学习框架不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行\"></a>不过有一个问题是，分式的值可能是小数（除不尽的情况），这时需要采取报错等对策，采取的深度学习框架不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行</h4></blockquote>\n</blockquote>\n<h4 id=\"3维数据的卷积运算\"><a href=\"#3维数据的卷积运算\" class=\"headerlink\" title=\"3维数据的卷积运算\"></a>3维数据的卷积运算</h4><blockquote>\n<h4 id=\"对于3维数据的卷积运算，相比于2维数据的，其在纵深方向（通道方向）上特征图增加了\"><a href=\"#对于3维数据的卷积运算，相比于2维数据的，其在纵深方向（通道方向）上特征图增加了\" class=\"headerlink\" title=\"对于3维数据的卷积运算，相比于2维数据的，其在纵深方向（通道方向）上特征图增加了\"></a>对于3维数据的卷积运算，相比于2维数据的，其在纵深方向（通道方向）上特征图增加了</h4><h4 id=\"通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出\"><a href=\"#通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出\" class=\"headerlink\" title=\"通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出\"></a>通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/8.png\"></p>\n<h4 id=\"需要注意的是，输入数据和滤波器的通道数要设为相同的值\"><a href=\"#需要注意的是，输入数据和滤波器的通道数要设为相同的值\" class=\"headerlink\" title=\"需要注意的是，输入数据和滤波器的通道数要设为相同的值\"></a>需要注意的是，输入数据和滤波器的通道数要设为相同的值</h4><h4 id=\"通过立体方块来理解\"><a href=\"#通过立体方块来理解\" class=\"headerlink\" title=\"通过立体方块来理解\"></a>通过立体方块来理解</h4><blockquote>\n<h4 id=\"以3维数据为例，把3维数据表示为多维数组时，书写顺序是-channel-height-weight-或-C-H-W-，滤波器-卷积核-也是写作这样的形式-C-FH-FW-，如下\"><a href=\"#以3维数据为例，把3维数据表示为多维数组时，书写顺序是-channel-height-weight-或-C-H-W-，滤波器-卷积核-也是写作这样的形式-C-FH-FW-，如下\" class=\"headerlink\" title=\"以3维数据为例，把3维数据表示为多维数组时，书写顺序是 $(channel, height, weight)$ 或 $(C,H,W)$，滤波器(卷积核)也是写作这样的形式 $(C,FH,FW)$ ，如下\"></a>以3维数据为例，把3维数据表示为多维数组时，书写顺序是 $(channel, height, weight)$ 或 $(C,H,W)$，滤波器(卷积核)也是写作这样的形式 $(C,FH,FW)$ ，如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/16.png\"></p>\n<h4 id=\"上面的输出数据是一张特征图，也即通道数为-1-的特征图，如果要在通道方向上也有多个卷积核运算的输出，就需要用到多个滤波器（权重），如下\"><a href=\"#上面的输出数据是一张特征图，也即通道数为-1-的特征图，如果要在通道方向上也有多个卷积核运算的输出，就需要用到多个滤波器（权重），如下\" class=\"headerlink\" title=\"上面的输出数据是一张特征图，也即通道数为 1 的特征图，如果要在通道方向上也有多个卷积核运算的输出，就需要用到多个滤波器（权重），如下\"></a>上面的输出数据是一张特征图，也即通道数为 1 的特征图，如果要在通道方向上也有多个卷积核运算的输出，就需要用到多个滤波器（权重），如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/17.png\"></p>\n<h4 id=\"通过使用FN个卷积核，对应的输出特征图也变成了FN个，将这FN个特征图汇集在一起，就得到了一个形状为-FN-OH-OW-的方块，再将这个方块传递到下一层，就形成了CNN的处理流\"><a href=\"#通过使用FN个卷积核，对应的输出特征图也变成了FN个，将这FN个特征图汇集在一起，就得到了一个形状为-FN-OH-OW-的方块，再将这个方块传递到下一层，就形成了CNN的处理流\" class=\"headerlink\" title=\"通过使用FN个卷积核，对应的输出特征图也变成了FN个，将这FN个特征图汇集在一起，就得到了一个形状为 $(FN,OH,OW)$ 的方块，再将这个方块传递到下一层，就形成了CNN的处理流\"></a>通过使用FN个卷积核，对应的输出特征图也变成了FN个，将这FN个特征图汇集在一起，就得到了一个形状为 $(FN,OH,OW)$ 的方块，再将这个方块传递到下一层，就形成了CNN的处理流</h4><h4 id=\"那么4维的数据，卷积核的权重数据的书写顺序应该是-FC-CC-FH-FW-，即卷积核个数、卷积核通道数、卷积核高、卷积核宽\"><a href=\"#那么4维的数据，卷积核的权重数据的书写顺序应该是-FC-CC-FH-FW-，即卷积核个数、卷积核通道数、卷积核高、卷积核宽\" class=\"headerlink\" title=\"那么4维的数据，卷积核的权重数据的书写顺序应该是 $(FC,CC,FH,FW)$ ，即卷积核个数、卷积核通道数、卷积核高、卷积核宽\"></a>那么4维的数据，卷积核的权重数据的书写顺序应该是 $(FC,CC,FH,FW)$ ，即卷积核个数、卷积核通道数、卷积核高、卷积核宽</h4><h4 id=\"同样，也要加上偏置，如下\"><a href=\"#同样，也要加上偏置，如下\" class=\"headerlink\" title=\"同样，也要加上偏置，如下\"></a>同样，也要加上偏置，如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/18.png\"></p>\n<h4 id=\"注意，偏置的通道数要与卷积核的个数相同\"><a href=\"#注意，偏置的通道数要与卷积核的个数相同\" class=\"headerlink\" title=\"注意，偏置的通道数要与卷积核的个数相同\"></a>注意，偏置的通道数要与卷积核的个数相同</h4></blockquote>\n<h4 id=\"批处理\"><a href=\"#批处理\" class=\"headerlink\" title=\"批处理\"></a>批处理</h4><blockquote>\n<h4 id=\"通过批处理，能够实现处理的高效化和学习时对mini-batch的对应。在卷积运算中应用批处理，需要将在各层间传递的数据保存为4维数据，即-batch-num-channel-height-weight\"><a href=\"#通过批处理，能够实现处理的高效化和学习时对mini-batch的对应。在卷积运算中应用批处理，需要将在各层间传递的数据保存为4维数据，即-batch-num-channel-height-weight\" class=\"headerlink\" title=\"通过批处理，能够实现处理的高效化和学习时对mini-batch的对应。在卷积运算中应用批处理，需要将在各层间传递的数据保存为4维数据，即 $(batch-num,channel,height,weight)$\"></a>通过批处理，能够实现处理的高效化和学习时对mini-batch的对应。在卷积运算中应用批处理，需要将在各层间传递的数据保存为4维数据，即 $(batch-num,channel,height,weight)$</h4><h4 id=\"N个数据的批处理如下，\"><a href=\"#N个数据的批处理如下，\" class=\"headerlink\" title=\"N个数据的批处理如下，\"></a>N个数据的批处理如下，</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/19.png\"></p>\n<h4 id=\"批处理将N次处理汇总成了一次进行\"><a href=\"#批处理将N次处理汇总成了一次进行\" class=\"headerlink\" title=\"批处理将N次处理汇总成了一次进行\"></a>批处理将N次处理汇总成了一次进行</h4></blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n<h3 id=\"池化层\"><a href=\"#池化层\" class=\"headerlink\" title=\"池化层\"></a>池化层</h3><blockquote>\n<h4 id=\"池化是缩小高、长方向上的空间的运算\"><a href=\"#池化是缩小高、长方向上的空间的运算\" class=\"headerlink\" title=\"池化是缩小高、长方向上的空间的运算\"></a>池化是缩小高、长方向上的空间的运算</h4><h4 id=\"它的主要作用是对输出特征图进行下采样-subsampling-or-downsampling\"><a href=\"#它的主要作用是对输出特征图进行下采样-subsampling-or-downsampling\" class=\"headerlink\" title=\"它的主要作用是对输出特征图进行下采样(subsampling or downsampling)\"></a>它的主要作用是对输出特征图进行下采样(subsampling or downsampling)</h4><h4 id=\"它的作用是\"><a href=\"#它的作用是\" class=\"headerlink\" title=\"它的作用是\"></a>它的作用是</h4><blockquote>\n<h4 id=\"降维、减少计算量和内存开销，当原始输出特征图尺寸较大时，池化可以有效减小后续网络层的输入大小，从而减少参数数量和计算量\"><a href=\"#降维、减少计算量和内存开销，当原始输出特征图尺寸较大时，池化可以有效减小后续网络层的输入大小，从而减少参数数量和计算量\" class=\"headerlink\" title=\"降维、减少计算量和内存开销，当原始输出特征图尺寸较大时，池化可以有效减小后续网络层的输入大小，从而减少参数数量和计算量\"></a>降维、减少计算量和内存开销，当原始输出特征图尺寸较大时，池化可以有效减小后续网络层的输入大小，从而减少参数数量和计算量</h4><h4 id=\"提取主要特征、抑制噪声，如Max池化、Average池化、Global池化等\"><a href=\"#提取主要特征、抑制噪声，如Max池化、Average池化、Global池化等\" class=\"headerlink\" title=\"提取主要特征、抑制噪声，如Max池化、Average池化、Global池化等\"></a>提取主要特征、抑制噪声，如Max池化、Average池化、Global池化等</h4><h4 id=\"提高模型泛化能力，池化引入了某种程度的位置不变性-translation-invariance-，使模型对小范围的平移、旋转、缩放更具鲁棒性\"><a href=\"#提高模型泛化能力，池化引入了某种程度的位置不变性-translation-invariance-，使模型对小范围的平移、旋转、缩放更具鲁棒性\" class=\"headerlink\" title=\"提高模型泛化能力，池化引入了某种程度的位置不变性(translation invariance)，使模型对小范围的平移、旋转、缩放更具鲁棒性\"></a>提高模型泛化能力，池化引入了某种程度的位置不变性(translation invariance)，使模型对小范围的平移、旋转、缩放更具鲁棒性</h4><h4 id=\"防止过拟合，减少特征维度，限制了网络学习过于复杂的模式，有助于抑制过拟合\"><a href=\"#防止过拟合，减少特征维度，限制了网络学习过于复杂的模式，有助于抑制过拟合\" class=\"headerlink\" title=\"防止过拟合，减少特征维度，限制了网络学习过于复杂的模式，有助于抑制过拟合\"></a>防止过拟合，减少特征维度，限制了网络学习过于复杂的模式，有助于抑制过拟合</h4></blockquote>\n<h4 id=\"池化的目的是减小特征尺寸、增强抽象能力、提高效率\"><a href=\"#池化的目的是减小特征尺寸、增强抽象能力、提高效率\" class=\"headerlink\" title=\"池化的目的是减小特征尺寸、增强抽象能力、提高效率\"></a>池化的目的是减小特征尺寸、增强抽象能力、提高效率</h4><h4 id=\"补充：池化处理的就是张量，张量（tensor）就是一个具有维度的数值数组，0维是标量，1维是向量，2维是矩阵，3维及以上就是张量\"><a href=\"#补充：池化处理的就是张量，张量（tensor）就是一个具有维度的数值数组，0维是标量，1维是向量，2维是矩阵，3维及以上就是张量\" class=\"headerlink\" title=\"补充：池化处理的就是张量，张量（tensor）就是一个具有维度的数值数组，0维是标量，1维是向量，2维是矩阵，3维及以上就是张量\"></a>补充：池化处理的就是张量，张量（tensor）就是一个具有维度的数值数组，0维是标量，1维是向量，2维是矩阵，3维及以上就是张量</h4><h4 id=\"下面是Max池化的处理顺序\"><a href=\"#下面是Max池化的处理顺序\" class=\"headerlink\" title=\"下面是Max池化的处理顺序\"></a>下面是Max池化的处理顺序</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/20.png\"></p>\n<h4 id=\"这个例子是按照步幅为2进行池化窗口为-2x2-的Max池化操作的，处理的是输出特征图，Max池化是从目标区域（池化窗口）中取出最大值，提取显著特征\"><a href=\"#这个例子是按照步幅为2进行池化窗口为-2x2-的Max池化操作的，处理的是输出特征图，Max池化是从目标区域（池化窗口）中取出最大值，提取显著特征\" class=\"headerlink\" title=\"这个例子是按照步幅为2进行池化窗口为 2x2 的Max池化操作的，处理的是输出特征图，Max池化是从目标区域（池化窗口）中取出最大值，提取显著特征\"></a>这个例子是按照步幅为2进行池化窗口为 2x2 的Max池化操作的，处理的是输出特征图，Max池化是从目标区域（池化窗口）中取出最大值，提取显著特征</h4><h4 id=\"一般来说，池化的窗口大小会和步幅设定为相同值\"><a href=\"#一般来说，池化的窗口大小会和步幅设定为相同值\" class=\"headerlink\" title=\"一般来说，池化的窗口大小会和步幅设定为相同值\"></a>一般来说，池化的窗口大小会和步幅设定为相同值</h4><h4 id=\"除了Max池化，还有Average池化和Global池化，Average池化则是计算目标区域的平均值，用于平滑特征图；而Global池化是对整个特征图求平均或最大值，常用于分类前最后一层\"><a href=\"#除了Max池化，还有Average池化和Global池化，Average池化则是计算目标区域的平均值，用于平滑特征图；而Global池化是对整个特征图求平均或最大值，常用于分类前最后一层\" class=\"headerlink\" title=\"除了Max池化，还有Average池化和Global池化，Average池化则是计算目标区域的平均值，用于平滑特征图；而Global池化是对整个特征图求平均或最大值，常用于分类前最后一层\"></a>除了Max池化，还有Average池化和Global池化，Average池化则是计算目标区域的平均值，用于平滑特征图；而Global池化是对整个特征图求平均或最大值，常用于分类前最后一层</h4><h4 id=\"池化层的特征\"><a href=\"#池化层的特征\" class=\"headerlink\" title=\"池化层的特征\"></a>池化层的特征</h4><blockquote>\n<h4 id=\"没有要学习的参数，池化层和卷积层不同，没有要学习的参数\"><a href=\"#没有要学习的参数，池化层和卷积层不同，没有要学习的参数\" class=\"headerlink\" title=\"没有要学习的参数，池化层和卷积层不同，没有要学习的参数\"></a>没有要学习的参数，池化层和卷积层不同，没有要学习的参数</h4><h4 id=\"通道数不发生变化，经过池化运算，输入数据和输出数据的通道数不会发生变化\"><a href=\"#通道数不发生变化，经过池化运算，输入数据和输出数据的通道数不会发生变化\" class=\"headerlink\" title=\"通道数不发生变化，经过池化运算，输入数据和输出数据的通道数不会发生变化\"></a>通道数不发生变化，经过池化运算，输入数据和输出数据的通道数不会发生变化</h4><h4 id=\"对微小的位置变化具有鲁棒性，输入数据发生微小偏差时，池化仍会返回相同的结果，如下面的例子\"><a href=\"#对微小的位置变化具有鲁棒性，输入数据发生微小偏差时，池化仍会返回相同的结果，如下面的例子\" class=\"headerlink\" title=\"对微小的位置变化具有鲁棒性，输入数据发生微小偏差时，池化仍会返回相同的结果，如下面的例子\"></a>对微小的位置变化具有鲁棒性，输入数据发生微小偏差时，池化仍会返回相同的结果，如下面的例子</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/21.png\"></p>\n<h4 id=\"当输入数据在宽度方向上只偏离1个元素时，输出仍为相同的结果，当然，根据数据的不同，有时结果也不相同\"><a href=\"#当输入数据在宽度方向上只偏离1个元素时，输出仍为相同的结果，当然，根据数据的不同，有时结果也不相同\" class=\"headerlink\" title=\"当输入数据在宽度方向上只偏离1个元素时，输出仍为相同的结果，当然，根据数据的不同，有时结果也不相同\"></a>当输入数据在宽度方向上只偏离1个元素时，输出仍为相同的结果，当然，根据数据的不同，有时结果也不相同</h4></blockquote>\n</blockquote>\n<h3 id=\"卷积层和池化层的实现\"><a href=\"#卷积层和池化层的实现\" class=\"headerlink\" title=\"卷积层和池化层的实现\"></a>卷积层和池化层的实现</h3><blockquote>\n<h4 id=\"CNN中各层间传递的数据是4维数据，即-数据个数-通道数-高-宽\"><a href=\"#CNN中各层间传递的数据是4维数据，即-数据个数-通道数-高-宽\" class=\"headerlink\" title=\"CNN中各层间传递的数据是4维数据，即 $(数据个数,通道数,高,宽)$\"></a>CNN中各层间传递的数据是4维数据，即 $(数据个数,通道数,高,宽)$</h4><h4 id=\"基于im2col的展开\"><a href=\"#基于im2col的展开\" class=\"headerlink\" title=\"基于im2col的展开\"></a>基于im2col的展开</h4><blockquote>\n<h4 id=\"im2col是一个函数，将输入数据展开以适合滤波器（权重），对于输入数据，将应用卷积核的区域-3维方块-横向展开为1列，im2col会在所有应用卷积核的区域进行这个展开处理，如下所示\"><a href=\"#im2col是一个函数，将输入数据展开以适合滤波器（权重），对于输入数据，将应用卷积核的区域-3维方块-横向展开为1列，im2col会在所有应用卷积核的区域进行这个展开处理，如下所示\" class=\"headerlink\" title=\"im2col是一个函数，将输入数据展开以适合滤波器（权重），对于输入数据，将应用卷积核的区域(3维方块)横向展开为1列，im2col会在所有应用卷积核的区域进行这个展开处理，如下所示\"></a>im2col是一个函数，将输入数据展开以适合滤波器（权重），对于输入数据，将应用卷积核的区域(3维方块)横向展开为1列，im2col会在所有应用卷积核的区域进行这个展开处理，如下所示</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/22.png\"></p>\n<h4 id=\"值得注意的是，滤波器的应用区域几乎都是重叠的，在此情况下，使用im2col展开后，展开后的元素个数会多于原方块-输入数据-的元素个数，因此，使用im2col的实现存在比普通的实现消耗更多内存的缺点。但，汇总成一个大的矩阵进行计算，对计算机的计算颇有益处，因为可以有效地利用线性代数库\"><a href=\"#值得注意的是，滤波器的应用区域几乎都是重叠的，在此情况下，使用im2col展开后，展开后的元素个数会多于原方块-输入数据-的元素个数，因此，使用im2col的实现存在比普通的实现消耗更多内存的缺点。但，汇总成一个大的矩阵进行计算，对计算机的计算颇有益处，因为可以有效地利用线性代数库\" class=\"headerlink\" title=\"值得注意的是，滤波器的应用区域几乎都是重叠的，在此情况下，使用im2col展开后，展开后的元素个数会多于原方块(输入数据)的元素个数，因此，使用im2col的实现存在比普通的实现消耗更多内存的缺点。但，汇总成一个大的矩阵进行计算，对计算机的计算颇有益处，因为可以有效地利用线性代数库\"></a>值得注意的是，滤波器的应用区域几乎都是重叠的，在此情况下，使用im2col展开后，展开后的元素个数会多于原方块(输入数据)的元素个数，因此，使用im2col的实现存在比普通的实现消耗更多内存的缺点。但，汇总成一个大的矩阵进行计算，对计算机的计算颇有益处，因为可以有效地利用线性代数库</h4><h4 id=\"另外，im2col-意为-image-to-column，即图像到矩阵\"><a href=\"#另外，im2col-意为-image-to-column，即图像到矩阵\" class=\"headerlink\" title=\"另外，im2col 意为 image to column，即图像到矩阵\"></a>另外，im2col 意为 image to column，即图像到矩阵</h4><h4 id=\"im2col的处理过程如下\"><a href=\"#im2col的处理过程如下\" class=\"headerlink\" title=\"im2col的处理过程如下\"></a>im2col的处理过程如下</h4><blockquote>\n<ol>\n<li><h4 id=\"从原始输入图像数据中提取所有滑动窗口区域（patch）\"><a href=\"#从原始输入图像数据中提取所有滑动窗口区域（patch）\" class=\"headerlink\" title=\"从原始输入图像数据中提取所有滑动窗口区域（patch）\"></a>从原始输入图像数据中提取所有滑动窗口区域（patch）</h4></li>\n<li><h4 id=\"将每个patch展开成1行\"><a href=\"#将每个patch展开成1行\" class=\"headerlink\" title=\"将每个patch展开成1行\"></a>将每个patch展开成1行</h4></li>\n<li><h4 id=\"再将所有的patch行拼接成一个矩阵（im2col矩阵）\"><a href=\"#再将所有的patch行拼接成一个矩阵（im2col矩阵）\" class=\"headerlink\" title=\"再将所有的patch行拼接成一个矩阵（im2col矩阵）\"></a>再将所有的patch行拼接成一个矩阵（im2col矩阵）</h4></li>\n<li><h4 id=\"将卷积核拉平成一个列向量\"><a href=\"#将卷积核拉平成一个列向量\" class=\"headerlink\" title=\"将卷积核拉平成一个列向量\"></a>将卷积核拉平成一个列向量</h4></li>\n<li><h4 id=\"进行矩阵乘法（类似于Affine层进行的处理）\"><a href=\"#进行矩阵乘法（类似于Affine层进行的处理）\" class=\"headerlink\" title=\"进行矩阵乘法（类似于Affine层进行的处理）\"></a>进行矩阵乘法（类似于Affine层进行的处理）</h4></li>\n<li><h4 id=\"再reshape成输出特征图的形状\"><a href=\"#再reshape成输出特征图的形状\" class=\"headerlink\" title=\"再reshape成输出特征图的形状\"></a>再reshape成输出特征图的形状</h4></li>\n</ol>\n</blockquote>\n<h4 id=\"使用im2col展开输入数据后，之后就只需将卷积层的滤波器（权重）纵向展开为1列，并计算2个矩阵的乘积即可，这和全连接层的Affi-ne层进行的处理基本相同\"><a href=\"#使用im2col展开输入数据后，之后就只需将卷积层的滤波器（权重）纵向展开为1列，并计算2个矩阵的乘积即可，这和全连接层的Affi-ne层进行的处理基本相同\" class=\"headerlink\" title=\"使用im2col展开输入数据后，之后就只需将卷积层的滤波器（权重）纵向展开为1列，并计算2个矩阵的乘积即可，这和全连接层的Affi ne层进行的处理基本相同\"></a>使用im2col展开输入数据后，之后就只需将卷积层的滤波器（权重）纵向展开为1列，并计算2个矩阵的乘积即可，这和全连接层的Affi ne层进行的处理基本相同</h4><h4 id=\"使用im2col的卷积运算的卷积核处理过程如下\"><a href=\"#使用im2col的卷积运算的卷积核处理过程如下\" class=\"headerlink\" title=\"使用im2col的卷积运算的卷积核处理过程如下\"></a>使用im2col的卷积运算的卷积核处理过程如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/13.png\"></p>\n<h4 id=\"因为CNN中数据会保存为4维数组，所以要将2维输出数据转换-reshape-为合适的形状，这就是卷积层的实现流程\"><a href=\"#因为CNN中数据会保存为4维数组，所以要将2维输出数据转换-reshape-为合适的形状，这就是卷积层的实现流程\" class=\"headerlink\" title=\"因为CNN中数据会保存为4维数组，所以要将2维输出数据转换(reshape)为合适的形状，这就是卷积层的实现流程\"></a>因为CNN中数据会保存为4维数组，所以要将2维输出数据转换(reshape)为合适的形状，这就是卷积层的实现流程</h4></blockquote>\n<h4 id=\"卷积层的实现\"><a href=\"#卷积层的实现\" class=\"headerlink\" title=\"卷积层的实现\"></a>卷积层的实现</h4><blockquote>\n<h4 id=\"在卷积层的实现过程中，有以下几个点需要注意\"><a href=\"#在卷积层的实现过程中，有以下几个点需要注意\" class=\"headerlink\" title=\"在卷积层的实现过程中，有以下几个点需要注意\"></a>在卷积层的实现过程中，有以下几个点需要注意</h4><blockquote>\n<ol>\n<li><h4 id=\"im2col函数得到的是col，这个col表示-输出区域数-每个区域展开后元素个数-，这个输出区域数其实就是-H-out-W-out，它就是经过im2col展开后得到的行数。而第2个参数就是-卷积核大小-通道数，即-FH-FW-C\"><a href=\"#im2col函数得到的是col，这个col表示-输出区域数-每个区域展开后元素个数-，这个输出区域数其实就是-H-out-W-out，它就是经过im2col展开后得到的行数。而第2个参数就是-卷积核大小-通道数，即-FH-FW-C\" class=\"headerlink\" title=\"im2col函数得到的是col，这个col表示 $(输出区域数,每个区域展开后元素个数)$ ，这个输出区域数其实就是 H_out * W_out，它就是经过im2col展开后得到的行数。而第2个参数就是 卷积核大小 * 通道数，即 $FH * FW * C$\"></a>im2col函数得到的是col，这个col表示 $(输出区域数,每个区域展开后元素个数)$ ，这个输出区域数其实就是 H_out * W_out，它就是经过im2col展开后得到的行数。而第2个参数就是 卷积核大小 * 通道数，即 $FH * FW * C$</h4></li>\n<li><h4 id=\"卷积层的实现的类中的forward方法里，im2col展开处理后的数据形状-col-shape-N-out-h-out-w-C-FH-FW-，卷积核展开之后的形状-col-W-shape-C-FH-FW-FN-，那么-dot-col-col-W-得到的输出的形状就是-out-shape-N-out-h-out-w-FN-，这表示为：每一张输入图像，每一个输出位置-out-h-out-w-个，对每一个卷积核（共FN个）都得到一个输出数值。再将得到的输出out还原为-4D-卷积输出张量结构。注意：这里的N是batch-size，也是输入图像的个数；FN是卷积核的个数，也是这层卷积层的输出通道数\"><a href=\"#卷积层的实现的类中的forward方法里，im2col展开处理后的数据形状-col-shape-N-out-h-out-w-C-FH-FW-，卷积核展开之后的形状-col-W-shape-C-FH-FW-FN-，那么-dot-col-col-W-得到的输出的形状就是-out-shape-N-out-h-out-w-FN-，这表示为：每一张输入图像，每一个输出位置-out-h-out-w-个，对每一个卷积核（共FN个）都得到一个输出数值。再将得到的输出out还原为-4D-卷积输出张量结构。注意：这里的N是batch-size，也是输入图像的个数；FN是卷积核的个数，也是这层卷积层的输出通道数\" class=\"headerlink\" title=\"卷积层的实现的类中的forward方法里，im2col展开处理后的数据形状 col.shape&#x3D;(N*out_h*out_w, C*FH*FW)，卷积核展开之后的形状 col_W.shape &#x3D; (C*FH*FW, FN)，那么 dot(col, col_W) 得到的输出的形状就是 out.shape &#x3D; (N*out_h*out_w, FN)，这表示为：每一张输入图像，每一个输出位置(out_h*out_w)个，对每一个卷积核（共FN个）都得到一个输出数值。再将得到的输出out还原为 4D 卷积输出张量结构。注意：这里的N是batch_size，也是输入图像的个数；FN是卷积核的个数，也是这层卷积层的输出通道数\"></a>卷积层的实现的类中的forward方法里，im2col展开处理后的数据形状 col.shape&#x3D;(N*out_h*out_w, C*FH*FW)，卷积核展开之后的形状 col_W.shape &#x3D; (C*FH*FW, FN)，那么 dot(col, col_W) 得到的输出的形状就是 out.shape &#x3D; (N*out_h*out_w, FN)，这表示为：每一张输入图像，每一个输出位置(out_h*out_w)个，对每一个卷积核（共FN个）都得到一个输出数值。再将得到的输出out还原为 4D 卷积输出张量结构。注意：这里的N是batch_size，也是输入图像的个数；FN是卷积核的个数，也是这层卷积层的输出通道数</h4></li>\n<li><h4 id=\"将out还原为4D卷积输出张量结构，这里就有一个新的问题，为什么不能直接使用-reshape-将out-shape变为-N-FN-out-h-out-w-？这是因为-reshape-只是重新排列元素的形状，不能改变维度的排列顺序（轴顺序），要改变轴顺序必须使用-transpose，out-shape-N-out-h-out-w-FN-，axis-0-N-out-h-out-w，axis-1-FN，reshape不会打乱数据顺序，而transpose是要调整轴的排列的，transpose-0-3-1-2-这里的数值是reshape之后-0N1H2W3C-的下标，不是实际值，最后要调整为NCHW（0-3-1-2），如下图\"><a href=\"#将out还原为4D卷积输出张量结构，这里就有一个新的问题，为什么不能直接使用-reshape-将out-shape变为-N-FN-out-h-out-w-？这是因为-reshape-只是重新排列元素的形状，不能改变维度的排列顺序（轴顺序），要改变轴顺序必须使用-transpose，out-shape-N-out-h-out-w-FN-，axis-0-N-out-h-out-w，axis-1-FN，reshape不会打乱数据顺序，而transpose是要调整轴的排列的，transpose-0-3-1-2-这里的数值是reshape之后-0N1H2W3C-的下标，不是实际值，最后要调整为NCHW（0-3-1-2），如下图\" class=\"headerlink\" title=\"将out还原为4D卷积输出张量结构，这里就有一个新的问题，为什么不能直接使用 reshape 将out.shape变为 (N, FN, out_h, out_w) ？这是因为 reshape 只是重新排列元素的形状，不能改变维度的排列顺序（轴顺序），要改变轴顺序必须使用 transpose，out.shape &#x3D; ( N * out_h * out_w, FN )，axis&#x3D;0:N*out_h*out_w，axis&#x3D;1:FN，reshape不会打乱数据顺序，而transpose是要调整轴的排列的，transpose(0,3,1,2)这里的数值是reshape之后 0N1H2W3C 的下标，不是实际值，最后要调整为NCHW（0,3,1,2），如下图\"></a>将out还原为4D卷积输出张量结构，这里就有一个新的问题，为什么不能直接使用 reshape 将out.shape变为 (N, FN, out_h, out_w) ？这是因为 reshape 只是重新排列元素的形状，不能改变维度的排列顺序（轴顺序），要改变轴顺序必须使用 transpose，out.shape &#x3D; ( N * out_h * out_w, FN )，axis&#x3D;0:N*out_h*out_w，axis&#x3D;1:FN，reshape不会打乱数据顺序，而transpose是要调整轴的排列的，transpose(0,3,1,2)这里的数值是reshape之后 0N1H2W3C 的下标，不是实际值，最后要调整为NCHW（0,3,1,2），如下图</h4></li>\n<li><h4 id=\"在进行卷积层的反向传播时，必须进行im2col的逆处理，这里使用一个col2im函数，代码在code里\"><a href=\"#在进行卷积层的反向传播时，必须进行im2col的逆处理，这里使用一个col2im函数，代码在code里\" class=\"headerlink\" title=\"在进行卷积层的反向传播时，必须进行im2col的逆处理，这里使用一个col2im函数，代码在code里\"></a>在进行卷积层的反向传播时，必须进行im2col的逆处理，这里使用一个col2im函数，代码在code里</h4></li>\n</ol>\n<p><img src=\"http://picbed.yanzu.tech/img/DL/6/23.png\"></p>\n</blockquote>\n</blockquote>\n<h4 id=\"池化层的实现\"><a href=\"#池化层的实现\" class=\"headerlink\" title=\"池化层的实现\"></a>池化层的实现</h4><blockquote>\n<h4 id=\"池化层的实现和卷积层相同，都使用im2col展开输入数据，不过，池化的情况下，在通道方向上是独立的，这一点和卷积层不同，也即池化的应用区域按通道单独展开\"><a href=\"#池化层的实现和卷积层相同，都使用im2col展开输入数据，不过，池化的情况下，在通道方向上是独立的，这一点和卷积层不同，也即池化的应用区域按通道单独展开\" class=\"headerlink\" title=\"池化层的实现和卷积层相同，都使用im2col展开输入数据，不过，池化的情况下，在通道方向上是独立的，这一点和卷积层不同，也即池化的应用区域按通道单独展开\"></a>池化层的实现和卷积层相同，都使用im2col展开输入数据，不过，池化的情况下，在通道方向上是独立的，这一点和卷积层不同，也即池化的应用区域按通道单独展开</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/14.png\"></p>\n<h4 id=\"然后进行Max池化\"><a href=\"#然后进行Max池化\" class=\"headerlink\" title=\"然后进行Max池化\"></a>然后进行Max池化</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/15.png\"></p>\n</blockquote>\n</blockquote>\n<h3 id=\"CNN的实现\"><a href=\"#CNN的实现\" class=\"headerlink\" title=\"CNN的实现\"></a>CNN的实现</h3><blockquote>\n<h4 id=\"有几点需要注意\"><a href=\"#有几点需要注意\" class=\"headerlink\" title=\"有几点需要注意\"></a>有几点需要注意</h4><blockquote>\n<ol>\n<li><h4 id=\"初始化方法中，池化输出大小的代码如下\"><a href=\"#初始化方法中，池化输出大小的代码如下\" class=\"headerlink\" title=\"初始化方法中，池化输出大小的代码如下\"></a>初始化方法中，池化输出大小的代码如下</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pool_output_size = <span class=\"built_in\">int</span>(filter_num * (conv_output_size / <span class=\"number\">2</span>) * (conv_output_size / <span class=\"number\">2</span>))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"其中-conv-output-size-2-是因为后面定义的池化窗口大小为2x2，且stride-2，所以池化后输出的高宽会变为原来的一半，池化层的设定如下\"><a href=\"#其中-conv-output-size-2-是因为后面定义的池化窗口大小为2x2，且stride-2，所以池化后输出的高宽会变为原来的一半，池化层的设定如下\" class=\"headerlink\" title=\"其中 conv_output_size&#x2F;2 是因为后面定义的池化窗口大小为2x2，且stride&#x3D;2，所以池化后输出的高宽会变为原来的一半，池化层的设定如下\"></a>其中 conv_output_size&#x2F;2 是因为后面定义的池化窗口大小为2x2，且stride&#x3D;2，所以池化后输出的高宽会变为原来的一半，池化层的设定如下</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Pool1&#x27;</span>] = Pooling(pool_h=<span class=\"number\">2</span>, pool_w=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>)</span><br></pre></td></tr></table></figure>\n</li>\n<li><h4 id=\"初始化方法中，参数-weight-init-std-0-01-的作用，这是为了控制权重初始化的“数值范围”，防止信号和梯度在网络中“放大或消失”，从而保持网络稳定训练，np-random-randn-是均值为0、标准差为1的正态分布，数值范围在-3到3，这有可能会导致梯度消失或爆炸。而乘以0-01后就变为了均值为0，标准差为0-01的正态分布，则会更稳定一点\"><a href=\"#初始化方法中，参数-weight-init-std-0-01-的作用，这是为了控制权重初始化的“数值范围”，防止信号和梯度在网络中“放大或消失”，从而保持网络稳定训练，np-random-randn-是均值为0、标准差为1的正态分布，数值范围在-3到3，这有可能会导致梯度消失或爆炸。而乘以0-01后就变为了均值为0，标准差为0-01的正态分布，则会更稳定一点\" class=\"headerlink\" title=\"初始化方法中，参数 weight_init_std&#x3D;0.01 的作用，这是为了控制权重初始化的“数值范围”，防止信号和梯度在网络中“放大或消失”，从而保持网络稳定训练，np.random.randn()是均值为0、标准差为1的正态分布，数值范围在-3到3，这有可能会导致梯度消失或爆炸。而乘以0.01后就变为了均值为0，标准差为0.01的正态分布，则会更稳定一点\"></a>初始化方法中，参数 weight_init_std&#x3D;0.01 的作用，这是为了控制权重初始化的“数值范围”，防止信号和梯度在网络中“放大或消失”，从而保持网络稳定训练，np.random.randn()是均值为0、标准差为1的正态分布，数值范围在-3到3，这有可能会导致梯度消失或爆炸。而乘以0.01后就变为了均值为0，标准差为0.01的正态分布，则会更稳定一点</h4></li>\n</ol>\n</blockquote>\n</blockquote>\n<h3 id=\"CNN的可视化\"><a href=\"#CNN的可视化\" class=\"headerlink\" title=\"CNN的可视化\"></a>CNN的可视化</h3><blockquote>\n<h4 id=\"卷积层的滤波器会提取边缘或斑块等原始信息\"><a href=\"#卷积层的滤波器会提取边缘或斑块等原始信息\" class=\"headerlink\" title=\"卷积层的滤波器会提取边缘或斑块等原始信息\"></a>卷积层的滤波器会提取边缘或斑块等原始信息</h4></blockquote>\n<h3 id=\"具有代表性的CNN\"><a href=\"#具有代表性的CNN\" class=\"headerlink\" title=\"具有代表性的CNN\"></a>具有代表性的CNN</h3><blockquote>\n<h4 id=\"LeNet\"><a href=\"#LeNet\" class=\"headerlink\" title=\"LeNet\"></a>LeNet</h4><h4 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h4></blockquote>\n<h3 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h3><blockquote>\n<h4 id=\"卷积层的实现-1\"><a href=\"#卷积层的实现-1\" class=\"headerlink\" title=\"卷积层的实现\"></a>卷积层的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Convolution</span>:</span><br><span class=\"line\"> <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\"> 卷积层，实现前向和反向传播。</span></span><br><span class=\"line\"><span class=\"string\"> &quot;&quot;&quot;</span></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, W, b, stride=<span class=\"number\">1</span>, pad=<span class=\"number\">0</span></span>):</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.W = W  <span class=\"comment\"># 卷积核权重，形状(FN, C, FH, FW)</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.b = b  <span class=\"comment\"># 偏置，形状(FN,)</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.stride = stride  <span class=\"comment\"># 步幅</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.pad = pad        <span class=\"comment\"># 填充</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.x = <span class=\"literal\">None</span>   <span class=\"comment\"># 输入</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.col = <span class=\"literal\">None</span> <span class=\"comment\"># im2col展开后的输入</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.col_W = <span class=\"literal\">None</span> <span class=\"comment\"># 展开后的卷积核</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.dW = <span class=\"literal\">None</span>  <span class=\"comment\"># 权重梯度</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.db = <span class=\"literal\">None</span>  <span class=\"comment\"># 偏置梯度</span></span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">     <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">     前向传播：im2col展开输入，矩阵乘法实现卷积。</span></span><br><span class=\"line\"><span class=\"string\">     &quot;&quot;&quot;</span></span><br><span class=\"line\">     FN, C, FH, FW = <span class=\"variable language_\">self</span>.W.shape  <span class=\"comment\"># 卷积核参数</span></span><br><span class=\"line\">     N, C, H, W = x.shape          <span class=\"comment\"># 输入参数</span></span><br><span class=\"line\">     out_h = <span class=\"number\">1</span> + <span class=\"built_in\">int</span>((H + <span class=\"number\">2</span>*<span class=\"variable language_\">self</span>.pad - FH) / <span class=\"variable language_\">self</span>.stride)  <span class=\"comment\"># 输出高</span></span><br><span class=\"line\">     out_w = <span class=\"number\">1</span> + <span class=\"built_in\">int</span>((W + <span class=\"number\">2</span>*<span class=\"variable language_\">self</span>.pad - FW) / <span class=\"variable language_\">self</span>.stride)  <span class=\"comment\"># 输出宽</span></span><br><span class=\"line\">     col = im2col(x, FH, FW, <span class=\"variable language_\">self</span>.stride, <span class=\"variable language_\">self</span>.pad)  <span class=\"comment\"># 输入展开</span></span><br><span class=\"line\">     col_W = <span class=\"variable language_\">self</span>.W.reshape(FN, -<span class=\"number\">1</span>).T  <span class=\"comment\"># 卷积核展开</span></span><br><span class=\"line\">     out = np.dot(col, col_W) + <span class=\"variable language_\">self</span>.b  <span class=\"comment\"># 矩阵乘法+偏置</span></span><br><span class=\"line\">     out = out.reshape(N, out_h, out_w, -<span class=\"number\">1</span>).transpose(<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>)  <span class=\"comment\"># 还原输出形状</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.x = x</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.col = col</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.col_W = col_W</span><br><span class=\"line\">     <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">backward</span>(<span class=\"params\">self, dout</span>):</span><br><span class=\"line\">     <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">     反向传播：计算输入、权重、偏置的梯度。</span></span><br><span class=\"line\"><span class=\"string\">     &quot;&quot;&quot;</span></span><br><span class=\"line\">     FN, C, FH, FW = <span class=\"variable language_\">self</span>.W.shape</span><br><span class=\"line\">     dout = dout.transpose(<span class=\"number\">0</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">1</span>).reshape(-<span class=\"number\">1</span>, FN)  <span class=\"comment\"># 调整dout形状</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.db = np.<span class=\"built_in\">sum</span>(dout, axis=<span class=\"number\">0</span>)  <span class=\"comment\"># 偏置梯度</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.dW = np.dot(<span class=\"variable language_\">self</span>.col.T, dout)  <span class=\"comment\"># 权重梯度</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.dW = <span class=\"variable language_\">self</span>.dW.transpose(<span class=\"number\">1</span>, <span class=\"number\">0</span>).reshape(FN, C, FH, FW)  <span class=\"comment\"># 还原权重形状</span></span><br><span class=\"line\">     dcol = np.dot(dout, <span class=\"variable language_\">self</span>.col_W.T)  <span class=\"comment\"># 输入展开后的梯度</span></span><br><span class=\"line\">     dx = col2im(dcol, <span class=\"variable language_\">self</span>.x.shape, FH, FW, <span class=\"variable language_\">self</span>.stride, <span class=\"variable language_\">self</span>.pad)  <span class=\"comment\"># 还原输入形状</span></span><br><span class=\"line\">     <span class=\"keyword\">return</span> dx</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"col2im\"><a href=\"#col2im\" class=\"headerlink\" title=\"col2im\"></a>col2im</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># col2im: 将im2col展开的二维矩阵还原为原始多维图像数据</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">col2im</span>(<span class=\"params\">col, input_shape, filter_h, filter_w, stride=<span class=\"number\">1</span>, pad=<span class=\"number\">0</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    将im2col展开的二维矩阵还原为原始的多维图像数据。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    col : 展开后的二维矩阵</span></span><br><span class=\"line\"><span class=\"string\">    input_shape : 原始输入数据的形状（如：(10, 1, 28, 28)）</span></span><br><span class=\"line\"><span class=\"string\">    filter_h : 滤波器高度</span></span><br><span class=\"line\"><span class=\"string\">    filter_w : 滤波器宽度</span></span><br><span class=\"line\"><span class=\"string\">    stride : 步幅</span></span><br><span class=\"line\"><span class=\"string\">    pad : 填充</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    -------</span></span><br><span class=\"line\"><span class=\"string\">    img : 还原后的多维图像数据</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    N, C, H, W = input_shape</span><br><span class=\"line\">    out_h = (H + <span class=\"number\">2</span>*pad - filter_h)//stride + <span class=\"number\">1</span>  <span class=\"comment\"># 输出高度</span></span><br><span class=\"line\">    out_w = (W + <span class=\"number\">2</span>*pad - filter_w)//stride + <span class=\"number\">1</span>  <span class=\"comment\"># 输出宽度</span></span><br><span class=\"line\">    <span class=\"comment\"># 还原为im2col前的形状</span></span><br><span class=\"line\">    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 初始化还原后的图像，注意填充和步幅的影响</span></span><br><span class=\"line\">    img = np.zeros((N, C, H + <span class=\"number\">2</span>*pad + stride - <span class=\"number\">1</span>, W + <span class=\"number\">2</span>*pad + stride - <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"comment\"># 将每个patch累加到对应的位置</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> y <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(filter_h):</span><br><span class=\"line\">        y_max = y + stride*out_h</span><br><span class=\"line\">        <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(filter_w):</span><br><span class=\"line\">            x_max = x + stride*out_w</span><br><span class=\"line\">            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 去除填充部分，返回原始大小</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> img[:, :, pad:H + pad, pad:W + pad]</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"池化层的实现-1\"><a href=\"#池化层的实现-1\" class=\"headerlink\" title=\"池化层的实现\"></a>池化层的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Pooling</span>:</span><br><span class=\"line\"> <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\"> 池化层，实现最大池化的前向和反向传播。</span></span><br><span class=\"line\"><span class=\"string\"> &quot;&quot;&quot;</span></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, pool_h, pool_w, stride=<span class=\"number\">2</span>, pad=<span class=\"number\">0</span></span>):</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.pool_h = pool_h  <span class=\"comment\"># 池化窗口高</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.pool_w = pool_w  <span class=\"comment\"># 池化窗口宽</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.stride = stride  <span class=\"comment\"># 步幅</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.pad = pad        <span class=\"comment\"># 填充</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.x = <span class=\"literal\">None</span>         <span class=\"comment\"># 输入</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.arg_max = <span class=\"literal\">None</span>   <span class=\"comment\"># 最大值索引</span></span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">     <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">     前向传播：im2col展开后做最大池化。</span></span><br><span class=\"line\"><span class=\"string\">     &quot;&quot;&quot;</span></span><br><span class=\"line\">     N, C, H, W = x.shape</span><br><span class=\"line\">     out_h = <span class=\"built_in\">int</span>(<span class=\"number\">1</span> + (H - <span class=\"variable language_\">self</span>.pool_h) / <span class=\"variable language_\">self</span>.stride)  <span class=\"comment\"># 输出高</span></span><br><span class=\"line\">     out_w = <span class=\"built_in\">int</span>(<span class=\"number\">1</span> + (W - <span class=\"variable language_\">self</span>.pool_w) / <span class=\"variable language_\">self</span>.stride)  <span class=\"comment\"># 输出宽</span></span><br><span class=\"line\">     col = im2col(x, <span class=\"variable language_\">self</span>.pool_h, <span class=\"variable language_\">self</span>.pool_w, <span class=\"variable language_\">self</span>.stride, <span class=\"variable language_\">self</span>.pad)  <span class=\"comment\"># 输入展开</span></span><br><span class=\"line\">     col = col.reshape(-<span class=\"number\">1</span>, <span class=\"variable language_\">self</span>.pool_h*<span class=\"variable language_\">self</span>.pool_w)  <span class=\"comment\"># 每个池化区域展平成一行</span></span><br><span class=\"line\">     arg_max = np.argmax(col, axis=<span class=\"number\">1</span>)  <span class=\"comment\"># 最大值索引</span></span><br><span class=\"line\">     out = np.<span class=\"built_in\">max</span>(col, axis=<span class=\"number\">1</span>)         <span class=\"comment\"># 最大值</span></span><br><span class=\"line\">     out = out.reshape(N, out_h, out_w, C).transpose(<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>)  <span class=\"comment\"># 还原输出形状</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.x = x</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.arg_max = arg_max</span><br><span class=\"line\">     <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">backward</span>(<span class=\"params\">self, dout</span>):</span><br><span class=\"line\">     <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">     反向传播：最大池化的反向传播，将梯度传递到最大值位置。</span></span><br><span class=\"line\"><span class=\"string\">     &quot;&quot;&quot;</span></span><br><span class=\"line\">     dout = dout.transpose(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>)  <span class=\"comment\"># 调整dout形状</span></span><br><span class=\"line\">     pool_size = <span class=\"variable language_\">self</span>.pool_h * <span class=\"variable language_\">self</span>.pool_w</span><br><span class=\"line\">     dmax = np.zeros((dout.size, pool_size))  <span class=\"comment\"># 初始化梯度</span></span><br><span class=\"line\">     dmax[np.arange(<span class=\"variable language_\">self</span>.arg_max.size), <span class=\"variable language_\">self</span>.arg_max.flatten()] = dout.flatten()  <span class=\"comment\"># 只在最大值位置赋值</span></span><br><span class=\"line\">     dmax = dmax.reshape(dout.shape + (pool_size,)) </span><br><span class=\"line\">     dcol = dmax.reshape(dmax.shape[<span class=\"number\">0</span>] * dmax.shape[<span class=\"number\">1</span>] * dmax.shape[<span class=\"number\">2</span>], -<span class=\"number\">1</span>)  <span class=\"comment\"># 展平成二维</span></span><br><span class=\"line\">     dx = col2im(dcol, <span class=\"variable language_\">self</span>.x.shape, <span class=\"variable language_\">self</span>.pool_h, <span class=\"variable language_\">self</span>.pool_w, <span class=\"variable language_\">self</span>.stride, <span class=\"variable language_\">self</span>.pad)  <span class=\"comment\"># 还原输入形状</span></span><br><span class=\"line\">     <span class=\"keyword\">return</span> dx</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"简单的CNN例子\"><a href=\"#简单的CNN例子\" class=\"headerlink\" title=\"简单的CNN例子\"></a>简单的CNN例子</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys, os</span><br><span class=\"line\">sys.path.append(os.pardir)  <span class=\"comment\"># 添加父目录到模块搜索路径，便于导入common模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> OrderedDict</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.layers <span class=\"keyword\">import</span> *</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.gradient <span class=\"keyword\">import</span> numerical_gradient</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SimpleConvNet</span>:</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    简单卷积神经网络（ConvNet）实现：</span></span><br><span class=\"line\"><span class=\"string\">    网络结构为 conv - relu - pool - affine - relu - affine - softmax。</span></span><br><span class=\"line\"><span class=\"string\">    支持参数保存与加载，支持数值梯度和反向传播梯度。</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, input_dim=(<span class=\"params\"><span class=\"number\">1</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span></span>), </span></span><br><span class=\"line\"><span class=\"params\">                 conv_param=&#123;<span class=\"string\">&#x27;filter_num&#x27;</span>:<span class=\"number\">30</span>, <span class=\"string\">&#x27;filter_size&#x27;</span>:<span class=\"number\">5</span>, <span class=\"string\">&#x27;pad&#x27;</span>:<span class=\"number\">0</span>, <span class=\"string\">&#x27;stride&#x27;</span>:<span class=\"number\">1</span>&#125;,</span></span><br><span class=\"line\"><span class=\"params\">                 hidden_size=<span class=\"number\">100</span>, output_size=<span class=\"number\">10</span>, weight_init_std=<span class=\"number\">0.01</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        初始化网络参数和各层。</span></span><br><span class=\"line\"><span class=\"string\">        input_dim: 输入数据的形状 (通道数, 高, 宽)</span></span><br><span class=\"line\"><span class=\"string\">        conv_param: 卷积层参数字典，包括filter_num, filter_size, pad, stride</span></span><br><span class=\"line\"><span class=\"string\">        hidden_size: 隐藏层神经元数</span></span><br><span class=\"line\"><span class=\"string\">        output_size: 输出类别数</span></span><br><span class=\"line\"><span class=\"string\">        weight_init_std: 权重初始化标准差</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        filter_num = conv_param[<span class=\"string\">&#x27;filter_num&#x27;</span>]</span><br><span class=\"line\">        filter_size = conv_param[<span class=\"string\">&#x27;filter_size&#x27;</span>]</span><br><span class=\"line\">        filter_pad = conv_param[<span class=\"string\">&#x27;pad&#x27;</span>]</span><br><span class=\"line\">        filter_stride = conv_param[<span class=\"string\">&#x27;stride&#x27;</span>]</span><br><span class=\"line\">        input_size = input_dim[<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"comment\"># 计算卷积层输出尺寸</span></span><br><span class=\"line\">        conv_output_size = (input_size - filter_size + <span class=\"number\">2</span>*filter_pad) / filter_stride + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"comment\"># 计算池化层输出尺寸（池化窗口为2x2，步幅2）</span></span><br><span class=\"line\">        pool_output_size = <span class=\"built_in\">int</span>(filter_num * (conv_output_size/<span class=\"number\">2</span>) * (conv_output_size/<span class=\"number\">2</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 权重初始化</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params = &#123;&#125;</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W1&#x27;</span>] = weight_init_std * \\</span><br><span class=\"line\">                            np.random.randn(filter_num, input_dim[<span class=\"number\">0</span>], filter_size, filter_size)  <span class=\"comment\"># 卷积核权重</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b1&#x27;</span>] = np.zeros(filter_num)  <span class=\"comment\"># 卷积核偏置</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W2&#x27;</span>] = weight_init_std * \\</span><br><span class=\"line\">                            np.random.randn(pool_output_size, hidden_size)  <span class=\"comment\"># 全连接层1权重</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b2&#x27;</span>] = np.zeros(hidden_size)  <span class=\"comment\"># 全连接层1偏置</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W3&#x27;</span>] = weight_init_std * \\</span><br><span class=\"line\">                            np.random.randn(hidden_size, output_size)  <span class=\"comment\"># 全连接层2权重</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b3&#x27;</span>] = np.zeros(output_size)  <span class=\"comment\"># 全连接层2偏置</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 构建网络层（有序字典保证前向/反向顺序）</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.layers = OrderedDict()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Conv1&#x27;</span>] = Convolution(<span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W1&#x27;</span>], <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b1&#x27;</span>],</span><br><span class=\"line\">                                           conv_param[<span class=\"string\">&#x27;stride&#x27;</span>], conv_param[<span class=\"string\">&#x27;pad&#x27;</span>])  <span class=\"comment\"># 卷积层</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Relu1&#x27;</span>] = Relu()  <span class=\"comment\"># 激活层</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Pool1&#x27;</span>] = Pooling(pool_h=<span class=\"number\">2</span>, pool_w=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>)  <span class=\"comment\"># 池化层</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine1&#x27;</span>] = Affine(<span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W2&#x27;</span>], <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b2&#x27;</span>])  <span class=\"comment\"># 全连接层1</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Relu2&#x27;</span>] = Relu()  <span class=\"comment\"># 激活层</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine2&#x27;</span>] = Affine(<span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W3&#x27;</span>], <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b3&#x27;</span>])  <span class=\"comment\"># 全连接层2</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.last_layer = SoftmaxWithLoss()  <span class=\"comment\"># 最后一层为softmax+损失</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        前向传播，输出网络预测结果。</span></span><br><span class=\"line\"><span class=\"string\">        x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">        返回：输出结果</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>.layers.values():</span><br><span class=\"line\">            x = layer.forward(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">loss</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        计算损失函数值。</span></span><br><span class=\"line\"><span class=\"string\">        x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">        t: 标签（监督信号）</span></span><br><span class=\"line\"><span class=\"string\">        返回：损失值</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        y = <span class=\"variable language_\">self</span>.predict(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.last_layer.forward(y, t)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">accuracy</span>(<span class=\"params\">self, x, t, batch_size=<span class=\"number\">100</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        计算预测精度。</span></span><br><span class=\"line\"><span class=\"string\">        x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">        t: 标签</span></span><br><span class=\"line\"><span class=\"string\">        batch_size: 批大小</span></span><br><span class=\"line\"><span class=\"string\">        返回：精度（0~1）</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> t.ndim != <span class=\"number\">1</span> : t = np.argmax(t, axis=<span class=\"number\">1</span>)  <span class=\"comment\"># 若为one-hot则转为标签索引</span></span><br><span class=\"line\">        acc = <span class=\"number\">0.0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">int</span>(x.shape[<span class=\"number\">0</span>] / batch_size)):</span><br><span class=\"line\">            tx = x[i*batch_size:(i+<span class=\"number\">1</span>)*batch_size]</span><br><span class=\"line\">            tt = t[i*batch_size:(i+<span class=\"number\">1</span>)*batch_size]</span><br><span class=\"line\">            y = <span class=\"variable language_\">self</span>.predict(tx)</span><br><span class=\"line\">            y = np.argmax(y, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">            acc += np.<span class=\"built_in\">sum</span>(y == tt)  <span class=\"comment\"># 统计预测正确个数</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> acc / x.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">numerical_gradient</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        用数值微分法计算各层参数的梯度。</span></span><br><span class=\"line\"><span class=\"string\">        x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">        t: 标签</span></span><br><span class=\"line\"><span class=\"string\">        返回：包含各层权重和偏置梯度的字典</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        loss_w = <span class=\"keyword\">lambda</span> w: <span class=\"variable language_\">self</span>.loss(x, t)</span><br><span class=\"line\">        grads = &#123;&#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> idx <span class=\"keyword\">in</span> (<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>):</span><br><span class=\"line\">            grads[<span class=\"string\">&#x27;W&#x27;</span> + <span class=\"built_in\">str</span>(idx)] = numerical_gradient(loss_w, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W&#x27;</span> + <span class=\"built_in\">str</span>(idx)])  <span class=\"comment\"># 权重梯度</span></span><br><span class=\"line\">            grads[<span class=\"string\">&#x27;b&#x27;</span> + <span class=\"built_in\">str</span>(idx)] = numerical_gradient(loss_w, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b&#x27;</span> + <span class=\"built_in\">str</span>(idx)])  <span class=\"comment\"># 偏置梯度</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> grads</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">gradient</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        用反向传播法计算各层参数的梯度。</span></span><br><span class=\"line\"><span class=\"string\">        x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">        t: 标签</span></span><br><span class=\"line\"><span class=\"string\">        返回：包含各层权重和偏置梯度的字典</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"comment\"># 前向传播，计算损失</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.loss(x, t)</span><br><span class=\"line\">        <span class=\"comment\"># 反向传播，计算梯度</span></span><br><span class=\"line\">        dout = <span class=\"number\">1</span></span><br><span class=\"line\">        dout = <span class=\"variable language_\">self</span>.last_layer.backward(dout)</span><br><span class=\"line\">        layers = <span class=\"built_in\">list</span>(<span class=\"variable language_\">self</span>.layers.values())</span><br><span class=\"line\">        layers.reverse()  <span class=\"comment\"># 反向传播需逆序</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> layers:</span><br><span class=\"line\">            dout = layer.backward(dout)</span><br><span class=\"line\">        <span class=\"comment\"># 收集各层的权重和偏置梯度</span></span><br><span class=\"line\">        grads = &#123;&#125;</span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;W1&#x27;</span>], grads[<span class=\"string\">&#x27;b1&#x27;</span>] = <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Conv1&#x27;</span>].dW, <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Conv1&#x27;</span>].db</span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;W2&#x27;</span>], grads[<span class=\"string\">&#x27;b2&#x27;</span>] = <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine1&#x27;</span>].dW, <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine1&#x27;</span>].db</span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;W3&#x27;</span>], grads[<span class=\"string\">&#x27;b3&#x27;</span>] = <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine2&#x27;</span>].dW, <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine2&#x27;</span>].db</span><br><span class=\"line\">        <span class=\"keyword\">return</span> grads</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">save_params</span>(<span class=\"params\">self, file_name=<span class=\"string\">&quot;params.pkl&quot;</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        保存网络参数到文件。</span></span><br><span class=\"line\"><span class=\"string\">        file_name: 文件名</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        params = &#123;&#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> key, val <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>.params.items():</span><br><span class=\"line\">            params[key] = val</span><br><span class=\"line\">        <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(file_name, <span class=\"string\">&#x27;wb&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">            pickle.dump(params, f)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">load_params</span>(<span class=\"params\">self, file_name=<span class=\"string\">&quot;params.pkl&quot;</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        从文件加载网络参数。</span></span><br><span class=\"line\"><span class=\"string\">        file_name: 文件名</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(file_name, <span class=\"string\">&#x27;rb&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">            params = pickle.load(f)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> key, val <span class=\"keyword\">in</span> params.items():</span><br><span class=\"line\">            <span class=\"variable language_\">self</span>.params[key] = val</span><br><span class=\"line\">        <span class=\"comment\"># 更新各层的权重和偏置</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i, key <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>([<span class=\"string\">&#x27;Conv1&#x27;</span>, <span class=\"string\">&#x27;Affine1&#x27;</span>, <span class=\"string\">&#x27;Affine2&#x27;</span>]):</span><br><span class=\"line\">            <span class=\"variable language_\">self</span>.layers[key].W = <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W&#x27;</span> + <span class=\"built_in\">str</span>(i+<span class=\"number\">1</span>)]</span><br><span class=\"line\">            <span class=\"variable language_\">self</span>.layers[key].b = <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b&#x27;</span> + <span class=\"built_in\">str</span>(i+<span class=\"number\">1</span>)]</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"手写数字识别的CNN\"><a href=\"#手写数字识别的CNN\" class=\"headerlink\" title=\"手写数字识别的CNN\"></a>手写数字识别的CNN</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys, os</span><br><span class=\"line\">sys.path.append(os.pardir)  <span class=\"comment\"># 将父目录添加到sys.path，便于导入模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist</span><br><span class=\"line\"><span class=\"keyword\">from</span> simple_convnet <span class=\"keyword\">import</span> SimpleConvNet</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.trainer <span class=\"keyword\">import</span> Trainer</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载MNIST数据集（不展开，保持(N, 1, 28, 28)的形状）</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(flatten=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 如果训练速度较慢，可以通过取消注释下方代码来减少数据量</span></span><br><span class=\"line\"><span class=\"comment\"># x_train, t_train = x_train[:5000], t_train[:5000]</span></span><br><span class=\"line\"><span class=\"comment\"># x_test, t_test = x_test[:1000], t_test[:1000]</span></span><br><span class=\"line\"></span><br><span class=\"line\">max_epochs = <span class=\"number\">20</span>  <span class=\"comment\"># 训练的总轮数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 初始化卷积神经网络</span></span><br><span class=\"line\">network = SimpleConvNet(</span><br><span class=\"line\">    input_dim=(<span class=\"number\">1</span>,<span class=\"number\">28</span>,<span class=\"number\">28</span>),  <span class=\"comment\"># 输入数据的形状：(通道数，高，宽)</span></span><br><span class=\"line\">    conv_param=&#123;<span class=\"string\">&#x27;filter_num&#x27;</span>: <span class=\"number\">30</span>, <span class=\"string\">&#x27;filter_size&#x27;</span>: <span class=\"number\">5</span>, <span class=\"string\">&#x27;pad&#x27;</span>: <span class=\"number\">0</span>, <span class=\"string\">&#x27;stride&#x27;</span>: <span class=\"number\">1</span>&#125;,  <span class=\"comment\"># 卷积层参数</span></span><br><span class=\"line\">    hidden_size=<span class=\"number\">100</span>,  <span class=\"comment\"># 隐藏（全连接）层的神经元数量</span></span><br><span class=\"line\">    output_size=<span class=\"number\">10</span>,   <span class=\"comment\"># 输出类别数（数字0-9）</span></span><br><span class=\"line\">    weight_init_std=<span class=\"number\">0.01</span>  <span class=\"comment\"># 权重初始化的标准差</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置训练器</span></span><br><span class=\"line\">trainer = Trainer(</span><br><span class=\"line\">    network, x_train, t_train, x_test, t_test,</span><br><span class=\"line\">    epochs=max_epochs, mini_batch_size=<span class=\"number\">100</span>,  <span class=\"comment\"># 训练轮数和每个小批量的样本数</span></span><br><span class=\"line\">    optimizer=<span class=\"string\">&#x27;Adam&#x27;</span>, optimizer_param=&#123;<span class=\"string\">&#x27;lr&#x27;</span>: <span class=\"number\">0.001</span>&#125;,  <span class=\"comment\"># 优化器及其学习率</span></span><br><span class=\"line\">    evaluate_sample_num_per_epoch=<span class=\"number\">1000</span>  <span class=\"comment\"># 每轮评估的样本数</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 开始训练</span></span><br><span class=\"line\">trainer.train()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 保存训练好的网络参数到文件</span></span><br><span class=\"line\">network.save_params(<span class=\"string\">&quot;params.pkl&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Saved Network Parameters!&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制训练集和测试集的准确率随轮数变化的曲线</span></span><br><span class=\"line\">markers = &#123;<span class=\"string\">&#x27;train&#x27;</span>: <span class=\"string\">&#x27;o&#x27;</span>, <span class=\"string\">&#x27;test&#x27;</span>: <span class=\"string\">&#x27;s&#x27;</span>&#125;</span><br><span class=\"line\">x = np.arange(max_epochs)</span><br><span class=\"line\">plt.plot(x, trainer.train_acc_list, marker=<span class=\"string\">&#x27;o&#x27;</span>, label=<span class=\"string\">&#x27;train&#x27;</span>, markevery=<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.plot(x, trainer.test_acc_list, marker=<span class=\"string\">&#x27;s&#x27;</span>, label=<span class=\"string\">&#x27;test&#x27;</span>, markevery=<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;epochs&quot;</span>)  <span class=\"comment\"># 横坐标为训练轮数</span></span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;accuracy&quot;</span>)  <span class=\"comment\"># 纵坐标为准确率</span></span><br><span class=\"line\">plt.ylim(<span class=\"number\">0</span>, <span class=\"number\">1.0</span>)  <span class=\"comment\"># 设置y轴范围</span></span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;lower right&#x27;</span>)  <span class=\"comment\"># 图例位置</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"CNN的可视化-1\"><a href=\"#CNN的可视化-1\" class=\"headerlink\" title=\"CNN的可视化\"></a>CNN的可视化</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os, sys</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">sys.path.append(<span class=\"string\">&#x27;../../py_pro/DL/ch07/&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> simple_convnet <span class=\"keyword\">import</span> SimpleConvNet</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 可视化卷积核（滤波器）权重的函数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">filter_show</span>(<span class=\"params\">filters, nx=<span class=\"number\">8</span>, margin=<span class=\"number\">3</span>, scale=<span class=\"number\">10</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    显示卷积层的滤波器（权重），以灰度图形式排列。</span></span><br><span class=\"line\"><span class=\"string\">    参数：</span></span><br><span class=\"line\"><span class=\"string\">        filters: 卷积核权重，形状为 (FN, C, FH, FW)</span></span><br><span class=\"line\"><span class=\"string\">        nx: 每行显示的滤波器数量</span></span><br><span class=\"line\"><span class=\"string\">        margin: 图像之间的间隔（未使用）</span></span><br><span class=\"line\"><span class=\"string\">        scale: 图像缩放比例（未使用）</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    FN, C, FH, FW = filters.shape  <span class=\"comment\"># FN:滤波器个数, C:通道数, FH:高, FW:宽</span></span><br><span class=\"line\">    ny = <span class=\"built_in\">int</span>(np.ceil(FN / nx))     <span class=\"comment\"># 计算需要的行数</span></span><br><span class=\"line\"></span><br><span class=\"line\">    fig = plt.figure()</span><br><span class=\"line\">    <span class=\"comment\"># 调整子图间距，去除边距和间隔</span></span><br><span class=\"line\">    fig.subplots_adjust(left=<span class=\"number\">0</span>, right=<span class=\"number\">1</span>, bottom=<span class=\"number\">0</span>, top=<span class=\"number\">1</span>, hspace=<span class=\"number\">0.05</span>, wspace=<span class=\"number\">0.05</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(FN):</span><br><span class=\"line\">        <span class=\"comment\"># 在ny行nx列的子图中添加第i+1个子图，不显示坐标轴刻度</span></span><br><span class=\"line\">        ax = fig.add_subplot(ny, nx, i+<span class=\"number\">1</span>, xticks=[], yticks=[])</span><br><span class=\"line\">        <span class=\"comment\"># 显示第i个滤波器的第一个通道（通常为灰度图）</span></span><br><span class=\"line\">        ax.imshow(filters[i, <span class=\"number\">0</span>], cmap=plt.cm.gray_r, interpolation=<span class=\"string\">&#x27;nearest&#x27;</span>)</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建卷积神经网络实例</span></span><br><span class=\"line\">network = SimpleConvNet()</span><br><span class=\"line\"><span class=\"comment\"># 显示随机初始化后的第一层卷积核权重</span></span><br><span class=\"line\">filter_show(network.params[<span class=\"string\">&#x27;W1&#x27;</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载训练后保存的参数</span></span><br><span class=\"line\">network.load_params(<span class=\"string\">&quot;../../py_pro/DL/ch07/params.pkl&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 显示训练后第一层卷积核权重</span></span><br><span class=\"line\">filter_show(network.params[<span class=\"string\">&#x27;W1&#x27;</span>])</span><br></pre></td></tr></table></figure>\n\n\n</blockquote>\n<h3 id=\"如懂！！！\"><a href=\"#如懂！！！\" class=\"headerlink\" title=\"如懂！！！\"></a>如懂！！！</h3>","cover_type":"img","excerpt":"","more":"<h1 id=\"卷积神经网络\"><a href=\"#卷积神经网络\" class=\"headerlink\" title=\"卷积神经网络\"></a>卷积神经网络</h1><blockquote>\n<h4 id=\"卷积神经网络（Convolutional-Neural-Network，CNN），用于图像识别、语音识别等场景\"><a href=\"#卷积神经网络（Convolutional-Neural-Network，CNN），用于图像识别、语音识别等场景\" class=\"headerlink\" title=\"卷积神经网络（Convolutional Neural Network，CNN），用于图像识别、语音识别等场景\"></a>卷积神经网络（Convolutional Neural Network，CNN），用于图像识别、语音识别等场景</h4></blockquote>\n<blockquote>\n<h4 id=\"相邻层的所有神经元之间都有连接，称之为全连接，使用Affine层实现全连接层\"><a href=\"#相邻层的所有神经元之间都有连接，称之为全连接，使用Affine层实现全连接层\" class=\"headerlink\" title=\"相邻层的所有神经元之间都有连接，称之为全连接，使用Affine层实现全连接层\"></a>相邻层的所有神经元之间都有连接，称之为全连接，使用Affine层实现全连接层</h4><h4 id=\"下面是一个5层的全连接神经网络\"><a href=\"#下面是一个5层的全连接神经网络\" class=\"headerlink\" title=\"下面是一个5层的全连接神经网络\"></a>下面是一个5层的全连接神经网络</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/1.png\"></p>\n<h4 id=\"可以看到，每个Affine层后面跟着激活函数层\"><a href=\"#可以看到，每个Affine层后面跟着激活函数层\" class=\"headerlink\" title=\"可以看到，每个Affine层后面跟着激活函数层\"></a>可以看到，每个Affine层后面跟着激活函数层</h4><h4 id=\"而基于CNN的神经网络结构如下\"><a href=\"#而基于CNN的神经网络结构如下\" class=\"headerlink\" title=\"而基于CNN的神经网络结构如下\"></a>而基于CNN的神经网络结构如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/2.png\"></p>\n<h4 id=\"可以看到，CNN的层的连接顺序是-Convolution-ReLU-Pooling-，Pooling层有时会被省略，另外，靠近输出的层中使用了之前使用了-“Affine-ReLU”组合\"><a href=\"#可以看到，CNN的层的连接顺序是-Convolution-ReLU-Pooling-，Pooling层有时会被省略，另外，靠近输出的层中使用了之前使用了-“Affine-ReLU”组合\" class=\"headerlink\" title=\"可以看到，CNN的层的连接顺序是 $Convolution - ReLU-Pooling$ ，Pooling层有时会被省略，另外，靠近输出的层中使用了之前使用了 “Affine-ReLU”组合\"></a>可以看到，CNN的层的连接顺序是 $Convolution - ReLU-Pooling$ ，Pooling层有时会被省略，另外，靠近输出的层中使用了之前使用了 “Affine-ReLU”组合</h4><h4 id=\"卷积层\"><a href=\"#卷积层\" class=\"headerlink\" title=\"卷积层\"></a>卷积层</h4><blockquote>\n<h4 id=\"回顾Affine层：\"><a href=\"#回顾Affine层：\" class=\"headerlink\" title=\"回顾Affine层：\"></a>回顾Affine层：</h4><blockquote>\n<h4 id=\"在全连接层中，相邻层的神经元全部连接在一起，输出的数量可以任意决定\"><a href=\"#在全连接层中，相邻层的神经元全部连接在一起，输出的数量可以任意决定\" class=\"headerlink\" title=\"在全连接层中，相邻层的神经元全部连接在一起，输出的数量可以任意决定\"></a>在全连接层中，相邻层的神经元全部连接在一起，输出的数量可以任意决定</h4><h4 id=\"它存在的一个问题是，忽视了输入数据的形状，如输入数据是图像（高、长、通道方向上的3维形状）时，输入到Affine层会将3维数据拉成1维的，无法利用与形状相关的信息\"><a href=\"#它存在的一个问题是，忽视了输入数据的形状，如输入数据是图像（高、长、通道方向上的3维形状）时，输入到Affine层会将3维数据拉成1维的，无法利用与形状相关的信息\" class=\"headerlink\" title=\"它存在的一个问题是，忽视了输入数据的形状，如输入数据是图像（高、长、通道方向上的3维形状）时，输入到Affine层会将3维数据拉成1维的，无法利用与形状相关的信息\"></a>它存在的一个问题是，忽视了输入数据的形状，如输入数据是图像（高、长、通道方向上的3维形状）时，输入到Affine层会将3维数据拉成1维的，无法利用与形状相关的信息</h4></blockquote>\n<h4 id=\"而卷积层Conv可以保持形状不变，当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层\"><a href=\"#而卷积层Conv可以保持形状不变，当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层\" class=\"headerlink\" title=\"而卷积层Conv可以保持形状不变，当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层\"></a>而卷积层Conv可以保持形状不变，当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层</h4><h4 id=\"在CNN中，有时将卷积层Conv的输入输出数据称为特征图（feature-map），输入数据就叫输入特征图，输出就叫输出特征图\"><a href=\"#在CNN中，有时将卷积层Conv的输入输出数据称为特征图（feature-map），输入数据就叫输入特征图，输出就叫输出特征图\" class=\"headerlink\" title=\"在CNN中，有时将卷积层Conv的输入输出数据称为特征图（feature map），输入数据就叫输入特征图，输出就叫输出特征图\"></a>在CNN中，有时将卷积层Conv的输入输出数据称为特征图（feature map），输入数据就叫输入特征图，输出就叫输出特征图</h4><h4 id=\"卷积运算\"><a href=\"#卷积运算\" class=\"headerlink\" title=\"卷积运算\"></a>卷积运算</h4><blockquote>\n<h4 id=\"卷积层进行的处理就是卷积运算，结合例子说明（输入数据和滤波器之间的符号表示卷积运算）\"><a href=\"#卷积层进行的处理就是卷积运算，结合例子说明（输入数据和滤波器之间的符号表示卷积运算）\" class=\"headerlink\" title=\"卷积层进行的处理就是卷积运算，结合例子说明（输入数据和滤波器之间的符号表示卷积运算）\"></a>卷积层进行的处理就是卷积运算，结合例子说明（输入数据和滤波器之间的符号表示卷积运算）</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/3.png\"></p>\n<h4 id=\"这里的滤波器（filter）也被称为“卷积核（convolution-kernel）”，滤波器是一个结构单位，里面的值就是权重。另外，输入数据和卷积核都是有高宽方向的形状的数据-height-weight-，卷积运算的过程如下\"><a href=\"#这里的滤波器（filter）也被称为“卷积核（convolution-kernel）”，滤波器是一个结构单位，里面的值就是权重。另外，输入数据和卷积核都是有高宽方向的形状的数据-height-weight-，卷积运算的过程如下\" class=\"headerlink\" title=\"这里的滤波器（filter）也被称为“卷积核（convolution kernel）”，滤波器是一个结构单位，里面的值就是权重。另外，输入数据和卷积核都是有高宽方向的形状的数据(height, weight)，卷积运算的过程如下\"></a>这里的滤波器（filter）也被称为“卷积核（convolution kernel）”，滤波器是一个结构单位，里面的值就是权重。另外，输入数据和卷积核都是有高宽方向的形状的数据(height, weight)，卷积运算的过程如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/4.png\"></p>\n<h4 id=\"对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（乘积累加运算），然后，将这个结果保存到输出的对应位置，所有位置进行一次，就可以得到卷积运算的输出\"><a href=\"#对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（乘积累加运算），然后，将这个结果保存到输出的对应位置，所有位置进行一次，就可以得到卷积运算的输出\" class=\"headerlink\" title=\"对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（乘积累加运算），然后，将这个结果保存到输出的对应位置，所有位置进行一次，就可以得到卷积运算的输出\"></a>对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（乘积累加运算），然后，将这个结果保存到输出的对应位置，所有位置进行一次，就可以得到卷积运算的输出</h4><h4 id=\"在全连接的神经网络中，除了权重参数，还存在偏置；在CNN中，滤波器的参数就对应之前的权重，且也存在偏置，包含偏执的卷积运算处理如下\"><a href=\"#在全连接的神经网络中，除了权重参数，还存在偏置；在CNN中，滤波器的参数就对应之前的权重，且也存在偏置，包含偏执的卷积运算处理如下\" class=\"headerlink\" title=\"在全连接的神经网络中，除了权重参数，还存在偏置；在CNN中，滤波器的参数就对应之前的权重，且也存在偏置，包含偏执的卷积运算处理如下\"></a>在全连接的神经网络中，除了权重参数，还存在偏置；在CNN中，滤波器的参数就对应之前的权重，且也存在偏置，包含偏执的卷积运算处理如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/5.png\"></p>\n<h4 id=\"上面的操作，本质上就是在进行多个-y-W-cdot-x-b-运算，所以所有的输出都要-b\"><a href=\"#上面的操作，本质上就是在进行多个-y-W-cdot-x-b-运算，所以所有的输出都要-b\" class=\"headerlink\" title=\"上面的操作，本质上就是在进行多个 $ y &#x3D; W \\cdot x + b$ 运算，所以所有的输出都要 +b\"></a>上面的操作，本质上就是在进行多个 $ y &#x3D; W \\cdot x + b$ 运算，所以所有的输出都要 +b</h4></blockquote>\n<h4 id=\"填充\"><a href=\"#填充\" class=\"headerlink\" title=\"填充\"></a>填充</h4><blockquote>\n<h4 id=\"在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（如0等），这称之为填充（padding）\"><a href=\"#在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（如0等），这称之为填充（padding）\" class=\"headerlink\" title=\"在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（如0等），这称之为填充（padding）\"></a>在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（如0等），这称之为填充（padding）</h4><h4 id=\"为什么要进行填充操作呢？\"><a href=\"#为什么要进行填充操作呢？\" class=\"headerlink\" title=\"为什么要进行填充操作呢？\"></a>为什么要进行填充操作呢？</h4><blockquote>\n<h4 id=\"通过之前的卷积操作过程，注意到，经过卷积运算的输入数据得到的输出数据，大小（空间）缩小了，而卷积层往往不止一层，多次卷积操作之后，可能会使得输出数据的大小变为1从而导致不能再进行卷积操作了（赘述）\"><a href=\"#通过之前的卷积操作过程，注意到，经过卷积运算的输入数据得到的输出数据，大小（空间）缩小了，而卷积层往往不止一层，多次卷积操作之后，可能会使得输出数据的大小变为1从而导致不能再进行卷积操作了（赘述）\" class=\"headerlink\" title=\"通过之前的卷积操作过程，注意到，经过卷积运算的输入数据得到的输出数据，大小（空间）缩小了，而卷积层往往不止一层，多次卷积操作之后，可能会使得输出数据的大小变为1从而导致不能再进行卷积操作了（赘述）\"></a>通过之前的卷积操作过程，注意到，经过卷积运算的输入数据得到的输出数据，大小（空间）缩小了，而卷积层往往不止一层，多次卷积操作之后，可能会使得输出数据的大小变为1从而导致不能再进行卷积操作了（赘述）</h4><h4 id=\"所以，使用填充主要是为了调整输出的大小，之所以要进行填充，是因为如果每次进行卷积运算都会缩小空间（如-4-4-的输入，-3-3-的滤波器，输出是-2-2-），那么在某个时刻输出大小就有可能变为-1，导致无法再应用卷积运算\"><a href=\"#所以，使用填充主要是为了调整输出的大小，之所以要进行填充，是因为如果每次进行卷积运算都会缩小空间（如-4-4-的输入，-3-3-的滤波器，输出是-2-2-），那么在某个时刻输出大小就有可能变为-1，导致无法再应用卷积运算\" class=\"headerlink\" title=\"所以，使用填充主要是为了调整输出的大小，之所以要进行填充，是因为如果每次进行卷积运算都会缩小空间（如(4,4)的输入，(3,3)的滤波器，输出是(2,2)），那么在某个时刻输出大小就有可能变为 1，导致无法再应用卷积运算\"></a>所以，使用填充主要是为了调整输出的大小，之所以要进行填充，是因为如果每次进行卷积运算都会缩小空间（如(4,4)的输入，(3,3)的滤波器，输出是(2,2)），那么在某个时刻输出大小就有可能变为 1，导致无法再应用卷积运算</h4></blockquote>\n<h4 id=\"填充如何实现的，例：对大小为（4，4）的输入数据应用了幅度为1的填充\"><a href=\"#填充如何实现的，例：对大小为（4，4）的输入数据应用了幅度为1的填充\" class=\"headerlink\" title=\"填充如何实现的，例：对大小为（4，4）的输入数据应用了幅度为1的填充\"></a>填充如何实现的，例：对大小为（4，4）的输入数据应用了幅度为1的填充</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/6.png\"></p>\n<h4 id=\"“幅度为1的填充”是指用幅度为1像素的0填充周围，填0的话直接省略不写，填充后，-4-4-的输入数据变成了-6-6-的形状\"><a href=\"#“幅度为1的填充”是指用幅度为1像素的0填充周围，填0的话直接省略不写，填充后，-4-4-的输入数据变成了-6-6-的形状\" class=\"headerlink\" title=\"“幅度为1的填充”是指用幅度为1像素的0填充周围，填0的话直接省略不写，填充后，(4,4)的输入数据变成了(6,6)的形状\"></a>“幅度为1的填充”是指用幅度为1像素的0填充周围，填0的话直接省略不写，填充后，(4,4)的输入数据变成了(6,6)的形状</h4></blockquote>\n<h4 id=\"步幅\"><a href=\"#步幅\" class=\"headerlink\" title=\"步幅\"></a>步幅</h4><blockquote>\n<h4 id=\"应用滤波器（卷积核）的位置间隔称为步幅（stride）\"><a href=\"#应用滤波器（卷积核）的位置间隔称为步幅（stride）\" class=\"headerlink\" title=\"应用滤波器（卷积核）的位置间隔称为步幅（stride）\"></a>应用滤波器（卷积核）的位置间隔称为步幅（stride）</h4><h4 id=\"下面将应用滤波器的窗口的间隔变为2个元素\"><a href=\"#下面将应用滤波器的窗口的间隔变为2个元素\" class=\"headerlink\" title=\"下面将应用滤波器的窗口的间隔变为2个元素\"></a>下面将应用滤波器的窗口的间隔变为2个元素</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/7.png\"></p>\n<h4 id=\"注意，步幅为2，意味着各个方向上（横向纵向）的移动都是2个元素\"><a href=\"#注意，步幅为2，意味着各个方向上（横向纵向）的移动都是2个元素\" class=\"headerlink\" title=\"注意，步幅为2，意味着各个方向上（横向纵向）的移动都是2个元素\"></a>注意，步幅为2，意味着各个方向上（横向纵向）的移动都是2个元素</h4><h4 id=\"增大步幅后，输出大小会变小。而增大填充后，输出大小会变大，对于步幅和填充，计算输出大小\"><a href=\"#增大步幅后，输出大小会变小。而增大填充后，输出大小会变大，对于步幅和填充，计算输出大小\" class=\"headerlink\" title=\"增大步幅后，输出大小会变小。而增大填充后，输出大小会变大，对于步幅和填充，计算输出大小\"></a>增大步幅后，输出大小会变小。而增大填充后，输出大小会变大，对于步幅和填充，计算输出大小</h4><blockquote>\n<h4 id=\"若输入大小为-H-W-，卷积核大小为（FH-FW），输出大小为-OH-OW-，填充为P，步幅为S，此时，输出大小可通过以下式子计算\"><a href=\"#若输入大小为-H-W-，卷积核大小为（FH-FW），输出大小为-OH-OW-，填充为P，步幅为S，此时，输出大小可通过以下式子计算\" class=\"headerlink\" title=\"若输入大小为(H,W)，卷积核大小为（FH,FW），输出大小为(OH,OW)，填充为P，步幅为S，此时，输出大小可通过以下式子计算\"></a>若输入大小为(H,W)，卷积核大小为（FH,FW），输出大小为(OH,OW)，填充为P，步幅为S，此时，输出大小可通过以下式子计算</h4><p>$$<br>OH &#x3D; \\frac {H + 2P - FH} {S} + 1 \\<br>OW &#x3D; \\frac {W + 2P - FW} {S} + 1<br>$$</p>\n<h4 id=\"不过有一个问题是，分式的值可能是小数（除不尽的情况），这时需要采取报错等对策，采取的深度学习框架不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行\"><a href=\"#不过有一个问题是，分式的值可能是小数（除不尽的情况），这时需要采取报错等对策，采取的深度学习框架不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行\" class=\"headerlink\" title=\"不过有一个问题是，分式的值可能是小数（除不尽的情况），这时需要采取报错等对策，采取的深度学习框架不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行\"></a>不过有一个问题是，分式的值可能是小数（除不尽的情况），这时需要采取报错等对策，采取的深度学习框架不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行</h4></blockquote>\n</blockquote>\n<h4 id=\"3维数据的卷积运算\"><a href=\"#3维数据的卷积运算\" class=\"headerlink\" title=\"3维数据的卷积运算\"></a>3维数据的卷积运算</h4><blockquote>\n<h4 id=\"对于3维数据的卷积运算，相比于2维数据的，其在纵深方向（通道方向）上特征图增加了\"><a href=\"#对于3维数据的卷积运算，相比于2维数据的，其在纵深方向（通道方向）上特征图增加了\" class=\"headerlink\" title=\"对于3维数据的卷积运算，相比于2维数据的，其在纵深方向（通道方向）上特征图增加了\"></a>对于3维数据的卷积运算，相比于2维数据的，其在纵深方向（通道方向）上特征图增加了</h4><h4 id=\"通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出\"><a href=\"#通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出\" class=\"headerlink\" title=\"通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出\"></a>通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/8.png\"></p>\n<h4 id=\"需要注意的是，输入数据和滤波器的通道数要设为相同的值\"><a href=\"#需要注意的是，输入数据和滤波器的通道数要设为相同的值\" class=\"headerlink\" title=\"需要注意的是，输入数据和滤波器的通道数要设为相同的值\"></a>需要注意的是，输入数据和滤波器的通道数要设为相同的值</h4><h4 id=\"通过立体方块来理解\"><a href=\"#通过立体方块来理解\" class=\"headerlink\" title=\"通过立体方块来理解\"></a>通过立体方块来理解</h4><blockquote>\n<h4 id=\"以3维数据为例，把3维数据表示为多维数组时，书写顺序是-channel-height-weight-或-C-H-W-，滤波器-卷积核-也是写作这样的形式-C-FH-FW-，如下\"><a href=\"#以3维数据为例，把3维数据表示为多维数组时，书写顺序是-channel-height-weight-或-C-H-W-，滤波器-卷积核-也是写作这样的形式-C-FH-FW-，如下\" class=\"headerlink\" title=\"以3维数据为例，把3维数据表示为多维数组时，书写顺序是 $(channel, height, weight)$ 或 $(C,H,W)$，滤波器(卷积核)也是写作这样的形式 $(C,FH,FW)$ ，如下\"></a>以3维数据为例，把3维数据表示为多维数组时，书写顺序是 $(channel, height, weight)$ 或 $(C,H,W)$，滤波器(卷积核)也是写作这样的形式 $(C,FH,FW)$ ，如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/16.png\"></p>\n<h4 id=\"上面的输出数据是一张特征图，也即通道数为-1-的特征图，如果要在通道方向上也有多个卷积核运算的输出，就需要用到多个滤波器（权重），如下\"><a href=\"#上面的输出数据是一张特征图，也即通道数为-1-的特征图，如果要在通道方向上也有多个卷积核运算的输出，就需要用到多个滤波器（权重），如下\" class=\"headerlink\" title=\"上面的输出数据是一张特征图，也即通道数为 1 的特征图，如果要在通道方向上也有多个卷积核运算的输出，就需要用到多个滤波器（权重），如下\"></a>上面的输出数据是一张特征图，也即通道数为 1 的特征图，如果要在通道方向上也有多个卷积核运算的输出，就需要用到多个滤波器（权重），如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/17.png\"></p>\n<h4 id=\"通过使用FN个卷积核，对应的输出特征图也变成了FN个，将这FN个特征图汇集在一起，就得到了一个形状为-FN-OH-OW-的方块，再将这个方块传递到下一层，就形成了CNN的处理流\"><a href=\"#通过使用FN个卷积核，对应的输出特征图也变成了FN个，将这FN个特征图汇集在一起，就得到了一个形状为-FN-OH-OW-的方块，再将这个方块传递到下一层，就形成了CNN的处理流\" class=\"headerlink\" title=\"通过使用FN个卷积核，对应的输出特征图也变成了FN个，将这FN个特征图汇集在一起，就得到了一个形状为 $(FN,OH,OW)$ 的方块，再将这个方块传递到下一层，就形成了CNN的处理流\"></a>通过使用FN个卷积核，对应的输出特征图也变成了FN个，将这FN个特征图汇集在一起，就得到了一个形状为 $(FN,OH,OW)$ 的方块，再将这个方块传递到下一层，就形成了CNN的处理流</h4><h4 id=\"那么4维的数据，卷积核的权重数据的书写顺序应该是-FC-CC-FH-FW-，即卷积核个数、卷积核通道数、卷积核高、卷积核宽\"><a href=\"#那么4维的数据，卷积核的权重数据的书写顺序应该是-FC-CC-FH-FW-，即卷积核个数、卷积核通道数、卷积核高、卷积核宽\" class=\"headerlink\" title=\"那么4维的数据，卷积核的权重数据的书写顺序应该是 $(FC,CC,FH,FW)$ ，即卷积核个数、卷积核通道数、卷积核高、卷积核宽\"></a>那么4维的数据，卷积核的权重数据的书写顺序应该是 $(FC,CC,FH,FW)$ ，即卷积核个数、卷积核通道数、卷积核高、卷积核宽</h4><h4 id=\"同样，也要加上偏置，如下\"><a href=\"#同样，也要加上偏置，如下\" class=\"headerlink\" title=\"同样，也要加上偏置，如下\"></a>同样，也要加上偏置，如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/18.png\"></p>\n<h4 id=\"注意，偏置的通道数要与卷积核的个数相同\"><a href=\"#注意，偏置的通道数要与卷积核的个数相同\" class=\"headerlink\" title=\"注意，偏置的通道数要与卷积核的个数相同\"></a>注意，偏置的通道数要与卷积核的个数相同</h4></blockquote>\n<h4 id=\"批处理\"><a href=\"#批处理\" class=\"headerlink\" title=\"批处理\"></a>批处理</h4><blockquote>\n<h4 id=\"通过批处理，能够实现处理的高效化和学习时对mini-batch的对应。在卷积运算中应用批处理，需要将在各层间传递的数据保存为4维数据，即-batch-num-channel-height-weight\"><a href=\"#通过批处理，能够实现处理的高效化和学习时对mini-batch的对应。在卷积运算中应用批处理，需要将在各层间传递的数据保存为4维数据，即-batch-num-channel-height-weight\" class=\"headerlink\" title=\"通过批处理，能够实现处理的高效化和学习时对mini-batch的对应。在卷积运算中应用批处理，需要将在各层间传递的数据保存为4维数据，即 $(batch-num,channel,height,weight)$\"></a>通过批处理，能够实现处理的高效化和学习时对mini-batch的对应。在卷积运算中应用批处理，需要将在各层间传递的数据保存为4维数据，即 $(batch-num,channel,height,weight)$</h4><h4 id=\"N个数据的批处理如下，\"><a href=\"#N个数据的批处理如下，\" class=\"headerlink\" title=\"N个数据的批处理如下，\"></a>N个数据的批处理如下，</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/19.png\"></p>\n<h4 id=\"批处理将N次处理汇总成了一次进行\"><a href=\"#批处理将N次处理汇总成了一次进行\" class=\"headerlink\" title=\"批处理将N次处理汇总成了一次进行\"></a>批处理将N次处理汇总成了一次进行</h4></blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n<h3 id=\"池化层\"><a href=\"#池化层\" class=\"headerlink\" title=\"池化层\"></a>池化层</h3><blockquote>\n<h4 id=\"池化是缩小高、长方向上的空间的运算\"><a href=\"#池化是缩小高、长方向上的空间的运算\" class=\"headerlink\" title=\"池化是缩小高、长方向上的空间的运算\"></a>池化是缩小高、长方向上的空间的运算</h4><h4 id=\"它的主要作用是对输出特征图进行下采样-subsampling-or-downsampling\"><a href=\"#它的主要作用是对输出特征图进行下采样-subsampling-or-downsampling\" class=\"headerlink\" title=\"它的主要作用是对输出特征图进行下采样(subsampling or downsampling)\"></a>它的主要作用是对输出特征图进行下采样(subsampling or downsampling)</h4><h4 id=\"它的作用是\"><a href=\"#它的作用是\" class=\"headerlink\" title=\"它的作用是\"></a>它的作用是</h4><blockquote>\n<h4 id=\"降维、减少计算量和内存开销，当原始输出特征图尺寸较大时，池化可以有效减小后续网络层的输入大小，从而减少参数数量和计算量\"><a href=\"#降维、减少计算量和内存开销，当原始输出特征图尺寸较大时，池化可以有效减小后续网络层的输入大小，从而减少参数数量和计算量\" class=\"headerlink\" title=\"降维、减少计算量和内存开销，当原始输出特征图尺寸较大时，池化可以有效减小后续网络层的输入大小，从而减少参数数量和计算量\"></a>降维、减少计算量和内存开销，当原始输出特征图尺寸较大时，池化可以有效减小后续网络层的输入大小，从而减少参数数量和计算量</h4><h4 id=\"提取主要特征、抑制噪声，如Max池化、Average池化、Global池化等\"><a href=\"#提取主要特征、抑制噪声，如Max池化、Average池化、Global池化等\" class=\"headerlink\" title=\"提取主要特征、抑制噪声，如Max池化、Average池化、Global池化等\"></a>提取主要特征、抑制噪声，如Max池化、Average池化、Global池化等</h4><h4 id=\"提高模型泛化能力，池化引入了某种程度的位置不变性-translation-invariance-，使模型对小范围的平移、旋转、缩放更具鲁棒性\"><a href=\"#提高模型泛化能力，池化引入了某种程度的位置不变性-translation-invariance-，使模型对小范围的平移、旋转、缩放更具鲁棒性\" class=\"headerlink\" title=\"提高模型泛化能力，池化引入了某种程度的位置不变性(translation invariance)，使模型对小范围的平移、旋转、缩放更具鲁棒性\"></a>提高模型泛化能力，池化引入了某种程度的位置不变性(translation invariance)，使模型对小范围的平移、旋转、缩放更具鲁棒性</h4><h4 id=\"防止过拟合，减少特征维度，限制了网络学习过于复杂的模式，有助于抑制过拟合\"><a href=\"#防止过拟合，减少特征维度，限制了网络学习过于复杂的模式，有助于抑制过拟合\" class=\"headerlink\" title=\"防止过拟合，减少特征维度，限制了网络学习过于复杂的模式，有助于抑制过拟合\"></a>防止过拟合，减少特征维度，限制了网络学习过于复杂的模式，有助于抑制过拟合</h4></blockquote>\n<h4 id=\"池化的目的是减小特征尺寸、增强抽象能力、提高效率\"><a href=\"#池化的目的是减小特征尺寸、增强抽象能力、提高效率\" class=\"headerlink\" title=\"池化的目的是减小特征尺寸、增强抽象能力、提高效率\"></a>池化的目的是减小特征尺寸、增强抽象能力、提高效率</h4><h4 id=\"补充：池化处理的就是张量，张量（tensor）就是一个具有维度的数值数组，0维是标量，1维是向量，2维是矩阵，3维及以上就是张量\"><a href=\"#补充：池化处理的就是张量，张量（tensor）就是一个具有维度的数值数组，0维是标量，1维是向量，2维是矩阵，3维及以上就是张量\" class=\"headerlink\" title=\"补充：池化处理的就是张量，张量（tensor）就是一个具有维度的数值数组，0维是标量，1维是向量，2维是矩阵，3维及以上就是张量\"></a>补充：池化处理的就是张量，张量（tensor）就是一个具有维度的数值数组，0维是标量，1维是向量，2维是矩阵，3维及以上就是张量</h4><h4 id=\"下面是Max池化的处理顺序\"><a href=\"#下面是Max池化的处理顺序\" class=\"headerlink\" title=\"下面是Max池化的处理顺序\"></a>下面是Max池化的处理顺序</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/20.png\"></p>\n<h4 id=\"这个例子是按照步幅为2进行池化窗口为-2x2-的Max池化操作的，处理的是输出特征图，Max池化是从目标区域（池化窗口）中取出最大值，提取显著特征\"><a href=\"#这个例子是按照步幅为2进行池化窗口为-2x2-的Max池化操作的，处理的是输出特征图，Max池化是从目标区域（池化窗口）中取出最大值，提取显著特征\" class=\"headerlink\" title=\"这个例子是按照步幅为2进行池化窗口为 2x2 的Max池化操作的，处理的是输出特征图，Max池化是从目标区域（池化窗口）中取出最大值，提取显著特征\"></a>这个例子是按照步幅为2进行池化窗口为 2x2 的Max池化操作的，处理的是输出特征图，Max池化是从目标区域（池化窗口）中取出最大值，提取显著特征</h4><h4 id=\"一般来说，池化的窗口大小会和步幅设定为相同值\"><a href=\"#一般来说，池化的窗口大小会和步幅设定为相同值\" class=\"headerlink\" title=\"一般来说，池化的窗口大小会和步幅设定为相同值\"></a>一般来说，池化的窗口大小会和步幅设定为相同值</h4><h4 id=\"除了Max池化，还有Average池化和Global池化，Average池化则是计算目标区域的平均值，用于平滑特征图；而Global池化是对整个特征图求平均或最大值，常用于分类前最后一层\"><a href=\"#除了Max池化，还有Average池化和Global池化，Average池化则是计算目标区域的平均值，用于平滑特征图；而Global池化是对整个特征图求平均或最大值，常用于分类前最后一层\" class=\"headerlink\" title=\"除了Max池化，还有Average池化和Global池化，Average池化则是计算目标区域的平均值，用于平滑特征图；而Global池化是对整个特征图求平均或最大值，常用于分类前最后一层\"></a>除了Max池化，还有Average池化和Global池化，Average池化则是计算目标区域的平均值，用于平滑特征图；而Global池化是对整个特征图求平均或最大值，常用于分类前最后一层</h4><h4 id=\"池化层的特征\"><a href=\"#池化层的特征\" class=\"headerlink\" title=\"池化层的特征\"></a>池化层的特征</h4><blockquote>\n<h4 id=\"没有要学习的参数，池化层和卷积层不同，没有要学习的参数\"><a href=\"#没有要学习的参数，池化层和卷积层不同，没有要学习的参数\" class=\"headerlink\" title=\"没有要学习的参数，池化层和卷积层不同，没有要学习的参数\"></a>没有要学习的参数，池化层和卷积层不同，没有要学习的参数</h4><h4 id=\"通道数不发生变化，经过池化运算，输入数据和输出数据的通道数不会发生变化\"><a href=\"#通道数不发生变化，经过池化运算，输入数据和输出数据的通道数不会发生变化\" class=\"headerlink\" title=\"通道数不发生变化，经过池化运算，输入数据和输出数据的通道数不会发生变化\"></a>通道数不发生变化，经过池化运算，输入数据和输出数据的通道数不会发生变化</h4><h4 id=\"对微小的位置变化具有鲁棒性，输入数据发生微小偏差时，池化仍会返回相同的结果，如下面的例子\"><a href=\"#对微小的位置变化具有鲁棒性，输入数据发生微小偏差时，池化仍会返回相同的结果，如下面的例子\" class=\"headerlink\" title=\"对微小的位置变化具有鲁棒性，输入数据发生微小偏差时，池化仍会返回相同的结果，如下面的例子\"></a>对微小的位置变化具有鲁棒性，输入数据发生微小偏差时，池化仍会返回相同的结果，如下面的例子</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/21.png\"></p>\n<h4 id=\"当输入数据在宽度方向上只偏离1个元素时，输出仍为相同的结果，当然，根据数据的不同，有时结果也不相同\"><a href=\"#当输入数据在宽度方向上只偏离1个元素时，输出仍为相同的结果，当然，根据数据的不同，有时结果也不相同\" class=\"headerlink\" title=\"当输入数据在宽度方向上只偏离1个元素时，输出仍为相同的结果，当然，根据数据的不同，有时结果也不相同\"></a>当输入数据在宽度方向上只偏离1个元素时，输出仍为相同的结果，当然，根据数据的不同，有时结果也不相同</h4></blockquote>\n</blockquote>\n<h3 id=\"卷积层和池化层的实现\"><a href=\"#卷积层和池化层的实现\" class=\"headerlink\" title=\"卷积层和池化层的实现\"></a>卷积层和池化层的实现</h3><blockquote>\n<h4 id=\"CNN中各层间传递的数据是4维数据，即-数据个数-通道数-高-宽\"><a href=\"#CNN中各层间传递的数据是4维数据，即-数据个数-通道数-高-宽\" class=\"headerlink\" title=\"CNN中各层间传递的数据是4维数据，即 $(数据个数,通道数,高,宽)$\"></a>CNN中各层间传递的数据是4维数据，即 $(数据个数,通道数,高,宽)$</h4><h4 id=\"基于im2col的展开\"><a href=\"#基于im2col的展开\" class=\"headerlink\" title=\"基于im2col的展开\"></a>基于im2col的展开</h4><blockquote>\n<h4 id=\"im2col是一个函数，将输入数据展开以适合滤波器（权重），对于输入数据，将应用卷积核的区域-3维方块-横向展开为1列，im2col会在所有应用卷积核的区域进行这个展开处理，如下所示\"><a href=\"#im2col是一个函数，将输入数据展开以适合滤波器（权重），对于输入数据，将应用卷积核的区域-3维方块-横向展开为1列，im2col会在所有应用卷积核的区域进行这个展开处理，如下所示\" class=\"headerlink\" title=\"im2col是一个函数，将输入数据展开以适合滤波器（权重），对于输入数据，将应用卷积核的区域(3维方块)横向展开为1列，im2col会在所有应用卷积核的区域进行这个展开处理，如下所示\"></a>im2col是一个函数，将输入数据展开以适合滤波器（权重），对于输入数据，将应用卷积核的区域(3维方块)横向展开为1列，im2col会在所有应用卷积核的区域进行这个展开处理，如下所示</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/22.png\"></p>\n<h4 id=\"值得注意的是，滤波器的应用区域几乎都是重叠的，在此情况下，使用im2col展开后，展开后的元素个数会多于原方块-输入数据-的元素个数，因此，使用im2col的实现存在比普通的实现消耗更多内存的缺点。但，汇总成一个大的矩阵进行计算，对计算机的计算颇有益处，因为可以有效地利用线性代数库\"><a href=\"#值得注意的是，滤波器的应用区域几乎都是重叠的，在此情况下，使用im2col展开后，展开后的元素个数会多于原方块-输入数据-的元素个数，因此，使用im2col的实现存在比普通的实现消耗更多内存的缺点。但，汇总成一个大的矩阵进行计算，对计算机的计算颇有益处，因为可以有效地利用线性代数库\" class=\"headerlink\" title=\"值得注意的是，滤波器的应用区域几乎都是重叠的，在此情况下，使用im2col展开后，展开后的元素个数会多于原方块(输入数据)的元素个数，因此，使用im2col的实现存在比普通的实现消耗更多内存的缺点。但，汇总成一个大的矩阵进行计算，对计算机的计算颇有益处，因为可以有效地利用线性代数库\"></a>值得注意的是，滤波器的应用区域几乎都是重叠的，在此情况下，使用im2col展开后，展开后的元素个数会多于原方块(输入数据)的元素个数，因此，使用im2col的实现存在比普通的实现消耗更多内存的缺点。但，汇总成一个大的矩阵进行计算，对计算机的计算颇有益处，因为可以有效地利用线性代数库</h4><h4 id=\"另外，im2col-意为-image-to-column，即图像到矩阵\"><a href=\"#另外，im2col-意为-image-to-column，即图像到矩阵\" class=\"headerlink\" title=\"另外，im2col 意为 image to column，即图像到矩阵\"></a>另外，im2col 意为 image to column，即图像到矩阵</h4><h4 id=\"im2col的处理过程如下\"><a href=\"#im2col的处理过程如下\" class=\"headerlink\" title=\"im2col的处理过程如下\"></a>im2col的处理过程如下</h4><blockquote>\n<ol>\n<li><h4 id=\"从原始输入图像数据中提取所有滑动窗口区域（patch）\"><a href=\"#从原始输入图像数据中提取所有滑动窗口区域（patch）\" class=\"headerlink\" title=\"从原始输入图像数据中提取所有滑动窗口区域（patch）\"></a>从原始输入图像数据中提取所有滑动窗口区域（patch）</h4></li>\n<li><h4 id=\"将每个patch展开成1行\"><a href=\"#将每个patch展开成1行\" class=\"headerlink\" title=\"将每个patch展开成1行\"></a>将每个patch展开成1行</h4></li>\n<li><h4 id=\"再将所有的patch行拼接成一个矩阵（im2col矩阵）\"><a href=\"#再将所有的patch行拼接成一个矩阵（im2col矩阵）\" class=\"headerlink\" title=\"再将所有的patch行拼接成一个矩阵（im2col矩阵）\"></a>再将所有的patch行拼接成一个矩阵（im2col矩阵）</h4></li>\n<li><h4 id=\"将卷积核拉平成一个列向量\"><a href=\"#将卷积核拉平成一个列向量\" class=\"headerlink\" title=\"将卷积核拉平成一个列向量\"></a>将卷积核拉平成一个列向量</h4></li>\n<li><h4 id=\"进行矩阵乘法（类似于Affine层进行的处理）\"><a href=\"#进行矩阵乘法（类似于Affine层进行的处理）\" class=\"headerlink\" title=\"进行矩阵乘法（类似于Affine层进行的处理）\"></a>进行矩阵乘法（类似于Affine层进行的处理）</h4></li>\n<li><h4 id=\"再reshape成输出特征图的形状\"><a href=\"#再reshape成输出特征图的形状\" class=\"headerlink\" title=\"再reshape成输出特征图的形状\"></a>再reshape成输出特征图的形状</h4></li>\n</ol>\n</blockquote>\n<h4 id=\"使用im2col展开输入数据后，之后就只需将卷积层的滤波器（权重）纵向展开为1列，并计算2个矩阵的乘积即可，这和全连接层的Affi-ne层进行的处理基本相同\"><a href=\"#使用im2col展开输入数据后，之后就只需将卷积层的滤波器（权重）纵向展开为1列，并计算2个矩阵的乘积即可，这和全连接层的Affi-ne层进行的处理基本相同\" class=\"headerlink\" title=\"使用im2col展开输入数据后，之后就只需将卷积层的滤波器（权重）纵向展开为1列，并计算2个矩阵的乘积即可，这和全连接层的Affi ne层进行的处理基本相同\"></a>使用im2col展开输入数据后，之后就只需将卷积层的滤波器（权重）纵向展开为1列，并计算2个矩阵的乘积即可，这和全连接层的Affi ne层进行的处理基本相同</h4><h4 id=\"使用im2col的卷积运算的卷积核处理过程如下\"><a href=\"#使用im2col的卷积运算的卷积核处理过程如下\" class=\"headerlink\" title=\"使用im2col的卷积运算的卷积核处理过程如下\"></a>使用im2col的卷积运算的卷积核处理过程如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/13.png\"></p>\n<h4 id=\"因为CNN中数据会保存为4维数组，所以要将2维输出数据转换-reshape-为合适的形状，这就是卷积层的实现流程\"><a href=\"#因为CNN中数据会保存为4维数组，所以要将2维输出数据转换-reshape-为合适的形状，这就是卷积层的实现流程\" class=\"headerlink\" title=\"因为CNN中数据会保存为4维数组，所以要将2维输出数据转换(reshape)为合适的形状，这就是卷积层的实现流程\"></a>因为CNN中数据会保存为4维数组，所以要将2维输出数据转换(reshape)为合适的形状，这就是卷积层的实现流程</h4></blockquote>\n<h4 id=\"卷积层的实现\"><a href=\"#卷积层的实现\" class=\"headerlink\" title=\"卷积层的实现\"></a>卷积层的实现</h4><blockquote>\n<h4 id=\"在卷积层的实现过程中，有以下几个点需要注意\"><a href=\"#在卷积层的实现过程中，有以下几个点需要注意\" class=\"headerlink\" title=\"在卷积层的实现过程中，有以下几个点需要注意\"></a>在卷积层的实现过程中，有以下几个点需要注意</h4><blockquote>\n<ol>\n<li><h4 id=\"im2col函数得到的是col，这个col表示-输出区域数-每个区域展开后元素个数-，这个输出区域数其实就是-H-out-W-out，它就是经过im2col展开后得到的行数。而第2个参数就是-卷积核大小-通道数，即-FH-FW-C\"><a href=\"#im2col函数得到的是col，这个col表示-输出区域数-每个区域展开后元素个数-，这个输出区域数其实就是-H-out-W-out，它就是经过im2col展开后得到的行数。而第2个参数就是-卷积核大小-通道数，即-FH-FW-C\" class=\"headerlink\" title=\"im2col函数得到的是col，这个col表示 $(输出区域数,每个区域展开后元素个数)$ ，这个输出区域数其实就是 H_out * W_out，它就是经过im2col展开后得到的行数。而第2个参数就是 卷积核大小 * 通道数，即 $FH * FW * C$\"></a>im2col函数得到的是col，这个col表示 $(输出区域数,每个区域展开后元素个数)$ ，这个输出区域数其实就是 H_out * W_out，它就是经过im2col展开后得到的行数。而第2个参数就是 卷积核大小 * 通道数，即 $FH * FW * C$</h4></li>\n<li><h4 id=\"卷积层的实现的类中的forward方法里，im2col展开处理后的数据形状-col-shape-N-out-h-out-w-C-FH-FW-，卷积核展开之后的形状-col-W-shape-C-FH-FW-FN-，那么-dot-col-col-W-得到的输出的形状就是-out-shape-N-out-h-out-w-FN-，这表示为：每一张输入图像，每一个输出位置-out-h-out-w-个，对每一个卷积核（共FN个）都得到一个输出数值。再将得到的输出out还原为-4D-卷积输出张量结构。注意：这里的N是batch-size，也是输入图像的个数；FN是卷积核的个数，也是这层卷积层的输出通道数\"><a href=\"#卷积层的实现的类中的forward方法里，im2col展开处理后的数据形状-col-shape-N-out-h-out-w-C-FH-FW-，卷积核展开之后的形状-col-W-shape-C-FH-FW-FN-，那么-dot-col-col-W-得到的输出的形状就是-out-shape-N-out-h-out-w-FN-，这表示为：每一张输入图像，每一个输出位置-out-h-out-w-个，对每一个卷积核（共FN个）都得到一个输出数值。再将得到的输出out还原为-4D-卷积输出张量结构。注意：这里的N是batch-size，也是输入图像的个数；FN是卷积核的个数，也是这层卷积层的输出通道数\" class=\"headerlink\" title=\"卷积层的实现的类中的forward方法里，im2col展开处理后的数据形状 col.shape&#x3D;(N*out_h*out_w, C*FH*FW)，卷积核展开之后的形状 col_W.shape &#x3D; (C*FH*FW, FN)，那么 dot(col, col_W) 得到的输出的形状就是 out.shape &#x3D; (N*out_h*out_w, FN)，这表示为：每一张输入图像，每一个输出位置(out_h*out_w)个，对每一个卷积核（共FN个）都得到一个输出数值。再将得到的输出out还原为 4D 卷积输出张量结构。注意：这里的N是batch_size，也是输入图像的个数；FN是卷积核的个数，也是这层卷积层的输出通道数\"></a>卷积层的实现的类中的forward方法里，im2col展开处理后的数据形状 col.shape&#x3D;(N*out_h*out_w, C*FH*FW)，卷积核展开之后的形状 col_W.shape &#x3D; (C*FH*FW, FN)，那么 dot(col, col_W) 得到的输出的形状就是 out.shape &#x3D; (N*out_h*out_w, FN)，这表示为：每一张输入图像，每一个输出位置(out_h*out_w)个，对每一个卷积核（共FN个）都得到一个输出数值。再将得到的输出out还原为 4D 卷积输出张量结构。注意：这里的N是batch_size，也是输入图像的个数；FN是卷积核的个数，也是这层卷积层的输出通道数</h4></li>\n<li><h4 id=\"将out还原为4D卷积输出张量结构，这里就有一个新的问题，为什么不能直接使用-reshape-将out-shape变为-N-FN-out-h-out-w-？这是因为-reshape-只是重新排列元素的形状，不能改变维度的排列顺序（轴顺序），要改变轴顺序必须使用-transpose，out-shape-N-out-h-out-w-FN-，axis-0-N-out-h-out-w，axis-1-FN，reshape不会打乱数据顺序，而transpose是要调整轴的排列的，transpose-0-3-1-2-这里的数值是reshape之后-0N1H2W3C-的下标，不是实际值，最后要调整为NCHW（0-3-1-2），如下图\"><a href=\"#将out还原为4D卷积输出张量结构，这里就有一个新的问题，为什么不能直接使用-reshape-将out-shape变为-N-FN-out-h-out-w-？这是因为-reshape-只是重新排列元素的形状，不能改变维度的排列顺序（轴顺序），要改变轴顺序必须使用-transpose，out-shape-N-out-h-out-w-FN-，axis-0-N-out-h-out-w，axis-1-FN，reshape不会打乱数据顺序，而transpose是要调整轴的排列的，transpose-0-3-1-2-这里的数值是reshape之后-0N1H2W3C-的下标，不是实际值，最后要调整为NCHW（0-3-1-2），如下图\" class=\"headerlink\" title=\"将out还原为4D卷积输出张量结构，这里就有一个新的问题，为什么不能直接使用 reshape 将out.shape变为 (N, FN, out_h, out_w) ？这是因为 reshape 只是重新排列元素的形状，不能改变维度的排列顺序（轴顺序），要改变轴顺序必须使用 transpose，out.shape &#x3D; ( N * out_h * out_w, FN )，axis&#x3D;0:N*out_h*out_w，axis&#x3D;1:FN，reshape不会打乱数据顺序，而transpose是要调整轴的排列的，transpose(0,3,1,2)这里的数值是reshape之后 0N1H2W3C 的下标，不是实际值，最后要调整为NCHW（0,3,1,2），如下图\"></a>将out还原为4D卷积输出张量结构，这里就有一个新的问题，为什么不能直接使用 reshape 将out.shape变为 (N, FN, out_h, out_w) ？这是因为 reshape 只是重新排列元素的形状，不能改变维度的排列顺序（轴顺序），要改变轴顺序必须使用 transpose，out.shape &#x3D; ( N * out_h * out_w, FN )，axis&#x3D;0:N*out_h*out_w，axis&#x3D;1:FN，reshape不会打乱数据顺序，而transpose是要调整轴的排列的，transpose(0,3,1,2)这里的数值是reshape之后 0N1H2W3C 的下标，不是实际值，最后要调整为NCHW（0,3,1,2），如下图</h4></li>\n<li><h4 id=\"在进行卷积层的反向传播时，必须进行im2col的逆处理，这里使用一个col2im函数，代码在code里\"><a href=\"#在进行卷积层的反向传播时，必须进行im2col的逆处理，这里使用一个col2im函数，代码在code里\" class=\"headerlink\" title=\"在进行卷积层的反向传播时，必须进行im2col的逆处理，这里使用一个col2im函数，代码在code里\"></a>在进行卷积层的反向传播时，必须进行im2col的逆处理，这里使用一个col2im函数，代码在code里</h4></li>\n</ol>\n<p><img src=\"http://picbed.yanzu.tech/img/DL/6/23.png\"></p>\n</blockquote>\n</blockquote>\n<h4 id=\"池化层的实现\"><a href=\"#池化层的实现\" class=\"headerlink\" title=\"池化层的实现\"></a>池化层的实现</h4><blockquote>\n<h4 id=\"池化层的实现和卷积层相同，都使用im2col展开输入数据，不过，池化的情况下，在通道方向上是独立的，这一点和卷积层不同，也即池化的应用区域按通道单独展开\"><a href=\"#池化层的实现和卷积层相同，都使用im2col展开输入数据，不过，池化的情况下，在通道方向上是独立的，这一点和卷积层不同，也即池化的应用区域按通道单独展开\" class=\"headerlink\" title=\"池化层的实现和卷积层相同，都使用im2col展开输入数据，不过，池化的情况下，在通道方向上是独立的，这一点和卷积层不同，也即池化的应用区域按通道单独展开\"></a>池化层的实现和卷积层相同，都使用im2col展开输入数据，不过，池化的情况下，在通道方向上是独立的，这一点和卷积层不同，也即池化的应用区域按通道单独展开</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/14.png\"></p>\n<h4 id=\"然后进行Max池化\"><a href=\"#然后进行Max池化\" class=\"headerlink\" title=\"然后进行Max池化\"></a>然后进行Max池化</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/6/15.png\"></p>\n</blockquote>\n</blockquote>\n<h3 id=\"CNN的实现\"><a href=\"#CNN的实现\" class=\"headerlink\" title=\"CNN的实现\"></a>CNN的实现</h3><blockquote>\n<h4 id=\"有几点需要注意\"><a href=\"#有几点需要注意\" class=\"headerlink\" title=\"有几点需要注意\"></a>有几点需要注意</h4><blockquote>\n<ol>\n<li><h4 id=\"初始化方法中，池化输出大小的代码如下\"><a href=\"#初始化方法中，池化输出大小的代码如下\" class=\"headerlink\" title=\"初始化方法中，池化输出大小的代码如下\"></a>初始化方法中，池化输出大小的代码如下</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pool_output_size = <span class=\"built_in\">int</span>(filter_num * (conv_output_size / <span class=\"number\">2</span>) * (conv_output_size / <span class=\"number\">2</span>))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"其中-conv-output-size-2-是因为后面定义的池化窗口大小为2x2，且stride-2，所以池化后输出的高宽会变为原来的一半，池化层的设定如下\"><a href=\"#其中-conv-output-size-2-是因为后面定义的池化窗口大小为2x2，且stride-2，所以池化后输出的高宽会变为原来的一半，池化层的设定如下\" class=\"headerlink\" title=\"其中 conv_output_size&#x2F;2 是因为后面定义的池化窗口大小为2x2，且stride&#x3D;2，所以池化后输出的高宽会变为原来的一半，池化层的设定如下\"></a>其中 conv_output_size&#x2F;2 是因为后面定义的池化窗口大小为2x2，且stride&#x3D;2，所以池化后输出的高宽会变为原来的一半，池化层的设定如下</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Pool1&#x27;</span>] = Pooling(pool_h=<span class=\"number\">2</span>, pool_w=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>)</span><br></pre></td></tr></table></figure>\n</li>\n<li><h4 id=\"初始化方法中，参数-weight-init-std-0-01-的作用，这是为了控制权重初始化的“数值范围”，防止信号和梯度在网络中“放大或消失”，从而保持网络稳定训练，np-random-randn-是均值为0、标准差为1的正态分布，数值范围在-3到3，这有可能会导致梯度消失或爆炸。而乘以0-01后就变为了均值为0，标准差为0-01的正态分布，则会更稳定一点\"><a href=\"#初始化方法中，参数-weight-init-std-0-01-的作用，这是为了控制权重初始化的“数值范围”，防止信号和梯度在网络中“放大或消失”，从而保持网络稳定训练，np-random-randn-是均值为0、标准差为1的正态分布，数值范围在-3到3，这有可能会导致梯度消失或爆炸。而乘以0-01后就变为了均值为0，标准差为0-01的正态分布，则会更稳定一点\" class=\"headerlink\" title=\"初始化方法中，参数 weight_init_std&#x3D;0.01 的作用，这是为了控制权重初始化的“数值范围”，防止信号和梯度在网络中“放大或消失”，从而保持网络稳定训练，np.random.randn()是均值为0、标准差为1的正态分布，数值范围在-3到3，这有可能会导致梯度消失或爆炸。而乘以0.01后就变为了均值为0，标准差为0.01的正态分布，则会更稳定一点\"></a>初始化方法中，参数 weight_init_std&#x3D;0.01 的作用，这是为了控制权重初始化的“数值范围”，防止信号和梯度在网络中“放大或消失”，从而保持网络稳定训练，np.random.randn()是均值为0、标准差为1的正态分布，数值范围在-3到3，这有可能会导致梯度消失或爆炸。而乘以0.01后就变为了均值为0，标准差为0.01的正态分布，则会更稳定一点</h4></li>\n</ol>\n</blockquote>\n</blockquote>\n<h3 id=\"CNN的可视化\"><a href=\"#CNN的可视化\" class=\"headerlink\" title=\"CNN的可视化\"></a>CNN的可视化</h3><blockquote>\n<h4 id=\"卷积层的滤波器会提取边缘或斑块等原始信息\"><a href=\"#卷积层的滤波器会提取边缘或斑块等原始信息\" class=\"headerlink\" title=\"卷积层的滤波器会提取边缘或斑块等原始信息\"></a>卷积层的滤波器会提取边缘或斑块等原始信息</h4></blockquote>\n<h3 id=\"具有代表性的CNN\"><a href=\"#具有代表性的CNN\" class=\"headerlink\" title=\"具有代表性的CNN\"></a>具有代表性的CNN</h3><blockquote>\n<h4 id=\"LeNet\"><a href=\"#LeNet\" class=\"headerlink\" title=\"LeNet\"></a>LeNet</h4><h4 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h4></blockquote>\n<h3 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h3><blockquote>\n<h4 id=\"卷积层的实现-1\"><a href=\"#卷积层的实现-1\" class=\"headerlink\" title=\"卷积层的实现\"></a>卷积层的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Convolution</span>:</span><br><span class=\"line\"> <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\"> 卷积层，实现前向和反向传播。</span></span><br><span class=\"line\"><span class=\"string\"> &quot;&quot;&quot;</span></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, W, b, stride=<span class=\"number\">1</span>, pad=<span class=\"number\">0</span></span>):</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.W = W  <span class=\"comment\"># 卷积核权重，形状(FN, C, FH, FW)</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.b = b  <span class=\"comment\"># 偏置，形状(FN,)</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.stride = stride  <span class=\"comment\"># 步幅</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.pad = pad        <span class=\"comment\"># 填充</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.x = <span class=\"literal\">None</span>   <span class=\"comment\"># 输入</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.col = <span class=\"literal\">None</span> <span class=\"comment\"># im2col展开后的输入</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.col_W = <span class=\"literal\">None</span> <span class=\"comment\"># 展开后的卷积核</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.dW = <span class=\"literal\">None</span>  <span class=\"comment\"># 权重梯度</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.db = <span class=\"literal\">None</span>  <span class=\"comment\"># 偏置梯度</span></span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">     <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">     前向传播：im2col展开输入，矩阵乘法实现卷积。</span></span><br><span class=\"line\"><span class=\"string\">     &quot;&quot;&quot;</span></span><br><span class=\"line\">     FN, C, FH, FW = <span class=\"variable language_\">self</span>.W.shape  <span class=\"comment\"># 卷积核参数</span></span><br><span class=\"line\">     N, C, H, W = x.shape          <span class=\"comment\"># 输入参数</span></span><br><span class=\"line\">     out_h = <span class=\"number\">1</span> + <span class=\"built_in\">int</span>((H + <span class=\"number\">2</span>*<span class=\"variable language_\">self</span>.pad - FH) / <span class=\"variable language_\">self</span>.stride)  <span class=\"comment\"># 输出高</span></span><br><span class=\"line\">     out_w = <span class=\"number\">1</span> + <span class=\"built_in\">int</span>((W + <span class=\"number\">2</span>*<span class=\"variable language_\">self</span>.pad - FW) / <span class=\"variable language_\">self</span>.stride)  <span class=\"comment\"># 输出宽</span></span><br><span class=\"line\">     col = im2col(x, FH, FW, <span class=\"variable language_\">self</span>.stride, <span class=\"variable language_\">self</span>.pad)  <span class=\"comment\"># 输入展开</span></span><br><span class=\"line\">     col_W = <span class=\"variable language_\">self</span>.W.reshape(FN, -<span class=\"number\">1</span>).T  <span class=\"comment\"># 卷积核展开</span></span><br><span class=\"line\">     out = np.dot(col, col_W) + <span class=\"variable language_\">self</span>.b  <span class=\"comment\"># 矩阵乘法+偏置</span></span><br><span class=\"line\">     out = out.reshape(N, out_h, out_w, -<span class=\"number\">1</span>).transpose(<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>)  <span class=\"comment\"># 还原输出形状</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.x = x</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.col = col</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.col_W = col_W</span><br><span class=\"line\">     <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">backward</span>(<span class=\"params\">self, dout</span>):</span><br><span class=\"line\">     <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">     反向传播：计算输入、权重、偏置的梯度。</span></span><br><span class=\"line\"><span class=\"string\">     &quot;&quot;&quot;</span></span><br><span class=\"line\">     FN, C, FH, FW = <span class=\"variable language_\">self</span>.W.shape</span><br><span class=\"line\">     dout = dout.transpose(<span class=\"number\">0</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">1</span>).reshape(-<span class=\"number\">1</span>, FN)  <span class=\"comment\"># 调整dout形状</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.db = np.<span class=\"built_in\">sum</span>(dout, axis=<span class=\"number\">0</span>)  <span class=\"comment\"># 偏置梯度</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.dW = np.dot(<span class=\"variable language_\">self</span>.col.T, dout)  <span class=\"comment\"># 权重梯度</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.dW = <span class=\"variable language_\">self</span>.dW.transpose(<span class=\"number\">1</span>, <span class=\"number\">0</span>).reshape(FN, C, FH, FW)  <span class=\"comment\"># 还原权重形状</span></span><br><span class=\"line\">     dcol = np.dot(dout, <span class=\"variable language_\">self</span>.col_W.T)  <span class=\"comment\"># 输入展开后的梯度</span></span><br><span class=\"line\">     dx = col2im(dcol, <span class=\"variable language_\">self</span>.x.shape, FH, FW, <span class=\"variable language_\">self</span>.stride, <span class=\"variable language_\">self</span>.pad)  <span class=\"comment\"># 还原输入形状</span></span><br><span class=\"line\">     <span class=\"keyword\">return</span> dx</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"col2im\"><a href=\"#col2im\" class=\"headerlink\" title=\"col2im\"></a>col2im</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># col2im: 将im2col展开的二维矩阵还原为原始多维图像数据</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">col2im</span>(<span class=\"params\">col, input_shape, filter_h, filter_w, stride=<span class=\"number\">1</span>, pad=<span class=\"number\">0</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    将im2col展开的二维矩阵还原为原始的多维图像数据。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Parameters</span></span><br><span class=\"line\"><span class=\"string\">    ----------</span></span><br><span class=\"line\"><span class=\"string\">    col : 展开后的二维矩阵</span></span><br><span class=\"line\"><span class=\"string\">    input_shape : 原始输入数据的形状（如：(10, 1, 28, 28)）</span></span><br><span class=\"line\"><span class=\"string\">    filter_h : 滤波器高度</span></span><br><span class=\"line\"><span class=\"string\">    filter_w : 滤波器宽度</span></span><br><span class=\"line\"><span class=\"string\">    stride : 步幅</span></span><br><span class=\"line\"><span class=\"string\">    pad : 填充</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns</span></span><br><span class=\"line\"><span class=\"string\">    -------</span></span><br><span class=\"line\"><span class=\"string\">    img : 还原后的多维图像数据</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    N, C, H, W = input_shape</span><br><span class=\"line\">    out_h = (H + <span class=\"number\">2</span>*pad - filter_h)//stride + <span class=\"number\">1</span>  <span class=\"comment\"># 输出高度</span></span><br><span class=\"line\">    out_w = (W + <span class=\"number\">2</span>*pad - filter_w)//stride + <span class=\"number\">1</span>  <span class=\"comment\"># 输出宽度</span></span><br><span class=\"line\">    <span class=\"comment\"># 还原为im2col前的形状</span></span><br><span class=\"line\">    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 初始化还原后的图像，注意填充和步幅的影响</span></span><br><span class=\"line\">    img = np.zeros((N, C, H + <span class=\"number\">2</span>*pad + stride - <span class=\"number\">1</span>, W + <span class=\"number\">2</span>*pad + stride - <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"comment\"># 将每个patch累加到对应的位置</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> y <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(filter_h):</span><br><span class=\"line\">        y_max = y + stride*out_h</span><br><span class=\"line\">        <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(filter_w):</span><br><span class=\"line\">            x_max = x + stride*out_w</span><br><span class=\"line\">            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 去除填充部分，返回原始大小</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> img[:, :, pad:H + pad, pad:W + pad]</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"池化层的实现-1\"><a href=\"#池化层的实现-1\" class=\"headerlink\" title=\"池化层的实现\"></a>池化层的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Pooling</span>:</span><br><span class=\"line\"> <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\"> 池化层，实现最大池化的前向和反向传播。</span></span><br><span class=\"line\"><span class=\"string\"> &quot;&quot;&quot;</span></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, pool_h, pool_w, stride=<span class=\"number\">2</span>, pad=<span class=\"number\">0</span></span>):</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.pool_h = pool_h  <span class=\"comment\"># 池化窗口高</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.pool_w = pool_w  <span class=\"comment\"># 池化窗口宽</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.stride = stride  <span class=\"comment\"># 步幅</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.pad = pad        <span class=\"comment\"># 填充</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.x = <span class=\"literal\">None</span>         <span class=\"comment\"># 输入</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.arg_max = <span class=\"literal\">None</span>   <span class=\"comment\"># 最大值索引</span></span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">     <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">     前向传播：im2col展开后做最大池化。</span></span><br><span class=\"line\"><span class=\"string\">     &quot;&quot;&quot;</span></span><br><span class=\"line\">     N, C, H, W = x.shape</span><br><span class=\"line\">     out_h = <span class=\"built_in\">int</span>(<span class=\"number\">1</span> + (H - <span class=\"variable language_\">self</span>.pool_h) / <span class=\"variable language_\">self</span>.stride)  <span class=\"comment\"># 输出高</span></span><br><span class=\"line\">     out_w = <span class=\"built_in\">int</span>(<span class=\"number\">1</span> + (W - <span class=\"variable language_\">self</span>.pool_w) / <span class=\"variable language_\">self</span>.stride)  <span class=\"comment\"># 输出宽</span></span><br><span class=\"line\">     col = im2col(x, <span class=\"variable language_\">self</span>.pool_h, <span class=\"variable language_\">self</span>.pool_w, <span class=\"variable language_\">self</span>.stride, <span class=\"variable language_\">self</span>.pad)  <span class=\"comment\"># 输入展开</span></span><br><span class=\"line\">     col = col.reshape(-<span class=\"number\">1</span>, <span class=\"variable language_\">self</span>.pool_h*<span class=\"variable language_\">self</span>.pool_w)  <span class=\"comment\"># 每个池化区域展平成一行</span></span><br><span class=\"line\">     arg_max = np.argmax(col, axis=<span class=\"number\">1</span>)  <span class=\"comment\"># 最大值索引</span></span><br><span class=\"line\">     out = np.<span class=\"built_in\">max</span>(col, axis=<span class=\"number\">1</span>)         <span class=\"comment\"># 最大值</span></span><br><span class=\"line\">     out = out.reshape(N, out_h, out_w, C).transpose(<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>)  <span class=\"comment\"># 还原输出形状</span></span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.x = x</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.arg_max = arg_max</span><br><span class=\"line\">     <span class=\"keyword\">return</span> out</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">backward</span>(<span class=\"params\">self, dout</span>):</span><br><span class=\"line\">     <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">     反向传播：最大池化的反向传播，将梯度传递到最大值位置。</span></span><br><span class=\"line\"><span class=\"string\">     &quot;&quot;&quot;</span></span><br><span class=\"line\">     dout = dout.transpose(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>)  <span class=\"comment\"># 调整dout形状</span></span><br><span class=\"line\">     pool_size = <span class=\"variable language_\">self</span>.pool_h * <span class=\"variable language_\">self</span>.pool_w</span><br><span class=\"line\">     dmax = np.zeros((dout.size, pool_size))  <span class=\"comment\"># 初始化梯度</span></span><br><span class=\"line\">     dmax[np.arange(<span class=\"variable language_\">self</span>.arg_max.size), <span class=\"variable language_\">self</span>.arg_max.flatten()] = dout.flatten()  <span class=\"comment\"># 只在最大值位置赋值</span></span><br><span class=\"line\">     dmax = dmax.reshape(dout.shape + (pool_size,)) </span><br><span class=\"line\">     dcol = dmax.reshape(dmax.shape[<span class=\"number\">0</span>] * dmax.shape[<span class=\"number\">1</span>] * dmax.shape[<span class=\"number\">2</span>], -<span class=\"number\">1</span>)  <span class=\"comment\"># 展平成二维</span></span><br><span class=\"line\">     dx = col2im(dcol, <span class=\"variable language_\">self</span>.x.shape, <span class=\"variable language_\">self</span>.pool_h, <span class=\"variable language_\">self</span>.pool_w, <span class=\"variable language_\">self</span>.stride, <span class=\"variable language_\">self</span>.pad)  <span class=\"comment\"># 还原输入形状</span></span><br><span class=\"line\">     <span class=\"keyword\">return</span> dx</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"简单的CNN例子\"><a href=\"#简单的CNN例子\" class=\"headerlink\" title=\"简单的CNN例子\"></a>简单的CNN例子</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys, os</span><br><span class=\"line\">sys.path.append(os.pardir)  <span class=\"comment\"># 添加父目录到模块搜索路径，便于导入common模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> OrderedDict</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.layers <span class=\"keyword\">import</span> *</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.gradient <span class=\"keyword\">import</span> numerical_gradient</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SimpleConvNet</span>:</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    简单卷积神经网络（ConvNet）实现：</span></span><br><span class=\"line\"><span class=\"string\">    网络结构为 conv - relu - pool - affine - relu - affine - softmax。</span></span><br><span class=\"line\"><span class=\"string\">    支持参数保存与加载，支持数值梯度和反向传播梯度。</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, input_dim=(<span class=\"params\"><span class=\"number\">1</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span></span>), </span></span><br><span class=\"line\"><span class=\"params\">                 conv_param=&#123;<span class=\"string\">&#x27;filter_num&#x27;</span>:<span class=\"number\">30</span>, <span class=\"string\">&#x27;filter_size&#x27;</span>:<span class=\"number\">5</span>, <span class=\"string\">&#x27;pad&#x27;</span>:<span class=\"number\">0</span>, <span class=\"string\">&#x27;stride&#x27;</span>:<span class=\"number\">1</span>&#125;,</span></span><br><span class=\"line\"><span class=\"params\">                 hidden_size=<span class=\"number\">100</span>, output_size=<span class=\"number\">10</span>, weight_init_std=<span class=\"number\">0.01</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        初始化网络参数和各层。</span></span><br><span class=\"line\"><span class=\"string\">        input_dim: 输入数据的形状 (通道数, 高, 宽)</span></span><br><span class=\"line\"><span class=\"string\">        conv_param: 卷积层参数字典，包括filter_num, filter_size, pad, stride</span></span><br><span class=\"line\"><span class=\"string\">        hidden_size: 隐藏层神经元数</span></span><br><span class=\"line\"><span class=\"string\">        output_size: 输出类别数</span></span><br><span class=\"line\"><span class=\"string\">        weight_init_std: 权重初始化标准差</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        filter_num = conv_param[<span class=\"string\">&#x27;filter_num&#x27;</span>]</span><br><span class=\"line\">        filter_size = conv_param[<span class=\"string\">&#x27;filter_size&#x27;</span>]</span><br><span class=\"line\">        filter_pad = conv_param[<span class=\"string\">&#x27;pad&#x27;</span>]</span><br><span class=\"line\">        filter_stride = conv_param[<span class=\"string\">&#x27;stride&#x27;</span>]</span><br><span class=\"line\">        input_size = input_dim[<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"comment\"># 计算卷积层输出尺寸</span></span><br><span class=\"line\">        conv_output_size = (input_size - filter_size + <span class=\"number\">2</span>*filter_pad) / filter_stride + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"comment\"># 计算池化层输出尺寸（池化窗口为2x2，步幅2）</span></span><br><span class=\"line\">        pool_output_size = <span class=\"built_in\">int</span>(filter_num * (conv_output_size/<span class=\"number\">2</span>) * (conv_output_size/<span class=\"number\">2</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 权重初始化</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params = &#123;&#125;</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W1&#x27;</span>] = weight_init_std * \\</span><br><span class=\"line\">                            np.random.randn(filter_num, input_dim[<span class=\"number\">0</span>], filter_size, filter_size)  <span class=\"comment\"># 卷积核权重</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b1&#x27;</span>] = np.zeros(filter_num)  <span class=\"comment\"># 卷积核偏置</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W2&#x27;</span>] = weight_init_std * \\</span><br><span class=\"line\">                            np.random.randn(pool_output_size, hidden_size)  <span class=\"comment\"># 全连接层1权重</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b2&#x27;</span>] = np.zeros(hidden_size)  <span class=\"comment\"># 全连接层1偏置</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W3&#x27;</span>] = weight_init_std * \\</span><br><span class=\"line\">                            np.random.randn(hidden_size, output_size)  <span class=\"comment\"># 全连接层2权重</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b3&#x27;</span>] = np.zeros(output_size)  <span class=\"comment\"># 全连接层2偏置</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 构建网络层（有序字典保证前向/反向顺序）</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.layers = OrderedDict()</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Conv1&#x27;</span>] = Convolution(<span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W1&#x27;</span>], <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b1&#x27;</span>],</span><br><span class=\"line\">                                           conv_param[<span class=\"string\">&#x27;stride&#x27;</span>], conv_param[<span class=\"string\">&#x27;pad&#x27;</span>])  <span class=\"comment\"># 卷积层</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Relu1&#x27;</span>] = Relu()  <span class=\"comment\"># 激活层</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Pool1&#x27;</span>] = Pooling(pool_h=<span class=\"number\">2</span>, pool_w=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>)  <span class=\"comment\"># 池化层</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine1&#x27;</span>] = Affine(<span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W2&#x27;</span>], <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b2&#x27;</span>])  <span class=\"comment\"># 全连接层1</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Relu2&#x27;</span>] = Relu()  <span class=\"comment\"># 激活层</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine2&#x27;</span>] = Affine(<span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W3&#x27;</span>], <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b3&#x27;</span>])  <span class=\"comment\"># 全连接层2</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.last_layer = SoftmaxWithLoss()  <span class=\"comment\"># 最后一层为softmax+损失</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        前向传播，输出网络预测结果。</span></span><br><span class=\"line\"><span class=\"string\">        x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">        返回：输出结果</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>.layers.values():</span><br><span class=\"line\">            x = layer.forward(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">loss</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        计算损失函数值。</span></span><br><span class=\"line\"><span class=\"string\">        x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">        t: 标签（监督信号）</span></span><br><span class=\"line\"><span class=\"string\">        返回：损失值</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        y = <span class=\"variable language_\">self</span>.predict(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.last_layer.forward(y, t)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">accuracy</span>(<span class=\"params\">self, x, t, batch_size=<span class=\"number\">100</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        计算预测精度。</span></span><br><span class=\"line\"><span class=\"string\">        x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">        t: 标签</span></span><br><span class=\"line\"><span class=\"string\">        batch_size: 批大小</span></span><br><span class=\"line\"><span class=\"string\">        返回：精度（0~1）</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> t.ndim != <span class=\"number\">1</span> : t = np.argmax(t, axis=<span class=\"number\">1</span>)  <span class=\"comment\"># 若为one-hot则转为标签索引</span></span><br><span class=\"line\">        acc = <span class=\"number\">0.0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">int</span>(x.shape[<span class=\"number\">0</span>] / batch_size)):</span><br><span class=\"line\">            tx = x[i*batch_size:(i+<span class=\"number\">1</span>)*batch_size]</span><br><span class=\"line\">            tt = t[i*batch_size:(i+<span class=\"number\">1</span>)*batch_size]</span><br><span class=\"line\">            y = <span class=\"variable language_\">self</span>.predict(tx)</span><br><span class=\"line\">            y = np.argmax(y, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">            acc += np.<span class=\"built_in\">sum</span>(y == tt)  <span class=\"comment\"># 统计预测正确个数</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> acc / x.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">numerical_gradient</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        用数值微分法计算各层参数的梯度。</span></span><br><span class=\"line\"><span class=\"string\">        x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">        t: 标签</span></span><br><span class=\"line\"><span class=\"string\">        返回：包含各层权重和偏置梯度的字典</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        loss_w = <span class=\"keyword\">lambda</span> w: <span class=\"variable language_\">self</span>.loss(x, t)</span><br><span class=\"line\">        grads = &#123;&#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> idx <span class=\"keyword\">in</span> (<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>):</span><br><span class=\"line\">            grads[<span class=\"string\">&#x27;W&#x27;</span> + <span class=\"built_in\">str</span>(idx)] = numerical_gradient(loss_w, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W&#x27;</span> + <span class=\"built_in\">str</span>(idx)])  <span class=\"comment\"># 权重梯度</span></span><br><span class=\"line\">            grads[<span class=\"string\">&#x27;b&#x27;</span> + <span class=\"built_in\">str</span>(idx)] = numerical_gradient(loss_w, <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b&#x27;</span> + <span class=\"built_in\">str</span>(idx)])  <span class=\"comment\"># 偏置梯度</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> grads</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">gradient</span>(<span class=\"params\">self, x, t</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        用反向传播法计算各层参数的梯度。</span></span><br><span class=\"line\"><span class=\"string\">        x: 输入数据</span></span><br><span class=\"line\"><span class=\"string\">        t: 标签</span></span><br><span class=\"line\"><span class=\"string\">        返回：包含各层权重和偏置梯度的字典</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"comment\"># 前向传播，计算损失</span></span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.loss(x, t)</span><br><span class=\"line\">        <span class=\"comment\"># 反向传播，计算梯度</span></span><br><span class=\"line\">        dout = <span class=\"number\">1</span></span><br><span class=\"line\">        dout = <span class=\"variable language_\">self</span>.last_layer.backward(dout)</span><br><span class=\"line\">        layers = <span class=\"built_in\">list</span>(<span class=\"variable language_\">self</span>.layers.values())</span><br><span class=\"line\">        layers.reverse()  <span class=\"comment\"># 反向传播需逆序</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> layers:</span><br><span class=\"line\">            dout = layer.backward(dout)</span><br><span class=\"line\">        <span class=\"comment\"># 收集各层的权重和偏置梯度</span></span><br><span class=\"line\">        grads = &#123;&#125;</span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;W1&#x27;</span>], grads[<span class=\"string\">&#x27;b1&#x27;</span>] = <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Conv1&#x27;</span>].dW, <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Conv1&#x27;</span>].db</span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;W2&#x27;</span>], grads[<span class=\"string\">&#x27;b2&#x27;</span>] = <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine1&#x27;</span>].dW, <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine1&#x27;</span>].db</span><br><span class=\"line\">        grads[<span class=\"string\">&#x27;W3&#x27;</span>], grads[<span class=\"string\">&#x27;b3&#x27;</span>] = <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine2&#x27;</span>].dW, <span class=\"variable language_\">self</span>.layers[<span class=\"string\">&#x27;Affine2&#x27;</span>].db</span><br><span class=\"line\">        <span class=\"keyword\">return</span> grads</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">save_params</span>(<span class=\"params\">self, file_name=<span class=\"string\">&quot;params.pkl&quot;</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        保存网络参数到文件。</span></span><br><span class=\"line\"><span class=\"string\">        file_name: 文件名</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        params = &#123;&#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> key, val <span class=\"keyword\">in</span> <span class=\"variable language_\">self</span>.params.items():</span><br><span class=\"line\">            params[key] = val</span><br><span class=\"line\">        <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(file_name, <span class=\"string\">&#x27;wb&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">            pickle.dump(params, f)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">load_params</span>(<span class=\"params\">self, file_name=<span class=\"string\">&quot;params.pkl&quot;</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        从文件加载网络参数。</span></span><br><span class=\"line\"><span class=\"string\">        file_name: 文件名</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(file_name, <span class=\"string\">&#x27;rb&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">            params = pickle.load(f)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> key, val <span class=\"keyword\">in</span> params.items():</span><br><span class=\"line\">            <span class=\"variable language_\">self</span>.params[key] = val</span><br><span class=\"line\">        <span class=\"comment\"># 更新各层的权重和偏置</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i, key <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>([<span class=\"string\">&#x27;Conv1&#x27;</span>, <span class=\"string\">&#x27;Affine1&#x27;</span>, <span class=\"string\">&#x27;Affine2&#x27;</span>]):</span><br><span class=\"line\">            <span class=\"variable language_\">self</span>.layers[key].W = <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;W&#x27;</span> + <span class=\"built_in\">str</span>(i+<span class=\"number\">1</span>)]</span><br><span class=\"line\">            <span class=\"variable language_\">self</span>.layers[key].b = <span class=\"variable language_\">self</span>.params[<span class=\"string\">&#x27;b&#x27;</span> + <span class=\"built_in\">str</span>(i+<span class=\"number\">1</span>)]</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"手写数字识别的CNN\"><a href=\"#手写数字识别的CNN\" class=\"headerlink\" title=\"手写数字识别的CNN\"></a>手写数字识别的CNN</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys, os</span><br><span class=\"line\">sys.path.append(os.pardir)  <span class=\"comment\"># 将父目录添加到sys.path，便于导入模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist</span><br><span class=\"line\"><span class=\"keyword\">from</span> simple_convnet <span class=\"keyword\">import</span> SimpleConvNet</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.trainer <span class=\"keyword\">import</span> Trainer</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载MNIST数据集（不展开，保持(N, 1, 28, 28)的形状）</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(flatten=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 如果训练速度较慢，可以通过取消注释下方代码来减少数据量</span></span><br><span class=\"line\"><span class=\"comment\"># x_train, t_train = x_train[:5000], t_train[:5000]</span></span><br><span class=\"line\"><span class=\"comment\"># x_test, t_test = x_test[:1000], t_test[:1000]</span></span><br><span class=\"line\"></span><br><span class=\"line\">max_epochs = <span class=\"number\">20</span>  <span class=\"comment\"># 训练的总轮数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 初始化卷积神经网络</span></span><br><span class=\"line\">network = SimpleConvNet(</span><br><span class=\"line\">    input_dim=(<span class=\"number\">1</span>,<span class=\"number\">28</span>,<span class=\"number\">28</span>),  <span class=\"comment\"># 输入数据的形状：(通道数，高，宽)</span></span><br><span class=\"line\">    conv_param=&#123;<span class=\"string\">&#x27;filter_num&#x27;</span>: <span class=\"number\">30</span>, <span class=\"string\">&#x27;filter_size&#x27;</span>: <span class=\"number\">5</span>, <span class=\"string\">&#x27;pad&#x27;</span>: <span class=\"number\">0</span>, <span class=\"string\">&#x27;stride&#x27;</span>: <span class=\"number\">1</span>&#125;,  <span class=\"comment\"># 卷积层参数</span></span><br><span class=\"line\">    hidden_size=<span class=\"number\">100</span>,  <span class=\"comment\"># 隐藏（全连接）层的神经元数量</span></span><br><span class=\"line\">    output_size=<span class=\"number\">10</span>,   <span class=\"comment\"># 输出类别数（数字0-9）</span></span><br><span class=\"line\">    weight_init_std=<span class=\"number\">0.01</span>  <span class=\"comment\"># 权重初始化的标准差</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置训练器</span></span><br><span class=\"line\">trainer = Trainer(</span><br><span class=\"line\">    network, x_train, t_train, x_test, t_test,</span><br><span class=\"line\">    epochs=max_epochs, mini_batch_size=<span class=\"number\">100</span>,  <span class=\"comment\"># 训练轮数和每个小批量的样本数</span></span><br><span class=\"line\">    optimizer=<span class=\"string\">&#x27;Adam&#x27;</span>, optimizer_param=&#123;<span class=\"string\">&#x27;lr&#x27;</span>: <span class=\"number\">0.001</span>&#125;,  <span class=\"comment\"># 优化器及其学习率</span></span><br><span class=\"line\">    evaluate_sample_num_per_epoch=<span class=\"number\">1000</span>  <span class=\"comment\"># 每轮评估的样本数</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 开始训练</span></span><br><span class=\"line\">trainer.train()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 保存训练好的网络参数到文件</span></span><br><span class=\"line\">network.save_params(<span class=\"string\">&quot;params.pkl&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Saved Network Parameters!&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制训练集和测试集的准确率随轮数变化的曲线</span></span><br><span class=\"line\">markers = &#123;<span class=\"string\">&#x27;train&#x27;</span>: <span class=\"string\">&#x27;o&#x27;</span>, <span class=\"string\">&#x27;test&#x27;</span>: <span class=\"string\">&#x27;s&#x27;</span>&#125;</span><br><span class=\"line\">x = np.arange(max_epochs)</span><br><span class=\"line\">plt.plot(x, trainer.train_acc_list, marker=<span class=\"string\">&#x27;o&#x27;</span>, label=<span class=\"string\">&#x27;train&#x27;</span>, markevery=<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.plot(x, trainer.test_acc_list, marker=<span class=\"string\">&#x27;s&#x27;</span>, label=<span class=\"string\">&#x27;test&#x27;</span>, markevery=<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;epochs&quot;</span>)  <span class=\"comment\"># 横坐标为训练轮数</span></span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;accuracy&quot;</span>)  <span class=\"comment\"># 纵坐标为准确率</span></span><br><span class=\"line\">plt.ylim(<span class=\"number\">0</span>, <span class=\"number\">1.0</span>)  <span class=\"comment\"># 设置y轴范围</span></span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;lower right&#x27;</span>)  <span class=\"comment\"># 图例位置</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"CNN的可视化-1\"><a href=\"#CNN的可视化-1\" class=\"headerlink\" title=\"CNN的可视化\"></a>CNN的可视化</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os, sys</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">sys.path.append(<span class=\"string\">&#x27;../../py_pro/DL/ch07/&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> simple_convnet <span class=\"keyword\">import</span> SimpleConvNet</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 可视化卷积核（滤波器）权重的函数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">filter_show</span>(<span class=\"params\">filters, nx=<span class=\"number\">8</span>, margin=<span class=\"number\">3</span>, scale=<span class=\"number\">10</span></span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    显示卷积层的滤波器（权重），以灰度图形式排列。</span></span><br><span class=\"line\"><span class=\"string\">    参数：</span></span><br><span class=\"line\"><span class=\"string\">        filters: 卷积核权重，形状为 (FN, C, FH, FW)</span></span><br><span class=\"line\"><span class=\"string\">        nx: 每行显示的滤波器数量</span></span><br><span class=\"line\"><span class=\"string\">        margin: 图像之间的间隔（未使用）</span></span><br><span class=\"line\"><span class=\"string\">        scale: 图像缩放比例（未使用）</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    FN, C, FH, FW = filters.shape  <span class=\"comment\"># FN:滤波器个数, C:通道数, FH:高, FW:宽</span></span><br><span class=\"line\">    ny = <span class=\"built_in\">int</span>(np.ceil(FN / nx))     <span class=\"comment\"># 计算需要的行数</span></span><br><span class=\"line\"></span><br><span class=\"line\">    fig = plt.figure()</span><br><span class=\"line\">    <span class=\"comment\"># 调整子图间距，去除边距和间隔</span></span><br><span class=\"line\">    fig.subplots_adjust(left=<span class=\"number\">0</span>, right=<span class=\"number\">1</span>, bottom=<span class=\"number\">0</span>, top=<span class=\"number\">1</span>, hspace=<span class=\"number\">0.05</span>, wspace=<span class=\"number\">0.05</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(FN):</span><br><span class=\"line\">        <span class=\"comment\"># 在ny行nx列的子图中添加第i+1个子图，不显示坐标轴刻度</span></span><br><span class=\"line\">        ax = fig.add_subplot(ny, nx, i+<span class=\"number\">1</span>, xticks=[], yticks=[])</span><br><span class=\"line\">        <span class=\"comment\"># 显示第i个滤波器的第一个通道（通常为灰度图）</span></span><br><span class=\"line\">        ax.imshow(filters[i, <span class=\"number\">0</span>], cmap=plt.cm.gray_r, interpolation=<span class=\"string\">&#x27;nearest&#x27;</span>)</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建卷积神经网络实例</span></span><br><span class=\"line\">network = SimpleConvNet()</span><br><span class=\"line\"><span class=\"comment\"># 显示随机初始化后的第一层卷积核权重</span></span><br><span class=\"line\">filter_show(network.params[<span class=\"string\">&#x27;W1&#x27;</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载训练后保存的参数</span></span><br><span class=\"line\">network.load_params(<span class=\"string\">&quot;../../py_pro/DL/ch07/params.pkl&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 显示训练后第一层卷积核权重</span></span><br><span class=\"line\">filter_show(network.params[<span class=\"string\">&#x27;W1&#x27;</span>])</span><br></pre></td></tr></table></figure>\n\n\n</blockquote>\n<h3 id=\"如懂！！！\"><a href=\"#如懂！！！\" class=\"headerlink\" title=\"如懂！！！\"></a>如懂！！！</h3>"},{"title":"DL之路---啃鱼书（5）","data":"2025-06-20T07:21:00.000Z","updated":"2025-06-20T07:21:00.000Z","type":"DL","top_img":null,"cover":"http://picbed.yanzu.tech/img/post_cover/p24.png","_content":"\n# 与学习相关的技巧\n\n\n\n## 参数的更新\n\n> #### 神经网络的学习的目的是找到使损失函数的值尽可能小的参数，这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）\n>\n> #### SGD（随机梯度下降法）\n>\n> > #### SGD的缺点，如果函数的形状非均向（anisotropic），比如呈延伸状，搜索的路径就会非常低效，低效的根本原因是，梯度的方向并没有指向最小值的方向，如下图\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/5/1.png)\n> >\n> > #### 这会导致它的效率很低，基于SGD的最优化的更新路径如下图\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/5/2.png)\n> >\n> > #### 可以看到，它是Z字型的移动\n>\n> \n>\n> #### Momentum（带动量的随机梯度下降法）\n>\n> > #### 它是动量的意思，和物理有关，数学表达式如下\n> >\n> > $$\n> > v \\leftarrow \\alpha \\cdot v - \\eta \\cdot \\frac {\\partial L} {\\partial W} \\\\\n> > W \\leftarrow W + v\n> > $$\n> >\n> > #### W同样还是表示要更新的权重参数，$\\frac{\\partial L}{\\partial W}$ 表示损失函数关于W的梯度，$\\eta$ 是学习率，而 $v$ 是物理上的速度\n> >\n> > #### 第一个式子表示，物体在梯度方向上受力，在这个力的作用下，物体的速度增加这一物理法则，$\\alpha v$ 这一项，其中的 $\\alpha$ 是一个衰减因子，其作用是在物体不受力的作用的情况下，使得物体速度减小，一般设置为0.9之类的值，它对应物理中的地面摩擦或者空气阻力等\n> >\n> > \n> >\n> > #### 基于Momentum的最优化的更新路径如下图\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/5/3.png)\n> >\n> > #### 可以发现，和SGD相比，它的Z字型程度减轻了，这是因为，虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速；另一方面，虽然y轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它们会互相抵消，所以y轴方向上的速度不稳定\n>\n> \n>\n> #### AdaGrad（自适应梯度算法）\n>\n> > #### 学习率衰减（learning rate decay）：随着学习的进行，使学习率逐渐减小。它是针对全体参数的，它是将全体参数的学习率一起降低\n> >\n> > #### AdaGrad会为参数的每个元素适当地调整学习率，与此同时进行学习，Ada是取自Adaptive，即适当的\n> >\n> > #### 其数学表达式如下\n> >\n> > $$\n> > h \\leftarrow h + \\frac {\\partial L} {\\partial W} \\odot \\frac {\\partial L} {\\partial W} \\\\\n> > \\mathbf{W} \\leftarrow \\mathbf{W} - \\eta \\frac{1}{\\sqrt{\\mathbf{h}}} \\frac{\\partial L}{\\partial \\mathbf{W}}\n> > $$\n> >\n> > #### 需要说明的是，变量 $h$ ，它保存了以前的所有梯度值的平方和，$\\odot$ 表示对应矩阵元素的乘法，也就是实现了所有梯度值的平方和\n> >\n> > #### 另外，在更新参数时，通过乘以 $\\frac {1}{\\sqrt {h}}$ ，就可以调整学习的尺度，这意味着，参数的元素中变动较大（被大幅更新）的元素的学习率将变小，也即，可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小\n> >\n> > \n> >\n> > #### AdaGrad会记录过去所有梯度的平方和，因此，学习越深入，更新的幅度就越小，如果无休止的学习，最终更新量会变为0\n> >\n> > #### RMSProp方法，并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来，这样的操作被称为**指数移动平均**，呈指数函数式地减小过去的梯度的尺度\n> >\n> > \n> >\n> > #### 基于AdaGrad的最优化的更新路径如下图\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/5/4.png)\n> >\n> > #### 可以看到，函数的取值高效地向着最小值移动\n>\n> \n>\n> #### Adam（自适应矩估计算法）\n>\n> > #### 它融合了Momentum和AdaGrad，结合两者的优点，它还可以进行超参数的偏置矫正\n> >\n> > #### 基于Adam的最优化的更新路径如下图\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/5/5.png)\n> >\n> > #### Adam会设置3个超参数，一个是学习率 $\\eta$ ，另外两个是一次momentum系数 $\\beta_1$ 和二次momentum系数 $\\beta_2$ ，且标准的设定值是 $\\beta_1 = 0.9, \\beta_2 = 0.999$ \n>\n> \n> \n\n\n\n### 权重的初始值\n\n> #### 权值衰减（weight decay），就是一种以减小权重参数的值为目的进行学习的方法，通过减小权重参数的值来抑制过拟合的发生，以及提高泛化能力\n>\n> \n>\n> #### 在误差反向传播法中，所有的权重值都会进行相同的更新，所以不能把权重初始值设为一样的值（权重均一化）\n>\n> \n>\n> #### 激活函数使用sigmoid函数，随着输出不断地靠近0（或者靠近1），它的导数的值逐渐接近0，从而偏向0和1的数据分布会造成反向传播中梯度的值不断变小，最后消失，这就是所谓的**梯度消失**（gradient vanishing）\n>\n> #### 激活值在分布上有所偏向会出现表现了受限或梯度消失的问题\n>\n> \n>\n> #### Xavier初始值\n>\n> > #### 为了使各层的激活值呈现出具有相同广度的分布，推导了合适的权重尺度，结论是：如果前一层的节点数为n，则初始值使用标准差为 $\\frac{1}{\\sqrt{n}}$ 的分布\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/5/6.png)\n> >\n> > #### 可以将激活函数sigmoid改为tanh双曲线函数，两者同为S型曲线函数，但tanh是关于原点对称的S型曲线，而sigmoid是关于（0，0.5）对称的，而用作激活函数的函数最好具有关于原点对称的性质\n>\n> \n>\n> #### ReLU的权值重置\n>\n> > #### Xavier初始值是以激活函数是线性函数为前提而推导出来的，sigmoid函数和tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值\n> >\n> > #### 当激活函数使用ReLU时，一般推荐使用ReLU专用的初始值——He初始值\n> >\n> > #### 它是，当前一层的节点数为n时，He初始值使用标准差为 $\\sqrt{\\frac {2}{n}}$ 的高斯分布\n> >\n> > #### 相对于Xavier初始值的 $\\sqrt{\\frac {1}{n}}$ ，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数\n> >\n> > #### 当激活函数使用ReLU时，权重初始值使用He初始值，当激活函数为sigmoid或tanh等S型曲线函数时，初始值使用Xavier初始值\n>\n> \n\n\n\n### Batch Normalization\n\n> #### Batch Normalization的算法\n>\n> > #### 为了使各层拥有适当的广度，“强制性”地调整激活值的分布，这就是Batch Normalization方法\n> >\n> > #### 它有以下几个特点\n> >\n> > - #### 可以使学习快速进行（可以增大学习率）\n> >\n> > - #### 对初始值的依赖不大\n> >\n> > - #### 抑制过拟合\n> >\n> > #### Batch normalization层：对神经网络中的数据分布进行正规化的层\n> >\n> > #### 它以进行学习时的mini-batch为单位，按mini-batch进行正规化，具体来说就是，进行使数据分布的均值为0、方差为1的正规化，数学式表示如下\n> >\n> > $$\n> > \\mu_B \\leftarrow \\frac {1}{m} \\sum_{i=1}^m x_i \\\\\n> > \\sigma_B^2 \\leftarrow \\frac{1}{m} \\sum_{i=1}^m(x_i - \\mu_B)^2 \\\\\n> > \\hat x_i \\leftarrow \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\varepsilon}}\n> > $$\n> >\n> > #### 第1、2个式子是对mini-batch的m个输入数据的集合 $B={x_1,x_2...x_m}$ 求均值 $\\mu_B$ 和方差 $\\sigma_B^2$ ，第三个式子对输入数据进行均值为0、方差为1（合适的分布）的正规化，$\\varepsilon$ 是一个极小值，作用是防止除0\n> >\n> > #### 通过将这个处理插入到激活函数的前面（或者后面），可以减小数据分布的偏向\n> >\n> > #### 紧接着，Batch Norm层会对正规化后的数据进行缩放和平移的变换，数学式表示如下\n> >\n> > $$\n> > y_i \\leftarrow \\gamma \\hat x_i  + \\beta\n> > $$\n> >\n> > #### 其中， $\\gamma$ 和 $\\beta$ 是参数，初始分别为1和0，然后再通过学习调整到合适的值\n> >\n> > #### 它是神经网络上的正向传播，计算图如下\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/5/7.png)\n> >\n> > #### 留个坑：batch norm的反向传播\n>\n> \n>\n> #### Batch Normalization的评估\n\n\n\n### 正则化\n\n> #### 过拟合：只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态\n>\n> #### 发生过拟合的原因，一个是模型拥有大量参数、表现力强，一个是训练数据少\n>\n> \n>\n> #### 权值衰减\n>\n> > #### 权值衰减是一直以来经常被使用的一种抑制过拟合的方法，该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合\n> >\n> > #### 如，为损失函数加上权重的平方范数（L2范数），这样就可以抑制权重变大，将权重记为W，L2的范数的权值衰减就是 $\\frac{1}{2} \\lambda W^2$ ，然后将它加到损失函数上，$\\lambda$ 是控制正则化强度的超参数，$\\lambda$ 设置得越大，对大的权重施加的惩罚就越重，1/2是用于将 $\\frac{1}{2} \\lambda W^2$ 的求导结果变成 $\\lambda W$ 的调整常用量\n> >\n> > #### 对于所有权重，权值衰减方法都会为损失函数加上 $\\frac{1}{2} \\lambda W^2$ ，因此，在求权重梯度的计算中，要为之前的误差反向传播法的结果加上正则化项的导数 $\\lambda W$ \n> >\n> > #### L2范数相当于各元素的平方和，假设有权重 $W=(w_1,w_2...w_n)$ ，则L2范数可用 $\\sqrt{w_1^2+w_2^2+...w_n^2}$ 计算出来\n> >\n> > #### 除了L2范数，还有L1范数、$L \\infty$ 范数等，L1范数是各元素的绝对值之和，相当于 $|w_1|+|w_2|+...|w_n|$ ，$L\\infty$ 范数也称为Max范数，相当于各个元素的绝对值中最大的那一个\n> >\n> > #### 以上三种范数都可以用作正则项\n>\n> \n>\n> #### Dropout\n>\n> > #### 前面的为损失函数加上权重的L2范数的权值衰减方法，在网络模型变得复杂的情况下，就难以应对了\n> >\n> > #### 由此，引出了Dropout这种方法\n> >\n> > #### Dropout是一种在学习的过程中随机删除神经元的方法\n> >\n> > > #### 训练时，随机选出隐藏层的神经元，然后将其删除，被删除的神经元不再进行信号的传递\n> > >\n> > > #### 训练时，每传递一次数据，就会随机选择要删除的神经元\n> > >\n> > > #### 测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/5/8.png)\n> >\n> > \n> >\n> > #### 集成学习，就是让多个模型单独进行学习，推理时再取多个模型的输出的平均值，通过进行集成学习，神经网络的识别精度可以提高好几个百分点，Dropout将集成学习的效果（模拟地）通过一个网络实现了\n> >\n> > \n\n\n\n### 超参数的验证\n\n>#### 超参数是指各层的神经元数量、batch大小、参数更新时的学习率或权值衰减等\n>\n>#### 不能使用测试数据评估超参数的性能，因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合\n>\n>#### 用于调整超参数的数据，一般称为验证数据\n>\n>#### 训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估，为了确认泛化能力，要在最后使用测试数据\n>\n>\n>\n>#### 超参数的最优化\n>\n>> #### 进行超参数的最优化时，要逐渐缩小超参数的“好值”的存在范围\n>>\n>> #### 所谓逐渐缩小范围，是指一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样，最好是随机采样），用这个采样到的值进行识别精度的评估，重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围，最终确定超参数的合适范围(一般是 0.001~1000)\n>>\n>> #### 还可以使用贝叶斯最优化（Bayesian optimization）\n>>\n>> \n\n\n\n\n\n\n\n### code\n\n> #### SGD\n>\n> ```python\n> class SGD:\n> \"\"\"\n> 随机梯度下降法（Stochastic Gradient Descent）\n> 最基础的优化算法，直接使用梯度更新参数\n> \"\"\"\n> \n> def __init__(self, lr=0.01):\n> \"\"\"\n> 初始化SGD优化器\n> \n> 参数:\n> lr: 学习率，控制参数更新的步长\n> \"\"\"\n> self.lr = lr\n> \n> def update(self, params, grads):\n> \"\"\"\n> 更新参数\n> \n> 参数:\n> params: 需要更新的参数字典\n> grads: 对应的梯度字典\n> \"\"\"\n> for key in params.keys():\n> params[key] -= self.lr * grads[key]  # 参数更新公式：θ = θ - lr * ∇θ\n> ```\n>\n> \n>\n> #### Momentum\n>\n> ```python\n> class Momentum:\n> \"\"\"\n> 带动量的随机梯度下降法\n> 通过引入动量项来加速收敛，减少震荡\n> \"\"\"\n> \n> def __init__(self, lr=0.01, momentum=0.9):\n> \"\"\"\n> 初始化Momentum优化器\n> \n> 参数:\n> lr: 学习率\n> momentum: 动量系数，通常设为0.9\n> \"\"\"\n> self.lr = lr\n> self.momentum = momentum\n> self.v = None  # 速度向量\n> \n> def update(self, params, grads):\n> \"\"\"\n> 更新参数\n> \n> 参数:\n> params: 需要更新的参数字典\n> grads: 对应的梯度字典\n> \"\"\"\n> if self.v is None:\n> # 第一次调用时初始化速度向量\n> self.v = {}\n> for key, val in params.items():                                \n>     self.v[key] = np.zeros_like(val)\n>     \n> for key in params.keys():\n> # 更新速度：v = momentum * v - lr * grad\n> self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n> # 更新参数：θ = θ + v\n> params[key] += self.v[key]\n> ```\n>\n> \n>\n> #### AdaGrad\n>\n> ```python\n> class AdaGrad:\n> \"\"\"\n> AdaGrad（自适应梯度算法）\n> 为每个参数自适应地调整学习率，适合处理稀疏梯度\n> \"\"\"\n> \n> def __init__(self, lr=0.01):\n> \"\"\"\n> 初始化AdaGrad优化器\n> \n> 参数:\n> lr: 初始学习率\n> \"\"\"\n> self.lr = lr\n> self.h = None  # 累积梯度平方和\n> \n> def update(self, params, grads):\n> \"\"\"\n> 更新参数\n> \n> 参数:\n> params: 需要更新的参数字典\n> grads: 对应的梯度字典\n> \"\"\"\n> if self.h is None:\n> # 第一次调用时初始化累积梯度平方和\n> self.h = {}\n> for key, val in params.items():\n>     self.h[key] = np.zeros_like(val)\n> \n> for key in params.keys():\n> # 累积梯度平方和：h = h + grad^2\n> self.h[key] += grads[key] * grads[key]\n> # 参数更新：θ = θ - lr * grad / sqrt(h + ε)\n> params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n> ```\n>\n> \n>\n> #### Adam\n>\n> ```python\n> class Adam:\n> \"\"\"\n> Adam（自适应矩估计算法）\n> 结合了Momentum和RMSprop的优点，是目前最常用的优化算法之一\n> \"\"\"\n> \n> def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n> \"\"\"\n> 初始化Adam优化器\n> \n> 参数:\n> lr: 学习率\n> beta1: 一阶矩估计的指数衰减率\n> beta2: 二阶矩估计的指数衰减率\n> \"\"\"\n> self.lr = lr\n> self.beta1 = beta1\n> self.beta2 = beta2\n> self.iter = 0  # 迭代次数\n> self.m = None  # 一阶矩估计（梯度的指数移动平均）\n> self.v = None  # 二阶矩估计（梯度平方的指数移动平均）\n> \n> def update(self, params, grads):\n> \"\"\"\n> 更新参数\n> \n> 参数:\n> params: 需要更新的参数字典\n> grads: 对应的梯度字典\n> \"\"\"\n> if self.m is None:\n> # 第一次调用时初始化\n> self.m, self.v = {}, {}\n> for key, val in params.items():\n>     self.m[key] = np.zeros_like(val)\n>     self.v[key] = np.zeros_like(val)\n> \n> self.iter += 1\n> # 计算学习率的偏差修正\n> lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n> \n> for key in params.keys():\n> # 更新一阶矩估计：m = beta1 * m + (1 - beta1) * grad\n> #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n> # 更新二阶矩估计：v = beta2 * v + (1 - beta2) * grad^2\n> #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n> \n> # 偏差修正的更新方式\n> self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n> self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n> \n> # 参数更新：θ = θ - lr_t * m / sqrt(v + ε)\n> params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n> \n> # 另一种偏差修正的实现方式（注释掉）\n> #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n> #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n> #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\n> ```\n>\n> \n>\n> #### 几种更新方法的比较\n>\n> ```python\n> import sys, os\n> sys.path.append(os.pardir)  # 添加父目录到路径，以便导入common模块\n> import numpy as np\n> import matplotlib.pyplot as plt\n> from collections import OrderedDict\n> from common.optimizer import *  # 导入所有优化器类\n> \n> \n> def f(x, y):\n> \"\"\"\n> 定义目标函数：f(x,y) = x^2/20 + y^2\n> 这是一个简单的二次函数，用于测试优化算法的性能\n> \n> 参数:\n> x, y: 输入变量\n> 返回:\n> 函数值\n> \"\"\"\n> return x**2 / 20.0 + y**2\n> \n> \n> def df(x, y):\n> \"\"\"\n> 计算目标函数的梯度\n> ∂f/∂x = x/10, ∂f/∂y = 2y\n> \n> 参数:\n> x, y: 输入变量\n> 返回:\n> (∂f/∂x, ∂f/∂y): 梯度向量\n> \"\"\"\n> return x / 10.0, 2.0*y\n> \n> # 设置初始位置\n> init_pos = (-7.0, 2.0)  # 初始点坐标\n> \n> # 初始化参数字典\n> params = {}\n> params['x'], params['y'] = init_pos[0], init_pos[1]  # 将初始位置赋值给参数\n> \n> # 初始化梯度字典\n> grads = {}\n> grads['x'], grads['y'] = 0, 0  # 梯度初始化为0\n> \n> # 创建优化器字典，使用OrderedDict保持顺序\n> optimizers = OrderedDict()\n> optimizers[\"SGD\"] = SGD(lr=0.95)  # 随机梯度下降，学习率0.95\n> optimizers[\"Momentum\"] = Momentum(lr=0.1)  # 带动量的SGD，学习率0.1\n> optimizers[\"AdaGrad\"] = AdaGrad(lr=1.5)  # 自适应梯度算法，学习率1.5\n> optimizers[\"Adam\"] = Adam(lr=0.3)  # Adam优化器，学习率0.3\n> \n> idx = 1  # 子图索引\n> \n> # 遍历每种优化器\n> for key in optimizers:\n> optimizer = optimizers[key]  # 获取当前优化器\n> x_history = []  # 记录x坐标的历史轨迹\n> y_history = []  # 记录y坐标的历史轨迹\n> params['x'], params['y'] = init_pos[0], init_pos[1]  # 重置参数到初始位置\n> \n> # 进行30次优化迭代\n> for i in range(30):\n> x_history.append(params['x'])  # 记录当前x位置\n> y_history.append(params['y'])  # 记录当前y位置\n> \n> # 计算当前点的梯度\n> grads['x'], grads['y'] = df(params['x'], params['y'])\n> # 使用优化器更新参数\n> optimizer.update(params, grads)\n> \n> \n> # 创建网格用于绘制等高线图\n> x = np.arange(-10, 10, 0.01)  # x轴范围：-10到10，步长0.01\n> y = np.arange(-5, 5, 0.01)    # y轴范围：-5到5，步长0.01\n> \n> X, Y = np.meshgrid(x, y)  # 创建网格坐标\n> Z = f(X, Y)  # 计算网格上每个点的函数值\n> \n> # 为了简化等高线图，将大于7的值设为0\n> mask = Z > 7\n> Z[mask] = 0\n> \n> # 绘制子图\n> plt.subplot(2, 2, idx)  # 创建2x2的子图，当前是第idx个\n> idx += 1  # 子图索引递增\n> \n> # 绘制优化轨迹\n> plt.plot(x_history, y_history, 'o-', color=\"red\")  # 红色圆点连线表示优化路径\n> plt.contour(X, Y, Z)  # 绘制等高线\n> plt.ylim(-10, 10)  # 设置y轴范围\n> plt.xlim(-10, 10)  # 设置x轴范围\n> plt.plot(0, 0, '+')  # 在原点(0,0)绘制加号，表示全局最优点\n> #colorbar()  # 注释掉的颜色条\n> #spring()   # 注释掉的弹簧布局\n> plt.title(key)  # 设置子图标题为优化器名称\n> plt.xlabel(\"x\")  # 设置x轴标签\n> plt.ylabel(\"y\")  # 设置y轴标签\n> \n> plt.show()  # 显示所有子图\n> ```\n>\n> \n>\n> #### 基于MNIST数据集的更新方法的比较\n>\n> ```python\n> import os\n> import sys\n> sys.path.append(os.pardir)  # 添加父目录到路径，以便导入其他模块\n> import matplotlib.pyplot as plt\n> from dataset.mnist import load_mnist  # 导入MNIST数据集加载函数\n> from common.util import smooth_curve  # 导入平滑曲线函数，用于可视化\n> from common.multi_layer_net import MultiLayerNet  # 导入多层神经网络类\n> from common.optimizer import *  # 导入所有优化器类\n> \n> \n> # 0: 加载MNIST数据集 ==========\n> (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)  # 加载并归一化MNIST数据集\n> \n> train_size = x_train.shape[0]  # 训练集大小\n> batch_size = 128  # 批量大小\n> max_iterations = 2000  # 最大迭代次数\n> \n> \n> # 1: 实验设置 ==========\n> # 创建不同优化器的字典\n> optimizers = {}\n> optimizers['SGD'] = SGD()  # 随机梯度下降\n> optimizers['Momentum'] = Momentum()  # 带动量的SGD\n> optimizers['AdaGrad'] = AdaGrad()  # 自适应梯度算法\n> optimizers['Adam'] = Adam()  # Adam优化器\n> #optimizers['RMSprop'] = RMSprop()  # RMSprop优化器（已注释）\n> \n> # 为每个优化器创建对应的神经网络和损失记录\n> networks = {}  # 存储不同优化器对应的神经网络\n> train_loss = {}  # 存储不同优化器的训练损失\n> for key in optimizers.keys():\n> # 创建具有4个隐藏层（每层100个神经元）的多层神经网络\n> networks[key] = MultiLayerNet(\n> input_size=784,  # 输入层大小（28x28=784）\n> hidden_size_list=[100, 100, 100, 100],  # 4个隐藏层，每层100个神经元\n> output_size=10)  # 输出层大小（10个类别）\n> train_loss[key] = []  # 初始化损失列表\n> \n> \n> # 2: 开始训练 ==========\n> for i in range(max_iterations):\n> # 随机选择批量数据\n> batch_mask = np.random.choice(train_size, batch_size)  # 随机选择batch_size个样本的索引\n> x_batch = x_train[batch_mask]  # 获取批量输入数据\n> t_batch = t_train[batch_mask]  # 获取批量标签数据\n> \n> # 对每个优化器进行参数更新\n> for key in optimizers.keys():\n> # 计算梯度\n> grads = networks[key].gradient(x_batch, t_batch)  # 计算当前批量的梯度\n> # 更新参数\n> optimizers[key].update(networks[key].params, grads)  # 使用优化器更新网络参数\n> \n> # 计算损失\n> loss = networks[key].loss(x_batch, t_batch)  # 计算当前批量的损失\n> train_loss[key].append(loss)  # 记录损失\n> \n> # 每100次迭代打印一次训练状态\n> if i % 100 == 0:\n> print(\"===========\" + \"iteration:\" + str(i) + \"===========\")\n> for key in optimizers.keys():\n> loss = networks[key].loss(x_batch, t_batch)  # 计算当前批量的损失\n> print(key + \":\" + str(loss))  # 打印每个优化器的损失\n> \n> \n> # 3: 绘制图表 ==========\n> # 设置不同优化器的标记样式\n> markers = {\"SGD\": \"o\", \"Momentum\": \"x\", \"AdaGrad\": \"s\", \"Adam\": \"D\"}\n> x = np.arange(max_iterations)  # 创建x轴数据（迭代次数）\n> \n> # 绘制每个优化器的损失曲线\n> for key in optimizers.keys():\n> # 使用平滑曲线绘制损失，每100次迭代标记一次\n> plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n> \n> # 设置图表属性\n> plt.xlabel(\"iterations\")  # x轴标签：迭代次数\n> plt.ylabel(\"loss\")  # y轴标签：损失值\n> plt.ylim(0, 1)  # 设置y轴范围：0到1\n> plt.legend()  # 显示图例\n> plt.show()  # 显示图表\n> ```\n>\n> \n>\n> #### 隐藏层的激活值的分布（设计两种初始值Xavier和He初始值（作为权重的初始值），分别适用于不同的激活函数）\n>\n> ```python\n> # coding: utf-8\n> import numpy as np\n> import matplotlib.pyplot as plt\n> \n> \n> def sigmoid(x):\n> \"\"\"\n> Sigmoid激活函数\n> 将输入值压缩到(0,1)区间\n> \"\"\"\n> return 1 / (1 + np.exp(-x))\n> \n> \n> def ReLU(x):\n> \"\"\"\n> ReLU激活函数\n> 当输入大于0时返回原值，小于0时返回0\n> \"\"\"\n> return np.maximum(0, x)\n> \n> \n> def tanh(x):\n> \"\"\"\n> tanh激活函数\n> 将输入值压缩到(-1,1)区间\n> \"\"\"\n> return np.tanh(x)\n> \n> # 生成1000个样本，每个样本100个特征,x 就是高斯分布随机数\n> input_data = np.random.randn(1000, 100)  # 且符合标准正态分布，也就是高斯分布\n> node_num = 100\n> hidden_layer_size = 5\n> activations = {}\n> \n> x = input_data\n> \n> # 模拟5层神经网络的前向传播过程\n> for i in range(hidden_layer_size):\n> if i != 0:\n> x = activations[i-1]  # 使用上一层的输出作为当前层的输入\n> \n> # 权重初始化实验\n> # 可以尝试不同的权重初始化方法\n> w = np.random.randn(node_num, node_num) * 1  # 标准正态分布\n> # w = np.random.randn(node_num, node_num) * 0.01  # 缩小100倍\n> # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)  # Xavier初始化\n> # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)  # He初始化\n> \n> # 线性变换\n> a = np.dot(x, w)\n> \n> # 激活函数实验\n> # 可以尝试不同的激活函数\n> z = sigmoid(a)  # Sigmoid激活\n> # z = ReLU(a)    # ReLU激活\n> # z = tanh(a)    # tanh激活\n> \n> activations[i] = z  # 保存当前层的激活值\n> \n> # 绘制每层激活值的分布直方图\n> for i, a in activations.items():\n> plt.subplot(1, len(activations), i+1)  # 创建子图\n> plt.title(str(i+1) + \"-layer\")  # 设置标题\n> if i != 0: plt.yticks([], [])  # 除第一层外，隐藏y轴刻度\n> # plt.xlim(0.1, 1)  # 可以设置x轴范围\n> # plt.ylim(0, 7000)  # 可以设置y轴范围\n> plt.hist(a.flatten(), 30, range=(0,1))  # 绘制直方图，30个bin，范围0-1\n> plt.show()  # 显示图形\n> ```\n>\n> \n>\n> #### 基于MNIST数据集的权重初始值的比较\n>\n> ```python\n> import os\n> import sys\n> \n> # 添加父目录到系统路径，以便导入common模块\n> sys.path.append(os.pardir)\n> import numpy as np\n> import matplotlib.pyplot as plt\n> from dataset.mnist import load_mnist\n> from common.util import smooth_curve\n> from common.multi_layer_net import MultiLayerNet\n> from common.optimizer import SGD\n> \n> # 加载MNIST数据集并进行归一化处理\n> (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n> \n> # 设置训练参数\n> train_size = x_train.shape[0]  # 训练数据总数\n> batch_size = 128  # 批次大小\n> max_iterations = 2000  # 最大迭代次数\n> \n> # 定义不同的权重初始化方法\n> # std=0.01: 标准差为0.01的正态分布\n> # Xavier: Xavier初始化（适用于sigmoid激活函数）\n> # He: He初始化（适用于ReLU激活函数）\n> weight_init_types = {'std=0.01': 0.01, 'Xavier': 'sigmoid', 'He': 'relu'}\n> optimizer = SGD(lr=0.01)  # 随机梯度下降优化器，学习率为0.01\n> \n> # 创建不同初始化方法的神经网络和损失记录\n> networks = {}\n> train_loss = {}\n> for key, weight_type in weight_init_types.items():\n> # 创建多层神经网络：输入784维，4个隐藏层各100个神经元，输出10维\n> networks[key] = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100],\n>                             output_size=10, weight_init_std=weight_type)\n> train_loss[key] = []  # 初始化损失记录列表\n> \n> # 开始训练循环\n> for i in range(max_iterations):\n> # 随机选择批次数据\n> batch_mask = np.random.choice(train_size, batch_size)\n> x_batch = x_train[batch_mask]\n> t_batch = t_train[batch_mask]\n> \n> # 对每种权重初始化方法进行训练\n> for key in weight_init_types.keys():\n>   # 计算梯度\n>   grads = networks[key].gradient(x_batch, t_batch)\n>   # 更新网络参数\n>   optimizer.update(networks[key].params, grads)\n> \n>   # 计算并记录损失\n>   loss = networks[key].loss(x_batch, t_batch)\n>   train_loss[key].append(loss)\n> \n> # 每100次迭代打印一次损失值\n> if i % 100 == 0:\n>   print(\"===========\" + \"iteration:\" + str(i) + \"===========\")\n>   for key in weight_init_types.keys():\n>       loss = networks[key].loss(x_batch, t_batch)\n>       print(key + \":\" + str(loss))\n> \n> # 绘制训练损失曲线\n> markers = {'std=0.01': 'o', 'Xavier': 's', 'He': 'D'}  # 不同方法的标记符号\n> x = np.arange(max_iterations)  # x轴：迭代次数\n> for key in weight_init_types.keys():\n> # 绘制平滑后的损失曲线，每100次迭代显示一个标记\n> plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n> plt.xlabel(\"iterations\")  # x轴标签\n> plt.ylabel(\"loss\")  # y轴标签\n> plt.ylim(0, 2.5)  # 设置y轴范围\n> plt.legend()  # 显示图例\n> plt.show()  # 显示图形\n> ```\n>\n> \n>\n> #### Batch Normalization的评估\n>\n> ```python\n> # coding: utf-8\n> # 本脚本用于对比带有批归一化（Batch Normalization）和不带批归一化的多层神经网络在不同权重初始值下的训练表现。\n> # 通过在MNIST数据集上训练网络，观察批归一化对训练收敛速度和准确率的影响。\n> import sys, os\n> sys.path.append(os.pardir)  # 将父目录加入sys.path，便于导入上级目录中的模块\n> import numpy as np\n> import matplotlib.pyplot as plt\n> from dataset.mnist import load_mnist  # 导入MNIST数据集加载函数\n> from common.multi_layer_net_extend import MultiLayerNetExtend  # 导入可扩展多层网络实现\n> from common.optimizer import SGD, Adam  # 导入优化器\n> \n> # 加载MNIST数据集，并进行归一化处理\n> (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n> \n> # 为了加快实验速度，仅取前1000个训练样本\n> x_train = x_train[:1000]\n> t_train = t_train[:1000]\n> \n> max_epochs = 20  # 最大训练轮数\n> train_size = x_train.shape[0]  # 训练集样本数\n> batch_size = 100  # 每个批次的样本数\n> learning_rate = 0.01  # 学习率\n> \n> \n> def __train(weight_init_std):\n>  \"\"\"\n>  训练带有和不带有批归一化的神经网络。\n>  参数：\n>      weight_init_std: 权重初始化的标准差\n>  返回：\n>      train_acc_list: 不带批归一化的网络在每个epoch的训练准确率\n>      bn_train_acc_list: 带批归一化的网络在每个epoch的训练准确率\n>  \"\"\"\n>  # 构建带批归一化的网络\n>  bn_network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10, \n>                                  weight_init_std=weight_init_std, use_batchnorm=True)\n>  # 构建不带批归一化的网络\n>  network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10,\n>                              weight_init_std=weight_init_std)\n>  optimizer = SGD(lr=learning_rate)  # 使用SGD优化器\n>  \n>  train_acc_list = []  # 记录不带BN的准确率\n>  bn_train_acc_list = []  # 记录带BN的准确率\n>  \n>  iter_per_epoch = max(train_size / batch_size, 1)  # 每个epoch的迭代次数\n>  epoch_cnt = 0  # 当前epoch计数\n>  \n>  # 训练循环\n>  for i in range(1000000000):  # 迭代次数设置为极大，实际会在达到max_epochs时break\n>      # 随机采样一个batch\n>      batch_mask = np.random.choice(train_size, batch_size)\n>      x_batch = x_train[batch_mask]\n>      t_batch = t_train[batch_mask]\n>  \n>      # 对两个网络分别进行反向传播和参数更新\n>      for _network in (bn_network, network):\n>          grads = _network.gradient(x_batch, t_batch)\n>          optimizer.update(_network.params, grads)\n>  \n>      # 每经过一个epoch，记录一次准确率\n>      if i % iter_per_epoch == 0:\n>          train_acc = network.accuracy(x_train, t_train)\n>          bn_train_acc = bn_network.accuracy(x_train, t_train)\n>          train_acc_list.append(train_acc)\n>          bn_train_acc_list.append(bn_train_acc)\n>  \n>          print(\"epoch:\" + str(epoch_cnt) + \" | \" + str(train_acc) + \" - \" + str(bn_train_acc))\n>  \n>          epoch_cnt += 1\n>          if epoch_cnt >= max_epochs:\n>              break\n>              \n>  return train_acc_list, bn_train_acc_list\n> \n> \n> # 生成16个权重初始化标准差，从1到1e-4，等比数列\n> weight_scale_list = np.logspace(0, -4, num=16)\n> x = np.arange(max_epochs)  # x轴为epoch数\n> \n> # 对每个权重初始化标准差分别进行实验\n> for i, w in enumerate(weight_scale_list):\n>  print( \"============== \" + str(i+1) + \"/16\" + \" ==============\")\n>  train_acc_list, bn_train_acc_list = __train(w)\n>  \n>  # 绘制每组实验的准确率曲线\n>  plt.subplot(4,4,i+1)\n>  plt.title(\"W:\" + str(w))\n>  if i == 15:\n>      plt.plot(x, bn_train_acc_list, label='Batch Normalization', markevery=2)\n>      plt.plot(x, train_acc_list, linestyle = \"--\", label='Normal(without BatchNorm)', markevery=2)\n>  else:\n>      plt.plot(x, bn_train_acc_list, markevery=2)\n>      plt.plot(x, train_acc_list, linestyle=\"--\", markevery=2)\n> \n>  plt.ylim(0, 1.0)  # y轴范围\n>  if i % 4:\n>      plt.yticks([])\n>  else:\n>      plt.ylabel(\"accuracy\")\n>  if i < 12:\n>      plt.xticks([])\n>  else:\n>      plt.xlabel(\"epochs\")\n>  plt.legend(loc='lower right')\n>  \n> plt.show()  # 显示所有子图\n> ```\n>\n> \n>\n> #### Dropout的实现\n>\n> ```python\n> class Dropout:\n>  def __init__(self, dropout_ratio=0.5):\n>      slef.dropout_ratio = dropout_ratio\n>      self.mask = None\n> \n>  def forward(self,x, train_flag=True):\n>      if train_flag:\n>          self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n>          return x * self.mask\n>      else:\n>          return x * (1.0 - self.dropout_ratio)\n> \n>  def backward(self, dout):\n>      return dout * self.mask\n> ```\n>\n> #### 使用Mnist数据集进行验证Dropout的效果\n>\n> ```python\n> # coding: utf-8\n> # 本脚本用于实验Dropout对深层神经网络过拟合的抑制作用。\n> # 通过在MNIST数据集上训练一个较深的网络，观察在有无Dropout时训练集和测试集准确率的变化。\n> import os\n> import sys\n> sys.path.append(os.pardir)  # 将父目录加入sys.path，便于导入上级目录中的模块\n> import numpy as np\n> import matplotlib.pyplot as plt\n> from dataset.mnist import load_mnist  # 导入MNIST数据集加载函数\n> from common.multi_layer_net_extend import MultiLayerNetExtend  # 导入可扩展多层神经网络实现（支持Dropout）\n> from common.trainer import Trainer  # 导入训练器类，简化训练流程\n> \n> # 加载MNIST数据集，并进行归一化处理\n> (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n> \n> # 为了更容易出现过拟合，仅取前300个训练样本\n> x_train = x_train[:300]\n> t_train = t_train[:300]\n> \n> # 设置Dropout参数\n> use_dropout = True  # 是否使用Dropout\n> # dropout_ratio为每层神经元被随机丢弃的比例，常用0.2~0.5\n> # 若不使用Dropout，可将use_dropout设为False\n> # ====================================================\n> dropout_ratio = 0.2  # Dropout比例\n> \n> # 构建一个6层隐藏层的全连接神经网络，设置Dropout参数\n> network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n>                            output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n> # 使用Trainer类进行训练，自动完成mini-batch梯度下降、准确率记录等\n> trainer = Trainer(network, x_train, t_train, x_test, t_test,\n>                epochs=301, mini_batch_size=100,\n>                optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n> trainer.train()\n> \n> # 获取训练集和测试集的准确率变化曲线\n> train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n> \n> # 绘制训练集和测试集的准确率曲线\n> markers = {'train': 'o', 'test': 's'}\n> x = np.arange(len(train_acc_list))\n> plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n> plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n> plt.xlabel(\"epochs\")\n> plt.ylabel(\"accuracy\")\n> plt.ylim(0, 1.0)\n> plt.legend(loc='lower right')\n> plt.show()\n> ```\n>\n> \n>\n> #### 超参数最优化的实现\n>\n> ```python\n> # coding: utf-8\n> import sys, os\n> sys.path.append(os.pardir)  # 将父目录添加到sys.path，便于导入上级目录的模块\n> import numpy as np\n> import matplotlib.pyplot as plt\n> from dataset.mnist import load_mnist\n> from common.multi_layer_net import MultiLayerNet\n> from common.util import shuffle_dataset\n> from common.trainer import Trainer\n> \n> # 加载MNIST数据集，并进行归一化处理\n> (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n> \n> # 只取前500个训练样本，减少计算量，加快超参数搜索\n> x_train = x_train[:500]\n> t_train = t_train[:500]\n> \n> # 设置验证集比例为20%\n> validation_rate = 0.20\n> validation_num = int(x_train.shape[0] * validation_rate)\n> # 先打乱数据，保证训练集和验证集的分布一致\n> x_train, t_train = shuffle_dataset(x_train, t_train)\n> x_val = x_train[:validation_num]\n> t_val = t_train[:validation_num]\n> x_train = x_train[validation_num:]\n> t_train = t_train[validation_num:]\n> \n> # 定义训练函数，输入学习率和权重衰减，返回每轮的验证集和训练集准确率\n> def __train(lr, weight_decay, epocs=50):\n>     # 构建一个6层隐藏层的全连接神经网络，带权重衰减\n>     network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n>                             output_size=10, weight_decay_lambda=weight_decay)\n>     # 构建训练器，使用SGD优化器\n>     trainer = Trainer(network, x_train, t_train, x_val, t_val,\n>                       epochs=epocs, mini_batch_size=100,\n>                       optimizer='sgd', optimizer_param={'lr': lr}, verbose=False)\n>     trainer.train()\n> \n>     return trainer.test_acc_list, trainer.train_acc_list\n> \n> # 超参数优化实验次数\n> optimization_trial = 100\n> results_val = {}   # 存储每组超参数下的验证集准确率\n> results_train = {} # 存储每组超参数下的训练集准确率\n> for _ in range(optimization_trial):\n>     # 随机采样权重衰减和学习率（对数均匀分布采样，覆盖大范围）\n>     weight_decay = 10 ** np.random.uniform(-8, -4)\n>     lr = 10 ** np.random.uniform(-6, -2)\n>     # ================================================\n> \n>     # 训练网络并记录结果\n>     val_acc_list, train_acc_list = __train(lr, weight_decay)\n>     print(\"val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(lr) + \", weight decay:\" + str(weight_decay))\n>     key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n>     results_val[key] = val_acc_list\n>     results_train[key] = train_acc_list\n> \n> # 输出超参数优化结果\n> print(\"=========== Hyper-Parameter Optimization Result ===========\")\n> graph_draw_num = 20  # 最多画出前20组最优超参数的曲线\n> col_num = 5\n> row_num = int(np.ceil(graph_draw_num / col_num))\n> i = 0\n> \n> # 按验证集最终准确率从高到低排序，依次画出前20组的准确率曲线\n> for key, val_acc_list in sorted(results_val.items(), key=lambda x:x[1][-1], reverse=True):\n>     print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n> \n>     plt.subplot(row_num, col_num, i+1)\n>     plt.title(\"Best-\" + str(i+1))\n>     plt.ylim(0.0, 1.0)\n>     if i % 5: plt.yticks([])\n>     plt.xticks([])\n>     x = np.arange(len(val_acc_list))\n>     plt.plot(x, val_acc_list)           # 验证集准确率\n>     plt.plot(x, results_train[key], \"--\") # 训练集准确率（虚线）\n>     i += 1\n> \n>     if i >= graph_draw_num:\n>         break\n> \n> plt.show()\n> ```\n>\n> \n\n\n\n### 晕了，完全晕了！","source":"_posts/24.md","raw":"---\ntitle: DL之路---啃鱼书（5）\ndata: 2025-06-20 15:21:00\nupdated: 2025-06-20 15:21:00\ntype: DL\ntop_img:\ncover: http://picbed.yanzu.tech/img/post_cover/p24.png\ntags:\n  - DL\n  - Learning\n  - gnaw_book\n---\n\n# 与学习相关的技巧\n\n\n\n## 参数的更新\n\n> #### 神经网络的学习的目的是找到使损失函数的值尽可能小的参数，这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）\n>\n> #### SGD（随机梯度下降法）\n>\n> > #### SGD的缺点，如果函数的形状非均向（anisotropic），比如呈延伸状，搜索的路径就会非常低效，低效的根本原因是，梯度的方向并没有指向最小值的方向，如下图\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/5/1.png)\n> >\n> > #### 这会导致它的效率很低，基于SGD的最优化的更新路径如下图\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/5/2.png)\n> >\n> > #### 可以看到，它是Z字型的移动\n>\n> \n>\n> #### Momentum（带动量的随机梯度下降法）\n>\n> > #### 它是动量的意思，和物理有关，数学表达式如下\n> >\n> > $$\n> > v \\leftarrow \\alpha \\cdot v - \\eta \\cdot \\frac {\\partial L} {\\partial W} \\\\\n> > W \\leftarrow W + v\n> > $$\n> >\n> > #### W同样还是表示要更新的权重参数，$\\frac{\\partial L}{\\partial W}$ 表示损失函数关于W的梯度，$\\eta$ 是学习率，而 $v$ 是物理上的速度\n> >\n> > #### 第一个式子表示，物体在梯度方向上受力，在这个力的作用下，物体的速度增加这一物理法则，$\\alpha v$ 这一项，其中的 $\\alpha$ 是一个衰减因子，其作用是在物体不受力的作用的情况下，使得物体速度减小，一般设置为0.9之类的值，它对应物理中的地面摩擦或者空气阻力等\n> >\n> > \n> >\n> > #### 基于Momentum的最优化的更新路径如下图\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/5/3.png)\n> >\n> > #### 可以发现，和SGD相比，它的Z字型程度减轻了，这是因为，虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速；另一方面，虽然y轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它们会互相抵消，所以y轴方向上的速度不稳定\n>\n> \n>\n> #### AdaGrad（自适应梯度算法）\n>\n> > #### 学习率衰减（learning rate decay）：随着学习的进行，使学习率逐渐减小。它是针对全体参数的，它是将全体参数的学习率一起降低\n> >\n> > #### AdaGrad会为参数的每个元素适当地调整学习率，与此同时进行学习，Ada是取自Adaptive，即适当的\n> >\n> > #### 其数学表达式如下\n> >\n> > $$\n> > h \\leftarrow h + \\frac {\\partial L} {\\partial W} \\odot \\frac {\\partial L} {\\partial W} \\\\\n> > \\mathbf{W} \\leftarrow \\mathbf{W} - \\eta \\frac{1}{\\sqrt{\\mathbf{h}}} \\frac{\\partial L}{\\partial \\mathbf{W}}\n> > $$\n> >\n> > #### 需要说明的是，变量 $h$ ，它保存了以前的所有梯度值的平方和，$\\odot$ 表示对应矩阵元素的乘法，也就是实现了所有梯度值的平方和\n> >\n> > #### 另外，在更新参数时，通过乘以 $\\frac {1}{\\sqrt {h}}$ ，就可以调整学习的尺度，这意味着，参数的元素中变动较大（被大幅更新）的元素的学习率将变小，也即，可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小\n> >\n> > \n> >\n> > #### AdaGrad会记录过去所有梯度的平方和，因此，学习越深入，更新的幅度就越小，如果无休止的学习，最终更新量会变为0\n> >\n> > #### RMSProp方法，并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来，这样的操作被称为**指数移动平均**，呈指数函数式地减小过去的梯度的尺度\n> >\n> > \n> >\n> > #### 基于AdaGrad的最优化的更新路径如下图\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/5/4.png)\n> >\n> > #### 可以看到，函数的取值高效地向着最小值移动\n>\n> \n>\n> #### Adam（自适应矩估计算法）\n>\n> > #### 它融合了Momentum和AdaGrad，结合两者的优点，它还可以进行超参数的偏置矫正\n> >\n> > #### 基于Adam的最优化的更新路径如下图\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/5/5.png)\n> >\n> > #### Adam会设置3个超参数，一个是学习率 $\\eta$ ，另外两个是一次momentum系数 $\\beta_1$ 和二次momentum系数 $\\beta_2$ ，且标准的设定值是 $\\beta_1 = 0.9, \\beta_2 = 0.999$ \n>\n> \n> \n\n\n\n### 权重的初始值\n\n> #### 权值衰减（weight decay），就是一种以减小权重参数的值为目的进行学习的方法，通过减小权重参数的值来抑制过拟合的发生，以及提高泛化能力\n>\n> \n>\n> #### 在误差反向传播法中，所有的权重值都会进行相同的更新，所以不能把权重初始值设为一样的值（权重均一化）\n>\n> \n>\n> #### 激活函数使用sigmoid函数，随着输出不断地靠近0（或者靠近1），它的导数的值逐渐接近0，从而偏向0和1的数据分布会造成反向传播中梯度的值不断变小，最后消失，这就是所谓的**梯度消失**（gradient vanishing）\n>\n> #### 激活值在分布上有所偏向会出现表现了受限或梯度消失的问题\n>\n> \n>\n> #### Xavier初始值\n>\n> > #### 为了使各层的激活值呈现出具有相同广度的分布，推导了合适的权重尺度，结论是：如果前一层的节点数为n，则初始值使用标准差为 $\\frac{1}{\\sqrt{n}}$ 的分布\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/5/6.png)\n> >\n> > #### 可以将激活函数sigmoid改为tanh双曲线函数，两者同为S型曲线函数，但tanh是关于原点对称的S型曲线，而sigmoid是关于（0，0.5）对称的，而用作激活函数的函数最好具有关于原点对称的性质\n>\n> \n>\n> #### ReLU的权值重置\n>\n> > #### Xavier初始值是以激活函数是线性函数为前提而推导出来的，sigmoid函数和tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值\n> >\n> > #### 当激活函数使用ReLU时，一般推荐使用ReLU专用的初始值——He初始值\n> >\n> > #### 它是，当前一层的节点数为n时，He初始值使用标准差为 $\\sqrt{\\frac {2}{n}}$ 的高斯分布\n> >\n> > #### 相对于Xavier初始值的 $\\sqrt{\\frac {1}{n}}$ ，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数\n> >\n> > #### 当激活函数使用ReLU时，权重初始值使用He初始值，当激活函数为sigmoid或tanh等S型曲线函数时，初始值使用Xavier初始值\n>\n> \n\n\n\n### Batch Normalization\n\n> #### Batch Normalization的算法\n>\n> > #### 为了使各层拥有适当的广度，“强制性”地调整激活值的分布，这就是Batch Normalization方法\n> >\n> > #### 它有以下几个特点\n> >\n> > - #### 可以使学习快速进行（可以增大学习率）\n> >\n> > - #### 对初始值的依赖不大\n> >\n> > - #### 抑制过拟合\n> >\n> > #### Batch normalization层：对神经网络中的数据分布进行正规化的层\n> >\n> > #### 它以进行学习时的mini-batch为单位，按mini-batch进行正规化，具体来说就是，进行使数据分布的均值为0、方差为1的正规化，数学式表示如下\n> >\n> > $$\n> > \\mu_B \\leftarrow \\frac {1}{m} \\sum_{i=1}^m x_i \\\\\n> > \\sigma_B^2 \\leftarrow \\frac{1}{m} \\sum_{i=1}^m(x_i - \\mu_B)^2 \\\\\n> > \\hat x_i \\leftarrow \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\varepsilon}}\n> > $$\n> >\n> > #### 第1、2个式子是对mini-batch的m个输入数据的集合 $B={x_1,x_2...x_m}$ 求均值 $\\mu_B$ 和方差 $\\sigma_B^2$ ，第三个式子对输入数据进行均值为0、方差为1（合适的分布）的正规化，$\\varepsilon$ 是一个极小值，作用是防止除0\n> >\n> > #### 通过将这个处理插入到激活函数的前面（或者后面），可以减小数据分布的偏向\n> >\n> > #### 紧接着，Batch Norm层会对正规化后的数据进行缩放和平移的变换，数学式表示如下\n> >\n> > $$\n> > y_i \\leftarrow \\gamma \\hat x_i  + \\beta\n> > $$\n> >\n> > #### 其中， $\\gamma$ 和 $\\beta$ 是参数，初始分别为1和0，然后再通过学习调整到合适的值\n> >\n> > #### 它是神经网络上的正向传播，计算图如下\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/5/7.png)\n> >\n> > #### 留个坑：batch norm的反向传播\n>\n> \n>\n> #### Batch Normalization的评估\n\n\n\n### 正则化\n\n> #### 过拟合：只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态\n>\n> #### 发生过拟合的原因，一个是模型拥有大量参数、表现力强，一个是训练数据少\n>\n> \n>\n> #### 权值衰减\n>\n> > #### 权值衰减是一直以来经常被使用的一种抑制过拟合的方法，该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合\n> >\n> > #### 如，为损失函数加上权重的平方范数（L2范数），这样就可以抑制权重变大，将权重记为W，L2的范数的权值衰减就是 $\\frac{1}{2} \\lambda W^2$ ，然后将它加到损失函数上，$\\lambda$ 是控制正则化强度的超参数，$\\lambda$ 设置得越大，对大的权重施加的惩罚就越重，1/2是用于将 $\\frac{1}{2} \\lambda W^2$ 的求导结果变成 $\\lambda W$ 的调整常用量\n> >\n> > #### 对于所有权重，权值衰减方法都会为损失函数加上 $\\frac{1}{2} \\lambda W^2$ ，因此，在求权重梯度的计算中，要为之前的误差反向传播法的结果加上正则化项的导数 $\\lambda W$ \n> >\n> > #### L2范数相当于各元素的平方和，假设有权重 $W=(w_1,w_2...w_n)$ ，则L2范数可用 $\\sqrt{w_1^2+w_2^2+...w_n^2}$ 计算出来\n> >\n> > #### 除了L2范数，还有L1范数、$L \\infty$ 范数等，L1范数是各元素的绝对值之和，相当于 $|w_1|+|w_2|+...|w_n|$ ，$L\\infty$ 范数也称为Max范数，相当于各个元素的绝对值中最大的那一个\n> >\n> > #### 以上三种范数都可以用作正则项\n>\n> \n>\n> #### Dropout\n>\n> > #### 前面的为损失函数加上权重的L2范数的权值衰减方法，在网络模型变得复杂的情况下，就难以应对了\n> >\n> > #### 由此，引出了Dropout这种方法\n> >\n> > #### Dropout是一种在学习的过程中随机删除神经元的方法\n> >\n> > > #### 训练时，随机选出隐藏层的神经元，然后将其删除，被删除的神经元不再进行信号的传递\n> > >\n> > > #### 训练时，每传递一次数据，就会随机选择要删除的神经元\n> > >\n> > > #### 测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出\n> >\n> > ![](http://picbed.yanzu.tech/img/DL/5/8.png)\n> >\n> > \n> >\n> > #### 集成学习，就是让多个模型单独进行学习，推理时再取多个模型的输出的平均值，通过进行集成学习，神经网络的识别精度可以提高好几个百分点，Dropout将集成学习的效果（模拟地）通过一个网络实现了\n> >\n> > \n\n\n\n### 超参数的验证\n\n>#### 超参数是指各层的神经元数量、batch大小、参数更新时的学习率或权值衰减等\n>\n>#### 不能使用测试数据评估超参数的性能，因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合\n>\n>#### 用于调整超参数的数据，一般称为验证数据\n>\n>#### 训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估，为了确认泛化能力，要在最后使用测试数据\n>\n>\n>\n>#### 超参数的最优化\n>\n>> #### 进行超参数的最优化时，要逐渐缩小超参数的“好值”的存在范围\n>>\n>> #### 所谓逐渐缩小范围，是指一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样，最好是随机采样），用这个采样到的值进行识别精度的评估，重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围，最终确定超参数的合适范围(一般是 0.001~1000)\n>>\n>> #### 还可以使用贝叶斯最优化（Bayesian optimization）\n>>\n>> \n\n\n\n\n\n\n\n### code\n\n> #### SGD\n>\n> ```python\n> class SGD:\n> \"\"\"\n> 随机梯度下降法（Stochastic Gradient Descent）\n> 最基础的优化算法，直接使用梯度更新参数\n> \"\"\"\n> \n> def __init__(self, lr=0.01):\n> \"\"\"\n> 初始化SGD优化器\n> \n> 参数:\n> lr: 学习率，控制参数更新的步长\n> \"\"\"\n> self.lr = lr\n> \n> def update(self, params, grads):\n> \"\"\"\n> 更新参数\n> \n> 参数:\n> params: 需要更新的参数字典\n> grads: 对应的梯度字典\n> \"\"\"\n> for key in params.keys():\n> params[key] -= self.lr * grads[key]  # 参数更新公式：θ = θ - lr * ∇θ\n> ```\n>\n> \n>\n> #### Momentum\n>\n> ```python\n> class Momentum:\n> \"\"\"\n> 带动量的随机梯度下降法\n> 通过引入动量项来加速收敛，减少震荡\n> \"\"\"\n> \n> def __init__(self, lr=0.01, momentum=0.9):\n> \"\"\"\n> 初始化Momentum优化器\n> \n> 参数:\n> lr: 学习率\n> momentum: 动量系数，通常设为0.9\n> \"\"\"\n> self.lr = lr\n> self.momentum = momentum\n> self.v = None  # 速度向量\n> \n> def update(self, params, grads):\n> \"\"\"\n> 更新参数\n> \n> 参数:\n> params: 需要更新的参数字典\n> grads: 对应的梯度字典\n> \"\"\"\n> if self.v is None:\n> # 第一次调用时初始化速度向量\n> self.v = {}\n> for key, val in params.items():                                \n>     self.v[key] = np.zeros_like(val)\n>     \n> for key in params.keys():\n> # 更新速度：v = momentum * v - lr * grad\n> self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n> # 更新参数：θ = θ + v\n> params[key] += self.v[key]\n> ```\n>\n> \n>\n> #### AdaGrad\n>\n> ```python\n> class AdaGrad:\n> \"\"\"\n> AdaGrad（自适应梯度算法）\n> 为每个参数自适应地调整学习率，适合处理稀疏梯度\n> \"\"\"\n> \n> def __init__(self, lr=0.01):\n> \"\"\"\n> 初始化AdaGrad优化器\n> \n> 参数:\n> lr: 初始学习率\n> \"\"\"\n> self.lr = lr\n> self.h = None  # 累积梯度平方和\n> \n> def update(self, params, grads):\n> \"\"\"\n> 更新参数\n> \n> 参数:\n> params: 需要更新的参数字典\n> grads: 对应的梯度字典\n> \"\"\"\n> if self.h is None:\n> # 第一次调用时初始化累积梯度平方和\n> self.h = {}\n> for key, val in params.items():\n>     self.h[key] = np.zeros_like(val)\n> \n> for key in params.keys():\n> # 累积梯度平方和：h = h + grad^2\n> self.h[key] += grads[key] * grads[key]\n> # 参数更新：θ = θ - lr * grad / sqrt(h + ε)\n> params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n> ```\n>\n> \n>\n> #### Adam\n>\n> ```python\n> class Adam:\n> \"\"\"\n> Adam（自适应矩估计算法）\n> 结合了Momentum和RMSprop的优点，是目前最常用的优化算法之一\n> \"\"\"\n> \n> def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n> \"\"\"\n> 初始化Adam优化器\n> \n> 参数:\n> lr: 学习率\n> beta1: 一阶矩估计的指数衰减率\n> beta2: 二阶矩估计的指数衰减率\n> \"\"\"\n> self.lr = lr\n> self.beta1 = beta1\n> self.beta2 = beta2\n> self.iter = 0  # 迭代次数\n> self.m = None  # 一阶矩估计（梯度的指数移动平均）\n> self.v = None  # 二阶矩估计（梯度平方的指数移动平均）\n> \n> def update(self, params, grads):\n> \"\"\"\n> 更新参数\n> \n> 参数:\n> params: 需要更新的参数字典\n> grads: 对应的梯度字典\n> \"\"\"\n> if self.m is None:\n> # 第一次调用时初始化\n> self.m, self.v = {}, {}\n> for key, val in params.items():\n>     self.m[key] = np.zeros_like(val)\n>     self.v[key] = np.zeros_like(val)\n> \n> self.iter += 1\n> # 计算学习率的偏差修正\n> lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n> \n> for key in params.keys():\n> # 更新一阶矩估计：m = beta1 * m + (1 - beta1) * grad\n> #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n> # 更新二阶矩估计：v = beta2 * v + (1 - beta2) * grad^2\n> #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n> \n> # 偏差修正的更新方式\n> self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n> self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n> \n> # 参数更新：θ = θ - lr_t * m / sqrt(v + ε)\n> params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n> \n> # 另一种偏差修正的实现方式（注释掉）\n> #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n> #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n> #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\n> ```\n>\n> \n>\n> #### 几种更新方法的比较\n>\n> ```python\n> import sys, os\n> sys.path.append(os.pardir)  # 添加父目录到路径，以便导入common模块\n> import numpy as np\n> import matplotlib.pyplot as plt\n> from collections import OrderedDict\n> from common.optimizer import *  # 导入所有优化器类\n> \n> \n> def f(x, y):\n> \"\"\"\n> 定义目标函数：f(x,y) = x^2/20 + y^2\n> 这是一个简单的二次函数，用于测试优化算法的性能\n> \n> 参数:\n> x, y: 输入变量\n> 返回:\n> 函数值\n> \"\"\"\n> return x**2 / 20.0 + y**2\n> \n> \n> def df(x, y):\n> \"\"\"\n> 计算目标函数的梯度\n> ∂f/∂x = x/10, ∂f/∂y = 2y\n> \n> 参数:\n> x, y: 输入变量\n> 返回:\n> (∂f/∂x, ∂f/∂y): 梯度向量\n> \"\"\"\n> return x / 10.0, 2.0*y\n> \n> # 设置初始位置\n> init_pos = (-7.0, 2.0)  # 初始点坐标\n> \n> # 初始化参数字典\n> params = {}\n> params['x'], params['y'] = init_pos[0], init_pos[1]  # 将初始位置赋值给参数\n> \n> # 初始化梯度字典\n> grads = {}\n> grads['x'], grads['y'] = 0, 0  # 梯度初始化为0\n> \n> # 创建优化器字典，使用OrderedDict保持顺序\n> optimizers = OrderedDict()\n> optimizers[\"SGD\"] = SGD(lr=0.95)  # 随机梯度下降，学习率0.95\n> optimizers[\"Momentum\"] = Momentum(lr=0.1)  # 带动量的SGD，学习率0.1\n> optimizers[\"AdaGrad\"] = AdaGrad(lr=1.5)  # 自适应梯度算法，学习率1.5\n> optimizers[\"Adam\"] = Adam(lr=0.3)  # Adam优化器，学习率0.3\n> \n> idx = 1  # 子图索引\n> \n> # 遍历每种优化器\n> for key in optimizers:\n> optimizer = optimizers[key]  # 获取当前优化器\n> x_history = []  # 记录x坐标的历史轨迹\n> y_history = []  # 记录y坐标的历史轨迹\n> params['x'], params['y'] = init_pos[0], init_pos[1]  # 重置参数到初始位置\n> \n> # 进行30次优化迭代\n> for i in range(30):\n> x_history.append(params['x'])  # 记录当前x位置\n> y_history.append(params['y'])  # 记录当前y位置\n> \n> # 计算当前点的梯度\n> grads['x'], grads['y'] = df(params['x'], params['y'])\n> # 使用优化器更新参数\n> optimizer.update(params, grads)\n> \n> \n> # 创建网格用于绘制等高线图\n> x = np.arange(-10, 10, 0.01)  # x轴范围：-10到10，步长0.01\n> y = np.arange(-5, 5, 0.01)    # y轴范围：-5到5，步长0.01\n> \n> X, Y = np.meshgrid(x, y)  # 创建网格坐标\n> Z = f(X, Y)  # 计算网格上每个点的函数值\n> \n> # 为了简化等高线图，将大于7的值设为0\n> mask = Z > 7\n> Z[mask] = 0\n> \n> # 绘制子图\n> plt.subplot(2, 2, idx)  # 创建2x2的子图，当前是第idx个\n> idx += 1  # 子图索引递增\n> \n> # 绘制优化轨迹\n> plt.plot(x_history, y_history, 'o-', color=\"red\")  # 红色圆点连线表示优化路径\n> plt.contour(X, Y, Z)  # 绘制等高线\n> plt.ylim(-10, 10)  # 设置y轴范围\n> plt.xlim(-10, 10)  # 设置x轴范围\n> plt.plot(0, 0, '+')  # 在原点(0,0)绘制加号，表示全局最优点\n> #colorbar()  # 注释掉的颜色条\n> #spring()   # 注释掉的弹簧布局\n> plt.title(key)  # 设置子图标题为优化器名称\n> plt.xlabel(\"x\")  # 设置x轴标签\n> plt.ylabel(\"y\")  # 设置y轴标签\n> \n> plt.show()  # 显示所有子图\n> ```\n>\n> \n>\n> #### 基于MNIST数据集的更新方法的比较\n>\n> ```python\n> import os\n> import sys\n> sys.path.append(os.pardir)  # 添加父目录到路径，以便导入其他模块\n> import matplotlib.pyplot as plt\n> from dataset.mnist import load_mnist  # 导入MNIST数据集加载函数\n> from common.util import smooth_curve  # 导入平滑曲线函数，用于可视化\n> from common.multi_layer_net import MultiLayerNet  # 导入多层神经网络类\n> from common.optimizer import *  # 导入所有优化器类\n> \n> \n> # 0: 加载MNIST数据集 ==========\n> (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)  # 加载并归一化MNIST数据集\n> \n> train_size = x_train.shape[0]  # 训练集大小\n> batch_size = 128  # 批量大小\n> max_iterations = 2000  # 最大迭代次数\n> \n> \n> # 1: 实验设置 ==========\n> # 创建不同优化器的字典\n> optimizers = {}\n> optimizers['SGD'] = SGD()  # 随机梯度下降\n> optimizers['Momentum'] = Momentum()  # 带动量的SGD\n> optimizers['AdaGrad'] = AdaGrad()  # 自适应梯度算法\n> optimizers['Adam'] = Adam()  # Adam优化器\n> #optimizers['RMSprop'] = RMSprop()  # RMSprop优化器（已注释）\n> \n> # 为每个优化器创建对应的神经网络和损失记录\n> networks = {}  # 存储不同优化器对应的神经网络\n> train_loss = {}  # 存储不同优化器的训练损失\n> for key in optimizers.keys():\n> # 创建具有4个隐藏层（每层100个神经元）的多层神经网络\n> networks[key] = MultiLayerNet(\n> input_size=784,  # 输入层大小（28x28=784）\n> hidden_size_list=[100, 100, 100, 100],  # 4个隐藏层，每层100个神经元\n> output_size=10)  # 输出层大小（10个类别）\n> train_loss[key] = []  # 初始化损失列表\n> \n> \n> # 2: 开始训练 ==========\n> for i in range(max_iterations):\n> # 随机选择批量数据\n> batch_mask = np.random.choice(train_size, batch_size)  # 随机选择batch_size个样本的索引\n> x_batch = x_train[batch_mask]  # 获取批量输入数据\n> t_batch = t_train[batch_mask]  # 获取批量标签数据\n> \n> # 对每个优化器进行参数更新\n> for key in optimizers.keys():\n> # 计算梯度\n> grads = networks[key].gradient(x_batch, t_batch)  # 计算当前批量的梯度\n> # 更新参数\n> optimizers[key].update(networks[key].params, grads)  # 使用优化器更新网络参数\n> \n> # 计算损失\n> loss = networks[key].loss(x_batch, t_batch)  # 计算当前批量的损失\n> train_loss[key].append(loss)  # 记录损失\n> \n> # 每100次迭代打印一次训练状态\n> if i % 100 == 0:\n> print(\"===========\" + \"iteration:\" + str(i) + \"===========\")\n> for key in optimizers.keys():\n> loss = networks[key].loss(x_batch, t_batch)  # 计算当前批量的损失\n> print(key + \":\" + str(loss))  # 打印每个优化器的损失\n> \n> \n> # 3: 绘制图表 ==========\n> # 设置不同优化器的标记样式\n> markers = {\"SGD\": \"o\", \"Momentum\": \"x\", \"AdaGrad\": \"s\", \"Adam\": \"D\"}\n> x = np.arange(max_iterations)  # 创建x轴数据（迭代次数）\n> \n> # 绘制每个优化器的损失曲线\n> for key in optimizers.keys():\n> # 使用平滑曲线绘制损失，每100次迭代标记一次\n> plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n> \n> # 设置图表属性\n> plt.xlabel(\"iterations\")  # x轴标签：迭代次数\n> plt.ylabel(\"loss\")  # y轴标签：损失值\n> plt.ylim(0, 1)  # 设置y轴范围：0到1\n> plt.legend()  # 显示图例\n> plt.show()  # 显示图表\n> ```\n>\n> \n>\n> #### 隐藏层的激活值的分布（设计两种初始值Xavier和He初始值（作为权重的初始值），分别适用于不同的激活函数）\n>\n> ```python\n> # coding: utf-8\n> import numpy as np\n> import matplotlib.pyplot as plt\n> \n> \n> def sigmoid(x):\n> \"\"\"\n> Sigmoid激活函数\n> 将输入值压缩到(0,1)区间\n> \"\"\"\n> return 1 / (1 + np.exp(-x))\n> \n> \n> def ReLU(x):\n> \"\"\"\n> ReLU激活函数\n> 当输入大于0时返回原值，小于0时返回0\n> \"\"\"\n> return np.maximum(0, x)\n> \n> \n> def tanh(x):\n> \"\"\"\n> tanh激活函数\n> 将输入值压缩到(-1,1)区间\n> \"\"\"\n> return np.tanh(x)\n> \n> # 生成1000个样本，每个样本100个特征,x 就是高斯分布随机数\n> input_data = np.random.randn(1000, 100)  # 且符合标准正态分布，也就是高斯分布\n> node_num = 100\n> hidden_layer_size = 5\n> activations = {}\n> \n> x = input_data\n> \n> # 模拟5层神经网络的前向传播过程\n> for i in range(hidden_layer_size):\n> if i != 0:\n> x = activations[i-1]  # 使用上一层的输出作为当前层的输入\n> \n> # 权重初始化实验\n> # 可以尝试不同的权重初始化方法\n> w = np.random.randn(node_num, node_num) * 1  # 标准正态分布\n> # w = np.random.randn(node_num, node_num) * 0.01  # 缩小100倍\n> # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)  # Xavier初始化\n> # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)  # He初始化\n> \n> # 线性变换\n> a = np.dot(x, w)\n> \n> # 激活函数实验\n> # 可以尝试不同的激活函数\n> z = sigmoid(a)  # Sigmoid激活\n> # z = ReLU(a)    # ReLU激活\n> # z = tanh(a)    # tanh激活\n> \n> activations[i] = z  # 保存当前层的激活值\n> \n> # 绘制每层激活值的分布直方图\n> for i, a in activations.items():\n> plt.subplot(1, len(activations), i+1)  # 创建子图\n> plt.title(str(i+1) + \"-layer\")  # 设置标题\n> if i != 0: plt.yticks([], [])  # 除第一层外，隐藏y轴刻度\n> # plt.xlim(0.1, 1)  # 可以设置x轴范围\n> # plt.ylim(0, 7000)  # 可以设置y轴范围\n> plt.hist(a.flatten(), 30, range=(0,1))  # 绘制直方图，30个bin，范围0-1\n> plt.show()  # 显示图形\n> ```\n>\n> \n>\n> #### 基于MNIST数据集的权重初始值的比较\n>\n> ```python\n> import os\n> import sys\n> \n> # 添加父目录到系统路径，以便导入common模块\n> sys.path.append(os.pardir)\n> import numpy as np\n> import matplotlib.pyplot as plt\n> from dataset.mnist import load_mnist\n> from common.util import smooth_curve\n> from common.multi_layer_net import MultiLayerNet\n> from common.optimizer import SGD\n> \n> # 加载MNIST数据集并进行归一化处理\n> (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n> \n> # 设置训练参数\n> train_size = x_train.shape[0]  # 训练数据总数\n> batch_size = 128  # 批次大小\n> max_iterations = 2000  # 最大迭代次数\n> \n> # 定义不同的权重初始化方法\n> # std=0.01: 标准差为0.01的正态分布\n> # Xavier: Xavier初始化（适用于sigmoid激活函数）\n> # He: He初始化（适用于ReLU激活函数）\n> weight_init_types = {'std=0.01': 0.01, 'Xavier': 'sigmoid', 'He': 'relu'}\n> optimizer = SGD(lr=0.01)  # 随机梯度下降优化器，学习率为0.01\n> \n> # 创建不同初始化方法的神经网络和损失记录\n> networks = {}\n> train_loss = {}\n> for key, weight_type in weight_init_types.items():\n> # 创建多层神经网络：输入784维，4个隐藏层各100个神经元，输出10维\n> networks[key] = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100],\n>                             output_size=10, weight_init_std=weight_type)\n> train_loss[key] = []  # 初始化损失记录列表\n> \n> # 开始训练循环\n> for i in range(max_iterations):\n> # 随机选择批次数据\n> batch_mask = np.random.choice(train_size, batch_size)\n> x_batch = x_train[batch_mask]\n> t_batch = t_train[batch_mask]\n> \n> # 对每种权重初始化方法进行训练\n> for key in weight_init_types.keys():\n>   # 计算梯度\n>   grads = networks[key].gradient(x_batch, t_batch)\n>   # 更新网络参数\n>   optimizer.update(networks[key].params, grads)\n> \n>   # 计算并记录损失\n>   loss = networks[key].loss(x_batch, t_batch)\n>   train_loss[key].append(loss)\n> \n> # 每100次迭代打印一次损失值\n> if i % 100 == 0:\n>   print(\"===========\" + \"iteration:\" + str(i) + \"===========\")\n>   for key in weight_init_types.keys():\n>       loss = networks[key].loss(x_batch, t_batch)\n>       print(key + \":\" + str(loss))\n> \n> # 绘制训练损失曲线\n> markers = {'std=0.01': 'o', 'Xavier': 's', 'He': 'D'}  # 不同方法的标记符号\n> x = np.arange(max_iterations)  # x轴：迭代次数\n> for key in weight_init_types.keys():\n> # 绘制平滑后的损失曲线，每100次迭代显示一个标记\n> plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n> plt.xlabel(\"iterations\")  # x轴标签\n> plt.ylabel(\"loss\")  # y轴标签\n> plt.ylim(0, 2.5)  # 设置y轴范围\n> plt.legend()  # 显示图例\n> plt.show()  # 显示图形\n> ```\n>\n> \n>\n> #### Batch Normalization的评估\n>\n> ```python\n> # coding: utf-8\n> # 本脚本用于对比带有批归一化（Batch Normalization）和不带批归一化的多层神经网络在不同权重初始值下的训练表现。\n> # 通过在MNIST数据集上训练网络，观察批归一化对训练收敛速度和准确率的影响。\n> import sys, os\n> sys.path.append(os.pardir)  # 将父目录加入sys.path，便于导入上级目录中的模块\n> import numpy as np\n> import matplotlib.pyplot as plt\n> from dataset.mnist import load_mnist  # 导入MNIST数据集加载函数\n> from common.multi_layer_net_extend import MultiLayerNetExtend  # 导入可扩展多层网络实现\n> from common.optimizer import SGD, Adam  # 导入优化器\n> \n> # 加载MNIST数据集，并进行归一化处理\n> (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n> \n> # 为了加快实验速度，仅取前1000个训练样本\n> x_train = x_train[:1000]\n> t_train = t_train[:1000]\n> \n> max_epochs = 20  # 最大训练轮数\n> train_size = x_train.shape[0]  # 训练集样本数\n> batch_size = 100  # 每个批次的样本数\n> learning_rate = 0.01  # 学习率\n> \n> \n> def __train(weight_init_std):\n>  \"\"\"\n>  训练带有和不带有批归一化的神经网络。\n>  参数：\n>      weight_init_std: 权重初始化的标准差\n>  返回：\n>      train_acc_list: 不带批归一化的网络在每个epoch的训练准确率\n>      bn_train_acc_list: 带批归一化的网络在每个epoch的训练准确率\n>  \"\"\"\n>  # 构建带批归一化的网络\n>  bn_network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10, \n>                                  weight_init_std=weight_init_std, use_batchnorm=True)\n>  # 构建不带批归一化的网络\n>  network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10,\n>                              weight_init_std=weight_init_std)\n>  optimizer = SGD(lr=learning_rate)  # 使用SGD优化器\n>  \n>  train_acc_list = []  # 记录不带BN的准确率\n>  bn_train_acc_list = []  # 记录带BN的准确率\n>  \n>  iter_per_epoch = max(train_size / batch_size, 1)  # 每个epoch的迭代次数\n>  epoch_cnt = 0  # 当前epoch计数\n>  \n>  # 训练循环\n>  for i in range(1000000000):  # 迭代次数设置为极大，实际会在达到max_epochs时break\n>      # 随机采样一个batch\n>      batch_mask = np.random.choice(train_size, batch_size)\n>      x_batch = x_train[batch_mask]\n>      t_batch = t_train[batch_mask]\n>  \n>      # 对两个网络分别进行反向传播和参数更新\n>      for _network in (bn_network, network):\n>          grads = _network.gradient(x_batch, t_batch)\n>          optimizer.update(_network.params, grads)\n>  \n>      # 每经过一个epoch，记录一次准确率\n>      if i % iter_per_epoch == 0:\n>          train_acc = network.accuracy(x_train, t_train)\n>          bn_train_acc = bn_network.accuracy(x_train, t_train)\n>          train_acc_list.append(train_acc)\n>          bn_train_acc_list.append(bn_train_acc)\n>  \n>          print(\"epoch:\" + str(epoch_cnt) + \" | \" + str(train_acc) + \" - \" + str(bn_train_acc))\n>  \n>          epoch_cnt += 1\n>          if epoch_cnt >= max_epochs:\n>              break\n>              \n>  return train_acc_list, bn_train_acc_list\n> \n> \n> # 生成16个权重初始化标准差，从1到1e-4，等比数列\n> weight_scale_list = np.logspace(0, -4, num=16)\n> x = np.arange(max_epochs)  # x轴为epoch数\n> \n> # 对每个权重初始化标准差分别进行实验\n> for i, w in enumerate(weight_scale_list):\n>  print( \"============== \" + str(i+1) + \"/16\" + \" ==============\")\n>  train_acc_list, bn_train_acc_list = __train(w)\n>  \n>  # 绘制每组实验的准确率曲线\n>  plt.subplot(4,4,i+1)\n>  plt.title(\"W:\" + str(w))\n>  if i == 15:\n>      plt.plot(x, bn_train_acc_list, label='Batch Normalization', markevery=2)\n>      plt.plot(x, train_acc_list, linestyle = \"--\", label='Normal(without BatchNorm)', markevery=2)\n>  else:\n>      plt.plot(x, bn_train_acc_list, markevery=2)\n>      plt.plot(x, train_acc_list, linestyle=\"--\", markevery=2)\n> \n>  plt.ylim(0, 1.0)  # y轴范围\n>  if i % 4:\n>      plt.yticks([])\n>  else:\n>      plt.ylabel(\"accuracy\")\n>  if i < 12:\n>      plt.xticks([])\n>  else:\n>      plt.xlabel(\"epochs\")\n>  plt.legend(loc='lower right')\n>  \n> plt.show()  # 显示所有子图\n> ```\n>\n> \n>\n> #### Dropout的实现\n>\n> ```python\n> class Dropout:\n>  def __init__(self, dropout_ratio=0.5):\n>      slef.dropout_ratio = dropout_ratio\n>      self.mask = None\n> \n>  def forward(self,x, train_flag=True):\n>      if train_flag:\n>          self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n>          return x * self.mask\n>      else:\n>          return x * (1.0 - self.dropout_ratio)\n> \n>  def backward(self, dout):\n>      return dout * self.mask\n> ```\n>\n> #### 使用Mnist数据集进行验证Dropout的效果\n>\n> ```python\n> # coding: utf-8\n> # 本脚本用于实验Dropout对深层神经网络过拟合的抑制作用。\n> # 通过在MNIST数据集上训练一个较深的网络，观察在有无Dropout时训练集和测试集准确率的变化。\n> import os\n> import sys\n> sys.path.append(os.pardir)  # 将父目录加入sys.path，便于导入上级目录中的模块\n> import numpy as np\n> import matplotlib.pyplot as plt\n> from dataset.mnist import load_mnist  # 导入MNIST数据集加载函数\n> from common.multi_layer_net_extend import MultiLayerNetExtend  # 导入可扩展多层神经网络实现（支持Dropout）\n> from common.trainer import Trainer  # 导入训练器类，简化训练流程\n> \n> # 加载MNIST数据集，并进行归一化处理\n> (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n> \n> # 为了更容易出现过拟合，仅取前300个训练样本\n> x_train = x_train[:300]\n> t_train = t_train[:300]\n> \n> # 设置Dropout参数\n> use_dropout = True  # 是否使用Dropout\n> # dropout_ratio为每层神经元被随机丢弃的比例，常用0.2~0.5\n> # 若不使用Dropout，可将use_dropout设为False\n> # ====================================================\n> dropout_ratio = 0.2  # Dropout比例\n> \n> # 构建一个6层隐藏层的全连接神经网络，设置Dropout参数\n> network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n>                            output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n> # 使用Trainer类进行训练，自动完成mini-batch梯度下降、准确率记录等\n> trainer = Trainer(network, x_train, t_train, x_test, t_test,\n>                epochs=301, mini_batch_size=100,\n>                optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n> trainer.train()\n> \n> # 获取训练集和测试集的准确率变化曲线\n> train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n> \n> # 绘制训练集和测试集的准确率曲线\n> markers = {'train': 'o', 'test': 's'}\n> x = np.arange(len(train_acc_list))\n> plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n> plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n> plt.xlabel(\"epochs\")\n> plt.ylabel(\"accuracy\")\n> plt.ylim(0, 1.0)\n> plt.legend(loc='lower right')\n> plt.show()\n> ```\n>\n> \n>\n> #### 超参数最优化的实现\n>\n> ```python\n> # coding: utf-8\n> import sys, os\n> sys.path.append(os.pardir)  # 将父目录添加到sys.path，便于导入上级目录的模块\n> import numpy as np\n> import matplotlib.pyplot as plt\n> from dataset.mnist import load_mnist\n> from common.multi_layer_net import MultiLayerNet\n> from common.util import shuffle_dataset\n> from common.trainer import Trainer\n> \n> # 加载MNIST数据集，并进行归一化处理\n> (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n> \n> # 只取前500个训练样本，减少计算量，加快超参数搜索\n> x_train = x_train[:500]\n> t_train = t_train[:500]\n> \n> # 设置验证集比例为20%\n> validation_rate = 0.20\n> validation_num = int(x_train.shape[0] * validation_rate)\n> # 先打乱数据，保证训练集和验证集的分布一致\n> x_train, t_train = shuffle_dataset(x_train, t_train)\n> x_val = x_train[:validation_num]\n> t_val = t_train[:validation_num]\n> x_train = x_train[validation_num:]\n> t_train = t_train[validation_num:]\n> \n> # 定义训练函数，输入学习率和权重衰减，返回每轮的验证集和训练集准确率\n> def __train(lr, weight_decay, epocs=50):\n>     # 构建一个6层隐藏层的全连接神经网络，带权重衰减\n>     network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n>                             output_size=10, weight_decay_lambda=weight_decay)\n>     # 构建训练器，使用SGD优化器\n>     trainer = Trainer(network, x_train, t_train, x_val, t_val,\n>                       epochs=epocs, mini_batch_size=100,\n>                       optimizer='sgd', optimizer_param={'lr': lr}, verbose=False)\n>     trainer.train()\n> \n>     return trainer.test_acc_list, trainer.train_acc_list\n> \n> # 超参数优化实验次数\n> optimization_trial = 100\n> results_val = {}   # 存储每组超参数下的验证集准确率\n> results_train = {} # 存储每组超参数下的训练集准确率\n> for _ in range(optimization_trial):\n>     # 随机采样权重衰减和学习率（对数均匀分布采样，覆盖大范围）\n>     weight_decay = 10 ** np.random.uniform(-8, -4)\n>     lr = 10 ** np.random.uniform(-6, -2)\n>     # ================================================\n> \n>     # 训练网络并记录结果\n>     val_acc_list, train_acc_list = __train(lr, weight_decay)\n>     print(\"val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(lr) + \", weight decay:\" + str(weight_decay))\n>     key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n>     results_val[key] = val_acc_list\n>     results_train[key] = train_acc_list\n> \n> # 输出超参数优化结果\n> print(\"=========== Hyper-Parameter Optimization Result ===========\")\n> graph_draw_num = 20  # 最多画出前20组最优超参数的曲线\n> col_num = 5\n> row_num = int(np.ceil(graph_draw_num / col_num))\n> i = 0\n> \n> # 按验证集最终准确率从高到低排序，依次画出前20组的准确率曲线\n> for key, val_acc_list in sorted(results_val.items(), key=lambda x:x[1][-1], reverse=True):\n>     print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n> \n>     plt.subplot(row_num, col_num, i+1)\n>     plt.title(\"Best-\" + str(i+1))\n>     plt.ylim(0.0, 1.0)\n>     if i % 5: plt.yticks([])\n>     plt.xticks([])\n>     x = np.arange(len(val_acc_list))\n>     plt.plot(x, val_acc_list)           # 验证集准确率\n>     plt.plot(x, results_train[key], \"--\") # 训练集准确率（虚线）\n>     i += 1\n> \n>     if i >= graph_draw_num:\n>         break\n> \n> plt.show()\n> ```\n>\n> \n\n\n\n### 晕了，完全晕了！","slug":"24","published":1,"date":"2025-06-20T07:20:58.064Z","comments":1,"layout":"post","photos":[],"_id":"cmd5f7crq003kiku4598cdc5q","content":"<h1 id=\"与学习相关的技巧\"><a href=\"#与学习相关的技巧\" class=\"headerlink\" title=\"与学习相关的技巧\"></a>与学习相关的技巧</h1><h2 id=\"参数的更新\"><a href=\"#参数的更新\" class=\"headerlink\" title=\"参数的更新\"></a>参数的更新</h2><blockquote>\n<h4 id=\"神经网络的学习的目的是找到使损失函数的值尽可能小的参数，这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）\"><a href=\"#神经网络的学习的目的是找到使损失函数的值尽可能小的参数，这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）\" class=\"headerlink\" title=\"神经网络的学习的目的是找到使损失函数的值尽可能小的参数，这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）\"></a>神经网络的学习的目的是找到使损失函数的值尽可能小的参数，这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）</h4><h4 id=\"SGD（随机梯度下降法）\"><a href=\"#SGD（随机梯度下降法）\" class=\"headerlink\" title=\"SGD（随机梯度下降法）\"></a>SGD（随机梯度下降法）</h4><blockquote>\n<h4 id=\"SGD的缺点，如果函数的形状非均向（anisotropic），比如呈延伸状，搜索的路径就会非常低效，低效的根本原因是，梯度的方向并没有指向最小值的方向，如下图\"><a href=\"#SGD的缺点，如果函数的形状非均向（anisotropic），比如呈延伸状，搜索的路径就会非常低效，低效的根本原因是，梯度的方向并没有指向最小值的方向，如下图\" class=\"headerlink\" title=\"SGD的缺点，如果函数的形状非均向（anisotropic），比如呈延伸状，搜索的路径就会非常低效，低效的根本原因是，梯度的方向并没有指向最小值的方向，如下图\"></a>SGD的缺点，如果函数的形状非均向（anisotropic），比如呈延伸状，搜索的路径就会非常低效，低效的根本原因是，梯度的方向并没有指向最小值的方向，如下图</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/5/1.png\"></p>\n<h4 id=\"这会导致它的效率很低，基于SGD的最优化的更新路径如下图\"><a href=\"#这会导致它的效率很低，基于SGD的最优化的更新路径如下图\" class=\"headerlink\" title=\"这会导致它的效率很低，基于SGD的最优化的更新路径如下图\"></a>这会导致它的效率很低，基于SGD的最优化的更新路径如下图</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/5/2.png\"></p>\n<h4 id=\"可以看到，它是Z字型的移动\"><a href=\"#可以看到，它是Z字型的移动\" class=\"headerlink\" title=\"可以看到，它是Z字型的移动\"></a>可以看到，它是Z字型的移动</h4></blockquote>\n<h4 id=\"Momentum（带动量的随机梯度下降法）\"><a href=\"#Momentum（带动量的随机梯度下降法）\" class=\"headerlink\" title=\"Momentum（带动量的随机梯度下降法）\"></a>Momentum（带动量的随机梯度下降法）</h4><blockquote>\n<h4 id=\"它是动量的意思，和物理有关，数学表达式如下\"><a href=\"#它是动量的意思，和物理有关，数学表达式如下\" class=\"headerlink\" title=\"它是动量的意思，和物理有关，数学表达式如下\"></a>它是动量的意思，和物理有关，数学表达式如下</h4><p>$$<br>v \\leftarrow \\alpha \\cdot v - \\eta \\cdot \\frac {\\partial L} {\\partial W} \\<br>W \\leftarrow W + v<br>$$</p>\n<h4 id=\"W同样还是表示要更新的权重参数，-frac-partial-L-partial-W-表示损失函数关于W的梯度，-eta-是学习率，而-v-是物理上的速度\"><a href=\"#W同样还是表示要更新的权重参数，-frac-partial-L-partial-W-表示损失函数关于W的梯度，-eta-是学习率，而-v-是物理上的速度\" class=\"headerlink\" title=\"W同样还是表示要更新的权重参数，$\\frac{\\partial L}{\\partial W}$ 表示损失函数关于W的梯度，$\\eta$ 是学习率，而 $v$ 是物理上的速度\"></a>W同样还是表示要更新的权重参数，$\\frac{\\partial L}{\\partial W}$ 表示损失函数关于W的梯度，$\\eta$ 是学习率，而 $v$ 是物理上的速度</h4><h4 id=\"第一个式子表示，物体在梯度方向上受力，在这个力的作用下，物体的速度增加这一物理法则，-alpha-v-这一项，其中的-alpha-是一个衰减因子，其作用是在物体不受力的作用的情况下，使得物体速度减小，一般设置为0-9之类的值，它对应物理中的地面摩擦或者空气阻力等\"><a href=\"#第一个式子表示，物体在梯度方向上受力，在这个力的作用下，物体的速度增加这一物理法则，-alpha-v-这一项，其中的-alpha-是一个衰减因子，其作用是在物体不受力的作用的情况下，使得物体速度减小，一般设置为0-9之类的值，它对应物理中的地面摩擦或者空气阻力等\" class=\"headerlink\" title=\"第一个式子表示，物体在梯度方向上受力，在这个力的作用下，物体的速度增加这一物理法则，$\\alpha v$ 这一项，其中的 $\\alpha$ 是一个衰减因子，其作用是在物体不受力的作用的情况下，使得物体速度减小，一般设置为0.9之类的值，它对应物理中的地面摩擦或者空气阻力等\"></a>第一个式子表示，物体在梯度方向上受力，在这个力的作用下，物体的速度增加这一物理法则，$\\alpha v$ 这一项，其中的 $\\alpha$ 是一个衰减因子，其作用是在物体不受力的作用的情况下，使得物体速度减小，一般设置为0.9之类的值，它对应物理中的地面摩擦或者空气阻力等</h4><h4 id=\"基于Momentum的最优化的更新路径如下图\"><a href=\"#基于Momentum的最优化的更新路径如下图\" class=\"headerlink\" title=\"基于Momentum的最优化的更新路径如下图\"></a>基于Momentum的最优化的更新路径如下图</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/5/3.png\"></p>\n<h4 id=\"可以发现，和SGD相比，它的Z字型程度减轻了，这是因为，虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速；另一方面，虽然y轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它们会互相抵消，所以y轴方向上的速度不稳定\"><a href=\"#可以发现，和SGD相比，它的Z字型程度减轻了，这是因为，虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速；另一方面，虽然y轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它们会互相抵消，所以y轴方向上的速度不稳定\" class=\"headerlink\" title=\"可以发现，和SGD相比，它的Z字型程度减轻了，这是因为，虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速；另一方面，虽然y轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它们会互相抵消，所以y轴方向上的速度不稳定\"></a>可以发现，和SGD相比，它的Z字型程度减轻了，这是因为，虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速；另一方面，虽然y轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它们会互相抵消，所以y轴方向上的速度不稳定</h4></blockquote>\n<h4 id=\"AdaGrad（自适应梯度算法）\"><a href=\"#AdaGrad（自适应梯度算法）\" class=\"headerlink\" title=\"AdaGrad（自适应梯度算法）\"></a>AdaGrad（自适应梯度算法）</h4><blockquote>\n<h4 id=\"学习率衰减（learning-rate-decay）：随着学习的进行，使学习率逐渐减小。它是针对全体参数的，它是将全体参数的学习率一起降低\"><a href=\"#学习率衰减（learning-rate-decay）：随着学习的进行，使学习率逐渐减小。它是针对全体参数的，它是将全体参数的学习率一起降低\" class=\"headerlink\" title=\"学习率衰减（learning rate decay）：随着学习的进行，使学习率逐渐减小。它是针对全体参数的，它是将全体参数的学习率一起降低\"></a>学习率衰减（learning rate decay）：随着学习的进行，使学习率逐渐减小。它是针对全体参数的，它是将全体参数的学习率一起降低</h4><h4 id=\"AdaGrad会为参数的每个元素适当地调整学习率，与此同时进行学习，Ada是取自Adaptive，即适当的\"><a href=\"#AdaGrad会为参数的每个元素适当地调整学习率，与此同时进行学习，Ada是取自Adaptive，即适当的\" class=\"headerlink\" title=\"AdaGrad会为参数的每个元素适当地调整学习率，与此同时进行学习，Ada是取自Adaptive，即适当的\"></a>AdaGrad会为参数的每个元素适当地调整学习率，与此同时进行学习，Ada是取自Adaptive，即适当的</h4><h4 id=\"其数学表达式如下\"><a href=\"#其数学表达式如下\" class=\"headerlink\" title=\"其数学表达式如下\"></a>其数学表达式如下</h4><p>$$<br>h \\leftarrow h + \\frac {\\partial L} {\\partial W} \\odot \\frac {\\partial L} {\\partial W} \\<br>\\mathbf{W} \\leftarrow \\mathbf{W} - \\eta \\frac{1}{\\sqrt{\\mathbf{h}}} \\frac{\\partial L}{\\partial \\mathbf{W}}<br>$$</p>\n<h4 id=\"需要说明的是，变量-h-，它保存了以前的所有梯度值的平方和，-odot-表示对应矩阵元素的乘法，也就是实现了所有梯度值的平方和\"><a href=\"#需要说明的是，变量-h-，它保存了以前的所有梯度值的平方和，-odot-表示对应矩阵元素的乘法，也就是实现了所有梯度值的平方和\" class=\"headerlink\" title=\"需要说明的是，变量 $h$ ，它保存了以前的所有梯度值的平方和，$\\odot$ 表示对应矩阵元素的乘法，也就是实现了所有梯度值的平方和\"></a>需要说明的是，变量 $h$ ，它保存了以前的所有梯度值的平方和，$\\odot$ 表示对应矩阵元素的乘法，也就是实现了所有梯度值的平方和</h4><h4 id=\"另外，在更新参数时，通过乘以-frac-1-sqrt-h-，就可以调整学习的尺度，这意味着，参数的元素中变动较大（被大幅更新）的元素的学习率将变小，也即，可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小\"><a href=\"#另外，在更新参数时，通过乘以-frac-1-sqrt-h-，就可以调整学习的尺度，这意味着，参数的元素中变动较大（被大幅更新）的元素的学习率将变小，也即，可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小\" class=\"headerlink\" title=\"另外，在更新参数时，通过乘以 $\\frac {1}{\\sqrt {h}}$ ，就可以调整学习的尺度，这意味着，参数的元素中变动较大（被大幅更新）的元素的学习率将变小，也即，可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小\"></a>另外，在更新参数时，通过乘以 $\\frac {1}{\\sqrt {h}}$ ，就可以调整学习的尺度，这意味着，参数的元素中变动较大（被大幅更新）的元素的学习率将变小，也即，可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小</h4><h4 id=\"AdaGrad会记录过去所有梯度的平方和，因此，学习越深入，更新的幅度就越小，如果无休止的学习，最终更新量会变为0\"><a href=\"#AdaGrad会记录过去所有梯度的平方和，因此，学习越深入，更新的幅度就越小，如果无休止的学习，最终更新量会变为0\" class=\"headerlink\" title=\"AdaGrad会记录过去所有梯度的平方和，因此，学习越深入，更新的幅度就越小，如果无休止的学习，最终更新量会变为0\"></a>AdaGrad会记录过去所有梯度的平方和，因此，学习越深入，更新的幅度就越小，如果无休止的学习，最终更新量会变为0</h4><h4 id=\"RMSProp方法，并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来，这样的操作被称为指数移动平均，呈指数函数式地减小过去的梯度的尺度\"><a href=\"#RMSProp方法，并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来，这样的操作被称为指数移动平均，呈指数函数式地减小过去的梯度的尺度\" class=\"headerlink\" title=\"RMSProp方法，并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来，这样的操作被称为指数移动平均，呈指数函数式地减小过去的梯度的尺度\"></a>RMSProp方法，并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来，这样的操作被称为<strong>指数移动平均</strong>，呈指数函数式地减小过去的梯度的尺度</h4><h4 id=\"基于AdaGrad的最优化的更新路径如下图\"><a href=\"#基于AdaGrad的最优化的更新路径如下图\" class=\"headerlink\" title=\"基于AdaGrad的最优化的更新路径如下图\"></a>基于AdaGrad的最优化的更新路径如下图</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/5/4.png\"></p>\n<h4 id=\"可以看到，函数的取值高效地向着最小值移动\"><a href=\"#可以看到，函数的取值高效地向着最小值移动\" class=\"headerlink\" title=\"可以看到，函数的取值高效地向着最小值移动\"></a>可以看到，函数的取值高效地向着最小值移动</h4></blockquote>\n<h4 id=\"Adam（自适应矩估计算法）\"><a href=\"#Adam（自适应矩估计算法）\" class=\"headerlink\" title=\"Adam（自适应矩估计算法）\"></a>Adam（自适应矩估计算法）</h4><blockquote>\n<h4 id=\"它融合了Momentum和AdaGrad，结合两者的优点，它还可以进行超参数的偏置矫正\"><a href=\"#它融合了Momentum和AdaGrad，结合两者的优点，它还可以进行超参数的偏置矫正\" class=\"headerlink\" title=\"它融合了Momentum和AdaGrad，结合两者的优点，它还可以进行超参数的偏置矫正\"></a>它融合了Momentum和AdaGrad，结合两者的优点，它还可以进行超参数的偏置矫正</h4><h4 id=\"基于Adam的最优化的更新路径如下图\"><a href=\"#基于Adam的最优化的更新路径如下图\" class=\"headerlink\" title=\"基于Adam的最优化的更新路径如下图\"></a>基于Adam的最优化的更新路径如下图</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/5/5.png\"></p>\n<h4 id=\"Adam会设置3个超参数，一个是学习率-eta-，另外两个是一次momentum系数-beta-1-和二次momentum系数-beta-2-，且标准的设定值是-beta-1-0-9-beta-2-0-999\"><a href=\"#Adam会设置3个超参数，一个是学习率-eta-，另外两个是一次momentum系数-beta-1-和二次momentum系数-beta-2-，且标准的设定值是-beta-1-0-9-beta-2-0-999\" class=\"headerlink\" title=\"Adam会设置3个超参数，一个是学习率 $\\eta$ ，另外两个是一次momentum系数 $\\beta_1$ 和二次momentum系数 $\\beta_2$ ，且标准的设定值是 $\\beta_1 &#x3D; 0.9, \\beta_2 &#x3D; 0.999$\"></a>Adam会设置3个超参数，一个是学习率 $\\eta$ ，另外两个是一次momentum系数 $\\beta_1$ 和二次momentum系数 $\\beta_2$ ，且标准的设定值是 $\\beta_1 &#x3D; 0.9, \\beta_2 &#x3D; 0.999$</h4></blockquote>\n</blockquote>\n<h3 id=\"权重的初始值\"><a href=\"#权重的初始值\" class=\"headerlink\" title=\"权重的初始值\"></a>权重的初始值</h3><blockquote>\n<h4 id=\"权值衰减（weight-decay），就是一种以减小权重参数的值为目的进行学习的方法，通过减小权重参数的值来抑制过拟合的发生，以及提高泛化能力\"><a href=\"#权值衰减（weight-decay），就是一种以减小权重参数的值为目的进行学习的方法，通过减小权重参数的值来抑制过拟合的发生，以及提高泛化能力\" class=\"headerlink\" title=\"权值衰减（weight decay），就是一种以减小权重参数的值为目的进行学习的方法，通过减小权重参数的值来抑制过拟合的发生，以及提高泛化能力\"></a>权值衰减（weight decay），就是一种以减小权重参数的值为目的进行学习的方法，通过减小权重参数的值来抑制过拟合的发生，以及提高泛化能力</h4><h4 id=\"在误差反向传播法中，所有的权重值都会进行相同的更新，所以不能把权重初始值设为一样的值（权重均一化）\"><a href=\"#在误差反向传播法中，所有的权重值都会进行相同的更新，所以不能把权重初始值设为一样的值（权重均一化）\" class=\"headerlink\" title=\"在误差反向传播法中，所有的权重值都会进行相同的更新，所以不能把权重初始值设为一样的值（权重均一化）\"></a>在误差反向传播法中，所有的权重值都会进行相同的更新，所以不能把权重初始值设为一样的值（权重均一化）</h4><h4 id=\"激活函数使用sigmoid函数，随着输出不断地靠近0（或者靠近1），它的导数的值逐渐接近0，从而偏向0和1的数据分布会造成反向传播中梯度的值不断变小，最后消失，这就是所谓的梯度消失（gradient-vanishing）\"><a href=\"#激活函数使用sigmoid函数，随着输出不断地靠近0（或者靠近1），它的导数的值逐渐接近0，从而偏向0和1的数据分布会造成反向传播中梯度的值不断变小，最后消失，这就是所谓的梯度消失（gradient-vanishing）\" class=\"headerlink\" title=\"激活函数使用sigmoid函数，随着输出不断地靠近0（或者靠近1），它的导数的值逐渐接近0，从而偏向0和1的数据分布会造成反向传播中梯度的值不断变小，最后消失，这就是所谓的梯度消失（gradient vanishing）\"></a>激活函数使用sigmoid函数，随着输出不断地靠近0（或者靠近1），它的导数的值逐渐接近0，从而偏向0和1的数据分布会造成反向传播中梯度的值不断变小，最后消失，这就是所谓的<strong>梯度消失</strong>（gradient vanishing）</h4><h4 id=\"激活值在分布上有所偏向会出现表现了受限或梯度消失的问题\"><a href=\"#激活值在分布上有所偏向会出现表现了受限或梯度消失的问题\" class=\"headerlink\" title=\"激活值在分布上有所偏向会出现表现了受限或梯度消失的问题\"></a>激活值在分布上有所偏向会出现表现了受限或梯度消失的问题</h4><h4 id=\"Xavier初始值\"><a href=\"#Xavier初始值\" class=\"headerlink\" title=\"Xavier初始值\"></a>Xavier初始值</h4><blockquote>\n<h4 id=\"为了使各层的激活值呈现出具有相同广度的分布，推导了合适的权重尺度，结论是：如果前一层的节点数为n，则初始值使用标准差为-frac-1-sqrt-n-的分布\"><a href=\"#为了使各层的激活值呈现出具有相同广度的分布，推导了合适的权重尺度，结论是：如果前一层的节点数为n，则初始值使用标准差为-frac-1-sqrt-n-的分布\" class=\"headerlink\" title=\"为了使各层的激活值呈现出具有相同广度的分布，推导了合适的权重尺度，结论是：如果前一层的节点数为n，则初始值使用标准差为 $\\frac{1}{\\sqrt{n}}$ 的分布\"></a>为了使各层的激活值呈现出具有相同广度的分布，推导了合适的权重尺度，结论是：如果前一层的节点数为n，则初始值使用标准差为 $\\frac{1}{\\sqrt{n}}$ 的分布</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/5/6.png\"></p>\n<h4 id=\"可以将激活函数sigmoid改为tanh双曲线函数，两者同为S型曲线函数，但tanh是关于原点对称的S型曲线，而sigmoid是关于（0，0-5）对称的，而用作激活函数的函数最好具有关于原点对称的性质\"><a href=\"#可以将激活函数sigmoid改为tanh双曲线函数，两者同为S型曲线函数，但tanh是关于原点对称的S型曲线，而sigmoid是关于（0，0-5）对称的，而用作激活函数的函数最好具有关于原点对称的性质\" class=\"headerlink\" title=\"可以将激活函数sigmoid改为tanh双曲线函数，两者同为S型曲线函数，但tanh是关于原点对称的S型曲线，而sigmoid是关于（0，0.5）对称的，而用作激活函数的函数最好具有关于原点对称的性质\"></a>可以将激活函数sigmoid改为tanh双曲线函数，两者同为S型曲线函数，但tanh是关于原点对称的S型曲线，而sigmoid是关于（0，0.5）对称的，而用作激活函数的函数最好具有关于原点对称的性质</h4></blockquote>\n<h4 id=\"ReLU的权值重置\"><a href=\"#ReLU的权值重置\" class=\"headerlink\" title=\"ReLU的权值重置\"></a>ReLU的权值重置</h4><blockquote>\n<h4 id=\"Xavier初始值是以激活函数是线性函数为前提而推导出来的，sigmoid函数和tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值\"><a href=\"#Xavier初始值是以激活函数是线性函数为前提而推导出来的，sigmoid函数和tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值\" class=\"headerlink\" title=\"Xavier初始值是以激活函数是线性函数为前提而推导出来的，sigmoid函数和tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值\"></a>Xavier初始值是以激活函数是线性函数为前提而推导出来的，sigmoid函数和tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值</h4><h4 id=\"当激活函数使用ReLU时，一般推荐使用ReLU专用的初始值——He初始值\"><a href=\"#当激活函数使用ReLU时，一般推荐使用ReLU专用的初始值——He初始值\" class=\"headerlink\" title=\"当激活函数使用ReLU时，一般推荐使用ReLU专用的初始值——He初始值\"></a>当激活函数使用ReLU时，一般推荐使用ReLU专用的初始值——He初始值</h4><h4 id=\"它是，当前一层的节点数为n时，He初始值使用标准差为-sqrt-frac-2-n-的高斯分布\"><a href=\"#它是，当前一层的节点数为n时，He初始值使用标准差为-sqrt-frac-2-n-的高斯分布\" class=\"headerlink\" title=\"它是，当前一层的节点数为n时，He初始值使用标准差为 $\\sqrt{\\frac {2}{n}}$ 的高斯分布\"></a>它是，当前一层的节点数为n时，He初始值使用标准差为 $\\sqrt{\\frac {2}{n}}$ 的高斯分布</h4><h4 id=\"相对于Xavier初始值的-sqrt-frac-1-n-，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数\"><a href=\"#相对于Xavier初始值的-sqrt-frac-1-n-，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数\" class=\"headerlink\" title=\"相对于Xavier初始值的 $\\sqrt{\\frac {1}{n}}$ ，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数\"></a>相对于Xavier初始值的 $\\sqrt{\\frac {1}{n}}$ ，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数</h4><h4 id=\"当激活函数使用ReLU时，权重初始值使用He初始值，当激活函数为sigmoid或tanh等S型曲线函数时，初始值使用Xavier初始值\"><a href=\"#当激活函数使用ReLU时，权重初始值使用He初始值，当激活函数为sigmoid或tanh等S型曲线函数时，初始值使用Xavier初始值\" class=\"headerlink\" title=\"当激活函数使用ReLU时，权重初始值使用He初始值，当激活函数为sigmoid或tanh等S型曲线函数时，初始值使用Xavier初始值\"></a>当激活函数使用ReLU时，权重初始值使用He初始值，当激活函数为sigmoid或tanh等S型曲线函数时，初始值使用Xavier初始值</h4></blockquote>\n</blockquote>\n<h3 id=\"Batch-Normalization\"><a href=\"#Batch-Normalization\" class=\"headerlink\" title=\"Batch Normalization\"></a>Batch Normalization</h3><blockquote>\n<h4 id=\"Batch-Normalization的算法\"><a href=\"#Batch-Normalization的算法\" class=\"headerlink\" title=\"Batch Normalization的算法\"></a>Batch Normalization的算法</h4><blockquote>\n<h4 id=\"为了使各层拥有适当的广度，“强制性”地调整激活值的分布，这就是Batch-Normalization方法\"><a href=\"#为了使各层拥有适当的广度，“强制性”地调整激活值的分布，这就是Batch-Normalization方法\" class=\"headerlink\" title=\"为了使各层拥有适当的广度，“强制性”地调整激活值的分布，这就是Batch Normalization方法\"></a>为了使各层拥有适当的广度，“强制性”地调整激活值的分布，这就是Batch Normalization方法</h4><h4 id=\"它有以下几个特点\"><a href=\"#它有以下几个特点\" class=\"headerlink\" title=\"它有以下几个特点\"></a>它有以下几个特点</h4><ul>\n<li><h4 id=\"可以使学习快速进行（可以增大学习率）\"><a href=\"#可以使学习快速进行（可以增大学习率）\" class=\"headerlink\" title=\"可以使学习快速进行（可以增大学习率）\"></a>可以使学习快速进行（可以增大学习率）</h4></li>\n<li><h4 id=\"对初始值的依赖不大\"><a href=\"#对初始值的依赖不大\" class=\"headerlink\" title=\"对初始值的依赖不大\"></a>对初始值的依赖不大</h4></li>\n<li><h4 id=\"抑制过拟合\"><a href=\"#抑制过拟合\" class=\"headerlink\" title=\"抑制过拟合\"></a>抑制过拟合</h4></li>\n</ul>\n<h4 id=\"Batch-normalization层：对神经网络中的数据分布进行正规化的层\"><a href=\"#Batch-normalization层：对神经网络中的数据分布进行正规化的层\" class=\"headerlink\" title=\"Batch normalization层：对神经网络中的数据分布进行正规化的层\"></a>Batch normalization层：对神经网络中的数据分布进行正规化的层</h4><h4 id=\"它以进行学习时的mini-batch为单位，按mini-batch进行正规化，具体来说就是，进行使数据分布的均值为0、方差为1的正规化，数学式表示如下\"><a href=\"#它以进行学习时的mini-batch为单位，按mini-batch进行正规化，具体来说就是，进行使数据分布的均值为0、方差为1的正规化，数学式表示如下\" class=\"headerlink\" title=\"它以进行学习时的mini-batch为单位，按mini-batch进行正规化，具体来说就是，进行使数据分布的均值为0、方差为1的正规化，数学式表示如下\"></a>它以进行学习时的mini-batch为单位，按mini-batch进行正规化，具体来说就是，进行使数据分布的均值为0、方差为1的正规化，数学式表示如下</h4><p>$$<br>\\mu_B \\leftarrow \\frac {1}{m} \\sum_{i&#x3D;1}^m x_i \\<br>\\sigma_B^2 \\leftarrow \\frac{1}{m} \\sum_{i&#x3D;1}^m(x_i - \\mu_B)^2 \\<br>\\hat x_i \\leftarrow \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\varepsilon}}<br>$$</p>\n<h4 id=\"第1、2个式子是对mini-batch的m个输入数据的集合-B-x-1-x-2…x-m-求均值-mu-B-和方差-sigma-B-2-，第三个式子对输入数据进行均值为0、方差为1（合适的分布）的正规化，-varepsilon-是一个极小值，作用是防止除0\"><a href=\"#第1、2个式子是对mini-batch的m个输入数据的集合-B-x-1-x-2…x-m-求均值-mu-B-和方差-sigma-B-2-，第三个式子对输入数据进行均值为0、方差为1（合适的分布）的正规化，-varepsilon-是一个极小值，作用是防止除0\" class=\"headerlink\" title=\"第1、2个式子是对mini-batch的m个输入数据的集合 $B&#x3D;{x_1,x_2…x_m}$ 求均值 $\\mu_B$ 和方差 $\\sigma_B^2$ ，第三个式子对输入数据进行均值为0、方差为1（合适的分布）的正规化，$\\varepsilon$ 是一个极小值，作用是防止除0\"></a>第1、2个式子是对mini-batch的m个输入数据的集合 $B&#x3D;{x_1,x_2…x_m}$ 求均值 $\\mu_B$ 和方差 $\\sigma_B^2$ ，第三个式子对输入数据进行均值为0、方差为1（合适的分布）的正规化，$\\varepsilon$ 是一个极小值，作用是防止除0</h4><h4 id=\"通过将这个处理插入到激活函数的前面（或者后面），可以减小数据分布的偏向\"><a href=\"#通过将这个处理插入到激活函数的前面（或者后面），可以减小数据分布的偏向\" class=\"headerlink\" title=\"通过将这个处理插入到激活函数的前面（或者后面），可以减小数据分布的偏向\"></a>通过将这个处理插入到激活函数的前面（或者后面），可以减小数据分布的偏向</h4><h4 id=\"紧接着，Batch-Norm层会对正规化后的数据进行缩放和平移的变换，数学式表示如下\"><a href=\"#紧接着，Batch-Norm层会对正规化后的数据进行缩放和平移的变换，数学式表示如下\" class=\"headerlink\" title=\"紧接着，Batch Norm层会对正规化后的数据进行缩放和平移的变换，数学式表示如下\"></a>紧接着，Batch Norm层会对正规化后的数据进行缩放和平移的变换，数学式表示如下</h4><p>$$<br>y_i \\leftarrow \\gamma \\hat x_i  + \\beta<br>$$</p>\n<h4 id=\"其中，-gamma-和-beta-是参数，初始分别为1和0，然后再通过学习调整到合适的值\"><a href=\"#其中，-gamma-和-beta-是参数，初始分别为1和0，然后再通过学习调整到合适的值\" class=\"headerlink\" title=\"其中， $\\gamma$ 和 $\\beta$ 是参数，初始分别为1和0，然后再通过学习调整到合适的值\"></a>其中， $\\gamma$ 和 $\\beta$ 是参数，初始分别为1和0，然后再通过学习调整到合适的值</h4><h4 id=\"它是神经网络上的正向传播，计算图如下\"><a href=\"#它是神经网络上的正向传播，计算图如下\" class=\"headerlink\" title=\"它是神经网络上的正向传播，计算图如下\"></a>它是神经网络上的正向传播，计算图如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/5/7.png\"></p>\n<h4 id=\"留个坑：batch-norm的反向传播\"><a href=\"#留个坑：batch-norm的反向传播\" class=\"headerlink\" title=\"留个坑：batch norm的反向传播\"></a>留个坑：batch norm的反向传播</h4></blockquote>\n<h4 id=\"Batch-Normalization的评估\"><a href=\"#Batch-Normalization的评估\" class=\"headerlink\" title=\"Batch Normalization的评估\"></a>Batch Normalization的评估</h4></blockquote>\n<h3 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a>正则化</h3><blockquote>\n<h4 id=\"过拟合：只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态\"><a href=\"#过拟合：只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态\" class=\"headerlink\" title=\"过拟合：只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态\"></a>过拟合：只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态</h4><h4 id=\"发生过拟合的原因，一个是模型拥有大量参数、表现力强，一个是训练数据少\"><a href=\"#发生过拟合的原因，一个是模型拥有大量参数、表现力强，一个是训练数据少\" class=\"headerlink\" title=\"发生过拟合的原因，一个是模型拥有大量参数、表现力强，一个是训练数据少\"></a>发生过拟合的原因，一个是模型拥有大量参数、表现力强，一个是训练数据少</h4><h4 id=\"权值衰减\"><a href=\"#权值衰减\" class=\"headerlink\" title=\"权值衰减\"></a>权值衰减</h4><blockquote>\n<h4 id=\"权值衰减是一直以来经常被使用的一种抑制过拟合的方法，该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合\"><a href=\"#权值衰减是一直以来经常被使用的一种抑制过拟合的方法，该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合\" class=\"headerlink\" title=\"权值衰减是一直以来经常被使用的一种抑制过拟合的方法，该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合\"></a>权值衰减是一直以来经常被使用的一种抑制过拟合的方法，该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合</h4><h4 id=\"如，为损失函数加上权重的平方范数（L2范数），这样就可以抑制权重变大，将权重记为W，L2的范数的权值衰减就是-frac-1-2-lambda-W-2-，然后将它加到损失函数上，-lambda-是控制正则化强度的超参数，-lambda-设置得越大，对大的权重施加的惩罚就越重，1-2是用于将-frac-1-2-lambda-W-2-的求导结果变成-lambda-W-的调整常用量\"><a href=\"#如，为损失函数加上权重的平方范数（L2范数），这样就可以抑制权重变大，将权重记为W，L2的范数的权值衰减就是-frac-1-2-lambda-W-2-，然后将它加到损失函数上，-lambda-是控制正则化强度的超参数，-lambda-设置得越大，对大的权重施加的惩罚就越重，1-2是用于将-frac-1-2-lambda-W-2-的求导结果变成-lambda-W-的调整常用量\" class=\"headerlink\" title=\"如，为损失函数加上权重的平方范数（L2范数），这样就可以抑制权重变大，将权重记为W，L2的范数的权值衰减就是 $\\frac{1}{2} \\lambda W^2$ ，然后将它加到损失函数上，$\\lambda$ 是控制正则化强度的超参数，$\\lambda$ 设置得越大，对大的权重施加的惩罚就越重，1&#x2F;2是用于将 $\\frac{1}{2} \\lambda W^2$ 的求导结果变成 $\\lambda W$ 的调整常用量\"></a>如，为损失函数加上权重的平方范数（L2范数），这样就可以抑制权重变大，将权重记为W，L2的范数的权值衰减就是 $\\frac{1}{2} \\lambda W^2$ ，然后将它加到损失函数上，$\\lambda$ 是控制正则化强度的超参数，$\\lambda$ 设置得越大，对大的权重施加的惩罚就越重，1&#x2F;2是用于将 $\\frac{1}{2} \\lambda W^2$ 的求导结果变成 $\\lambda W$ 的调整常用量</h4><h4 id=\"对于所有权重，权值衰减方法都会为损失函数加上-frac-1-2-lambda-W-2-，因此，在求权重梯度的计算中，要为之前的误差反向传播法的结果加上正则化项的导数-lambda-W\"><a href=\"#对于所有权重，权值衰减方法都会为损失函数加上-frac-1-2-lambda-W-2-，因此，在求权重梯度的计算中，要为之前的误差反向传播法的结果加上正则化项的导数-lambda-W\" class=\"headerlink\" title=\"对于所有权重，权值衰减方法都会为损失函数加上 $\\frac{1}{2} \\lambda W^2$ ，因此，在求权重梯度的计算中，要为之前的误差反向传播法的结果加上正则化项的导数 $\\lambda W$\"></a>对于所有权重，权值衰减方法都会为损失函数加上 $\\frac{1}{2} \\lambda W^2$ ，因此，在求权重梯度的计算中，要为之前的误差反向传播法的结果加上正则化项的导数 $\\lambda W$</h4><h4 id=\"L2范数相当于各元素的平方和，假设有权重-W-w-1-w-2…w-n-，则L2范数可用-sqrt-w-1-2-w-2-2-…w-n-2-计算出来\"><a href=\"#L2范数相当于各元素的平方和，假设有权重-W-w-1-w-2…w-n-，则L2范数可用-sqrt-w-1-2-w-2-2-…w-n-2-计算出来\" class=\"headerlink\" title=\"L2范数相当于各元素的平方和，假设有权重 $W&#x3D;(w_1,w_2…w_n)$ ，则L2范数可用 $\\sqrt{w_1^2+w_2^2+…w_n^2}$ 计算出来\"></a>L2范数相当于各元素的平方和，假设有权重 $W&#x3D;(w_1,w_2…w_n)$ ，则L2范数可用 $\\sqrt{w_1^2+w_2^2+…w_n^2}$ 计算出来</h4><h4 id=\"除了L2范数，还有L1范数、-L-infty-范数等，L1范数是各元素的绝对值之和，相当于-w-1-w-2-…-w-n-，-L-infty-范数也称为Max范数，相当于各个元素的绝对值中最大的那一个\"><a href=\"#除了L2范数，还有L1范数、-L-infty-范数等，L1范数是各元素的绝对值之和，相当于-w-1-w-2-…-w-n-，-L-infty-范数也称为Max范数，相当于各个元素的绝对值中最大的那一个\" class=\"headerlink\" title=\"除了L2范数，还有L1范数、$L \\infty$ 范数等，L1范数是各元素的绝对值之和，相当于 $|w_1|+|w_2|+…|w_n|$ ，$L\\infty$ 范数也称为Max范数，相当于各个元素的绝对值中最大的那一个\"></a>除了L2范数，还有L1范数、$L \\infty$ 范数等，L1范数是各元素的绝对值之和，相当于 $|w_1|+|w_2|+…|w_n|$ ，$L\\infty$ 范数也称为Max范数，相当于各个元素的绝对值中最大的那一个</h4><h4 id=\"以上三种范数都可以用作正则项\"><a href=\"#以上三种范数都可以用作正则项\" class=\"headerlink\" title=\"以上三种范数都可以用作正则项\"></a>以上三种范数都可以用作正则项</h4></blockquote>\n<h4 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h4><blockquote>\n<h4 id=\"前面的为损失函数加上权重的L2范数的权值衰减方法，在网络模型变得复杂的情况下，就难以应对了\"><a href=\"#前面的为损失函数加上权重的L2范数的权值衰减方法，在网络模型变得复杂的情况下，就难以应对了\" class=\"headerlink\" title=\"前面的为损失函数加上权重的L2范数的权值衰减方法，在网络模型变得复杂的情况下，就难以应对了\"></a>前面的为损失函数加上权重的L2范数的权值衰减方法，在网络模型变得复杂的情况下，就难以应对了</h4><h4 id=\"由此，引出了Dropout这种方法\"><a href=\"#由此，引出了Dropout这种方法\" class=\"headerlink\" title=\"由此，引出了Dropout这种方法\"></a>由此，引出了Dropout这种方法</h4><h4 id=\"Dropout是一种在学习的过程中随机删除神经元的方法\"><a href=\"#Dropout是一种在学习的过程中随机删除神经元的方法\" class=\"headerlink\" title=\"Dropout是一种在学习的过程中随机删除神经元的方法\"></a>Dropout是一种在学习的过程中随机删除神经元的方法</h4><blockquote>\n<h4 id=\"训练时，随机选出隐藏层的神经元，然后将其删除，被删除的神经元不再进行信号的传递\"><a href=\"#训练时，随机选出隐藏层的神经元，然后将其删除，被删除的神经元不再进行信号的传递\" class=\"headerlink\" title=\"训练时，随机选出隐藏层的神经元，然后将其删除，被删除的神经元不再进行信号的传递\"></a>训练时，随机选出隐藏层的神经元，然后将其删除，被删除的神经元不再进行信号的传递</h4><h4 id=\"训练时，每传递一次数据，就会随机选择要删除的神经元\"><a href=\"#训练时，每传递一次数据，就会随机选择要删除的神经元\" class=\"headerlink\" title=\"训练时，每传递一次数据，就会随机选择要删除的神经元\"></a>训练时，每传递一次数据，就会随机选择要删除的神经元</h4><h4 id=\"测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出\"><a href=\"#测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出\" class=\"headerlink\" title=\"测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出\"></a>测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出</h4></blockquote>\n<p><img src=\"http://picbed.yanzu.tech/img/DL/5/8.png\"></p>\n<h4 id=\"集成学习，就是让多个模型单独进行学习，推理时再取多个模型的输出的平均值，通过进行集成学习，神经网络的识别精度可以提高好几个百分点，Dropout将集成学习的效果（模拟地）通过一个网络实现了\"><a href=\"#集成学习，就是让多个模型单独进行学习，推理时再取多个模型的输出的平均值，通过进行集成学习，神经网络的识别精度可以提高好几个百分点，Dropout将集成学习的效果（模拟地）通过一个网络实现了\" class=\"headerlink\" title=\"集成学习，就是让多个模型单独进行学习，推理时再取多个模型的输出的平均值，通过进行集成学习，神经网络的识别精度可以提高好几个百分点，Dropout将集成学习的效果（模拟地）通过一个网络实现了\"></a>集成学习，就是让多个模型单独进行学习，推理时再取多个模型的输出的平均值，通过进行集成学习，神经网络的识别精度可以提高好几个百分点，Dropout将集成学习的效果（模拟地）通过一个网络实现了</h4></blockquote>\n</blockquote>\n<h3 id=\"超参数的验证\"><a href=\"#超参数的验证\" class=\"headerlink\" title=\"超参数的验证\"></a>超参数的验证</h3><blockquote>\n<h4 id=\"超参数是指各层的神经元数量、batch大小、参数更新时的学习率或权值衰减等\"><a href=\"#超参数是指各层的神经元数量、batch大小、参数更新时的学习率或权值衰减等\" class=\"headerlink\" title=\"超参数是指各层的神经元数量、batch大小、参数更新时的学习率或权值衰减等\"></a>超参数是指各层的神经元数量、batch大小、参数更新时的学习率或权值衰减等</h4><h4 id=\"不能使用测试数据评估超参数的性能，因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合\"><a href=\"#不能使用测试数据评估超参数的性能，因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合\" class=\"headerlink\" title=\"不能使用测试数据评估超参数的性能，因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合\"></a>不能使用测试数据评估超参数的性能，因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合</h4><h4 id=\"用于调整超参数的数据，一般称为验证数据\"><a href=\"#用于调整超参数的数据，一般称为验证数据\" class=\"headerlink\" title=\"用于调整超参数的数据，一般称为验证数据\"></a>用于调整超参数的数据，一般称为验证数据</h4><h4 id=\"训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估，为了确认泛化能力，要在最后使用测试数据\"><a href=\"#训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估，为了确认泛化能力，要在最后使用测试数据\" class=\"headerlink\" title=\"训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估，为了确认泛化能力，要在最后使用测试数据\"></a>训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估，为了确认泛化能力，要在最后使用测试数据</h4><h4 id=\"超参数的最优化\"><a href=\"#超参数的最优化\" class=\"headerlink\" title=\"超参数的最优化\"></a>超参数的最优化</h4><blockquote>\n<h4 id=\"进行超参数的最优化时，要逐渐缩小超参数的“好值”的存在范围\"><a href=\"#进行超参数的最优化时，要逐渐缩小超参数的“好值”的存在范围\" class=\"headerlink\" title=\"进行超参数的最优化时，要逐渐缩小超参数的“好值”的存在范围\"></a>进行超参数的最优化时，要逐渐缩小超参数的“好值”的存在范围</h4><h4 id=\"所谓逐渐缩小范围，是指一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样，最好是随机采样），用这个采样到的值进行识别精度的评估，重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围，最终确定超参数的合适范围-一般是-0-001-1000\"><a href=\"#所谓逐渐缩小范围，是指一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样，最好是随机采样），用这个采样到的值进行识别精度的评估，重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围，最终确定超参数的合适范围-一般是-0-001-1000\" class=\"headerlink\" title=\"所谓逐渐缩小范围，是指一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样，最好是随机采样），用这个采样到的值进行识别精度的评估，重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围，最终确定超参数的合适范围(一般是 0.001~1000)\"></a>所谓逐渐缩小范围，是指一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样，最好是随机采样），用这个采样到的值进行识别精度的评估，重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围，最终确定超参数的合适范围(一般是 0.001~1000)</h4><h4 id=\"还可以使用贝叶斯最优化（Bayesian-optimization）\"><a href=\"#还可以使用贝叶斯最优化（Bayesian-optimization）\" class=\"headerlink\" title=\"还可以使用贝叶斯最优化（Bayesian optimization）\"></a>还可以使用贝叶斯最优化（Bayesian optimization）</h4></blockquote>\n</blockquote>\n<h3 id=\"code\"><a href=\"#code\" class=\"headerlink\" title=\"code\"></a>code</h3><blockquote>\n<h4 id=\"SGD\"><a href=\"#SGD\" class=\"headerlink\" title=\"SGD\"></a>SGD</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SGD</span>:</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">随机梯度下降法（Stochastic Gradient Descent）</span></span><br><span class=\"line\"><span class=\"string\">最基础的优化算法，直接使用梯度更新参数</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, lr=<span class=\"number\">0.01</span></span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">初始化SGD优化器</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">lr: 学习率，控制参数更新的步长</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.lr = lr</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">update</span>(<span class=\"params\">self, params, grads</span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">更新参数</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">params: 需要更新的参数字典</span></span><br><span class=\"line\"><span class=\"string\">grads: 对应的梯度字典</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> params.keys():</span><br><span class=\"line\">params[key] -= <span class=\"variable language_\">self</span>.lr * grads[key]  <span class=\"comment\"># 参数更新公式：θ = θ - lr * ∇θ</span></span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"Momentum\"><a href=\"#Momentum\" class=\"headerlink\" title=\"Momentum\"></a>Momentum</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Momentum</span>:</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">带动量的随机梯度下降法</span></span><br><span class=\"line\"><span class=\"string\">通过引入动量项来加速收敛，减少震荡</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, lr=<span class=\"number\">0.01</span>, momentum=<span class=\"number\">0.9</span></span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">初始化Momentum优化器</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">lr: 学习率</span></span><br><span class=\"line\"><span class=\"string\">momentum: 动量系数，通常设为0.9</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.lr = lr</span><br><span class=\"line\"><span class=\"variable language_\">self</span>.momentum = momentum</span><br><span class=\"line\"><span class=\"variable language_\">self</span>.v = <span class=\"literal\">None</span>  <span class=\"comment\"># 速度向量</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">update</span>(<span class=\"params\">self, params, grads</span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">更新参数</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">params: 需要更新的参数字典</span></span><br><span class=\"line\"><span class=\"string\">grads: 对应的梯度字典</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"variable language_\">self</span>.v <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\"><span class=\"comment\"># 第一次调用时初始化速度向量</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.v = &#123;&#125;</span><br><span class=\"line\"><span class=\"keyword\">for</span> key, val <span class=\"keyword\">in</span> params.items():                                </span><br><span class=\"line\">    <span class=\"variable language_\">self</span>.v[key] = np.zeros_like(val)</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> params.keys():</span><br><span class=\"line\"><span class=\"comment\"># 更新速度：v = momentum * v - lr * grad</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.v[key] = <span class=\"variable language_\">self</span>.momentum*<span class=\"variable language_\">self</span>.v[key] - <span class=\"variable language_\">self</span>.lr*grads[key] </span><br><span class=\"line\"><span class=\"comment\"># 更新参数：θ = θ + v</span></span><br><span class=\"line\">params[key] += <span class=\"variable language_\">self</span>.v[key]</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"AdaGrad\"><a href=\"#AdaGrad\" class=\"headerlink\" title=\"AdaGrad\"></a>AdaGrad</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">AdaGrad</span>:</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">AdaGrad（自适应梯度算法）</span></span><br><span class=\"line\"><span class=\"string\">为每个参数自适应地调整学习率，适合处理稀疏梯度</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, lr=<span class=\"number\">0.01</span></span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">初始化AdaGrad优化器</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">lr: 初始学习率</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.lr = lr</span><br><span class=\"line\"><span class=\"variable language_\">self</span>.h = <span class=\"literal\">None</span>  <span class=\"comment\"># 累积梯度平方和</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">update</span>(<span class=\"params\">self, params, grads</span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">更新参数</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">params: 需要更新的参数字典</span></span><br><span class=\"line\"><span class=\"string\">grads: 对应的梯度字典</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"variable language_\">self</span>.h <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\"><span class=\"comment\"># 第一次调用时初始化累积梯度平方和</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.h = &#123;&#125;</span><br><span class=\"line\"><span class=\"keyword\">for</span> key, val <span class=\"keyword\">in</span> params.items():</span><br><span class=\"line\">    <span class=\"variable language_\">self</span>.h[key] = np.zeros_like(val)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> params.keys():</span><br><span class=\"line\"><span class=\"comment\"># 累积梯度平方和：h = h + grad^2</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.h[key] += grads[key] * grads[key]</span><br><span class=\"line\"><span class=\"comment\"># 参数更新：θ = θ - lr * grad / sqrt(h + ε)</span></span><br><span class=\"line\">params[key] -= <span class=\"variable language_\">self</span>.lr * grads[key] / (np.sqrt(<span class=\"variable language_\">self</span>.h[key]) + <span class=\"number\">1e-7</span>)</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Adam</span>:</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">Adam（自适应矩估计算法）</span></span><br><span class=\"line\"><span class=\"string\">结合了Momentum和RMSprop的优点，是目前最常用的优化算法之一</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, lr=<span class=\"number\">0.001</span>, beta1=<span class=\"number\">0.9</span>, beta2=<span class=\"number\">0.999</span></span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">初始化Adam优化器</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">lr: 学习率</span></span><br><span class=\"line\"><span class=\"string\">beta1: 一阶矩估计的指数衰减率</span></span><br><span class=\"line\"><span class=\"string\">beta2: 二阶矩估计的指数衰减率</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.lr = lr</span><br><span class=\"line\"><span class=\"variable language_\">self</span>.beta1 = beta1</span><br><span class=\"line\"><span class=\"variable language_\">self</span>.beta2 = beta2</span><br><span class=\"line\"><span class=\"variable language_\">self</span>.<span class=\"built_in\">iter</span> = <span class=\"number\">0</span>  <span class=\"comment\"># 迭代次数</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.m = <span class=\"literal\">None</span>  <span class=\"comment\"># 一阶矩估计（梯度的指数移动平均）</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.v = <span class=\"literal\">None</span>  <span class=\"comment\"># 二阶矩估计（梯度平方的指数移动平均）</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">update</span>(<span class=\"params\">self, params, grads</span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">更新参数</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">params: 需要更新的参数字典</span></span><br><span class=\"line\"><span class=\"string\">grads: 对应的梯度字典</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"variable language_\">self</span>.m <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\"><span class=\"comment\"># 第一次调用时初始化</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.m, <span class=\"variable language_\">self</span>.v = &#123;&#125;, &#123;&#125;</span><br><span class=\"line\"><span class=\"keyword\">for</span> key, val <span class=\"keyword\">in</span> params.items():</span><br><span class=\"line\">    <span class=\"variable language_\">self</span>.m[key] = np.zeros_like(val)</span><br><span class=\"line\">    <span class=\"variable language_\">self</span>.v[key] = np.zeros_like(val)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.<span class=\"built_in\">iter</span> += <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"comment\"># 计算学习率的偏差修正</span></span><br><span class=\"line\">lr_t  = <span class=\"variable language_\">self</span>.lr * np.sqrt(<span class=\"number\">1.0</span> - <span class=\"variable language_\">self</span>.beta2**<span class=\"variable language_\">self</span>.<span class=\"built_in\">iter</span>) / (<span class=\"number\">1.0</span> - <span class=\"variable language_\">self</span>.beta1**<span class=\"variable language_\">self</span>.<span class=\"built_in\">iter</span>)         </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> params.keys():</span><br><span class=\"line\"><span class=\"comment\"># 更新一阶矩估计：m = beta1 * m + (1 - beta1) * grad</span></span><br><span class=\"line\"><span class=\"comment\">#self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]</span></span><br><span class=\"line\"><span class=\"comment\"># 更新二阶矩估计：v = beta2 * v + (1 - beta2) * grad^2</span></span><br><span class=\"line\"><span class=\"comment\">#self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 偏差修正的更新方式</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.m[key] += (<span class=\"number\">1</span> - <span class=\"variable language_\">self</span>.beta1) * (grads[key] - <span class=\"variable language_\">self</span>.m[key])</span><br><span class=\"line\"><span class=\"variable language_\">self</span>.v[key] += (<span class=\"number\">1</span> - <span class=\"variable language_\">self</span>.beta2) * (grads[key]**<span class=\"number\">2</span> - <span class=\"variable language_\">self</span>.v[key])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 参数更新：θ = θ - lr_t * m / sqrt(v + ε)</span></span><br><span class=\"line\">params[key] -= lr_t * <span class=\"variable language_\">self</span>.m[key] / (np.sqrt(<span class=\"variable language_\">self</span>.v[key]) + <span class=\"number\">1e-7</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 另一种偏差修正的实现方式（注释掉）</span></span><br><span class=\"line\"><span class=\"comment\">#unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias</span></span><br><span class=\"line\"><span class=\"comment\">#unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias</span></span><br><span class=\"line\"><span class=\"comment\">#params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)</span></span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"几种更新方法的比较\"><a href=\"#几种更新方法的比较\" class=\"headerlink\" title=\"几种更新方法的比较\"></a>几种更新方法的比较</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys, os</span><br><span class=\"line\">sys.path.append(os.pardir)  <span class=\"comment\"># 添加父目录到路径，以便导入common模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> OrderedDict</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.optimizer <span class=\"keyword\">import</span> *  <span class=\"comment\"># 导入所有优化器类</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">x, y</span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">定义目标函数：f(x,y) = x^2/20 + y^2</span></span><br><span class=\"line\"><span class=\"string\">这是一个简单的二次函数，用于测试优化算法的性能</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">x, y: 输入变量</span></span><br><span class=\"line\"><span class=\"string\">返回:</span></span><br><span class=\"line\"><span class=\"string\">函数值</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> x**<span class=\"number\">2</span> / <span class=\"number\">20.0</span> + y**<span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">df</span>(<span class=\"params\">x, y</span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">计算目标函数的梯度</span></span><br><span class=\"line\"><span class=\"string\">∂f/∂x = x/10, ∂f/∂y = 2y</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">x, y: 输入变量</span></span><br><span class=\"line\"><span class=\"string\">返回:</span></span><br><span class=\"line\"><span class=\"string\">(∂f/∂x, ∂f/∂y): 梯度向量</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> x / <span class=\"number\">10.0</span>, <span class=\"number\">2.0</span>*y</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置初始位置</span></span><br><span class=\"line\">init_pos = (-<span class=\"number\">7.0</span>, <span class=\"number\">2.0</span>)  <span class=\"comment\"># 初始点坐标</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 初始化参数字典</span></span><br><span class=\"line\">params = &#123;&#125;</span><br><span class=\"line\">params[<span class=\"string\">&#x27;x&#x27;</span>], params[<span class=\"string\">&#x27;y&#x27;</span>] = init_pos[<span class=\"number\">0</span>], init_pos[<span class=\"number\">1</span>]  <span class=\"comment\"># 将初始位置赋值给参数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 初始化梯度字典</span></span><br><span class=\"line\">grads = &#123;&#125;</span><br><span class=\"line\">grads[<span class=\"string\">&#x27;x&#x27;</span>], grads[<span class=\"string\">&#x27;y&#x27;</span>] = <span class=\"number\">0</span>, <span class=\"number\">0</span>  <span class=\"comment\"># 梯度初始化为0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建优化器字典，使用OrderedDict保持顺序</span></span><br><span class=\"line\">optimizers = OrderedDict()</span><br><span class=\"line\">optimizers[<span class=\"string\">&quot;SGD&quot;</span>] = SGD(lr=<span class=\"number\">0.95</span>)  <span class=\"comment\"># 随机梯度下降，学习率0.95</span></span><br><span class=\"line\">optimizers[<span class=\"string\">&quot;Momentum&quot;</span>] = Momentum(lr=<span class=\"number\">0.1</span>)  <span class=\"comment\"># 带动量的SGD，学习率0.1</span></span><br><span class=\"line\">optimizers[<span class=\"string\">&quot;AdaGrad&quot;</span>] = AdaGrad(lr=<span class=\"number\">1.5</span>)  <span class=\"comment\"># 自适应梯度算法，学习率1.5</span></span><br><span class=\"line\">optimizers[<span class=\"string\">&quot;Adam&quot;</span>] = Adam(lr=<span class=\"number\">0.3</span>)  <span class=\"comment\"># Adam优化器，学习率0.3</span></span><br><span class=\"line\"></span><br><span class=\"line\">idx = <span class=\"number\">1</span>  <span class=\"comment\"># 子图索引</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 遍历每种优化器</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> optimizers:</span><br><span class=\"line\">optimizer = optimizers[key]  <span class=\"comment\"># 获取当前优化器</span></span><br><span class=\"line\">x_history = []  <span class=\"comment\"># 记录x坐标的历史轨迹</span></span><br><span class=\"line\">y_history = []  <span class=\"comment\"># 记录y坐标的历史轨迹</span></span><br><span class=\"line\">params[<span class=\"string\">&#x27;x&#x27;</span>], params[<span class=\"string\">&#x27;y&#x27;</span>] = init_pos[<span class=\"number\">0</span>], init_pos[<span class=\"number\">1</span>]  <span class=\"comment\"># 重置参数到初始位置</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 进行30次优化迭代</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">30</span>):</span><br><span class=\"line\">x_history.append(params[<span class=\"string\">&#x27;x&#x27;</span>])  <span class=\"comment\"># 记录当前x位置</span></span><br><span class=\"line\">y_history.append(params[<span class=\"string\">&#x27;y&#x27;</span>])  <span class=\"comment\"># 记录当前y位置</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算当前点的梯度</span></span><br><span class=\"line\">grads[<span class=\"string\">&#x27;x&#x27;</span>], grads[<span class=\"string\">&#x27;y&#x27;</span>] = df(params[<span class=\"string\">&#x27;x&#x27;</span>], params[<span class=\"string\">&#x27;y&#x27;</span>])</span><br><span class=\"line\"><span class=\"comment\"># 使用优化器更新参数</span></span><br><span class=\"line\">optimizer.update(params, grads)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建网格用于绘制等高线图</span></span><br><span class=\"line\">x = np.arange(-<span class=\"number\">10</span>, <span class=\"number\">10</span>, <span class=\"number\">0.01</span>)  <span class=\"comment\"># x轴范围：-10到10，步长0.01</span></span><br><span class=\"line\">y = np.arange(-<span class=\"number\">5</span>, <span class=\"number\">5</span>, <span class=\"number\">0.01</span>)    <span class=\"comment\"># y轴范围：-5到5，步长0.01</span></span><br><span class=\"line\"></span><br><span class=\"line\">X, Y = np.meshgrid(x, y)  <span class=\"comment\"># 创建网格坐标</span></span><br><span class=\"line\">Z = f(X, Y)  <span class=\"comment\"># 计算网格上每个点的函数值</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 为了简化等高线图，将大于7的值设为0</span></span><br><span class=\"line\">mask = Z &gt; <span class=\"number\">7</span></span><br><span class=\"line\">Z[mask] = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制子图</span></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>, <span class=\"number\">2</span>, idx)  <span class=\"comment\"># 创建2x2的子图，当前是第idx个</span></span><br><span class=\"line\">idx += <span class=\"number\">1</span>  <span class=\"comment\"># 子图索引递增</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制优化轨迹</span></span><br><span class=\"line\">plt.plot(x_history, y_history, <span class=\"string\">&#x27;o-&#x27;</span>, color=<span class=\"string\">&quot;red&quot;</span>)  <span class=\"comment\"># 红色圆点连线表示优化路径</span></span><br><span class=\"line\">plt.contour(X, Y, Z)  <span class=\"comment\"># 绘制等高线</span></span><br><span class=\"line\">plt.ylim(-<span class=\"number\">10</span>, <span class=\"number\">10</span>)  <span class=\"comment\"># 设置y轴范围</span></span><br><span class=\"line\">plt.xlim(-<span class=\"number\">10</span>, <span class=\"number\">10</span>)  <span class=\"comment\"># 设置x轴范围</span></span><br><span class=\"line\">plt.plot(<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"string\">&#x27;+&#x27;</span>)  <span class=\"comment\"># 在原点(0,0)绘制加号，表示全局最优点</span></span><br><span class=\"line\"><span class=\"comment\">#colorbar()  # 注释掉的颜色条</span></span><br><span class=\"line\"><span class=\"comment\">#spring()   # 注释掉的弹簧布局</span></span><br><span class=\"line\">plt.title(key)  <span class=\"comment\"># 设置子图标题为优化器名称</span></span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;x&quot;</span>)  <span class=\"comment\"># 设置x轴标签</span></span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;y&quot;</span>)  <span class=\"comment\"># 设置y轴标签</span></span><br><span class=\"line\"></span><br><span class=\"line\">plt.show()  <span class=\"comment\"># 显示所有子图</span></span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"基于MNIST数据集的更新方法的比较\"><a href=\"#基于MNIST数据集的更新方法的比较\" class=\"headerlink\" title=\"基于MNIST数据集的更新方法的比较\"></a>基于MNIST数据集的更新方法的比较</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">sys.path.append(os.pardir)  <span class=\"comment\"># 添加父目录到路径，以便导入其他模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist  <span class=\"comment\"># 导入MNIST数据集加载函数</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> common.util <span class=\"keyword\">import</span> smooth_curve  <span class=\"comment\"># 导入平滑曲线函数，用于可视化</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> common.multi_layer_net <span class=\"keyword\">import</span> MultiLayerNet  <span class=\"comment\"># 导入多层神经网络类</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> common.optimizer <span class=\"keyword\">import</span> *  <span class=\"comment\"># 导入所有优化器类</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 0: 加载MNIST数据集 ==========</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>)  <span class=\"comment\"># 加载并归一化MNIST数据集</span></span><br><span class=\"line\"></span><br><span class=\"line\">train_size = x_train.shape[<span class=\"number\">0</span>]  <span class=\"comment\"># 训练集大小</span></span><br><span class=\"line\">batch_size = <span class=\"number\">128</span>  <span class=\"comment\"># 批量大小</span></span><br><span class=\"line\">max_iterations = <span class=\"number\">2000</span>  <span class=\"comment\"># 最大迭代次数</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 1: 实验设置 ==========</span></span><br><span class=\"line\"><span class=\"comment\"># 创建不同优化器的字典</span></span><br><span class=\"line\">optimizers = &#123;&#125;</span><br><span class=\"line\">optimizers[<span class=\"string\">&#x27;SGD&#x27;</span>] = SGD()  <span class=\"comment\"># 随机梯度下降</span></span><br><span class=\"line\">optimizers[<span class=\"string\">&#x27;Momentum&#x27;</span>] = Momentum()  <span class=\"comment\"># 带动量的SGD</span></span><br><span class=\"line\">optimizers[<span class=\"string\">&#x27;AdaGrad&#x27;</span>] = AdaGrad()  <span class=\"comment\"># 自适应梯度算法</span></span><br><span class=\"line\">optimizers[<span class=\"string\">&#x27;Adam&#x27;</span>] = Adam()  <span class=\"comment\"># Adam优化器</span></span><br><span class=\"line\"><span class=\"comment\">#optimizers[&#x27;RMSprop&#x27;] = RMSprop()  # RMSprop优化器（已注释）</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 为每个优化器创建对应的神经网络和损失记录</span></span><br><span class=\"line\">networks = &#123;&#125;  <span class=\"comment\"># 存储不同优化器对应的神经网络</span></span><br><span class=\"line\">train_loss = &#123;&#125;  <span class=\"comment\"># 存储不同优化器的训练损失</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> optimizers.keys():</span><br><span class=\"line\"><span class=\"comment\"># 创建具有4个隐藏层（每层100个神经元）的多层神经网络</span></span><br><span class=\"line\">networks[key] = MultiLayerNet(</span><br><span class=\"line\">input_size=<span class=\"number\">784</span>,  <span class=\"comment\"># 输入层大小（28x28=784）</span></span><br><span class=\"line\">hidden_size_list=[<span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>],  <span class=\"comment\"># 4个隐藏层，每层100个神经元</span></span><br><span class=\"line\">output_size=<span class=\"number\">10</span>)  <span class=\"comment\"># 输出层大小（10个类别）</span></span><br><span class=\"line\">train_loss[key] = []  <span class=\"comment\"># 初始化损失列表</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2: 开始训练 ==========</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(max_iterations):</span><br><span class=\"line\"><span class=\"comment\"># 随机选择批量数据</span></span><br><span class=\"line\">batch_mask = np.random.choice(train_size, batch_size)  <span class=\"comment\"># 随机选择batch_size个样本的索引</span></span><br><span class=\"line\">x_batch = x_train[batch_mask]  <span class=\"comment\"># 获取批量输入数据</span></span><br><span class=\"line\">t_batch = t_train[batch_mask]  <span class=\"comment\"># 获取批量标签数据</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 对每个优化器进行参数更新</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> optimizers.keys():</span><br><span class=\"line\"><span class=\"comment\"># 计算梯度</span></span><br><span class=\"line\">grads = networks[key].gradient(x_batch, t_batch)  <span class=\"comment\"># 计算当前批量的梯度</span></span><br><span class=\"line\"><span class=\"comment\"># 更新参数</span></span><br><span class=\"line\">optimizers[key].update(networks[key].params, grads)  <span class=\"comment\"># 使用优化器更新网络参数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算损失</span></span><br><span class=\"line\">loss = networks[key].loss(x_batch, t_batch)  <span class=\"comment\"># 计算当前批量的损失</span></span><br><span class=\"line\">train_loss[key].append(loss)  <span class=\"comment\"># 记录损失</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 每100次迭代打印一次训练状态</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;===========&quot;</span> + <span class=\"string\">&quot;iteration:&quot;</span> + <span class=\"built_in\">str</span>(i) + <span class=\"string\">&quot;===========&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> optimizers.keys():</span><br><span class=\"line\">loss = networks[key].loss(x_batch, t_batch)  <span class=\"comment\"># 计算当前批量的损失</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(key + <span class=\"string\">&quot;:&quot;</span> + <span class=\"built_in\">str</span>(loss))  <span class=\"comment\"># 打印每个优化器的损失</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3: 绘制图表 ==========</span></span><br><span class=\"line\"><span class=\"comment\"># 设置不同优化器的标记样式</span></span><br><span class=\"line\">markers = &#123;<span class=\"string\">&quot;SGD&quot;</span>: <span class=\"string\">&quot;o&quot;</span>, <span class=\"string\">&quot;Momentum&quot;</span>: <span class=\"string\">&quot;x&quot;</span>, <span class=\"string\">&quot;AdaGrad&quot;</span>: <span class=\"string\">&quot;s&quot;</span>, <span class=\"string\">&quot;Adam&quot;</span>: <span class=\"string\">&quot;D&quot;</span>&#125;</span><br><span class=\"line\">x = np.arange(max_iterations)  <span class=\"comment\"># 创建x轴数据（迭代次数）</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制每个优化器的损失曲线</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> optimizers.keys():</span><br><span class=\"line\"><span class=\"comment\"># 使用平滑曲线绘制损失，每100次迭代标记一次</span></span><br><span class=\"line\">plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=<span class=\"number\">100</span>, label=key)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置图表属性</span></span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;iterations&quot;</span>)  <span class=\"comment\"># x轴标签：迭代次数</span></span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;loss&quot;</span>)  <span class=\"comment\"># y轴标签：损失值</span></span><br><span class=\"line\">plt.ylim(<span class=\"number\">0</span>, <span class=\"number\">1</span>)  <span class=\"comment\"># 设置y轴范围：0到1</span></span><br><span class=\"line\">plt.legend()  <span class=\"comment\"># 显示图例</span></span><br><span class=\"line\">plt.show()  <span class=\"comment\"># 显示图表</span></span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"隐藏层的激活值的分布（设计两种初始值Xavier和He初始值（作为权重的初始值），分别适用于不同的激活函数）\"><a href=\"#隐藏层的激活值的分布（设计两种初始值Xavier和He初始值（作为权重的初始值），分别适用于不同的激活函数）\" class=\"headerlink\" title=\"隐藏层的激活值的分布（设计两种初始值Xavier和He初始值（作为权重的初始值），分别适用于不同的激活函数）\"></a>隐藏层的激活值的分布（设计两种初始值Xavier和He初始值（作为权重的初始值），分别适用于不同的激活函数）</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sigmoid</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">Sigmoid激活函数</span></span><br><span class=\"line\"><span class=\"string\">将输入值压缩到(0,1)区间</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> <span class=\"number\">1</span> / (<span class=\"number\">1</span> + np.exp(-x))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">ReLU</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">ReLU激活函数</span></span><br><span class=\"line\"><span class=\"string\">当输入大于0时返回原值，小于0时返回0</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> np.maximum(<span class=\"number\">0</span>, x)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">tanh</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">tanh激活函数</span></span><br><span class=\"line\"><span class=\"string\">将输入值压缩到(-1,1)区间</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> np.tanh(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 生成1000个样本，每个样本100个特征,x 就是高斯分布随机数</span></span><br><span class=\"line\">input_data = np.random.randn(<span class=\"number\">1000</span>, <span class=\"number\">100</span>)  <span class=\"comment\"># 且符合标准正态分布，也就是高斯分布</span></span><br><span class=\"line\">node_num = <span class=\"number\">100</span></span><br><span class=\"line\">hidden_layer_size = <span class=\"number\">5</span></span><br><span class=\"line\">activations = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">x = input_data</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 模拟5层神经网络的前向传播过程</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(hidden_layer_size):</span><br><span class=\"line\"><span class=\"keyword\">if</span> i != <span class=\"number\">0</span>:</span><br><span class=\"line\">x = activations[i-<span class=\"number\">1</span>]  <span class=\"comment\"># 使用上一层的输出作为当前层的输入</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 权重初始化实验</span></span><br><span class=\"line\"><span class=\"comment\"># 可以尝试不同的权重初始化方法</span></span><br><span class=\"line\">w = np.random.randn(node_num, node_num) * <span class=\"number\">1</span>  <span class=\"comment\"># 标准正态分布</span></span><br><span class=\"line\"><span class=\"comment\"># w = np.random.randn(node_num, node_num) * 0.01  # 缩小100倍</span></span><br><span class=\"line\"><span class=\"comment\"># w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)  # Xavier初始化</span></span><br><span class=\"line\"><span class=\"comment\"># w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)  # He初始化</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 线性变换</span></span><br><span class=\"line\">a = np.dot(x, w)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 激活函数实验</span></span><br><span class=\"line\"><span class=\"comment\"># 可以尝试不同的激活函数</span></span><br><span class=\"line\">z = sigmoid(a)  <span class=\"comment\"># Sigmoid激活</span></span><br><span class=\"line\"><span class=\"comment\"># z = ReLU(a)    # ReLU激活</span></span><br><span class=\"line\"><span class=\"comment\"># z = tanh(a)    # tanh激活</span></span><br><span class=\"line\"></span><br><span class=\"line\">activations[i] = z  <span class=\"comment\"># 保存当前层的激活值</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制每层激活值的分布直方图</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i, a <span class=\"keyword\">in</span> activations.items():</span><br><span class=\"line\">plt.subplot(<span class=\"number\">1</span>, <span class=\"built_in\">len</span>(activations), i+<span class=\"number\">1</span>)  <span class=\"comment\"># 创建子图</span></span><br><span class=\"line\">plt.title(<span class=\"built_in\">str</span>(i+<span class=\"number\">1</span>) + <span class=\"string\">&quot;-layer&quot;</span>)  <span class=\"comment\"># 设置标题</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> i != <span class=\"number\">0</span>: plt.yticks([], [])  <span class=\"comment\"># 除第一层外，隐藏y轴刻度</span></span><br><span class=\"line\"><span class=\"comment\"># plt.xlim(0.1, 1)  # 可以设置x轴范围</span></span><br><span class=\"line\"><span class=\"comment\"># plt.ylim(0, 7000)  # 可以设置y轴范围</span></span><br><span class=\"line\">plt.hist(a.flatten(), <span class=\"number\">30</span>, <span class=\"built_in\">range</span>=(<span class=\"number\">0</span>,<span class=\"number\">1</span>))  <span class=\"comment\"># 绘制直方图，30个bin，范围0-1</span></span><br><span class=\"line\">plt.show()  <span class=\"comment\"># 显示图形</span></span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"基于MNIST数据集的权重初始值的比较\"><a href=\"#基于MNIST数据集的权重初始值的比较\" class=\"headerlink\" title=\"基于MNIST数据集的权重初始值的比较\"></a>基于MNIST数据集的权重初始值的比较</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 添加父目录到系统路径，以便导入common模块</span></span><br><span class=\"line\">sys.path.append(os.pardir)</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.util <span class=\"keyword\">import</span> smooth_curve</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.multi_layer_net <span class=\"keyword\">import</span> MultiLayerNet</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.optimizer <span class=\"keyword\">import</span> SGD</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载MNIST数据集并进行归一化处理</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置训练参数</span></span><br><span class=\"line\">train_size = x_train.shape[<span class=\"number\">0</span>]  <span class=\"comment\"># 训练数据总数</span></span><br><span class=\"line\">batch_size = <span class=\"number\">128</span>  <span class=\"comment\"># 批次大小</span></span><br><span class=\"line\">max_iterations = <span class=\"number\">2000</span>  <span class=\"comment\"># 最大迭代次数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义不同的权重初始化方法</span></span><br><span class=\"line\"><span class=\"comment\"># std=0.01: 标准差为0.01的正态分布</span></span><br><span class=\"line\"><span class=\"comment\"># Xavier: Xavier初始化（适用于sigmoid激活函数）</span></span><br><span class=\"line\"><span class=\"comment\"># He: He初始化（适用于ReLU激活函数）</span></span><br><span class=\"line\">weight_init_types = &#123;<span class=\"string\">&#x27;std=0.01&#x27;</span>: <span class=\"number\">0.01</span>, <span class=\"string\">&#x27;Xavier&#x27;</span>: <span class=\"string\">&#x27;sigmoid&#x27;</span>, <span class=\"string\">&#x27;He&#x27;</span>: <span class=\"string\">&#x27;relu&#x27;</span>&#125;</span><br><span class=\"line\">optimizer = SGD(lr=<span class=\"number\">0.01</span>)  <span class=\"comment\"># 随机梯度下降优化器，学习率为0.01</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建不同初始化方法的神经网络和损失记录</span></span><br><span class=\"line\">networks = &#123;&#125;</span><br><span class=\"line\">train_loss = &#123;&#125;</span><br><span class=\"line\"><span class=\"keyword\">for</span> key, weight_type <span class=\"keyword\">in</span> weight_init_types.items():</span><br><span class=\"line\"><span class=\"comment\"># 创建多层神经网络：输入784维，4个隐藏层各100个神经元，输出10维</span></span><br><span class=\"line\">networks[key] = MultiLayerNet(input_size=<span class=\"number\">784</span>, hidden_size_list=[<span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>],</span><br><span class=\"line\">                            output_size=<span class=\"number\">10</span>, weight_init_std=weight_type)</span><br><span class=\"line\">train_loss[key] = []  <span class=\"comment\"># 初始化损失记录列表</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 开始训练循环</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(max_iterations):</span><br><span class=\"line\"><span class=\"comment\"># 随机选择批次数据</span></span><br><span class=\"line\">batch_mask = np.random.choice(train_size, batch_size)</span><br><span class=\"line\">x_batch = x_train[batch_mask]</span><br><span class=\"line\">t_batch = t_train[batch_mask]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 对每种权重初始化方法进行训练</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> weight_init_types.keys():</span><br><span class=\"line\">  <span class=\"comment\"># 计算梯度</span></span><br><span class=\"line\">  grads = networks[key].gradient(x_batch, t_batch)</span><br><span class=\"line\">  <span class=\"comment\"># 更新网络参数</span></span><br><span class=\"line\">  optimizer.update(networks[key].params, grads)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\"># 计算并记录损失</span></span><br><span class=\"line\">  loss = networks[key].loss(x_batch, t_batch)</span><br><span class=\"line\">  train_loss[key].append(loss)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 每100次迭代打印一次损失值</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">  <span class=\"built_in\">print</span>(<span class=\"string\">&quot;===========&quot;</span> + <span class=\"string\">&quot;iteration:&quot;</span> + <span class=\"built_in\">str</span>(i) + <span class=\"string\">&quot;===========&quot;</span>)</span><br><span class=\"line\">  <span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> weight_init_types.keys():</span><br><span class=\"line\">      loss = networks[key].loss(x_batch, t_batch)</span><br><span class=\"line\">      <span class=\"built_in\">print</span>(key + <span class=\"string\">&quot;:&quot;</span> + <span class=\"built_in\">str</span>(loss))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制训练损失曲线</span></span><br><span class=\"line\">markers = &#123;<span class=\"string\">&#x27;std=0.01&#x27;</span>: <span class=\"string\">&#x27;o&#x27;</span>, <span class=\"string\">&#x27;Xavier&#x27;</span>: <span class=\"string\">&#x27;s&#x27;</span>, <span class=\"string\">&#x27;He&#x27;</span>: <span class=\"string\">&#x27;D&#x27;</span>&#125;  <span class=\"comment\"># 不同方法的标记符号</span></span><br><span class=\"line\">x = np.arange(max_iterations)  <span class=\"comment\"># x轴：迭代次数</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> weight_init_types.keys():</span><br><span class=\"line\"><span class=\"comment\"># 绘制平滑后的损失曲线，每100次迭代显示一个标记</span></span><br><span class=\"line\">plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=<span class=\"number\">100</span>, label=key)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;iterations&quot;</span>)  <span class=\"comment\"># x轴标签</span></span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;loss&quot;</span>)  <span class=\"comment\"># y轴标签</span></span><br><span class=\"line\">plt.ylim(<span class=\"number\">0</span>, <span class=\"number\">2.5</span>)  <span class=\"comment\"># 设置y轴范围</span></span><br><span class=\"line\">plt.legend()  <span class=\"comment\"># 显示图例</span></span><br><span class=\"line\">plt.show()  <span class=\"comment\"># 显示图形</span></span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"Batch-Normalization的评估-1\"><a href=\"#Batch-Normalization的评估-1\" class=\"headerlink\" title=\"Batch Normalization的评估\"></a>Batch Normalization的评估</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"><span class=\"comment\"># 本脚本用于对比带有批归一化（Batch Normalization）和不带批归一化的多层神经网络在不同权重初始值下的训练表现。</span></span><br><span class=\"line\"><span class=\"comment\"># 通过在MNIST数据集上训练网络，观察批归一化对训练收敛速度和准确率的影响。</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys, os</span><br><span class=\"line\">sys.path.append(os.pardir)  <span class=\"comment\"># 将父目录加入sys.path，便于导入上级目录中的模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist  <span class=\"comment\"># 导入MNIST数据集加载函数</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> common.multi_layer_net_extend <span class=\"keyword\">import</span> MultiLayerNetExtend  <span class=\"comment\"># 导入可扩展多层网络实现</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> common.optimizer <span class=\"keyword\">import</span> SGD, Adam  <span class=\"comment\"># 导入优化器</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载MNIST数据集，并进行归一化处理</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 为了加快实验速度，仅取前1000个训练样本</span></span><br><span class=\"line\">x_train = x_train[:<span class=\"number\">1000</span>]</span><br><span class=\"line\">t_train = t_train[:<span class=\"number\">1000</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">max_epochs = <span class=\"number\">20</span>  <span class=\"comment\"># 最大训练轮数</span></span><br><span class=\"line\">train_size = x_train.shape[<span class=\"number\">0</span>]  <span class=\"comment\"># 训练集样本数</span></span><br><span class=\"line\">batch_size = <span class=\"number\">100</span>  <span class=\"comment\"># 每个批次的样本数</span></span><br><span class=\"line\">learning_rate = <span class=\"number\">0.01</span>  <span class=\"comment\"># 学习率</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">__train</span>(<span class=\"params\">weight_init_std</span>):</span><br><span class=\"line\"> <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\"> 训练带有和不带有批归一化的神经网络。</span></span><br><span class=\"line\"><span class=\"string\"> 参数：</span></span><br><span class=\"line\"><span class=\"string\">     weight_init_std: 权重初始化的标准差</span></span><br><span class=\"line\"><span class=\"string\"> 返回：</span></span><br><span class=\"line\"><span class=\"string\">     train_acc_list: 不带批归一化的网络在每个epoch的训练准确率</span></span><br><span class=\"line\"><span class=\"string\">     bn_train_acc_list: 带批归一化的网络在每个epoch的训练准确率</span></span><br><span class=\"line\"><span class=\"string\"> &quot;&quot;&quot;</span></span><br><span class=\"line\"> <span class=\"comment\"># 构建带批归一化的网络</span></span><br><span class=\"line\"> bn_network = MultiLayerNetExtend(input_size=<span class=\"number\">784</span>, hidden_size_list=[<span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>], output_size=<span class=\"number\">10</span>, </span><br><span class=\"line\">                                 weight_init_std=weight_init_std, use_batchnorm=<span class=\"literal\">True</span>)</span><br><span class=\"line\"> <span class=\"comment\"># 构建不带批归一化的网络</span></span><br><span class=\"line\"> network = MultiLayerNetExtend(input_size=<span class=\"number\">784</span>, hidden_size_list=[<span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>], output_size=<span class=\"number\">10</span>,</span><br><span class=\"line\">                             weight_init_std=weight_init_std)</span><br><span class=\"line\"> optimizer = SGD(lr=learning_rate)  <span class=\"comment\"># 使用SGD优化器</span></span><br><span class=\"line\"> </span><br><span class=\"line\"> train_acc_list = []  <span class=\"comment\"># 记录不带BN的准确率</span></span><br><span class=\"line\"> bn_train_acc_list = []  <span class=\"comment\"># 记录带BN的准确率</span></span><br><span class=\"line\"> </span><br><span class=\"line\"> iter_per_epoch = <span class=\"built_in\">max</span>(train_size / batch_size, <span class=\"number\">1</span>)  <span class=\"comment\"># 每个epoch的迭代次数</span></span><br><span class=\"line\"> epoch_cnt = <span class=\"number\">0</span>  <span class=\"comment\"># 当前epoch计数</span></span><br><span class=\"line\"> </span><br><span class=\"line\"> <span class=\"comment\"># 训练循环</span></span><br><span class=\"line\"> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1000000000</span>):  <span class=\"comment\"># 迭代次数设置为极大，实际会在达到max_epochs时break</span></span><br><span class=\"line\">     <span class=\"comment\"># 随机采样一个batch</span></span><br><span class=\"line\">     batch_mask = np.random.choice(train_size, batch_size)</span><br><span class=\"line\">     x_batch = x_train[batch_mask]</span><br><span class=\"line\">     t_batch = t_train[batch_mask]</span><br><span class=\"line\"> </span><br><span class=\"line\">     <span class=\"comment\"># 对两个网络分别进行反向传播和参数更新</span></span><br><span class=\"line\">     <span class=\"keyword\">for</span> _network <span class=\"keyword\">in</span> (bn_network, network):</span><br><span class=\"line\">         grads = _network.gradient(x_batch, t_batch)</span><br><span class=\"line\">         optimizer.update(_network.params, grads)</span><br><span class=\"line\"> </span><br><span class=\"line\">     <span class=\"comment\"># 每经过一个epoch，记录一次准确率</span></span><br><span class=\"line\">     <span class=\"keyword\">if</span> i % iter_per_epoch == <span class=\"number\">0</span>:</span><br><span class=\"line\">         train_acc = network.accuracy(x_train, t_train)</span><br><span class=\"line\">         bn_train_acc = bn_network.accuracy(x_train, t_train)</span><br><span class=\"line\">         train_acc_list.append(train_acc)</span><br><span class=\"line\">         bn_train_acc_list.append(bn_train_acc)</span><br><span class=\"line\"> </span><br><span class=\"line\">         <span class=\"built_in\">print</span>(<span class=\"string\">&quot;epoch:&quot;</span> + <span class=\"built_in\">str</span>(epoch_cnt) + <span class=\"string\">&quot; | &quot;</span> + <span class=\"built_in\">str</span>(train_acc) + <span class=\"string\">&quot; - &quot;</span> + <span class=\"built_in\">str</span>(bn_train_acc))</span><br><span class=\"line\"> </span><br><span class=\"line\">         epoch_cnt += <span class=\"number\">1</span></span><br><span class=\"line\">         <span class=\"keyword\">if</span> epoch_cnt &gt;= max_epochs:</span><br><span class=\"line\">             <span class=\"keyword\">break</span></span><br><span class=\"line\">             </span><br><span class=\"line\"> <span class=\"keyword\">return</span> train_acc_list, bn_train_acc_list</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 生成16个权重初始化标准差，从1到1e-4，等比数列</span></span><br><span class=\"line\">weight_scale_list = np.logspace(<span class=\"number\">0</span>, -<span class=\"number\">4</span>, num=<span class=\"number\">16</span>)</span><br><span class=\"line\">x = np.arange(max_epochs)  <span class=\"comment\"># x轴为epoch数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 对每个权重初始化标准差分别进行实验</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i, w <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(weight_scale_list):</span><br><span class=\"line\"> <span class=\"built_in\">print</span>( <span class=\"string\">&quot;============== &quot;</span> + <span class=\"built_in\">str</span>(i+<span class=\"number\">1</span>) + <span class=\"string\">&quot;/16&quot;</span> + <span class=\"string\">&quot; ==============&quot;</span>)</span><br><span class=\"line\"> train_acc_list, bn_train_acc_list = __train(w)</span><br><span class=\"line\"> </span><br><span class=\"line\"> <span class=\"comment\"># 绘制每组实验的准确率曲线</span></span><br><span class=\"line\"> plt.subplot(<span class=\"number\">4</span>,<span class=\"number\">4</span>,i+<span class=\"number\">1</span>)</span><br><span class=\"line\"> plt.title(<span class=\"string\">&quot;W:&quot;</span> + <span class=\"built_in\">str</span>(w))</span><br><span class=\"line\"> <span class=\"keyword\">if</span> i == <span class=\"number\">15</span>:</span><br><span class=\"line\">     plt.plot(x, bn_train_acc_list, label=<span class=\"string\">&#x27;Batch Normalization&#x27;</span>, markevery=<span class=\"number\">2</span>)</span><br><span class=\"line\">     plt.plot(x, train_acc_list, linestyle = <span class=\"string\">&quot;--&quot;</span>, label=<span class=\"string\">&#x27;Normal(without BatchNorm)&#x27;</span>, markevery=<span class=\"number\">2</span>)</span><br><span class=\"line\"> <span class=\"keyword\">else</span>:</span><br><span class=\"line\">     plt.plot(x, bn_train_acc_list, markevery=<span class=\"number\">2</span>)</span><br><span class=\"line\">     plt.plot(x, train_acc_list, linestyle=<span class=\"string\">&quot;--&quot;</span>, markevery=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"> plt.ylim(<span class=\"number\">0</span>, <span class=\"number\">1.0</span>)  <span class=\"comment\"># y轴范围</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> i % <span class=\"number\">4</span>:</span><br><span class=\"line\">     plt.yticks([])</span><br><span class=\"line\"> <span class=\"keyword\">else</span>:</span><br><span class=\"line\">     plt.ylabel(<span class=\"string\">&quot;accuracy&quot;</span>)</span><br><span class=\"line\"> <span class=\"keyword\">if</span> i &lt; <span class=\"number\">12</span>:</span><br><span class=\"line\">     plt.xticks([])</span><br><span class=\"line\"> <span class=\"keyword\">else</span>:</span><br><span class=\"line\">     plt.xlabel(<span class=\"string\">&quot;epochs&quot;</span>)</span><br><span class=\"line\"> plt.legend(loc=<span class=\"string\">&#x27;lower right&#x27;</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">plt.show()  <span class=\"comment\"># 显示所有子图</span></span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"Dropout的实现\"><a href=\"#Dropout的实现\" class=\"headerlink\" title=\"Dropout的实现\"></a>Dropout的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Dropout</span>:</span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, dropout_ratio=<span class=\"number\">0.5</span></span>):</span><br><span class=\"line\">     slef.dropout_ratio = dropout_ratio</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.mask = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self,x, train_flag=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">     <span class=\"keyword\">if</span> train_flag:</span><br><span class=\"line\">         <span class=\"variable language_\">self</span>.mask = np.random.rand(*x.shape) &gt; <span class=\"variable language_\">self</span>.dropout_ratio</span><br><span class=\"line\">         <span class=\"keyword\">return</span> x * <span class=\"variable language_\">self</span>.mask</span><br><span class=\"line\">     <span class=\"keyword\">else</span>:</span><br><span class=\"line\">         <span class=\"keyword\">return</span> x * (<span class=\"number\">1.0</span> - <span class=\"variable language_\">self</span>.dropout_ratio)</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">backward</span>(<span class=\"params\">self, dout</span>):</span><br><span class=\"line\">     <span class=\"keyword\">return</span> dout * <span class=\"variable language_\">self</span>.mask</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"使用Mnist数据集进行验证Dropout的效果\"><a href=\"#使用Mnist数据集进行验证Dropout的效果\" class=\"headerlink\" title=\"使用Mnist数据集进行验证Dropout的效果\"></a>使用Mnist数据集进行验证Dropout的效果</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"><span class=\"comment\"># 本脚本用于实验Dropout对深层神经网络过拟合的抑制作用。</span></span><br><span class=\"line\"><span class=\"comment\"># 通过在MNIST数据集上训练一个较深的网络，观察在有无Dropout时训练集和测试集准确率的变化。</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">sys.path.append(os.pardir)  <span class=\"comment\"># 将父目录加入sys.path，便于导入上级目录中的模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist  <span class=\"comment\"># 导入MNIST数据集加载函数</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> common.multi_layer_net_extend <span class=\"keyword\">import</span> MultiLayerNetExtend  <span class=\"comment\"># 导入可扩展多层神经网络实现（支持Dropout）</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> common.trainer <span class=\"keyword\">import</span> Trainer  <span class=\"comment\"># 导入训练器类，简化训练流程</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载MNIST数据集，并进行归一化处理</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 为了更容易出现过拟合，仅取前300个训练样本</span></span><br><span class=\"line\">x_train = x_train[:<span class=\"number\">300</span>]</span><br><span class=\"line\">t_train = t_train[:<span class=\"number\">300</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置Dropout参数</span></span><br><span class=\"line\">use_dropout = <span class=\"literal\">True</span>  <span class=\"comment\"># 是否使用Dropout</span></span><br><span class=\"line\"><span class=\"comment\"># dropout_ratio为每层神经元被随机丢弃的比例，常用0.2~0.5</span></span><br><span class=\"line\"><span class=\"comment\"># 若不使用Dropout，可将use_dropout设为False</span></span><br><span class=\"line\"><span class=\"comment\"># ====================================================</span></span><br><span class=\"line\">dropout_ratio = <span class=\"number\">0.2</span>  <span class=\"comment\"># Dropout比例</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 构建一个6层隐藏层的全连接神经网络，设置Dropout参数</span></span><br><span class=\"line\">network = MultiLayerNetExtend(input_size=<span class=\"number\">784</span>, hidden_size_list=[<span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>],</span><br><span class=\"line\">                           output_size=<span class=\"number\">10</span>, use_dropout=use_dropout, dropout_ration=dropout_ratio)</span><br><span class=\"line\"><span class=\"comment\"># 使用Trainer类进行训练，自动完成mini-batch梯度下降、准确率记录等</span></span><br><span class=\"line\">trainer = Trainer(network, x_train, t_train, x_test, t_test,</span><br><span class=\"line\">               epochs=<span class=\"number\">301</span>, mini_batch_size=<span class=\"number\">100</span>,</span><br><span class=\"line\">               optimizer=<span class=\"string\">&#x27;sgd&#x27;</span>, optimizer_param=&#123;<span class=\"string\">&#x27;lr&#x27;</span>: <span class=\"number\">0.01</span>&#125;, verbose=<span class=\"literal\">True</span>)</span><br><span class=\"line\">trainer.train()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 获取训练集和测试集的准确率变化曲线</span></span><br><span class=\"line\">train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制训练集和测试集的准确率曲线</span></span><br><span class=\"line\">markers = &#123;<span class=\"string\">&#x27;train&#x27;</span>: <span class=\"string\">&#x27;o&#x27;</span>, <span class=\"string\">&#x27;test&#x27;</span>: <span class=\"string\">&#x27;s&#x27;</span>&#125;</span><br><span class=\"line\">x = np.arange(<span class=\"built_in\">len</span>(train_acc_list))</span><br><span class=\"line\">plt.plot(x, train_acc_list, marker=<span class=\"string\">&#x27;o&#x27;</span>, label=<span class=\"string\">&#x27;train&#x27;</span>, markevery=<span class=\"number\">10</span>)</span><br><span class=\"line\">plt.plot(x, test_acc_list, marker=<span class=\"string\">&#x27;s&#x27;</span>, label=<span class=\"string\">&#x27;test&#x27;</span>, markevery=<span class=\"number\">10</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;epochs&quot;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;accuracy&quot;</span>)</span><br><span class=\"line\">plt.ylim(<span class=\"number\">0</span>, <span class=\"number\">1.0</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;lower right&#x27;</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"超参数最优化的实现\"><a href=\"#超参数最优化的实现\" class=\"headerlink\" title=\"超参数最优化的实现\"></a>超参数最优化的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys, os</span><br><span class=\"line\">sys.path.append(os.pardir)  <span class=\"comment\"># 将父目录添加到sys.path，便于导入上级目录的模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.multi_layer_net <span class=\"keyword\">import</span> MultiLayerNet</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.util <span class=\"keyword\">import</span> shuffle_dataset</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.trainer <span class=\"keyword\">import</span> Trainer</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载MNIST数据集，并进行归一化处理</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 只取前500个训练样本，减少计算量，加快超参数搜索</span></span><br><span class=\"line\">x_train = x_train[:<span class=\"number\">500</span>]</span><br><span class=\"line\">t_train = t_train[:<span class=\"number\">500</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置验证集比例为20%</span></span><br><span class=\"line\">validation_rate = <span class=\"number\">0.20</span></span><br><span class=\"line\">validation_num = <span class=\"built_in\">int</span>(x_train.shape[<span class=\"number\">0</span>] * validation_rate)</span><br><span class=\"line\"><span class=\"comment\"># 先打乱数据，保证训练集和验证集的分布一致</span></span><br><span class=\"line\">x_train, t_train = shuffle_dataset(x_train, t_train)</span><br><span class=\"line\">x_val = x_train[:validation_num]</span><br><span class=\"line\">t_val = t_train[:validation_num]</span><br><span class=\"line\">x_train = x_train[validation_num:]</span><br><span class=\"line\">t_train = t_train[validation_num:]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义训练函数，输入学习率和权重衰减，返回每轮的验证集和训练集准确率</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">__train</span>(<span class=\"params\">lr, weight_decay, epocs=<span class=\"number\">50</span></span>):</span><br><span class=\"line\">    <span class=\"comment\"># 构建一个6层隐藏层的全连接神经网络，带权重衰减</span></span><br><span class=\"line\">    network = MultiLayerNet(input_size=<span class=\"number\">784</span>, hidden_size_list=[<span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>],</span><br><span class=\"line\">                            output_size=<span class=\"number\">10</span>, weight_decay_lambda=weight_decay)</span><br><span class=\"line\">    <span class=\"comment\"># 构建训练器，使用SGD优化器</span></span><br><span class=\"line\">    trainer = Trainer(network, x_train, t_train, x_val, t_val,</span><br><span class=\"line\">                      epochs=epocs, mini_batch_size=<span class=\"number\">100</span>,</span><br><span class=\"line\">                      optimizer=<span class=\"string\">&#x27;sgd&#x27;</span>, optimizer_param=&#123;<span class=\"string\">&#x27;lr&#x27;</span>: lr&#125;, verbose=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    trainer.train()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> trainer.test_acc_list, trainer.train_acc_list</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 超参数优化实验次数</span></span><br><span class=\"line\">optimization_trial = <span class=\"number\">100</span></span><br><span class=\"line\">results_val = &#123;&#125;   <span class=\"comment\"># 存储每组超参数下的验证集准确率</span></span><br><span class=\"line\">results_train = &#123;&#125; <span class=\"comment\"># 存储每组超参数下的训练集准确率</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(optimization_trial):</span><br><span class=\"line\">    <span class=\"comment\"># 随机采样权重衰减和学习率（对数均匀分布采样，覆盖大范围）</span></span><br><span class=\"line\">    weight_decay = <span class=\"number\">10</span> ** np.random.uniform(-<span class=\"number\">8</span>, -<span class=\"number\">4</span>)</span><br><span class=\"line\">    lr = <span class=\"number\">10</span> ** np.random.uniform(-<span class=\"number\">6</span>, -<span class=\"number\">2</span>)</span><br><span class=\"line\">    <span class=\"comment\"># ================================================</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 训练网络并记录结果</span></span><br><span class=\"line\">    val_acc_list, train_acc_list = __train(lr, weight_decay)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;val acc:&quot;</span> + <span class=\"built_in\">str</span>(val_acc_list[-<span class=\"number\">1</span>]) + <span class=\"string\">&quot; | lr:&quot;</span> + <span class=\"built_in\">str</span>(lr) + <span class=\"string\">&quot;, weight decay:&quot;</span> + <span class=\"built_in\">str</span>(weight_decay))</span><br><span class=\"line\">    key = <span class=\"string\">&quot;lr:&quot;</span> + <span class=\"built_in\">str</span>(lr) + <span class=\"string\">&quot;, weight decay:&quot;</span> + <span class=\"built_in\">str</span>(weight_decay)</span><br><span class=\"line\">    results_val[key] = val_acc_list</span><br><span class=\"line\">    results_train[key] = train_acc_list</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出超参数优化结果</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;=========== Hyper-Parameter Optimization Result ===========&quot;</span>)</span><br><span class=\"line\">graph_draw_num = <span class=\"number\">20</span>  <span class=\"comment\"># 最多画出前20组最优超参数的曲线</span></span><br><span class=\"line\">col_num = <span class=\"number\">5</span></span><br><span class=\"line\">row_num = <span class=\"built_in\">int</span>(np.ceil(graph_draw_num / col_num))</span><br><span class=\"line\">i = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 按验证集最终准确率从高到低排序，依次画出前20组的准确率曲线</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key, val_acc_list <span class=\"keyword\">in</span> <span class=\"built_in\">sorted</span>(results_val.items(), key=<span class=\"keyword\">lambda</span> x:x[<span class=\"number\">1</span>][-<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Best-&quot;</span> + <span class=\"built_in\">str</span>(i+<span class=\"number\">1</span>) + <span class=\"string\">&quot;(val acc:&quot;</span> + <span class=\"built_in\">str</span>(val_acc_list[-<span class=\"number\">1</span>]) + <span class=\"string\">&quot;) | &quot;</span> + key)</span><br><span class=\"line\"></span><br><span class=\"line\">    plt.subplot(row_num, col_num, i+<span class=\"number\">1</span>)</span><br><span class=\"line\">    plt.title(<span class=\"string\">&quot;Best-&quot;</span> + <span class=\"built_in\">str</span>(i+<span class=\"number\">1</span>))</span><br><span class=\"line\">    plt.ylim(<span class=\"number\">0.0</span>, <span class=\"number\">1.0</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> i % <span class=\"number\">5</span>: plt.yticks([])</span><br><span class=\"line\">    plt.xticks([])</span><br><span class=\"line\">    x = np.arange(<span class=\"built_in\">len</span>(val_acc_list))</span><br><span class=\"line\">    plt.plot(x, val_acc_list)           <span class=\"comment\"># 验证集准确率</span></span><br><span class=\"line\">    plt.plot(x, results_train[key], <span class=\"string\">&quot;--&quot;</span>) <span class=\"comment\"># 训练集准确率（虚线）</span></span><br><span class=\"line\">    i += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> i &gt;= graph_draw_num:</span><br><span class=\"line\">        <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n</blockquote>\n<h3 id=\"晕了，完全晕了！\"><a href=\"#晕了，完全晕了！\" class=\"headerlink\" title=\"晕了，完全晕了！\"></a>晕了，完全晕了！</h3>","cover_type":"img","excerpt":"","more":"<h1 id=\"与学习相关的技巧\"><a href=\"#与学习相关的技巧\" class=\"headerlink\" title=\"与学习相关的技巧\"></a>与学习相关的技巧</h1><h2 id=\"参数的更新\"><a href=\"#参数的更新\" class=\"headerlink\" title=\"参数的更新\"></a>参数的更新</h2><blockquote>\n<h4 id=\"神经网络的学习的目的是找到使损失函数的值尽可能小的参数，这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）\"><a href=\"#神经网络的学习的目的是找到使损失函数的值尽可能小的参数，这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）\" class=\"headerlink\" title=\"神经网络的学习的目的是找到使损失函数的值尽可能小的参数，这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）\"></a>神经网络的学习的目的是找到使损失函数的值尽可能小的参数，这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）</h4><h4 id=\"SGD（随机梯度下降法）\"><a href=\"#SGD（随机梯度下降法）\" class=\"headerlink\" title=\"SGD（随机梯度下降法）\"></a>SGD（随机梯度下降法）</h4><blockquote>\n<h4 id=\"SGD的缺点，如果函数的形状非均向（anisotropic），比如呈延伸状，搜索的路径就会非常低效，低效的根本原因是，梯度的方向并没有指向最小值的方向，如下图\"><a href=\"#SGD的缺点，如果函数的形状非均向（anisotropic），比如呈延伸状，搜索的路径就会非常低效，低效的根本原因是，梯度的方向并没有指向最小值的方向，如下图\" class=\"headerlink\" title=\"SGD的缺点，如果函数的形状非均向（anisotropic），比如呈延伸状，搜索的路径就会非常低效，低效的根本原因是，梯度的方向并没有指向最小值的方向，如下图\"></a>SGD的缺点，如果函数的形状非均向（anisotropic），比如呈延伸状，搜索的路径就会非常低效，低效的根本原因是，梯度的方向并没有指向最小值的方向，如下图</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/5/1.png\"></p>\n<h4 id=\"这会导致它的效率很低，基于SGD的最优化的更新路径如下图\"><a href=\"#这会导致它的效率很低，基于SGD的最优化的更新路径如下图\" class=\"headerlink\" title=\"这会导致它的效率很低，基于SGD的最优化的更新路径如下图\"></a>这会导致它的效率很低，基于SGD的最优化的更新路径如下图</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/5/2.png\"></p>\n<h4 id=\"可以看到，它是Z字型的移动\"><a href=\"#可以看到，它是Z字型的移动\" class=\"headerlink\" title=\"可以看到，它是Z字型的移动\"></a>可以看到，它是Z字型的移动</h4></blockquote>\n<h4 id=\"Momentum（带动量的随机梯度下降法）\"><a href=\"#Momentum（带动量的随机梯度下降法）\" class=\"headerlink\" title=\"Momentum（带动量的随机梯度下降法）\"></a>Momentum（带动量的随机梯度下降法）</h4><blockquote>\n<h4 id=\"它是动量的意思，和物理有关，数学表达式如下\"><a href=\"#它是动量的意思，和物理有关，数学表达式如下\" class=\"headerlink\" title=\"它是动量的意思，和物理有关，数学表达式如下\"></a>它是动量的意思，和物理有关，数学表达式如下</h4><p>$$<br>v \\leftarrow \\alpha \\cdot v - \\eta \\cdot \\frac {\\partial L} {\\partial W} \\<br>W \\leftarrow W + v<br>$$</p>\n<h4 id=\"W同样还是表示要更新的权重参数，-frac-partial-L-partial-W-表示损失函数关于W的梯度，-eta-是学习率，而-v-是物理上的速度\"><a href=\"#W同样还是表示要更新的权重参数，-frac-partial-L-partial-W-表示损失函数关于W的梯度，-eta-是学习率，而-v-是物理上的速度\" class=\"headerlink\" title=\"W同样还是表示要更新的权重参数，$\\frac{\\partial L}{\\partial W}$ 表示损失函数关于W的梯度，$\\eta$ 是学习率，而 $v$ 是物理上的速度\"></a>W同样还是表示要更新的权重参数，$\\frac{\\partial L}{\\partial W}$ 表示损失函数关于W的梯度，$\\eta$ 是学习率，而 $v$ 是物理上的速度</h4><h4 id=\"第一个式子表示，物体在梯度方向上受力，在这个力的作用下，物体的速度增加这一物理法则，-alpha-v-这一项，其中的-alpha-是一个衰减因子，其作用是在物体不受力的作用的情况下，使得物体速度减小，一般设置为0-9之类的值，它对应物理中的地面摩擦或者空气阻力等\"><a href=\"#第一个式子表示，物体在梯度方向上受力，在这个力的作用下，物体的速度增加这一物理法则，-alpha-v-这一项，其中的-alpha-是一个衰减因子，其作用是在物体不受力的作用的情况下，使得物体速度减小，一般设置为0-9之类的值，它对应物理中的地面摩擦或者空气阻力等\" class=\"headerlink\" title=\"第一个式子表示，物体在梯度方向上受力，在这个力的作用下，物体的速度增加这一物理法则，$\\alpha v$ 这一项，其中的 $\\alpha$ 是一个衰减因子，其作用是在物体不受力的作用的情况下，使得物体速度减小，一般设置为0.9之类的值，它对应物理中的地面摩擦或者空气阻力等\"></a>第一个式子表示，物体在梯度方向上受力，在这个力的作用下，物体的速度增加这一物理法则，$\\alpha v$ 这一项，其中的 $\\alpha$ 是一个衰减因子，其作用是在物体不受力的作用的情况下，使得物体速度减小，一般设置为0.9之类的值，它对应物理中的地面摩擦或者空气阻力等</h4><h4 id=\"基于Momentum的最优化的更新路径如下图\"><a href=\"#基于Momentum的最优化的更新路径如下图\" class=\"headerlink\" title=\"基于Momentum的最优化的更新路径如下图\"></a>基于Momentum的最优化的更新路径如下图</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/5/3.png\"></p>\n<h4 id=\"可以发现，和SGD相比，它的Z字型程度减轻了，这是因为，虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速；另一方面，虽然y轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它们会互相抵消，所以y轴方向上的速度不稳定\"><a href=\"#可以发现，和SGD相比，它的Z字型程度减轻了，这是因为，虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速；另一方面，虽然y轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它们会互相抵消，所以y轴方向上的速度不稳定\" class=\"headerlink\" title=\"可以发现，和SGD相比，它的Z字型程度减轻了，这是因为，虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速；另一方面，虽然y轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它们会互相抵消，所以y轴方向上的速度不稳定\"></a>可以发现，和SGD相比，它的Z字型程度减轻了，这是因为，虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速；另一方面，虽然y轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它们会互相抵消，所以y轴方向上的速度不稳定</h4></blockquote>\n<h4 id=\"AdaGrad（自适应梯度算法）\"><a href=\"#AdaGrad（自适应梯度算法）\" class=\"headerlink\" title=\"AdaGrad（自适应梯度算法）\"></a>AdaGrad（自适应梯度算法）</h4><blockquote>\n<h4 id=\"学习率衰减（learning-rate-decay）：随着学习的进行，使学习率逐渐减小。它是针对全体参数的，它是将全体参数的学习率一起降低\"><a href=\"#学习率衰减（learning-rate-decay）：随着学习的进行，使学习率逐渐减小。它是针对全体参数的，它是将全体参数的学习率一起降低\" class=\"headerlink\" title=\"学习率衰减（learning rate decay）：随着学习的进行，使学习率逐渐减小。它是针对全体参数的，它是将全体参数的学习率一起降低\"></a>学习率衰减（learning rate decay）：随着学习的进行，使学习率逐渐减小。它是针对全体参数的，它是将全体参数的学习率一起降低</h4><h4 id=\"AdaGrad会为参数的每个元素适当地调整学习率，与此同时进行学习，Ada是取自Adaptive，即适当的\"><a href=\"#AdaGrad会为参数的每个元素适当地调整学习率，与此同时进行学习，Ada是取自Adaptive，即适当的\" class=\"headerlink\" title=\"AdaGrad会为参数的每个元素适当地调整学习率，与此同时进行学习，Ada是取自Adaptive，即适当的\"></a>AdaGrad会为参数的每个元素适当地调整学习率，与此同时进行学习，Ada是取自Adaptive，即适当的</h4><h4 id=\"其数学表达式如下\"><a href=\"#其数学表达式如下\" class=\"headerlink\" title=\"其数学表达式如下\"></a>其数学表达式如下</h4><p>$$<br>h \\leftarrow h + \\frac {\\partial L} {\\partial W} \\odot \\frac {\\partial L} {\\partial W} \\<br>\\mathbf{W} \\leftarrow \\mathbf{W} - \\eta \\frac{1}{\\sqrt{\\mathbf{h}}} \\frac{\\partial L}{\\partial \\mathbf{W}}<br>$$</p>\n<h4 id=\"需要说明的是，变量-h-，它保存了以前的所有梯度值的平方和，-odot-表示对应矩阵元素的乘法，也就是实现了所有梯度值的平方和\"><a href=\"#需要说明的是，变量-h-，它保存了以前的所有梯度值的平方和，-odot-表示对应矩阵元素的乘法，也就是实现了所有梯度值的平方和\" class=\"headerlink\" title=\"需要说明的是，变量 $h$ ，它保存了以前的所有梯度值的平方和，$\\odot$ 表示对应矩阵元素的乘法，也就是实现了所有梯度值的平方和\"></a>需要说明的是，变量 $h$ ，它保存了以前的所有梯度值的平方和，$\\odot$ 表示对应矩阵元素的乘法，也就是实现了所有梯度值的平方和</h4><h4 id=\"另外，在更新参数时，通过乘以-frac-1-sqrt-h-，就可以调整学习的尺度，这意味着，参数的元素中变动较大（被大幅更新）的元素的学习率将变小，也即，可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小\"><a href=\"#另外，在更新参数时，通过乘以-frac-1-sqrt-h-，就可以调整学习的尺度，这意味着，参数的元素中变动较大（被大幅更新）的元素的学习率将变小，也即，可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小\" class=\"headerlink\" title=\"另外，在更新参数时，通过乘以 $\\frac {1}{\\sqrt {h}}$ ，就可以调整学习的尺度，这意味着，参数的元素中变动较大（被大幅更新）的元素的学习率将变小，也即，可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小\"></a>另外，在更新参数时，通过乘以 $\\frac {1}{\\sqrt {h}}$ ，就可以调整学习的尺度，这意味着，参数的元素中变动较大（被大幅更新）的元素的学习率将变小，也即，可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小</h4><h4 id=\"AdaGrad会记录过去所有梯度的平方和，因此，学习越深入，更新的幅度就越小，如果无休止的学习，最终更新量会变为0\"><a href=\"#AdaGrad会记录过去所有梯度的平方和，因此，学习越深入，更新的幅度就越小，如果无休止的学习，最终更新量会变为0\" class=\"headerlink\" title=\"AdaGrad会记录过去所有梯度的平方和，因此，学习越深入，更新的幅度就越小，如果无休止的学习，最终更新量会变为0\"></a>AdaGrad会记录过去所有梯度的平方和，因此，学习越深入，更新的幅度就越小，如果无休止的学习，最终更新量会变为0</h4><h4 id=\"RMSProp方法，并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来，这样的操作被称为指数移动平均，呈指数函数式地减小过去的梯度的尺度\"><a href=\"#RMSProp方法，并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来，这样的操作被称为指数移动平均，呈指数函数式地减小过去的梯度的尺度\" class=\"headerlink\" title=\"RMSProp方法，并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来，这样的操作被称为指数移动平均，呈指数函数式地减小过去的梯度的尺度\"></a>RMSProp方法，并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来，这样的操作被称为<strong>指数移动平均</strong>，呈指数函数式地减小过去的梯度的尺度</h4><h4 id=\"基于AdaGrad的最优化的更新路径如下图\"><a href=\"#基于AdaGrad的最优化的更新路径如下图\" class=\"headerlink\" title=\"基于AdaGrad的最优化的更新路径如下图\"></a>基于AdaGrad的最优化的更新路径如下图</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/5/4.png\"></p>\n<h4 id=\"可以看到，函数的取值高效地向着最小值移动\"><a href=\"#可以看到，函数的取值高效地向着最小值移动\" class=\"headerlink\" title=\"可以看到，函数的取值高效地向着最小值移动\"></a>可以看到，函数的取值高效地向着最小值移动</h4></blockquote>\n<h4 id=\"Adam（自适应矩估计算法）\"><a href=\"#Adam（自适应矩估计算法）\" class=\"headerlink\" title=\"Adam（自适应矩估计算法）\"></a>Adam（自适应矩估计算法）</h4><blockquote>\n<h4 id=\"它融合了Momentum和AdaGrad，结合两者的优点，它还可以进行超参数的偏置矫正\"><a href=\"#它融合了Momentum和AdaGrad，结合两者的优点，它还可以进行超参数的偏置矫正\" class=\"headerlink\" title=\"它融合了Momentum和AdaGrad，结合两者的优点，它还可以进行超参数的偏置矫正\"></a>它融合了Momentum和AdaGrad，结合两者的优点，它还可以进行超参数的偏置矫正</h4><h4 id=\"基于Adam的最优化的更新路径如下图\"><a href=\"#基于Adam的最优化的更新路径如下图\" class=\"headerlink\" title=\"基于Adam的最优化的更新路径如下图\"></a>基于Adam的最优化的更新路径如下图</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/5/5.png\"></p>\n<h4 id=\"Adam会设置3个超参数，一个是学习率-eta-，另外两个是一次momentum系数-beta-1-和二次momentum系数-beta-2-，且标准的设定值是-beta-1-0-9-beta-2-0-999\"><a href=\"#Adam会设置3个超参数，一个是学习率-eta-，另外两个是一次momentum系数-beta-1-和二次momentum系数-beta-2-，且标准的设定值是-beta-1-0-9-beta-2-0-999\" class=\"headerlink\" title=\"Adam会设置3个超参数，一个是学习率 $\\eta$ ，另外两个是一次momentum系数 $\\beta_1$ 和二次momentum系数 $\\beta_2$ ，且标准的设定值是 $\\beta_1 &#x3D; 0.9, \\beta_2 &#x3D; 0.999$\"></a>Adam会设置3个超参数，一个是学习率 $\\eta$ ，另外两个是一次momentum系数 $\\beta_1$ 和二次momentum系数 $\\beta_2$ ，且标准的设定值是 $\\beta_1 &#x3D; 0.9, \\beta_2 &#x3D; 0.999$</h4></blockquote>\n</blockquote>\n<h3 id=\"权重的初始值\"><a href=\"#权重的初始值\" class=\"headerlink\" title=\"权重的初始值\"></a>权重的初始值</h3><blockquote>\n<h4 id=\"权值衰减（weight-decay），就是一种以减小权重参数的值为目的进行学习的方法，通过减小权重参数的值来抑制过拟合的发生，以及提高泛化能力\"><a href=\"#权值衰减（weight-decay），就是一种以减小权重参数的值为目的进行学习的方法，通过减小权重参数的值来抑制过拟合的发生，以及提高泛化能力\" class=\"headerlink\" title=\"权值衰减（weight decay），就是一种以减小权重参数的值为目的进行学习的方法，通过减小权重参数的值来抑制过拟合的发生，以及提高泛化能力\"></a>权值衰减（weight decay），就是一种以减小权重参数的值为目的进行学习的方法，通过减小权重参数的值来抑制过拟合的发生，以及提高泛化能力</h4><h4 id=\"在误差反向传播法中，所有的权重值都会进行相同的更新，所以不能把权重初始值设为一样的值（权重均一化）\"><a href=\"#在误差反向传播法中，所有的权重值都会进行相同的更新，所以不能把权重初始值设为一样的值（权重均一化）\" class=\"headerlink\" title=\"在误差反向传播法中，所有的权重值都会进行相同的更新，所以不能把权重初始值设为一样的值（权重均一化）\"></a>在误差反向传播法中，所有的权重值都会进行相同的更新，所以不能把权重初始值设为一样的值（权重均一化）</h4><h4 id=\"激活函数使用sigmoid函数，随着输出不断地靠近0（或者靠近1），它的导数的值逐渐接近0，从而偏向0和1的数据分布会造成反向传播中梯度的值不断变小，最后消失，这就是所谓的梯度消失（gradient-vanishing）\"><a href=\"#激活函数使用sigmoid函数，随着输出不断地靠近0（或者靠近1），它的导数的值逐渐接近0，从而偏向0和1的数据分布会造成反向传播中梯度的值不断变小，最后消失，这就是所谓的梯度消失（gradient-vanishing）\" class=\"headerlink\" title=\"激活函数使用sigmoid函数，随着输出不断地靠近0（或者靠近1），它的导数的值逐渐接近0，从而偏向0和1的数据分布会造成反向传播中梯度的值不断变小，最后消失，这就是所谓的梯度消失（gradient vanishing）\"></a>激活函数使用sigmoid函数，随着输出不断地靠近0（或者靠近1），它的导数的值逐渐接近0，从而偏向0和1的数据分布会造成反向传播中梯度的值不断变小，最后消失，这就是所谓的<strong>梯度消失</strong>（gradient vanishing）</h4><h4 id=\"激活值在分布上有所偏向会出现表现了受限或梯度消失的问题\"><a href=\"#激活值在分布上有所偏向会出现表现了受限或梯度消失的问题\" class=\"headerlink\" title=\"激活值在分布上有所偏向会出现表现了受限或梯度消失的问题\"></a>激活值在分布上有所偏向会出现表现了受限或梯度消失的问题</h4><h4 id=\"Xavier初始值\"><a href=\"#Xavier初始值\" class=\"headerlink\" title=\"Xavier初始值\"></a>Xavier初始值</h4><blockquote>\n<h4 id=\"为了使各层的激活值呈现出具有相同广度的分布，推导了合适的权重尺度，结论是：如果前一层的节点数为n，则初始值使用标准差为-frac-1-sqrt-n-的分布\"><a href=\"#为了使各层的激活值呈现出具有相同广度的分布，推导了合适的权重尺度，结论是：如果前一层的节点数为n，则初始值使用标准差为-frac-1-sqrt-n-的分布\" class=\"headerlink\" title=\"为了使各层的激活值呈现出具有相同广度的分布，推导了合适的权重尺度，结论是：如果前一层的节点数为n，则初始值使用标准差为 $\\frac{1}{\\sqrt{n}}$ 的分布\"></a>为了使各层的激活值呈现出具有相同广度的分布，推导了合适的权重尺度，结论是：如果前一层的节点数为n，则初始值使用标准差为 $\\frac{1}{\\sqrt{n}}$ 的分布</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/5/6.png\"></p>\n<h4 id=\"可以将激活函数sigmoid改为tanh双曲线函数，两者同为S型曲线函数，但tanh是关于原点对称的S型曲线，而sigmoid是关于（0，0-5）对称的，而用作激活函数的函数最好具有关于原点对称的性质\"><a href=\"#可以将激活函数sigmoid改为tanh双曲线函数，两者同为S型曲线函数，但tanh是关于原点对称的S型曲线，而sigmoid是关于（0，0-5）对称的，而用作激活函数的函数最好具有关于原点对称的性质\" class=\"headerlink\" title=\"可以将激活函数sigmoid改为tanh双曲线函数，两者同为S型曲线函数，但tanh是关于原点对称的S型曲线，而sigmoid是关于（0，0.5）对称的，而用作激活函数的函数最好具有关于原点对称的性质\"></a>可以将激活函数sigmoid改为tanh双曲线函数，两者同为S型曲线函数，但tanh是关于原点对称的S型曲线，而sigmoid是关于（0，0.5）对称的，而用作激活函数的函数最好具有关于原点对称的性质</h4></blockquote>\n<h4 id=\"ReLU的权值重置\"><a href=\"#ReLU的权值重置\" class=\"headerlink\" title=\"ReLU的权值重置\"></a>ReLU的权值重置</h4><blockquote>\n<h4 id=\"Xavier初始值是以激活函数是线性函数为前提而推导出来的，sigmoid函数和tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值\"><a href=\"#Xavier初始值是以激活函数是线性函数为前提而推导出来的，sigmoid函数和tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值\" class=\"headerlink\" title=\"Xavier初始值是以激活函数是线性函数为前提而推导出来的，sigmoid函数和tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值\"></a>Xavier初始值是以激活函数是线性函数为前提而推导出来的，sigmoid函数和tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值</h4><h4 id=\"当激活函数使用ReLU时，一般推荐使用ReLU专用的初始值——He初始值\"><a href=\"#当激活函数使用ReLU时，一般推荐使用ReLU专用的初始值——He初始值\" class=\"headerlink\" title=\"当激活函数使用ReLU时，一般推荐使用ReLU专用的初始值——He初始值\"></a>当激活函数使用ReLU时，一般推荐使用ReLU专用的初始值——He初始值</h4><h4 id=\"它是，当前一层的节点数为n时，He初始值使用标准差为-sqrt-frac-2-n-的高斯分布\"><a href=\"#它是，当前一层的节点数为n时，He初始值使用标准差为-sqrt-frac-2-n-的高斯分布\" class=\"headerlink\" title=\"它是，当前一层的节点数为n时，He初始值使用标准差为 $\\sqrt{\\frac {2}{n}}$ 的高斯分布\"></a>它是，当前一层的节点数为n时，He初始值使用标准差为 $\\sqrt{\\frac {2}{n}}$ 的高斯分布</h4><h4 id=\"相对于Xavier初始值的-sqrt-frac-1-n-，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数\"><a href=\"#相对于Xavier初始值的-sqrt-frac-1-n-，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数\" class=\"headerlink\" title=\"相对于Xavier初始值的 $\\sqrt{\\frac {1}{n}}$ ，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数\"></a>相对于Xavier初始值的 $\\sqrt{\\frac {1}{n}}$ ，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数</h4><h4 id=\"当激活函数使用ReLU时，权重初始值使用He初始值，当激活函数为sigmoid或tanh等S型曲线函数时，初始值使用Xavier初始值\"><a href=\"#当激活函数使用ReLU时，权重初始值使用He初始值，当激活函数为sigmoid或tanh等S型曲线函数时，初始值使用Xavier初始值\" class=\"headerlink\" title=\"当激活函数使用ReLU时，权重初始值使用He初始值，当激活函数为sigmoid或tanh等S型曲线函数时，初始值使用Xavier初始值\"></a>当激活函数使用ReLU时，权重初始值使用He初始值，当激活函数为sigmoid或tanh等S型曲线函数时，初始值使用Xavier初始值</h4></blockquote>\n</blockquote>\n<h3 id=\"Batch-Normalization\"><a href=\"#Batch-Normalization\" class=\"headerlink\" title=\"Batch Normalization\"></a>Batch Normalization</h3><blockquote>\n<h4 id=\"Batch-Normalization的算法\"><a href=\"#Batch-Normalization的算法\" class=\"headerlink\" title=\"Batch Normalization的算法\"></a>Batch Normalization的算法</h4><blockquote>\n<h4 id=\"为了使各层拥有适当的广度，“强制性”地调整激活值的分布，这就是Batch-Normalization方法\"><a href=\"#为了使各层拥有适当的广度，“强制性”地调整激活值的分布，这就是Batch-Normalization方法\" class=\"headerlink\" title=\"为了使各层拥有适当的广度，“强制性”地调整激活值的分布，这就是Batch Normalization方法\"></a>为了使各层拥有适当的广度，“强制性”地调整激活值的分布，这就是Batch Normalization方法</h4><h4 id=\"它有以下几个特点\"><a href=\"#它有以下几个特点\" class=\"headerlink\" title=\"它有以下几个特点\"></a>它有以下几个特点</h4><ul>\n<li><h4 id=\"可以使学习快速进行（可以增大学习率）\"><a href=\"#可以使学习快速进行（可以增大学习率）\" class=\"headerlink\" title=\"可以使学习快速进行（可以增大学习率）\"></a>可以使学习快速进行（可以增大学习率）</h4></li>\n<li><h4 id=\"对初始值的依赖不大\"><a href=\"#对初始值的依赖不大\" class=\"headerlink\" title=\"对初始值的依赖不大\"></a>对初始值的依赖不大</h4></li>\n<li><h4 id=\"抑制过拟合\"><a href=\"#抑制过拟合\" class=\"headerlink\" title=\"抑制过拟合\"></a>抑制过拟合</h4></li>\n</ul>\n<h4 id=\"Batch-normalization层：对神经网络中的数据分布进行正规化的层\"><a href=\"#Batch-normalization层：对神经网络中的数据分布进行正规化的层\" class=\"headerlink\" title=\"Batch normalization层：对神经网络中的数据分布进行正规化的层\"></a>Batch normalization层：对神经网络中的数据分布进行正规化的层</h4><h4 id=\"它以进行学习时的mini-batch为单位，按mini-batch进行正规化，具体来说就是，进行使数据分布的均值为0、方差为1的正规化，数学式表示如下\"><a href=\"#它以进行学习时的mini-batch为单位，按mini-batch进行正规化，具体来说就是，进行使数据分布的均值为0、方差为1的正规化，数学式表示如下\" class=\"headerlink\" title=\"它以进行学习时的mini-batch为单位，按mini-batch进行正规化，具体来说就是，进行使数据分布的均值为0、方差为1的正规化，数学式表示如下\"></a>它以进行学习时的mini-batch为单位，按mini-batch进行正规化，具体来说就是，进行使数据分布的均值为0、方差为1的正规化，数学式表示如下</h4><p>$$<br>\\mu_B \\leftarrow \\frac {1}{m} \\sum_{i&#x3D;1}^m x_i \\<br>\\sigma_B^2 \\leftarrow \\frac{1}{m} \\sum_{i&#x3D;1}^m(x_i - \\mu_B)^2 \\<br>\\hat x_i \\leftarrow \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\varepsilon}}<br>$$</p>\n<h4 id=\"第1、2个式子是对mini-batch的m个输入数据的集合-B-x-1-x-2…x-m-求均值-mu-B-和方差-sigma-B-2-，第三个式子对输入数据进行均值为0、方差为1（合适的分布）的正规化，-varepsilon-是一个极小值，作用是防止除0\"><a href=\"#第1、2个式子是对mini-batch的m个输入数据的集合-B-x-1-x-2…x-m-求均值-mu-B-和方差-sigma-B-2-，第三个式子对输入数据进行均值为0、方差为1（合适的分布）的正规化，-varepsilon-是一个极小值，作用是防止除0\" class=\"headerlink\" title=\"第1、2个式子是对mini-batch的m个输入数据的集合 $B&#x3D;{x_1,x_2…x_m}$ 求均值 $\\mu_B$ 和方差 $\\sigma_B^2$ ，第三个式子对输入数据进行均值为0、方差为1（合适的分布）的正规化，$\\varepsilon$ 是一个极小值，作用是防止除0\"></a>第1、2个式子是对mini-batch的m个输入数据的集合 $B&#x3D;{x_1,x_2…x_m}$ 求均值 $\\mu_B$ 和方差 $\\sigma_B^2$ ，第三个式子对输入数据进行均值为0、方差为1（合适的分布）的正规化，$\\varepsilon$ 是一个极小值，作用是防止除0</h4><h4 id=\"通过将这个处理插入到激活函数的前面（或者后面），可以减小数据分布的偏向\"><a href=\"#通过将这个处理插入到激活函数的前面（或者后面），可以减小数据分布的偏向\" class=\"headerlink\" title=\"通过将这个处理插入到激活函数的前面（或者后面），可以减小数据分布的偏向\"></a>通过将这个处理插入到激活函数的前面（或者后面），可以减小数据分布的偏向</h4><h4 id=\"紧接着，Batch-Norm层会对正规化后的数据进行缩放和平移的变换，数学式表示如下\"><a href=\"#紧接着，Batch-Norm层会对正规化后的数据进行缩放和平移的变换，数学式表示如下\" class=\"headerlink\" title=\"紧接着，Batch Norm层会对正规化后的数据进行缩放和平移的变换，数学式表示如下\"></a>紧接着，Batch Norm层会对正规化后的数据进行缩放和平移的变换，数学式表示如下</h4><p>$$<br>y_i \\leftarrow \\gamma \\hat x_i  + \\beta<br>$$</p>\n<h4 id=\"其中，-gamma-和-beta-是参数，初始分别为1和0，然后再通过学习调整到合适的值\"><a href=\"#其中，-gamma-和-beta-是参数，初始分别为1和0，然后再通过学习调整到合适的值\" class=\"headerlink\" title=\"其中， $\\gamma$ 和 $\\beta$ 是参数，初始分别为1和0，然后再通过学习调整到合适的值\"></a>其中， $\\gamma$ 和 $\\beta$ 是参数，初始分别为1和0，然后再通过学习调整到合适的值</h4><h4 id=\"它是神经网络上的正向传播，计算图如下\"><a href=\"#它是神经网络上的正向传播，计算图如下\" class=\"headerlink\" title=\"它是神经网络上的正向传播，计算图如下\"></a>它是神经网络上的正向传播，计算图如下</h4><p><img src=\"http://picbed.yanzu.tech/img/DL/5/7.png\"></p>\n<h4 id=\"留个坑：batch-norm的反向传播\"><a href=\"#留个坑：batch-norm的反向传播\" class=\"headerlink\" title=\"留个坑：batch norm的反向传播\"></a>留个坑：batch norm的反向传播</h4></blockquote>\n<h4 id=\"Batch-Normalization的评估\"><a href=\"#Batch-Normalization的评估\" class=\"headerlink\" title=\"Batch Normalization的评估\"></a>Batch Normalization的评估</h4></blockquote>\n<h3 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a>正则化</h3><blockquote>\n<h4 id=\"过拟合：只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态\"><a href=\"#过拟合：只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态\" class=\"headerlink\" title=\"过拟合：只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态\"></a>过拟合：只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态</h4><h4 id=\"发生过拟合的原因，一个是模型拥有大量参数、表现力强，一个是训练数据少\"><a href=\"#发生过拟合的原因，一个是模型拥有大量参数、表现力强，一个是训练数据少\" class=\"headerlink\" title=\"发生过拟合的原因，一个是模型拥有大量参数、表现力强，一个是训练数据少\"></a>发生过拟合的原因，一个是模型拥有大量参数、表现力强，一个是训练数据少</h4><h4 id=\"权值衰减\"><a href=\"#权值衰减\" class=\"headerlink\" title=\"权值衰减\"></a>权值衰减</h4><blockquote>\n<h4 id=\"权值衰减是一直以来经常被使用的一种抑制过拟合的方法，该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合\"><a href=\"#权值衰减是一直以来经常被使用的一种抑制过拟合的方法，该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合\" class=\"headerlink\" title=\"权值衰减是一直以来经常被使用的一种抑制过拟合的方法，该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合\"></a>权值衰减是一直以来经常被使用的一种抑制过拟合的方法，该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合</h4><h4 id=\"如，为损失函数加上权重的平方范数（L2范数），这样就可以抑制权重变大，将权重记为W，L2的范数的权值衰减就是-frac-1-2-lambda-W-2-，然后将它加到损失函数上，-lambda-是控制正则化强度的超参数，-lambda-设置得越大，对大的权重施加的惩罚就越重，1-2是用于将-frac-1-2-lambda-W-2-的求导结果变成-lambda-W-的调整常用量\"><a href=\"#如，为损失函数加上权重的平方范数（L2范数），这样就可以抑制权重变大，将权重记为W，L2的范数的权值衰减就是-frac-1-2-lambda-W-2-，然后将它加到损失函数上，-lambda-是控制正则化强度的超参数，-lambda-设置得越大，对大的权重施加的惩罚就越重，1-2是用于将-frac-1-2-lambda-W-2-的求导结果变成-lambda-W-的调整常用量\" class=\"headerlink\" title=\"如，为损失函数加上权重的平方范数（L2范数），这样就可以抑制权重变大，将权重记为W，L2的范数的权值衰减就是 $\\frac{1}{2} \\lambda W^2$ ，然后将它加到损失函数上，$\\lambda$ 是控制正则化强度的超参数，$\\lambda$ 设置得越大，对大的权重施加的惩罚就越重，1&#x2F;2是用于将 $\\frac{1}{2} \\lambda W^2$ 的求导结果变成 $\\lambda W$ 的调整常用量\"></a>如，为损失函数加上权重的平方范数（L2范数），这样就可以抑制权重变大，将权重记为W，L2的范数的权值衰减就是 $\\frac{1}{2} \\lambda W^2$ ，然后将它加到损失函数上，$\\lambda$ 是控制正则化强度的超参数，$\\lambda$ 设置得越大，对大的权重施加的惩罚就越重，1&#x2F;2是用于将 $\\frac{1}{2} \\lambda W^2$ 的求导结果变成 $\\lambda W$ 的调整常用量</h4><h4 id=\"对于所有权重，权值衰减方法都会为损失函数加上-frac-1-2-lambda-W-2-，因此，在求权重梯度的计算中，要为之前的误差反向传播法的结果加上正则化项的导数-lambda-W\"><a href=\"#对于所有权重，权值衰减方法都会为损失函数加上-frac-1-2-lambda-W-2-，因此，在求权重梯度的计算中，要为之前的误差反向传播法的结果加上正则化项的导数-lambda-W\" class=\"headerlink\" title=\"对于所有权重，权值衰减方法都会为损失函数加上 $\\frac{1}{2} \\lambda W^2$ ，因此，在求权重梯度的计算中，要为之前的误差反向传播法的结果加上正则化项的导数 $\\lambda W$\"></a>对于所有权重，权值衰减方法都会为损失函数加上 $\\frac{1}{2} \\lambda W^2$ ，因此，在求权重梯度的计算中，要为之前的误差反向传播法的结果加上正则化项的导数 $\\lambda W$</h4><h4 id=\"L2范数相当于各元素的平方和，假设有权重-W-w-1-w-2…w-n-，则L2范数可用-sqrt-w-1-2-w-2-2-…w-n-2-计算出来\"><a href=\"#L2范数相当于各元素的平方和，假设有权重-W-w-1-w-2…w-n-，则L2范数可用-sqrt-w-1-2-w-2-2-…w-n-2-计算出来\" class=\"headerlink\" title=\"L2范数相当于各元素的平方和，假设有权重 $W&#x3D;(w_1,w_2…w_n)$ ，则L2范数可用 $\\sqrt{w_1^2+w_2^2+…w_n^2}$ 计算出来\"></a>L2范数相当于各元素的平方和，假设有权重 $W&#x3D;(w_1,w_2…w_n)$ ，则L2范数可用 $\\sqrt{w_1^2+w_2^2+…w_n^2}$ 计算出来</h4><h4 id=\"除了L2范数，还有L1范数、-L-infty-范数等，L1范数是各元素的绝对值之和，相当于-w-1-w-2-…-w-n-，-L-infty-范数也称为Max范数，相当于各个元素的绝对值中最大的那一个\"><a href=\"#除了L2范数，还有L1范数、-L-infty-范数等，L1范数是各元素的绝对值之和，相当于-w-1-w-2-…-w-n-，-L-infty-范数也称为Max范数，相当于各个元素的绝对值中最大的那一个\" class=\"headerlink\" title=\"除了L2范数，还有L1范数、$L \\infty$ 范数等，L1范数是各元素的绝对值之和，相当于 $|w_1|+|w_2|+…|w_n|$ ，$L\\infty$ 范数也称为Max范数，相当于各个元素的绝对值中最大的那一个\"></a>除了L2范数，还有L1范数、$L \\infty$ 范数等，L1范数是各元素的绝对值之和，相当于 $|w_1|+|w_2|+…|w_n|$ ，$L\\infty$ 范数也称为Max范数，相当于各个元素的绝对值中最大的那一个</h4><h4 id=\"以上三种范数都可以用作正则项\"><a href=\"#以上三种范数都可以用作正则项\" class=\"headerlink\" title=\"以上三种范数都可以用作正则项\"></a>以上三种范数都可以用作正则项</h4></blockquote>\n<h4 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h4><blockquote>\n<h4 id=\"前面的为损失函数加上权重的L2范数的权值衰减方法，在网络模型变得复杂的情况下，就难以应对了\"><a href=\"#前面的为损失函数加上权重的L2范数的权值衰减方法，在网络模型变得复杂的情况下，就难以应对了\" class=\"headerlink\" title=\"前面的为损失函数加上权重的L2范数的权值衰减方法，在网络模型变得复杂的情况下，就难以应对了\"></a>前面的为损失函数加上权重的L2范数的权值衰减方法，在网络模型变得复杂的情况下，就难以应对了</h4><h4 id=\"由此，引出了Dropout这种方法\"><a href=\"#由此，引出了Dropout这种方法\" class=\"headerlink\" title=\"由此，引出了Dropout这种方法\"></a>由此，引出了Dropout这种方法</h4><h4 id=\"Dropout是一种在学习的过程中随机删除神经元的方法\"><a href=\"#Dropout是一种在学习的过程中随机删除神经元的方法\" class=\"headerlink\" title=\"Dropout是一种在学习的过程中随机删除神经元的方法\"></a>Dropout是一种在学习的过程中随机删除神经元的方法</h4><blockquote>\n<h4 id=\"训练时，随机选出隐藏层的神经元，然后将其删除，被删除的神经元不再进行信号的传递\"><a href=\"#训练时，随机选出隐藏层的神经元，然后将其删除，被删除的神经元不再进行信号的传递\" class=\"headerlink\" title=\"训练时，随机选出隐藏层的神经元，然后将其删除，被删除的神经元不再进行信号的传递\"></a>训练时，随机选出隐藏层的神经元，然后将其删除，被删除的神经元不再进行信号的传递</h4><h4 id=\"训练时，每传递一次数据，就会随机选择要删除的神经元\"><a href=\"#训练时，每传递一次数据，就会随机选择要删除的神经元\" class=\"headerlink\" title=\"训练时，每传递一次数据，就会随机选择要删除的神经元\"></a>训练时，每传递一次数据，就会随机选择要删除的神经元</h4><h4 id=\"测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出\"><a href=\"#测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出\" class=\"headerlink\" title=\"测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出\"></a>测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出</h4></blockquote>\n<p><img src=\"http://picbed.yanzu.tech/img/DL/5/8.png\"></p>\n<h4 id=\"集成学习，就是让多个模型单独进行学习，推理时再取多个模型的输出的平均值，通过进行集成学习，神经网络的识别精度可以提高好几个百分点，Dropout将集成学习的效果（模拟地）通过一个网络实现了\"><a href=\"#集成学习，就是让多个模型单独进行学习，推理时再取多个模型的输出的平均值，通过进行集成学习，神经网络的识别精度可以提高好几个百分点，Dropout将集成学习的效果（模拟地）通过一个网络实现了\" class=\"headerlink\" title=\"集成学习，就是让多个模型单独进行学习，推理时再取多个模型的输出的平均值，通过进行集成学习，神经网络的识别精度可以提高好几个百分点，Dropout将集成学习的效果（模拟地）通过一个网络实现了\"></a>集成学习，就是让多个模型单独进行学习，推理时再取多个模型的输出的平均值，通过进行集成学习，神经网络的识别精度可以提高好几个百分点，Dropout将集成学习的效果（模拟地）通过一个网络实现了</h4></blockquote>\n</blockquote>\n<h3 id=\"超参数的验证\"><a href=\"#超参数的验证\" class=\"headerlink\" title=\"超参数的验证\"></a>超参数的验证</h3><blockquote>\n<h4 id=\"超参数是指各层的神经元数量、batch大小、参数更新时的学习率或权值衰减等\"><a href=\"#超参数是指各层的神经元数量、batch大小、参数更新时的学习率或权值衰减等\" class=\"headerlink\" title=\"超参数是指各层的神经元数量、batch大小、参数更新时的学习率或权值衰减等\"></a>超参数是指各层的神经元数量、batch大小、参数更新时的学习率或权值衰减等</h4><h4 id=\"不能使用测试数据评估超参数的性能，因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合\"><a href=\"#不能使用测试数据评估超参数的性能，因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合\" class=\"headerlink\" title=\"不能使用测试数据评估超参数的性能，因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合\"></a>不能使用测试数据评估超参数的性能，因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合</h4><h4 id=\"用于调整超参数的数据，一般称为验证数据\"><a href=\"#用于调整超参数的数据，一般称为验证数据\" class=\"headerlink\" title=\"用于调整超参数的数据，一般称为验证数据\"></a>用于调整超参数的数据，一般称为验证数据</h4><h4 id=\"训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估，为了确认泛化能力，要在最后使用测试数据\"><a href=\"#训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估，为了确认泛化能力，要在最后使用测试数据\" class=\"headerlink\" title=\"训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估，为了确认泛化能力，要在最后使用测试数据\"></a>训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估，为了确认泛化能力，要在最后使用测试数据</h4><h4 id=\"超参数的最优化\"><a href=\"#超参数的最优化\" class=\"headerlink\" title=\"超参数的最优化\"></a>超参数的最优化</h4><blockquote>\n<h4 id=\"进行超参数的最优化时，要逐渐缩小超参数的“好值”的存在范围\"><a href=\"#进行超参数的最优化时，要逐渐缩小超参数的“好值”的存在范围\" class=\"headerlink\" title=\"进行超参数的最优化时，要逐渐缩小超参数的“好值”的存在范围\"></a>进行超参数的最优化时，要逐渐缩小超参数的“好值”的存在范围</h4><h4 id=\"所谓逐渐缩小范围，是指一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样，最好是随机采样），用这个采样到的值进行识别精度的评估，重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围，最终确定超参数的合适范围-一般是-0-001-1000\"><a href=\"#所谓逐渐缩小范围，是指一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样，最好是随机采样），用这个采样到的值进行识别精度的评估，重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围，最终确定超参数的合适范围-一般是-0-001-1000\" class=\"headerlink\" title=\"所谓逐渐缩小范围，是指一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样，最好是随机采样），用这个采样到的值进行识别精度的评估，重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围，最终确定超参数的合适范围(一般是 0.001~1000)\"></a>所谓逐渐缩小范围，是指一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样，最好是随机采样），用这个采样到的值进行识别精度的评估，重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围，最终确定超参数的合适范围(一般是 0.001~1000)</h4><h4 id=\"还可以使用贝叶斯最优化（Bayesian-optimization）\"><a href=\"#还可以使用贝叶斯最优化（Bayesian-optimization）\" class=\"headerlink\" title=\"还可以使用贝叶斯最优化（Bayesian optimization）\"></a>还可以使用贝叶斯最优化（Bayesian optimization）</h4></blockquote>\n</blockquote>\n<h3 id=\"code\"><a href=\"#code\" class=\"headerlink\" title=\"code\"></a>code</h3><blockquote>\n<h4 id=\"SGD\"><a href=\"#SGD\" class=\"headerlink\" title=\"SGD\"></a>SGD</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SGD</span>:</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">随机梯度下降法（Stochastic Gradient Descent）</span></span><br><span class=\"line\"><span class=\"string\">最基础的优化算法，直接使用梯度更新参数</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, lr=<span class=\"number\">0.01</span></span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">初始化SGD优化器</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">lr: 学习率，控制参数更新的步长</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.lr = lr</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">update</span>(<span class=\"params\">self, params, grads</span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">更新参数</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">params: 需要更新的参数字典</span></span><br><span class=\"line\"><span class=\"string\">grads: 对应的梯度字典</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> params.keys():</span><br><span class=\"line\">params[key] -= <span class=\"variable language_\">self</span>.lr * grads[key]  <span class=\"comment\"># 参数更新公式：θ = θ - lr * ∇θ</span></span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"Momentum\"><a href=\"#Momentum\" class=\"headerlink\" title=\"Momentum\"></a>Momentum</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Momentum</span>:</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">带动量的随机梯度下降法</span></span><br><span class=\"line\"><span class=\"string\">通过引入动量项来加速收敛，减少震荡</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, lr=<span class=\"number\">0.01</span>, momentum=<span class=\"number\">0.9</span></span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">初始化Momentum优化器</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">lr: 学习率</span></span><br><span class=\"line\"><span class=\"string\">momentum: 动量系数，通常设为0.9</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.lr = lr</span><br><span class=\"line\"><span class=\"variable language_\">self</span>.momentum = momentum</span><br><span class=\"line\"><span class=\"variable language_\">self</span>.v = <span class=\"literal\">None</span>  <span class=\"comment\"># 速度向量</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">update</span>(<span class=\"params\">self, params, grads</span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">更新参数</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">params: 需要更新的参数字典</span></span><br><span class=\"line\"><span class=\"string\">grads: 对应的梯度字典</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"variable language_\">self</span>.v <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\"><span class=\"comment\"># 第一次调用时初始化速度向量</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.v = &#123;&#125;</span><br><span class=\"line\"><span class=\"keyword\">for</span> key, val <span class=\"keyword\">in</span> params.items():                                </span><br><span class=\"line\">    <span class=\"variable language_\">self</span>.v[key] = np.zeros_like(val)</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> params.keys():</span><br><span class=\"line\"><span class=\"comment\"># 更新速度：v = momentum * v - lr * grad</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.v[key] = <span class=\"variable language_\">self</span>.momentum*<span class=\"variable language_\">self</span>.v[key] - <span class=\"variable language_\">self</span>.lr*grads[key] </span><br><span class=\"line\"><span class=\"comment\"># 更新参数：θ = θ + v</span></span><br><span class=\"line\">params[key] += <span class=\"variable language_\">self</span>.v[key]</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"AdaGrad\"><a href=\"#AdaGrad\" class=\"headerlink\" title=\"AdaGrad\"></a>AdaGrad</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">AdaGrad</span>:</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">AdaGrad（自适应梯度算法）</span></span><br><span class=\"line\"><span class=\"string\">为每个参数自适应地调整学习率，适合处理稀疏梯度</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, lr=<span class=\"number\">0.01</span></span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">初始化AdaGrad优化器</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">lr: 初始学习率</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.lr = lr</span><br><span class=\"line\"><span class=\"variable language_\">self</span>.h = <span class=\"literal\">None</span>  <span class=\"comment\"># 累积梯度平方和</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">update</span>(<span class=\"params\">self, params, grads</span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">更新参数</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">params: 需要更新的参数字典</span></span><br><span class=\"line\"><span class=\"string\">grads: 对应的梯度字典</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"variable language_\">self</span>.h <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\"><span class=\"comment\"># 第一次调用时初始化累积梯度平方和</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.h = &#123;&#125;</span><br><span class=\"line\"><span class=\"keyword\">for</span> key, val <span class=\"keyword\">in</span> params.items():</span><br><span class=\"line\">    <span class=\"variable language_\">self</span>.h[key] = np.zeros_like(val)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> params.keys():</span><br><span class=\"line\"><span class=\"comment\"># 累积梯度平方和：h = h + grad^2</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.h[key] += grads[key] * grads[key]</span><br><span class=\"line\"><span class=\"comment\"># 参数更新：θ = θ - lr * grad / sqrt(h + ε)</span></span><br><span class=\"line\">params[key] -= <span class=\"variable language_\">self</span>.lr * grads[key] / (np.sqrt(<span class=\"variable language_\">self</span>.h[key]) + <span class=\"number\">1e-7</span>)</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Adam</span>:</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">Adam（自适应矩估计算法）</span></span><br><span class=\"line\"><span class=\"string\">结合了Momentum和RMSprop的优点，是目前最常用的优化算法之一</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, lr=<span class=\"number\">0.001</span>, beta1=<span class=\"number\">0.9</span>, beta2=<span class=\"number\">0.999</span></span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">初始化Adam优化器</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">lr: 学习率</span></span><br><span class=\"line\"><span class=\"string\">beta1: 一阶矩估计的指数衰减率</span></span><br><span class=\"line\"><span class=\"string\">beta2: 二阶矩估计的指数衰减率</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.lr = lr</span><br><span class=\"line\"><span class=\"variable language_\">self</span>.beta1 = beta1</span><br><span class=\"line\"><span class=\"variable language_\">self</span>.beta2 = beta2</span><br><span class=\"line\"><span class=\"variable language_\">self</span>.<span class=\"built_in\">iter</span> = <span class=\"number\">0</span>  <span class=\"comment\"># 迭代次数</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.m = <span class=\"literal\">None</span>  <span class=\"comment\"># 一阶矩估计（梯度的指数移动平均）</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.v = <span class=\"literal\">None</span>  <span class=\"comment\"># 二阶矩估计（梯度平方的指数移动平均）</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">update</span>(<span class=\"params\">self, params, grads</span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">更新参数</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">params: 需要更新的参数字典</span></span><br><span class=\"line\"><span class=\"string\">grads: 对应的梯度字典</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> <span class=\"variable language_\">self</span>.m <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\"><span class=\"comment\"># 第一次调用时初始化</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.m, <span class=\"variable language_\">self</span>.v = &#123;&#125;, &#123;&#125;</span><br><span class=\"line\"><span class=\"keyword\">for</span> key, val <span class=\"keyword\">in</span> params.items():</span><br><span class=\"line\">    <span class=\"variable language_\">self</span>.m[key] = np.zeros_like(val)</span><br><span class=\"line\">    <span class=\"variable language_\">self</span>.v[key] = np.zeros_like(val)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.<span class=\"built_in\">iter</span> += <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"comment\"># 计算学习率的偏差修正</span></span><br><span class=\"line\">lr_t  = <span class=\"variable language_\">self</span>.lr * np.sqrt(<span class=\"number\">1.0</span> - <span class=\"variable language_\">self</span>.beta2**<span class=\"variable language_\">self</span>.<span class=\"built_in\">iter</span>) / (<span class=\"number\">1.0</span> - <span class=\"variable language_\">self</span>.beta1**<span class=\"variable language_\">self</span>.<span class=\"built_in\">iter</span>)         </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> params.keys():</span><br><span class=\"line\"><span class=\"comment\"># 更新一阶矩估计：m = beta1 * m + (1 - beta1) * grad</span></span><br><span class=\"line\"><span class=\"comment\">#self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]</span></span><br><span class=\"line\"><span class=\"comment\"># 更新二阶矩估计：v = beta2 * v + (1 - beta2) * grad^2</span></span><br><span class=\"line\"><span class=\"comment\">#self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 偏差修正的更新方式</span></span><br><span class=\"line\"><span class=\"variable language_\">self</span>.m[key] += (<span class=\"number\">1</span> - <span class=\"variable language_\">self</span>.beta1) * (grads[key] - <span class=\"variable language_\">self</span>.m[key])</span><br><span class=\"line\"><span class=\"variable language_\">self</span>.v[key] += (<span class=\"number\">1</span> - <span class=\"variable language_\">self</span>.beta2) * (grads[key]**<span class=\"number\">2</span> - <span class=\"variable language_\">self</span>.v[key])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 参数更新：θ = θ - lr_t * m / sqrt(v + ε)</span></span><br><span class=\"line\">params[key] -= lr_t * <span class=\"variable language_\">self</span>.m[key] / (np.sqrt(<span class=\"variable language_\">self</span>.v[key]) + <span class=\"number\">1e-7</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 另一种偏差修正的实现方式（注释掉）</span></span><br><span class=\"line\"><span class=\"comment\">#unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias</span></span><br><span class=\"line\"><span class=\"comment\">#unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias</span></span><br><span class=\"line\"><span class=\"comment\">#params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)</span></span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"几种更新方法的比较\"><a href=\"#几种更新方法的比较\" class=\"headerlink\" title=\"几种更新方法的比较\"></a>几种更新方法的比较</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys, os</span><br><span class=\"line\">sys.path.append(os.pardir)  <span class=\"comment\"># 添加父目录到路径，以便导入common模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> OrderedDict</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.optimizer <span class=\"keyword\">import</span> *  <span class=\"comment\"># 导入所有优化器类</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">x, y</span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">定义目标函数：f(x,y) = x^2/20 + y^2</span></span><br><span class=\"line\"><span class=\"string\">这是一个简单的二次函数，用于测试优化算法的性能</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">x, y: 输入变量</span></span><br><span class=\"line\"><span class=\"string\">返回:</span></span><br><span class=\"line\"><span class=\"string\">函数值</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> x**<span class=\"number\">2</span> / <span class=\"number\">20.0</span> + y**<span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">df</span>(<span class=\"params\">x, y</span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">计算目标函数的梯度</span></span><br><span class=\"line\"><span class=\"string\">∂f/∂x = x/10, ∂f/∂y = 2y</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">参数:</span></span><br><span class=\"line\"><span class=\"string\">x, y: 输入变量</span></span><br><span class=\"line\"><span class=\"string\">返回:</span></span><br><span class=\"line\"><span class=\"string\">(∂f/∂x, ∂f/∂y): 梯度向量</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> x / <span class=\"number\">10.0</span>, <span class=\"number\">2.0</span>*y</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置初始位置</span></span><br><span class=\"line\">init_pos = (-<span class=\"number\">7.0</span>, <span class=\"number\">2.0</span>)  <span class=\"comment\"># 初始点坐标</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 初始化参数字典</span></span><br><span class=\"line\">params = &#123;&#125;</span><br><span class=\"line\">params[<span class=\"string\">&#x27;x&#x27;</span>], params[<span class=\"string\">&#x27;y&#x27;</span>] = init_pos[<span class=\"number\">0</span>], init_pos[<span class=\"number\">1</span>]  <span class=\"comment\"># 将初始位置赋值给参数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 初始化梯度字典</span></span><br><span class=\"line\">grads = &#123;&#125;</span><br><span class=\"line\">grads[<span class=\"string\">&#x27;x&#x27;</span>], grads[<span class=\"string\">&#x27;y&#x27;</span>] = <span class=\"number\">0</span>, <span class=\"number\">0</span>  <span class=\"comment\"># 梯度初始化为0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建优化器字典，使用OrderedDict保持顺序</span></span><br><span class=\"line\">optimizers = OrderedDict()</span><br><span class=\"line\">optimizers[<span class=\"string\">&quot;SGD&quot;</span>] = SGD(lr=<span class=\"number\">0.95</span>)  <span class=\"comment\"># 随机梯度下降，学习率0.95</span></span><br><span class=\"line\">optimizers[<span class=\"string\">&quot;Momentum&quot;</span>] = Momentum(lr=<span class=\"number\">0.1</span>)  <span class=\"comment\"># 带动量的SGD，学习率0.1</span></span><br><span class=\"line\">optimizers[<span class=\"string\">&quot;AdaGrad&quot;</span>] = AdaGrad(lr=<span class=\"number\">1.5</span>)  <span class=\"comment\"># 自适应梯度算法，学习率1.5</span></span><br><span class=\"line\">optimizers[<span class=\"string\">&quot;Adam&quot;</span>] = Adam(lr=<span class=\"number\">0.3</span>)  <span class=\"comment\"># Adam优化器，学习率0.3</span></span><br><span class=\"line\"></span><br><span class=\"line\">idx = <span class=\"number\">1</span>  <span class=\"comment\"># 子图索引</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 遍历每种优化器</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> optimizers:</span><br><span class=\"line\">optimizer = optimizers[key]  <span class=\"comment\"># 获取当前优化器</span></span><br><span class=\"line\">x_history = []  <span class=\"comment\"># 记录x坐标的历史轨迹</span></span><br><span class=\"line\">y_history = []  <span class=\"comment\"># 记录y坐标的历史轨迹</span></span><br><span class=\"line\">params[<span class=\"string\">&#x27;x&#x27;</span>], params[<span class=\"string\">&#x27;y&#x27;</span>] = init_pos[<span class=\"number\">0</span>], init_pos[<span class=\"number\">1</span>]  <span class=\"comment\"># 重置参数到初始位置</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 进行30次优化迭代</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">30</span>):</span><br><span class=\"line\">x_history.append(params[<span class=\"string\">&#x27;x&#x27;</span>])  <span class=\"comment\"># 记录当前x位置</span></span><br><span class=\"line\">y_history.append(params[<span class=\"string\">&#x27;y&#x27;</span>])  <span class=\"comment\"># 记录当前y位置</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算当前点的梯度</span></span><br><span class=\"line\">grads[<span class=\"string\">&#x27;x&#x27;</span>], grads[<span class=\"string\">&#x27;y&#x27;</span>] = df(params[<span class=\"string\">&#x27;x&#x27;</span>], params[<span class=\"string\">&#x27;y&#x27;</span>])</span><br><span class=\"line\"><span class=\"comment\"># 使用优化器更新参数</span></span><br><span class=\"line\">optimizer.update(params, grads)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建网格用于绘制等高线图</span></span><br><span class=\"line\">x = np.arange(-<span class=\"number\">10</span>, <span class=\"number\">10</span>, <span class=\"number\">0.01</span>)  <span class=\"comment\"># x轴范围：-10到10，步长0.01</span></span><br><span class=\"line\">y = np.arange(-<span class=\"number\">5</span>, <span class=\"number\">5</span>, <span class=\"number\">0.01</span>)    <span class=\"comment\"># y轴范围：-5到5，步长0.01</span></span><br><span class=\"line\"></span><br><span class=\"line\">X, Y = np.meshgrid(x, y)  <span class=\"comment\"># 创建网格坐标</span></span><br><span class=\"line\">Z = f(X, Y)  <span class=\"comment\"># 计算网格上每个点的函数值</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 为了简化等高线图，将大于7的值设为0</span></span><br><span class=\"line\">mask = Z &gt; <span class=\"number\">7</span></span><br><span class=\"line\">Z[mask] = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制子图</span></span><br><span class=\"line\">plt.subplot(<span class=\"number\">2</span>, <span class=\"number\">2</span>, idx)  <span class=\"comment\"># 创建2x2的子图，当前是第idx个</span></span><br><span class=\"line\">idx += <span class=\"number\">1</span>  <span class=\"comment\"># 子图索引递增</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制优化轨迹</span></span><br><span class=\"line\">plt.plot(x_history, y_history, <span class=\"string\">&#x27;o-&#x27;</span>, color=<span class=\"string\">&quot;red&quot;</span>)  <span class=\"comment\"># 红色圆点连线表示优化路径</span></span><br><span class=\"line\">plt.contour(X, Y, Z)  <span class=\"comment\"># 绘制等高线</span></span><br><span class=\"line\">plt.ylim(-<span class=\"number\">10</span>, <span class=\"number\">10</span>)  <span class=\"comment\"># 设置y轴范围</span></span><br><span class=\"line\">plt.xlim(-<span class=\"number\">10</span>, <span class=\"number\">10</span>)  <span class=\"comment\"># 设置x轴范围</span></span><br><span class=\"line\">plt.plot(<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"string\">&#x27;+&#x27;</span>)  <span class=\"comment\"># 在原点(0,0)绘制加号，表示全局最优点</span></span><br><span class=\"line\"><span class=\"comment\">#colorbar()  # 注释掉的颜色条</span></span><br><span class=\"line\"><span class=\"comment\">#spring()   # 注释掉的弹簧布局</span></span><br><span class=\"line\">plt.title(key)  <span class=\"comment\"># 设置子图标题为优化器名称</span></span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;x&quot;</span>)  <span class=\"comment\"># 设置x轴标签</span></span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;y&quot;</span>)  <span class=\"comment\"># 设置y轴标签</span></span><br><span class=\"line\"></span><br><span class=\"line\">plt.show()  <span class=\"comment\"># 显示所有子图</span></span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"基于MNIST数据集的更新方法的比较\"><a href=\"#基于MNIST数据集的更新方法的比较\" class=\"headerlink\" title=\"基于MNIST数据集的更新方法的比较\"></a>基于MNIST数据集的更新方法的比较</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">sys.path.append(os.pardir)  <span class=\"comment\"># 添加父目录到路径，以便导入其他模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist  <span class=\"comment\"># 导入MNIST数据集加载函数</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> common.util <span class=\"keyword\">import</span> smooth_curve  <span class=\"comment\"># 导入平滑曲线函数，用于可视化</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> common.multi_layer_net <span class=\"keyword\">import</span> MultiLayerNet  <span class=\"comment\"># 导入多层神经网络类</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> common.optimizer <span class=\"keyword\">import</span> *  <span class=\"comment\"># 导入所有优化器类</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 0: 加载MNIST数据集 ==========</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>)  <span class=\"comment\"># 加载并归一化MNIST数据集</span></span><br><span class=\"line\"></span><br><span class=\"line\">train_size = x_train.shape[<span class=\"number\">0</span>]  <span class=\"comment\"># 训练集大小</span></span><br><span class=\"line\">batch_size = <span class=\"number\">128</span>  <span class=\"comment\"># 批量大小</span></span><br><span class=\"line\">max_iterations = <span class=\"number\">2000</span>  <span class=\"comment\"># 最大迭代次数</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 1: 实验设置 ==========</span></span><br><span class=\"line\"><span class=\"comment\"># 创建不同优化器的字典</span></span><br><span class=\"line\">optimizers = &#123;&#125;</span><br><span class=\"line\">optimizers[<span class=\"string\">&#x27;SGD&#x27;</span>] = SGD()  <span class=\"comment\"># 随机梯度下降</span></span><br><span class=\"line\">optimizers[<span class=\"string\">&#x27;Momentum&#x27;</span>] = Momentum()  <span class=\"comment\"># 带动量的SGD</span></span><br><span class=\"line\">optimizers[<span class=\"string\">&#x27;AdaGrad&#x27;</span>] = AdaGrad()  <span class=\"comment\"># 自适应梯度算法</span></span><br><span class=\"line\">optimizers[<span class=\"string\">&#x27;Adam&#x27;</span>] = Adam()  <span class=\"comment\"># Adam优化器</span></span><br><span class=\"line\"><span class=\"comment\">#optimizers[&#x27;RMSprop&#x27;] = RMSprop()  # RMSprop优化器（已注释）</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 为每个优化器创建对应的神经网络和损失记录</span></span><br><span class=\"line\">networks = &#123;&#125;  <span class=\"comment\"># 存储不同优化器对应的神经网络</span></span><br><span class=\"line\">train_loss = &#123;&#125;  <span class=\"comment\"># 存储不同优化器的训练损失</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> optimizers.keys():</span><br><span class=\"line\"><span class=\"comment\"># 创建具有4个隐藏层（每层100个神经元）的多层神经网络</span></span><br><span class=\"line\">networks[key] = MultiLayerNet(</span><br><span class=\"line\">input_size=<span class=\"number\">784</span>,  <span class=\"comment\"># 输入层大小（28x28=784）</span></span><br><span class=\"line\">hidden_size_list=[<span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>],  <span class=\"comment\"># 4个隐藏层，每层100个神经元</span></span><br><span class=\"line\">output_size=<span class=\"number\">10</span>)  <span class=\"comment\"># 输出层大小（10个类别）</span></span><br><span class=\"line\">train_loss[key] = []  <span class=\"comment\"># 初始化损失列表</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2: 开始训练 ==========</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(max_iterations):</span><br><span class=\"line\"><span class=\"comment\"># 随机选择批量数据</span></span><br><span class=\"line\">batch_mask = np.random.choice(train_size, batch_size)  <span class=\"comment\"># 随机选择batch_size个样本的索引</span></span><br><span class=\"line\">x_batch = x_train[batch_mask]  <span class=\"comment\"># 获取批量输入数据</span></span><br><span class=\"line\">t_batch = t_train[batch_mask]  <span class=\"comment\"># 获取批量标签数据</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 对每个优化器进行参数更新</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> optimizers.keys():</span><br><span class=\"line\"><span class=\"comment\"># 计算梯度</span></span><br><span class=\"line\">grads = networks[key].gradient(x_batch, t_batch)  <span class=\"comment\"># 计算当前批量的梯度</span></span><br><span class=\"line\"><span class=\"comment\"># 更新参数</span></span><br><span class=\"line\">optimizers[key].update(networks[key].params, grads)  <span class=\"comment\"># 使用优化器更新网络参数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算损失</span></span><br><span class=\"line\">loss = networks[key].loss(x_batch, t_batch)  <span class=\"comment\"># 计算当前批量的损失</span></span><br><span class=\"line\">train_loss[key].append(loss)  <span class=\"comment\"># 记录损失</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 每100次迭代打印一次训练状态</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;===========&quot;</span> + <span class=\"string\">&quot;iteration:&quot;</span> + <span class=\"built_in\">str</span>(i) + <span class=\"string\">&quot;===========&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> optimizers.keys():</span><br><span class=\"line\">loss = networks[key].loss(x_batch, t_batch)  <span class=\"comment\"># 计算当前批量的损失</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(key + <span class=\"string\">&quot;:&quot;</span> + <span class=\"built_in\">str</span>(loss))  <span class=\"comment\"># 打印每个优化器的损失</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3: 绘制图表 ==========</span></span><br><span class=\"line\"><span class=\"comment\"># 设置不同优化器的标记样式</span></span><br><span class=\"line\">markers = &#123;<span class=\"string\">&quot;SGD&quot;</span>: <span class=\"string\">&quot;o&quot;</span>, <span class=\"string\">&quot;Momentum&quot;</span>: <span class=\"string\">&quot;x&quot;</span>, <span class=\"string\">&quot;AdaGrad&quot;</span>: <span class=\"string\">&quot;s&quot;</span>, <span class=\"string\">&quot;Adam&quot;</span>: <span class=\"string\">&quot;D&quot;</span>&#125;</span><br><span class=\"line\">x = np.arange(max_iterations)  <span class=\"comment\"># 创建x轴数据（迭代次数）</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制每个优化器的损失曲线</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> optimizers.keys():</span><br><span class=\"line\"><span class=\"comment\"># 使用平滑曲线绘制损失，每100次迭代标记一次</span></span><br><span class=\"line\">plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=<span class=\"number\">100</span>, label=key)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置图表属性</span></span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;iterations&quot;</span>)  <span class=\"comment\"># x轴标签：迭代次数</span></span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;loss&quot;</span>)  <span class=\"comment\"># y轴标签：损失值</span></span><br><span class=\"line\">plt.ylim(<span class=\"number\">0</span>, <span class=\"number\">1</span>)  <span class=\"comment\"># 设置y轴范围：0到1</span></span><br><span class=\"line\">plt.legend()  <span class=\"comment\"># 显示图例</span></span><br><span class=\"line\">plt.show()  <span class=\"comment\"># 显示图表</span></span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"隐藏层的激活值的分布（设计两种初始值Xavier和He初始值（作为权重的初始值），分别适用于不同的激活函数）\"><a href=\"#隐藏层的激活值的分布（设计两种初始值Xavier和He初始值（作为权重的初始值），分别适用于不同的激活函数）\" class=\"headerlink\" title=\"隐藏层的激活值的分布（设计两种初始值Xavier和He初始值（作为权重的初始值），分别适用于不同的激活函数）\"></a>隐藏层的激活值的分布（设计两种初始值Xavier和He初始值（作为权重的初始值），分别适用于不同的激活函数）</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sigmoid</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">Sigmoid激活函数</span></span><br><span class=\"line\"><span class=\"string\">将输入值压缩到(0,1)区间</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> <span class=\"number\">1</span> / (<span class=\"number\">1</span> + np.exp(-x))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">ReLU</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">ReLU激活函数</span></span><br><span class=\"line\"><span class=\"string\">当输入大于0时返回原值，小于0时返回0</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> np.maximum(<span class=\"number\">0</span>, x)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">tanh</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">tanh激活函数</span></span><br><span class=\"line\"><span class=\"string\">将输入值压缩到(-1,1)区间</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">return</span> np.tanh(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 生成1000个样本，每个样本100个特征,x 就是高斯分布随机数</span></span><br><span class=\"line\">input_data = np.random.randn(<span class=\"number\">1000</span>, <span class=\"number\">100</span>)  <span class=\"comment\"># 且符合标准正态分布，也就是高斯分布</span></span><br><span class=\"line\">node_num = <span class=\"number\">100</span></span><br><span class=\"line\">hidden_layer_size = <span class=\"number\">5</span></span><br><span class=\"line\">activations = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">x = input_data</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 模拟5层神经网络的前向传播过程</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(hidden_layer_size):</span><br><span class=\"line\"><span class=\"keyword\">if</span> i != <span class=\"number\">0</span>:</span><br><span class=\"line\">x = activations[i-<span class=\"number\">1</span>]  <span class=\"comment\"># 使用上一层的输出作为当前层的输入</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 权重初始化实验</span></span><br><span class=\"line\"><span class=\"comment\"># 可以尝试不同的权重初始化方法</span></span><br><span class=\"line\">w = np.random.randn(node_num, node_num) * <span class=\"number\">1</span>  <span class=\"comment\"># 标准正态分布</span></span><br><span class=\"line\"><span class=\"comment\"># w = np.random.randn(node_num, node_num) * 0.01  # 缩小100倍</span></span><br><span class=\"line\"><span class=\"comment\"># w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)  # Xavier初始化</span></span><br><span class=\"line\"><span class=\"comment\"># w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)  # He初始化</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 线性变换</span></span><br><span class=\"line\">a = np.dot(x, w)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 激活函数实验</span></span><br><span class=\"line\"><span class=\"comment\"># 可以尝试不同的激活函数</span></span><br><span class=\"line\">z = sigmoid(a)  <span class=\"comment\"># Sigmoid激活</span></span><br><span class=\"line\"><span class=\"comment\"># z = ReLU(a)    # ReLU激活</span></span><br><span class=\"line\"><span class=\"comment\"># z = tanh(a)    # tanh激活</span></span><br><span class=\"line\"></span><br><span class=\"line\">activations[i] = z  <span class=\"comment\"># 保存当前层的激活值</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制每层激活值的分布直方图</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i, a <span class=\"keyword\">in</span> activations.items():</span><br><span class=\"line\">plt.subplot(<span class=\"number\">1</span>, <span class=\"built_in\">len</span>(activations), i+<span class=\"number\">1</span>)  <span class=\"comment\"># 创建子图</span></span><br><span class=\"line\">plt.title(<span class=\"built_in\">str</span>(i+<span class=\"number\">1</span>) + <span class=\"string\">&quot;-layer&quot;</span>)  <span class=\"comment\"># 设置标题</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> i != <span class=\"number\">0</span>: plt.yticks([], [])  <span class=\"comment\"># 除第一层外，隐藏y轴刻度</span></span><br><span class=\"line\"><span class=\"comment\"># plt.xlim(0.1, 1)  # 可以设置x轴范围</span></span><br><span class=\"line\"><span class=\"comment\"># plt.ylim(0, 7000)  # 可以设置y轴范围</span></span><br><span class=\"line\">plt.hist(a.flatten(), <span class=\"number\">30</span>, <span class=\"built_in\">range</span>=(<span class=\"number\">0</span>,<span class=\"number\">1</span>))  <span class=\"comment\"># 绘制直方图，30个bin，范围0-1</span></span><br><span class=\"line\">plt.show()  <span class=\"comment\"># 显示图形</span></span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"基于MNIST数据集的权重初始值的比较\"><a href=\"#基于MNIST数据集的权重初始值的比较\" class=\"headerlink\" title=\"基于MNIST数据集的权重初始值的比较\"></a>基于MNIST数据集的权重初始值的比较</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 添加父目录到系统路径，以便导入common模块</span></span><br><span class=\"line\">sys.path.append(os.pardir)</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.util <span class=\"keyword\">import</span> smooth_curve</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.multi_layer_net <span class=\"keyword\">import</span> MultiLayerNet</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.optimizer <span class=\"keyword\">import</span> SGD</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载MNIST数据集并进行归一化处理</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置训练参数</span></span><br><span class=\"line\">train_size = x_train.shape[<span class=\"number\">0</span>]  <span class=\"comment\"># 训练数据总数</span></span><br><span class=\"line\">batch_size = <span class=\"number\">128</span>  <span class=\"comment\"># 批次大小</span></span><br><span class=\"line\">max_iterations = <span class=\"number\">2000</span>  <span class=\"comment\"># 最大迭代次数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义不同的权重初始化方法</span></span><br><span class=\"line\"><span class=\"comment\"># std=0.01: 标准差为0.01的正态分布</span></span><br><span class=\"line\"><span class=\"comment\"># Xavier: Xavier初始化（适用于sigmoid激活函数）</span></span><br><span class=\"line\"><span class=\"comment\"># He: He初始化（适用于ReLU激活函数）</span></span><br><span class=\"line\">weight_init_types = &#123;<span class=\"string\">&#x27;std=0.01&#x27;</span>: <span class=\"number\">0.01</span>, <span class=\"string\">&#x27;Xavier&#x27;</span>: <span class=\"string\">&#x27;sigmoid&#x27;</span>, <span class=\"string\">&#x27;He&#x27;</span>: <span class=\"string\">&#x27;relu&#x27;</span>&#125;</span><br><span class=\"line\">optimizer = SGD(lr=<span class=\"number\">0.01</span>)  <span class=\"comment\"># 随机梯度下降优化器，学习率为0.01</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建不同初始化方法的神经网络和损失记录</span></span><br><span class=\"line\">networks = &#123;&#125;</span><br><span class=\"line\">train_loss = &#123;&#125;</span><br><span class=\"line\"><span class=\"keyword\">for</span> key, weight_type <span class=\"keyword\">in</span> weight_init_types.items():</span><br><span class=\"line\"><span class=\"comment\"># 创建多层神经网络：输入784维，4个隐藏层各100个神经元，输出10维</span></span><br><span class=\"line\">networks[key] = MultiLayerNet(input_size=<span class=\"number\">784</span>, hidden_size_list=[<span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>],</span><br><span class=\"line\">                            output_size=<span class=\"number\">10</span>, weight_init_std=weight_type)</span><br><span class=\"line\">train_loss[key] = []  <span class=\"comment\"># 初始化损失记录列表</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 开始训练循环</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(max_iterations):</span><br><span class=\"line\"><span class=\"comment\"># 随机选择批次数据</span></span><br><span class=\"line\">batch_mask = np.random.choice(train_size, batch_size)</span><br><span class=\"line\">x_batch = x_train[batch_mask]</span><br><span class=\"line\">t_batch = t_train[batch_mask]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 对每种权重初始化方法进行训练</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> weight_init_types.keys():</span><br><span class=\"line\">  <span class=\"comment\"># 计算梯度</span></span><br><span class=\"line\">  grads = networks[key].gradient(x_batch, t_batch)</span><br><span class=\"line\">  <span class=\"comment\"># 更新网络参数</span></span><br><span class=\"line\">  optimizer.update(networks[key].params, grads)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\"># 计算并记录损失</span></span><br><span class=\"line\">  loss = networks[key].loss(x_batch, t_batch)</span><br><span class=\"line\">  train_loss[key].append(loss)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 每100次迭代打印一次损失值</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">  <span class=\"built_in\">print</span>(<span class=\"string\">&quot;===========&quot;</span> + <span class=\"string\">&quot;iteration:&quot;</span> + <span class=\"built_in\">str</span>(i) + <span class=\"string\">&quot;===========&quot;</span>)</span><br><span class=\"line\">  <span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> weight_init_types.keys():</span><br><span class=\"line\">      loss = networks[key].loss(x_batch, t_batch)</span><br><span class=\"line\">      <span class=\"built_in\">print</span>(key + <span class=\"string\">&quot;:&quot;</span> + <span class=\"built_in\">str</span>(loss))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制训练损失曲线</span></span><br><span class=\"line\">markers = &#123;<span class=\"string\">&#x27;std=0.01&#x27;</span>: <span class=\"string\">&#x27;o&#x27;</span>, <span class=\"string\">&#x27;Xavier&#x27;</span>: <span class=\"string\">&#x27;s&#x27;</span>, <span class=\"string\">&#x27;He&#x27;</span>: <span class=\"string\">&#x27;D&#x27;</span>&#125;  <span class=\"comment\"># 不同方法的标记符号</span></span><br><span class=\"line\">x = np.arange(max_iterations)  <span class=\"comment\"># x轴：迭代次数</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> weight_init_types.keys():</span><br><span class=\"line\"><span class=\"comment\"># 绘制平滑后的损失曲线，每100次迭代显示一个标记</span></span><br><span class=\"line\">plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=<span class=\"number\">100</span>, label=key)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;iterations&quot;</span>)  <span class=\"comment\"># x轴标签</span></span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;loss&quot;</span>)  <span class=\"comment\"># y轴标签</span></span><br><span class=\"line\">plt.ylim(<span class=\"number\">0</span>, <span class=\"number\">2.5</span>)  <span class=\"comment\"># 设置y轴范围</span></span><br><span class=\"line\">plt.legend()  <span class=\"comment\"># 显示图例</span></span><br><span class=\"line\">plt.show()  <span class=\"comment\"># 显示图形</span></span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"Batch-Normalization的评估-1\"><a href=\"#Batch-Normalization的评估-1\" class=\"headerlink\" title=\"Batch Normalization的评估\"></a>Batch Normalization的评估</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"><span class=\"comment\"># 本脚本用于对比带有批归一化（Batch Normalization）和不带批归一化的多层神经网络在不同权重初始值下的训练表现。</span></span><br><span class=\"line\"><span class=\"comment\"># 通过在MNIST数据集上训练网络，观察批归一化对训练收敛速度和准确率的影响。</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys, os</span><br><span class=\"line\">sys.path.append(os.pardir)  <span class=\"comment\"># 将父目录加入sys.path，便于导入上级目录中的模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist  <span class=\"comment\"># 导入MNIST数据集加载函数</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> common.multi_layer_net_extend <span class=\"keyword\">import</span> MultiLayerNetExtend  <span class=\"comment\"># 导入可扩展多层网络实现</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> common.optimizer <span class=\"keyword\">import</span> SGD, Adam  <span class=\"comment\"># 导入优化器</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载MNIST数据集，并进行归一化处理</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 为了加快实验速度，仅取前1000个训练样本</span></span><br><span class=\"line\">x_train = x_train[:<span class=\"number\">1000</span>]</span><br><span class=\"line\">t_train = t_train[:<span class=\"number\">1000</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">max_epochs = <span class=\"number\">20</span>  <span class=\"comment\"># 最大训练轮数</span></span><br><span class=\"line\">train_size = x_train.shape[<span class=\"number\">0</span>]  <span class=\"comment\"># 训练集样本数</span></span><br><span class=\"line\">batch_size = <span class=\"number\">100</span>  <span class=\"comment\"># 每个批次的样本数</span></span><br><span class=\"line\">learning_rate = <span class=\"number\">0.01</span>  <span class=\"comment\"># 学习率</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">__train</span>(<span class=\"params\">weight_init_std</span>):</span><br><span class=\"line\"> <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\"> 训练带有和不带有批归一化的神经网络。</span></span><br><span class=\"line\"><span class=\"string\"> 参数：</span></span><br><span class=\"line\"><span class=\"string\">     weight_init_std: 权重初始化的标准差</span></span><br><span class=\"line\"><span class=\"string\"> 返回：</span></span><br><span class=\"line\"><span class=\"string\">     train_acc_list: 不带批归一化的网络在每个epoch的训练准确率</span></span><br><span class=\"line\"><span class=\"string\">     bn_train_acc_list: 带批归一化的网络在每个epoch的训练准确率</span></span><br><span class=\"line\"><span class=\"string\"> &quot;&quot;&quot;</span></span><br><span class=\"line\"> <span class=\"comment\"># 构建带批归一化的网络</span></span><br><span class=\"line\"> bn_network = MultiLayerNetExtend(input_size=<span class=\"number\">784</span>, hidden_size_list=[<span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>], output_size=<span class=\"number\">10</span>, </span><br><span class=\"line\">                                 weight_init_std=weight_init_std, use_batchnorm=<span class=\"literal\">True</span>)</span><br><span class=\"line\"> <span class=\"comment\"># 构建不带批归一化的网络</span></span><br><span class=\"line\"> network = MultiLayerNetExtend(input_size=<span class=\"number\">784</span>, hidden_size_list=[<span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>], output_size=<span class=\"number\">10</span>,</span><br><span class=\"line\">                             weight_init_std=weight_init_std)</span><br><span class=\"line\"> optimizer = SGD(lr=learning_rate)  <span class=\"comment\"># 使用SGD优化器</span></span><br><span class=\"line\"> </span><br><span class=\"line\"> train_acc_list = []  <span class=\"comment\"># 记录不带BN的准确率</span></span><br><span class=\"line\"> bn_train_acc_list = []  <span class=\"comment\"># 记录带BN的准确率</span></span><br><span class=\"line\"> </span><br><span class=\"line\"> iter_per_epoch = <span class=\"built_in\">max</span>(train_size / batch_size, <span class=\"number\">1</span>)  <span class=\"comment\"># 每个epoch的迭代次数</span></span><br><span class=\"line\"> epoch_cnt = <span class=\"number\">0</span>  <span class=\"comment\"># 当前epoch计数</span></span><br><span class=\"line\"> </span><br><span class=\"line\"> <span class=\"comment\"># 训练循环</span></span><br><span class=\"line\"> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1000000000</span>):  <span class=\"comment\"># 迭代次数设置为极大，实际会在达到max_epochs时break</span></span><br><span class=\"line\">     <span class=\"comment\"># 随机采样一个batch</span></span><br><span class=\"line\">     batch_mask = np.random.choice(train_size, batch_size)</span><br><span class=\"line\">     x_batch = x_train[batch_mask]</span><br><span class=\"line\">     t_batch = t_train[batch_mask]</span><br><span class=\"line\"> </span><br><span class=\"line\">     <span class=\"comment\"># 对两个网络分别进行反向传播和参数更新</span></span><br><span class=\"line\">     <span class=\"keyword\">for</span> _network <span class=\"keyword\">in</span> (bn_network, network):</span><br><span class=\"line\">         grads = _network.gradient(x_batch, t_batch)</span><br><span class=\"line\">         optimizer.update(_network.params, grads)</span><br><span class=\"line\"> </span><br><span class=\"line\">     <span class=\"comment\"># 每经过一个epoch，记录一次准确率</span></span><br><span class=\"line\">     <span class=\"keyword\">if</span> i % iter_per_epoch == <span class=\"number\">0</span>:</span><br><span class=\"line\">         train_acc = network.accuracy(x_train, t_train)</span><br><span class=\"line\">         bn_train_acc = bn_network.accuracy(x_train, t_train)</span><br><span class=\"line\">         train_acc_list.append(train_acc)</span><br><span class=\"line\">         bn_train_acc_list.append(bn_train_acc)</span><br><span class=\"line\"> </span><br><span class=\"line\">         <span class=\"built_in\">print</span>(<span class=\"string\">&quot;epoch:&quot;</span> + <span class=\"built_in\">str</span>(epoch_cnt) + <span class=\"string\">&quot; | &quot;</span> + <span class=\"built_in\">str</span>(train_acc) + <span class=\"string\">&quot; - &quot;</span> + <span class=\"built_in\">str</span>(bn_train_acc))</span><br><span class=\"line\"> </span><br><span class=\"line\">         epoch_cnt += <span class=\"number\">1</span></span><br><span class=\"line\">         <span class=\"keyword\">if</span> epoch_cnt &gt;= max_epochs:</span><br><span class=\"line\">             <span class=\"keyword\">break</span></span><br><span class=\"line\">             </span><br><span class=\"line\"> <span class=\"keyword\">return</span> train_acc_list, bn_train_acc_list</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 生成16个权重初始化标准差，从1到1e-4，等比数列</span></span><br><span class=\"line\">weight_scale_list = np.logspace(<span class=\"number\">0</span>, -<span class=\"number\">4</span>, num=<span class=\"number\">16</span>)</span><br><span class=\"line\">x = np.arange(max_epochs)  <span class=\"comment\"># x轴为epoch数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 对每个权重初始化标准差分别进行实验</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i, w <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(weight_scale_list):</span><br><span class=\"line\"> <span class=\"built_in\">print</span>( <span class=\"string\">&quot;============== &quot;</span> + <span class=\"built_in\">str</span>(i+<span class=\"number\">1</span>) + <span class=\"string\">&quot;/16&quot;</span> + <span class=\"string\">&quot; ==============&quot;</span>)</span><br><span class=\"line\"> train_acc_list, bn_train_acc_list = __train(w)</span><br><span class=\"line\"> </span><br><span class=\"line\"> <span class=\"comment\"># 绘制每组实验的准确率曲线</span></span><br><span class=\"line\"> plt.subplot(<span class=\"number\">4</span>,<span class=\"number\">4</span>,i+<span class=\"number\">1</span>)</span><br><span class=\"line\"> plt.title(<span class=\"string\">&quot;W:&quot;</span> + <span class=\"built_in\">str</span>(w))</span><br><span class=\"line\"> <span class=\"keyword\">if</span> i == <span class=\"number\">15</span>:</span><br><span class=\"line\">     plt.plot(x, bn_train_acc_list, label=<span class=\"string\">&#x27;Batch Normalization&#x27;</span>, markevery=<span class=\"number\">2</span>)</span><br><span class=\"line\">     plt.plot(x, train_acc_list, linestyle = <span class=\"string\">&quot;--&quot;</span>, label=<span class=\"string\">&#x27;Normal(without BatchNorm)&#x27;</span>, markevery=<span class=\"number\">2</span>)</span><br><span class=\"line\"> <span class=\"keyword\">else</span>:</span><br><span class=\"line\">     plt.plot(x, bn_train_acc_list, markevery=<span class=\"number\">2</span>)</span><br><span class=\"line\">     plt.plot(x, train_acc_list, linestyle=<span class=\"string\">&quot;--&quot;</span>, markevery=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"> plt.ylim(<span class=\"number\">0</span>, <span class=\"number\">1.0</span>)  <span class=\"comment\"># y轴范围</span></span><br><span class=\"line\"> <span class=\"keyword\">if</span> i % <span class=\"number\">4</span>:</span><br><span class=\"line\">     plt.yticks([])</span><br><span class=\"line\"> <span class=\"keyword\">else</span>:</span><br><span class=\"line\">     plt.ylabel(<span class=\"string\">&quot;accuracy&quot;</span>)</span><br><span class=\"line\"> <span class=\"keyword\">if</span> i &lt; <span class=\"number\">12</span>:</span><br><span class=\"line\">     plt.xticks([])</span><br><span class=\"line\"> <span class=\"keyword\">else</span>:</span><br><span class=\"line\">     plt.xlabel(<span class=\"string\">&quot;epochs&quot;</span>)</span><br><span class=\"line\"> plt.legend(loc=<span class=\"string\">&#x27;lower right&#x27;</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">plt.show()  <span class=\"comment\"># 显示所有子图</span></span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"Dropout的实现\"><a href=\"#Dropout的实现\" class=\"headerlink\" title=\"Dropout的实现\"></a>Dropout的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Dropout</span>:</span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, dropout_ratio=<span class=\"number\">0.5</span></span>):</span><br><span class=\"line\">     slef.dropout_ratio = dropout_ratio</span><br><span class=\"line\">     <span class=\"variable language_\">self</span>.mask = <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self,x, train_flag=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">     <span class=\"keyword\">if</span> train_flag:</span><br><span class=\"line\">         <span class=\"variable language_\">self</span>.mask = np.random.rand(*x.shape) &gt; <span class=\"variable language_\">self</span>.dropout_ratio</span><br><span class=\"line\">         <span class=\"keyword\">return</span> x * <span class=\"variable language_\">self</span>.mask</span><br><span class=\"line\">     <span class=\"keyword\">else</span>:</span><br><span class=\"line\">         <span class=\"keyword\">return</span> x * (<span class=\"number\">1.0</span> - <span class=\"variable language_\">self</span>.dropout_ratio)</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"keyword\">def</span> <span class=\"title function_\">backward</span>(<span class=\"params\">self, dout</span>):</span><br><span class=\"line\">     <span class=\"keyword\">return</span> dout * <span class=\"variable language_\">self</span>.mask</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"使用Mnist数据集进行验证Dropout的效果\"><a href=\"#使用Mnist数据集进行验证Dropout的效果\" class=\"headerlink\" title=\"使用Mnist数据集进行验证Dropout的效果\"></a>使用Mnist数据集进行验证Dropout的效果</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"><span class=\"comment\"># 本脚本用于实验Dropout对深层神经网络过拟合的抑制作用。</span></span><br><span class=\"line\"><span class=\"comment\"># 通过在MNIST数据集上训练一个较深的网络，观察在有无Dropout时训练集和测试集准确率的变化。</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">sys.path.append(os.pardir)  <span class=\"comment\"># 将父目录加入sys.path，便于导入上级目录中的模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist  <span class=\"comment\"># 导入MNIST数据集加载函数</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> common.multi_layer_net_extend <span class=\"keyword\">import</span> MultiLayerNetExtend  <span class=\"comment\"># 导入可扩展多层神经网络实现（支持Dropout）</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> common.trainer <span class=\"keyword\">import</span> Trainer  <span class=\"comment\"># 导入训练器类，简化训练流程</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载MNIST数据集，并进行归一化处理</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 为了更容易出现过拟合，仅取前300个训练样本</span></span><br><span class=\"line\">x_train = x_train[:<span class=\"number\">300</span>]</span><br><span class=\"line\">t_train = t_train[:<span class=\"number\">300</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置Dropout参数</span></span><br><span class=\"line\">use_dropout = <span class=\"literal\">True</span>  <span class=\"comment\"># 是否使用Dropout</span></span><br><span class=\"line\"><span class=\"comment\"># dropout_ratio为每层神经元被随机丢弃的比例，常用0.2~0.5</span></span><br><span class=\"line\"><span class=\"comment\"># 若不使用Dropout，可将use_dropout设为False</span></span><br><span class=\"line\"><span class=\"comment\"># ====================================================</span></span><br><span class=\"line\">dropout_ratio = <span class=\"number\">0.2</span>  <span class=\"comment\"># Dropout比例</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 构建一个6层隐藏层的全连接神经网络，设置Dropout参数</span></span><br><span class=\"line\">network = MultiLayerNetExtend(input_size=<span class=\"number\">784</span>, hidden_size_list=[<span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>],</span><br><span class=\"line\">                           output_size=<span class=\"number\">10</span>, use_dropout=use_dropout, dropout_ration=dropout_ratio)</span><br><span class=\"line\"><span class=\"comment\"># 使用Trainer类进行训练，自动完成mini-batch梯度下降、准确率记录等</span></span><br><span class=\"line\">trainer = Trainer(network, x_train, t_train, x_test, t_test,</span><br><span class=\"line\">               epochs=<span class=\"number\">301</span>, mini_batch_size=<span class=\"number\">100</span>,</span><br><span class=\"line\">               optimizer=<span class=\"string\">&#x27;sgd&#x27;</span>, optimizer_param=&#123;<span class=\"string\">&#x27;lr&#x27;</span>: <span class=\"number\">0.01</span>&#125;, verbose=<span class=\"literal\">True</span>)</span><br><span class=\"line\">trainer.train()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 获取训练集和测试集的准确率变化曲线</span></span><br><span class=\"line\">train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制训练集和测试集的准确率曲线</span></span><br><span class=\"line\">markers = &#123;<span class=\"string\">&#x27;train&#x27;</span>: <span class=\"string\">&#x27;o&#x27;</span>, <span class=\"string\">&#x27;test&#x27;</span>: <span class=\"string\">&#x27;s&#x27;</span>&#125;</span><br><span class=\"line\">x = np.arange(<span class=\"built_in\">len</span>(train_acc_list))</span><br><span class=\"line\">plt.plot(x, train_acc_list, marker=<span class=\"string\">&#x27;o&#x27;</span>, label=<span class=\"string\">&#x27;train&#x27;</span>, markevery=<span class=\"number\">10</span>)</span><br><span class=\"line\">plt.plot(x, test_acc_list, marker=<span class=\"string\">&#x27;s&#x27;</span>, label=<span class=\"string\">&#x27;test&#x27;</span>, markevery=<span class=\"number\">10</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&quot;epochs&quot;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&quot;accuracy&quot;</span>)</span><br><span class=\"line\">plt.ylim(<span class=\"number\">0</span>, <span class=\"number\">1.0</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;lower right&#x27;</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n\n<h4 id=\"超参数最优化的实现\"><a href=\"#超参数最优化的实现\" class=\"headerlink\" title=\"超参数最优化的实现\"></a>超参数最优化的实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys, os</span><br><span class=\"line\">sys.path.append(os.pardir)  <span class=\"comment\"># 将父目录添加到sys.path，便于导入上级目录的模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> dataset.mnist <span class=\"keyword\">import</span> load_mnist</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.multi_layer_net <span class=\"keyword\">import</span> MultiLayerNet</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.util <span class=\"keyword\">import</span> shuffle_dataset</span><br><span class=\"line\"><span class=\"keyword\">from</span> common.trainer <span class=\"keyword\">import</span> Trainer</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载MNIST数据集，并进行归一化处理</span></span><br><span class=\"line\">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 只取前500个训练样本，减少计算量，加快超参数搜索</span></span><br><span class=\"line\">x_train = x_train[:<span class=\"number\">500</span>]</span><br><span class=\"line\">t_train = t_train[:<span class=\"number\">500</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置验证集比例为20%</span></span><br><span class=\"line\">validation_rate = <span class=\"number\">0.20</span></span><br><span class=\"line\">validation_num = <span class=\"built_in\">int</span>(x_train.shape[<span class=\"number\">0</span>] * validation_rate)</span><br><span class=\"line\"><span class=\"comment\"># 先打乱数据，保证训练集和验证集的分布一致</span></span><br><span class=\"line\">x_train, t_train = shuffle_dataset(x_train, t_train)</span><br><span class=\"line\">x_val = x_train[:validation_num]</span><br><span class=\"line\">t_val = t_train[:validation_num]</span><br><span class=\"line\">x_train = x_train[validation_num:]</span><br><span class=\"line\">t_train = t_train[validation_num:]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义训练函数，输入学习率和权重衰减，返回每轮的验证集和训练集准确率</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">__train</span>(<span class=\"params\">lr, weight_decay, epocs=<span class=\"number\">50</span></span>):</span><br><span class=\"line\">    <span class=\"comment\"># 构建一个6层隐藏层的全连接神经网络，带权重衰减</span></span><br><span class=\"line\">    network = MultiLayerNet(input_size=<span class=\"number\">784</span>, hidden_size_list=[<span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>, <span class=\"number\">100</span>],</span><br><span class=\"line\">                            output_size=<span class=\"number\">10</span>, weight_decay_lambda=weight_decay)</span><br><span class=\"line\">    <span class=\"comment\"># 构建训练器，使用SGD优化器</span></span><br><span class=\"line\">    trainer = Trainer(network, x_train, t_train, x_val, t_val,</span><br><span class=\"line\">                      epochs=epocs, mini_batch_size=<span class=\"number\">100</span>,</span><br><span class=\"line\">                      optimizer=<span class=\"string\">&#x27;sgd&#x27;</span>, optimizer_param=&#123;<span class=\"string\">&#x27;lr&#x27;</span>: lr&#125;, verbose=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    trainer.train()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> trainer.test_acc_list, trainer.train_acc_list</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 超参数优化实验次数</span></span><br><span class=\"line\">optimization_trial = <span class=\"number\">100</span></span><br><span class=\"line\">results_val = &#123;&#125;   <span class=\"comment\"># 存储每组超参数下的验证集准确率</span></span><br><span class=\"line\">results_train = &#123;&#125; <span class=\"comment\"># 存储每组超参数下的训练集准确率</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(optimization_trial):</span><br><span class=\"line\">    <span class=\"comment\"># 随机采样权重衰减和学习率（对数均匀分布采样，覆盖大范围）</span></span><br><span class=\"line\">    weight_decay = <span class=\"number\">10</span> ** np.random.uniform(-<span class=\"number\">8</span>, -<span class=\"number\">4</span>)</span><br><span class=\"line\">    lr = <span class=\"number\">10</span> ** np.random.uniform(-<span class=\"number\">6</span>, -<span class=\"number\">2</span>)</span><br><span class=\"line\">    <span class=\"comment\"># ================================================</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 训练网络并记录结果</span></span><br><span class=\"line\">    val_acc_list, train_acc_list = __train(lr, weight_decay)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;val acc:&quot;</span> + <span class=\"built_in\">str</span>(val_acc_list[-<span class=\"number\">1</span>]) + <span class=\"string\">&quot; | lr:&quot;</span> + <span class=\"built_in\">str</span>(lr) + <span class=\"string\">&quot;, weight decay:&quot;</span> + <span class=\"built_in\">str</span>(weight_decay))</span><br><span class=\"line\">    key = <span class=\"string\">&quot;lr:&quot;</span> + <span class=\"built_in\">str</span>(lr) + <span class=\"string\">&quot;, weight decay:&quot;</span> + <span class=\"built_in\">str</span>(weight_decay)</span><br><span class=\"line\">    results_val[key] = val_acc_list</span><br><span class=\"line\">    results_train[key] = train_acc_list</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出超参数优化结果</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;=========== Hyper-Parameter Optimization Result ===========&quot;</span>)</span><br><span class=\"line\">graph_draw_num = <span class=\"number\">20</span>  <span class=\"comment\"># 最多画出前20组最优超参数的曲线</span></span><br><span class=\"line\">col_num = <span class=\"number\">5</span></span><br><span class=\"line\">row_num = <span class=\"built_in\">int</span>(np.ceil(graph_draw_num / col_num))</span><br><span class=\"line\">i = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 按验证集最终准确率从高到低排序，依次画出前20组的准确率曲线</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key, val_acc_list <span class=\"keyword\">in</span> <span class=\"built_in\">sorted</span>(results_val.items(), key=<span class=\"keyword\">lambda</span> x:x[<span class=\"number\">1</span>][-<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Best-&quot;</span> + <span class=\"built_in\">str</span>(i+<span class=\"number\">1</span>) + <span class=\"string\">&quot;(val acc:&quot;</span> + <span class=\"built_in\">str</span>(val_acc_list[-<span class=\"number\">1</span>]) + <span class=\"string\">&quot;) | &quot;</span> + key)</span><br><span class=\"line\"></span><br><span class=\"line\">    plt.subplot(row_num, col_num, i+<span class=\"number\">1</span>)</span><br><span class=\"line\">    plt.title(<span class=\"string\">&quot;Best-&quot;</span> + <span class=\"built_in\">str</span>(i+<span class=\"number\">1</span>))</span><br><span class=\"line\">    plt.ylim(<span class=\"number\">0.0</span>, <span class=\"number\">1.0</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> i % <span class=\"number\">5</span>: plt.yticks([])</span><br><span class=\"line\">    plt.xticks([])</span><br><span class=\"line\">    x = np.arange(<span class=\"built_in\">len</span>(val_acc_list))</span><br><span class=\"line\">    plt.plot(x, val_acc_list)           <span class=\"comment\"># 验证集准确率</span></span><br><span class=\"line\">    plt.plot(x, results_train[key], <span class=\"string\">&quot;--&quot;</span>) <span class=\"comment\"># 训练集准确率（虚线）</span></span><br><span class=\"line\">    i += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> i &gt;= graph_draw_num:</span><br><span class=\"line\">        <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n</blockquote>\n<h3 id=\"晕了，完全晕了！\"><a href=\"#晕了，完全晕了！\" class=\"headerlink\" title=\"晕了，完全晕了！\"></a>晕了，完全晕了！</h3>"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cmd5f7cqp000diku4f8mk9wyx","tag_id":"cmd5f7cqj0004iku4cd3e9kdk","_id":"cmd5f7cqt000hiku49aka83ke"},{"post_id":"cmd5f7cqp000diku4f8mk9wyx","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7cqu000kiku40q2vhaok"},{"post_id":"cmd5f7cqf0001iku43zzhftob","tag_id":"cmd5f7cqj0004iku4cd3e9kdk","_id":"cmd5f7cqw000oiku4h68q3c2p"},{"post_id":"cmd5f7cqf0001iku43zzhftob","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7cqx000riku4hm6b4n6t"},{"post_id":"cmd5f7cqq000fiku46zl1fl8x","tag_id":"cmd5f7cqj0004iku4cd3e9kdk","_id":"cmd5f7cqz000viku4d0qr6d28"},{"post_id":"cmd5f7cqq000fiku46zl1fl8x","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7cr0000yiku4gx50acdq"},{"post_id":"cmd5f7cqv000miku4bhzec2en","tag_id":"cmd5f7cqj0004iku4cd3e9kdk","_id":"cmd5f7cr00011iku42wbk7vr5"},{"post_id":"cmd5f7cqv000miku4bhzec2en","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7cr10013iku4dgi814xa"},{"post_id":"cmd5f7cqw000qiku41p7ihznr","tag_id":"cmd5f7cqj0004iku4cd3e9kdk","_id":"cmd5f7cr20016iku4828mhhjt"},{"post_id":"cmd5f7cqw000qiku41p7ihznr","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7cr30018iku4h93sarf0"},{"post_id":"cmd5f7cqh0003iku42jy90mwy","tag_id":"cmd5f7cqj0004iku4cd3e9kdk","_id":"cmd5f7cr3001biku4gyxz6uz4"},{"post_id":"cmd5f7cqh0003iku42jy90mwy","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7cr4001diku4g4og4v92"},{"post_id":"cmd5f7cqx000tiku4540243f3","tag_id":"cmd5f7cqj0004iku4cd3e9kdk","_id":"cmd5f7cr5001fiku4efqo6e1z"},{"post_id":"cmd5f7cqx000tiku4540243f3","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7cr6001iiku49hew0qrm"},{"post_id":"cmd5f7cr0000ziku48qtn35ux","tag_id":"cmd5f7cqj0004iku4cd3e9kdk","_id":"cmd5f7cr6001jiku4d2nmgn6n"},{"post_id":"cmd5f7cr0000ziku48qtn35ux","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7cr6001liku47tkp3qdb"},{"post_id":"cmd5f7cqk0006iku4bysj6yuz","tag_id":"cmd5f7cqj0004iku4cd3e9kdk","_id":"cmd5f7cr6001miku4gwwbeein"},{"post_id":"cmd5f7cqk0006iku4bysj6yuz","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7cr7001oiku4di3d9brn"},{"post_id":"cmd5f7cql0008iku4amx06iqv","tag_id":"cmd5f7cr20015iku41u723nxx","_id":"cmd5f7cr7001piku461ok72j8"},{"post_id":"cmd5f7cql0008iku4amx06iqv","tag_id":"cmd5f7cr3001aiku4aszubicf","_id":"cmd5f7cr7001riku4aljlhuu3"},{"post_id":"cmd5f7cqm000aiku4e74o56a5","tag_id":"cmd5f7cqj0004iku4cd3e9kdk","_id":"cmd5f7cr7001siku45l9ia18m"},{"post_id":"cmd5f7cqm000aiku4e74o56a5","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7cr7001uiku49ngd6oqo"},{"post_id":"cmd5f7cqu000jiku4g9c52rex","tag_id":"cmd5f7cr20015iku41u723nxx","_id":"cmd5f7cr7001viku4cjvah0yz"},{"post_id":"cmd5f7cqu000jiku4g9c52rex","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7cr8001xiku4doayfqp5"},{"post_id":"cmd5f7cqz000xiku49hclctwg","tag_id":"cmd5f7cr6001niku4608h8mtt","_id":"cmd5f7cr8001yiku4843a9buk"},{"post_id":"cmd5f7cqz000xiku49hclctwg","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7cr8001ziku4664cc3zx"},{"post_id":"cmd5f7cr10012iku4d07mfvt0","tag_id":"cmd5f7cqj0004iku4cd3e9kdk","_id":"cmd5f7cr80021iku4b1kbfz0g"},{"post_id":"cmd5f7cr10012iku4d07mfvt0","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7cr80022iku47zk119q2"},{"post_id":"cmd5f7cr10012iku4d07mfvt0","tag_id":"cmd5f7cr7001qiku49drh2esy","_id":"cmd5f7crb0024iku4fuvj6atv"},{"post_id":"cmd5f7cr10014iku49dte6yzx","tag_id":"cmd5f7cr7001tiku4h47uhyrd","_id":"cmd5f7crb0025iku4eg837yez"},{"post_id":"cmd5f7cr10014iku49dte6yzx","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7crb0027iku46her3tcw"},{"post_id":"cmd5f7cr10014iku49dte6yzx","tag_id":"cmd5f7cr8001wiku43819a0nc","_id":"cmd5f7crb0028iku4dq3n5ir2"},{"post_id":"cmd5f7cr20017iku41vjdezvp","tag_id":"cmd5f7cr80020iku459d69sdw","_id":"cmd5f7crc002aiku44a280wue"},{"post_id":"cmd5f7cr20017iku41vjdezvp","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7crc002biku4709t7bwi"},{"post_id":"cmd5f7cr20017iku41vjdezvp","tag_id":"cmd5f7cr90023iku41a4mbje1","_id":"cmd5f7crc002ciku42rfwho3q"},{"post_id":"cmd5f7cr30019iku4a36x56zn","tag_id":"cmd5f7crb0026iku44c6038oq","_id":"cmd5f7crd002fiku4hrwqcy3u"},{"post_id":"cmd5f7cr30019iku4a36x56zn","tag_id":"cmd5f7crb0029iku40bs36ejf","_id":"cmd5f7crd002giku46aezfkfg"},{"post_id":"cmd5f7cr30019iku4a36x56zn","tag_id":"cmd5f7crc002diku45tz4129d","_id":"cmd5f7crd002iiku4gun81ont"},{"post_id":"cmd5f7cr30019iku4a36x56zn","tag_id":"cmd5f7cr20015iku41u723nxx","_id":"cmd5f7crd002jiku4hiivb1t5"},{"post_id":"cmd5f7cr4001ciku4534b5luk","tag_id":"cmd5f7cr7001tiku4h47uhyrd","_id":"cmd5f7cre002liku4bjdo7xyw"},{"post_id":"cmd5f7cr4001ciku4534b5luk","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7cre002miku4chmoahl1"},{"post_id":"cmd5f7cr4001ciku4534b5luk","tag_id":"cmd5f7crd002hiku43na9hzsf","_id":"cmd5f7cre002oiku45g4y4ad0"},{"post_id":"cmd5f7cr4001eiku49nfh3del","tag_id":"cmd5f7cr80020iku459d69sdw","_id":"cmd5f7crf002qiku4b2d3ajz9"},{"post_id":"cmd5f7cr4001eiku49nfh3del","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7crf002riku4bacxhzxg"},{"post_id":"cmd5f7cr4001eiku49nfh3del","tag_id":"cmd5f7cr90023iku41a4mbje1","_id":"cmd5f7crf002tiku4ht91eo98"},{"post_id":"cmd5f7cr5001hiku44y253u1z","tag_id":"cmd5f7cr7001tiku4h47uhyrd","_id":"cmd5f7crf002uiku47phuh2li"},{"post_id":"cmd5f7cr5001hiku44y253u1z","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7crf002viku45qeq3lex"},{"post_id":"cmd5f7cr5001hiku44y253u1z","tag_id":"cmd5f7crd002hiku43na9hzsf","_id":"cmd5f7crf002wiku4g4se1c2r"},{"post_id":"cmd5f7crh002xiku486hch15q","tag_id":"cmd5f7cr7001tiku4h47uhyrd","_id":"cmd5f7crj0031iku450sq0mk1"},{"post_id":"cmd5f7crh002xiku486hch15q","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7crj0032iku4a2cpfa63"},{"post_id":"cmd5f7crh002xiku486hch15q","tag_id":"cmd5f7cri002ziku4913k2662","_id":"cmd5f7crj0033iku44fovat04"},{"post_id":"cmd5f7crh002yiku40nxy02tk","tag_id":"cmd5f7cr80020iku459d69sdw","_id":"cmd5f7crj0034iku4704x2g7o"},{"post_id":"cmd5f7crh002yiku40nxy02tk","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7crj0035iku47tkq8799"},{"post_id":"cmd5f7crh002yiku40nxy02tk","tag_id":"cmd5f7cri0030iku4gnw9elcu","_id":"cmd5f7crj0036iku490oee54r"},{"post_id":"cmd5f7crh002yiku40nxy02tk","tag_id":"cmd5f7cr6001niku4608h8mtt","_id":"cmd5f7crj0037iku443jncqzg"},{"post_id":"cmd5f7crm0038iku43cwuebp0","tag_id":"cmd5f7cr7001tiku4h47uhyrd","_id":"cmd5f7crn003aiku4hxtea1va"},{"post_id":"cmd5f7crm0038iku43cwuebp0","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7cro003ciku4ai58hkk5"},{"post_id":"cmd5f7crm0038iku43cwuebp0","tag_id":"cmd5f7crd002hiku43na9hzsf","_id":"cmd5f7cro003diku4f65b7jn9"},{"post_id":"cmd5f7crn0039iku4a06j4efw","tag_id":"cmd5f7cr7001tiku4h47uhyrd","_id":"cmd5f7cro003eiku49tgs4zgd"},{"post_id":"cmd5f7crn0039iku4a06j4efw","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7cro003fiku4hrsw0bnj"},{"post_id":"cmd5f7crn0039iku4a06j4efw","tag_id":"cmd5f7crd002hiku43na9hzsf","_id":"cmd5f7cro003giku4g5v6hak6"},{"post_id":"cmd5f7crn003biku4dy86bbkb","tag_id":"cmd5f7cr7001tiku4h47uhyrd","_id":"cmd5f7cro003hiku4cug8ell6"},{"post_id":"cmd5f7crn003biku4dy86bbkb","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7cro003iiku4bkw5b9o2"},{"post_id":"cmd5f7crn003biku4dy86bbkb","tag_id":"cmd5f7crd002hiku43na9hzsf","_id":"cmd5f7crp003jiku4df4w1py7"},{"post_id":"cmd5f7crq003kiku4598cdc5q","tag_id":"cmd5f7cr7001tiku4h47uhyrd","_id":"cmd5f7crr003liku47x4y896h"},{"post_id":"cmd5f7crq003kiku4598cdc5q","tag_id":"cmd5f7cqn000biku43jafc9as","_id":"cmd5f7crr003miku475eo9x8n"},{"post_id":"cmd5f7crq003kiku4598cdc5q","tag_id":"cmd5f7crd002hiku43na9hzsf","_id":"cmd5f7crr003niku4aum65219"}],"Tag":[{"name":"ROS2","_id":"cmd5f7cqj0004iku4cd3e9kdk"},{"name":"Learning","_id":"cmd5f7cqn000biku43jafc9as"},{"name":"Ubuntu","_id":"cmd5f7cr20015iku41u723nxx"},{"name":"安装教程","_id":"cmd5f7cr3001aiku4aszubicf"},{"name":"ROS1","_id":"cmd5f7cr6001niku4608h8mtt"},{"name":"autoware","_id":"cmd5f7cr7001qiku49drh2esy"},{"name":"DL","_id":"cmd5f7cr7001tiku4h47uhyrd"},{"name":"Pytorch","_id":"cmd5f7cr8001wiku43819a0nc"},{"name":"RL","_id":"cmd5f7cr80020iku459d69sdw"},{"name":"math-theory","_id":"cmd5f7cr90023iku41a4mbje1"},{"name":"安装","_id":"cmd5f7crb0026iku44c6038oq"},{"name":"上网","_id":"cmd5f7crb0029iku40bs36ejf"},{"name":"踩坑","_id":"cmd5f7crc002diku45tz4129d"},{"name":"gnaw_book","_id":"cmd5f7crd002hiku43na9hzsf"},{"name":"title-tattle","_id":"cmd5f7cri002ziku4913k2662"},{"name":"paper-reading","_id":"cmd5f7cri0030iku4gnw9elcu"}]}}