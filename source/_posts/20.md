---
title: DL之路---啃鱼书（1）
data: 2025-06-10 17:20:00
updated: 2025-06-10 17:20:00
type: DL
top_img:
cover: http://picbed.yanzu.tech/img/post_cover/p20.png
tags:
  - DL
  - Learning
  - gnaw_book
---


# 感知机



> #### 感知机（Perceptron）是一种人工神经网络，用于解决二分类问题
>
> #### 它模拟的是生物神经元的行为，主要包括：
>
> > #### 输入：多个特征值 $ x_1,x_2...x_n $ 
> >
> > #### 权重：每个输入对应一个权重 $ \omega_1,\omega_2...\omega_3 $
> >
> > #### 偏置：一个常数项 $ b $
>
> #### 偏置是用于控制神经元被激活的容易程度
>
> #### 权重是用于控制各个信号的重要性的
>
> #### 而激活函数则是决定以什么方式来激活输入信号的总和
>
> #### 数学表达式为：
>
> $$
> y = sign(\omega \cdot x + b)
> $$
>
> #### 其中，$ \omega \cdot x = \sum_{i=1}^n \omega_i x_i $，sign函数定义为：
>
> $$
>  sign(z) = 
> \begin{cases}
> 1 & \text{if } z > 0, \\
> -1 & \text otherwise.
> \end{cases}
> $$
>
> 
>
> #### 工作原理：找到一个线性超平面（即一条直线或一个超平面）来将数据分为两类（+1 和 -1）。通过不断调整权重 $ \omega $ 和偏置 $ b $，使得训练数据被正确分类
>
> 
>
> #### 感知机学习算法
>
> > #### 它的学习规则是一种迭代算法，所谓的学习就是确定合适的参数的过程
> >
> > > #### 初始化：随机初始化权重 $ \omega $ 和偏置 $ b $
> > >
> > > #### 逐个遍历样本：对于每个训练样本 $ (x_i,y_i) $，如果预测值与真实值不一致（即 $ y_i(\omega \cdot x_i+b) \le 0 $ ），就更新参数
> > >
> > > $$
> > > \omega \leftarrow \omega + \eta y_i x_i \\
> > > b \leftarrow b + \eta y_i
> > > $$
> > >
> > > #### 其中，$ \eta $ 是学习率
> >
> > #### 重复训练，直到所有样本被正确分类或达到最大迭代次数



### code

> #### 与门实现
>
> ```python
> def AND(x1, x2):
>     w1, w2, theta = 0.5, 0.5, 0.7
>     tmp = x1 * w1 + x2 * w2
>     if tmp <= theta:
>         return 0
>     elif tmp > theta:
>         return 1
>     
> print(AND(0, 0))
> print(AND(0, 1))
> print(AND(1, 0))
> print(AND(1, 1))
> ```
>
> 
>
> #### 或门实现
>
> ```python
> def OR(x1, x2):
>     w1, w2, theta = 0.5, 0.5, 0.4
>     tmp = x1 * w1 + x2 * w2
>     if tmp <= theta:
>         return 0
>     else:
>         return 1
>     
> print(OR(0, 0))
> print(OR(0, 1))
> print(OR(1, 0))
> print(OR(1, 1))
> ```
>
> 
>
> #### 与非门实现
>
> ```python
> def NotAnd(x1, x2):
>     # 直接对与门取反
>     w1 , w2, theta = -0.5, -0.5, -0.7
>     tmp = w1 * x1 + w2 * x2
>     if tmp <= theta:
>         return 0
>     else:
>         return 1
>         
> print(NotAnd(0, 0))
> print(NotAnd(0, 1))
> print(NotAnd(1, 0))
> print(NotAnd(1, 1))
> ```
>
> 
>
> #### 或非门实现
>
> ```python
> def NotOr(x1, x2):
>     # 直接对或门取反
>     w1 , w2, theta = -0.5, -0.5, -0.4
>     tmp = w1 * x1 + w2 * x2
>     if tmp <= theta:
>         return 0
>     else:
>         return 1
> 
> print(NotOr(0, 0))
> print(NotOr(0, 1))
> print(NotOr(1, 0))
> print(NotOr(1, 1))
> ```
>
> 
>
> #### 导入权重和偏置之后
>
> ```python
> import numpy as np
> def And(x1, x2):
>     x = np.array([x1, x2])
>     w = np.array([0.5, 0.5])
>     b = -0.7
>     tmp = np.sum(w * x) + b
>     if tmp <= 0:
>         return 0
>     else:
>         return 1
> 
> print(And(0, 0))
> print(And(0, 1))
> print(And(1, 0))
> print(And(1, 1))
> 
> 
> def NOTAnd(x1, x2):
>     x = np.array([x1, x2])
>     w = np.array([-0.5, -0.5])
>     b = 0.7
>     tmp = np.sum(w * x) + b
>     if tmp <= 0:
>         return 0
>     else:
>         return 1
> 
> print(NOTAnd(0, 0))
> print(NOTAnd(0, 1))
> print(NOTAnd(1, 0))
> print(NOTAnd(1, 1))
> ```
>
> 
>
> #### 单层感知机无法表示异或门，单层感知机只能划分线性空间，无法划分非线性空间(单层感知机也叫朴素感知机)
>
> #### 多层感知机（神经网络）
>
> ```python
> # 与非门、或门和与门实现异或门
> def XOR(x1, x2):
>     s1 = NOTAnd(x1, x2)
>     s2 = OR(x1, x2)
>     y = And(s1, s2)
>     return y
> 
> print(XOR(0, 0))
> print(XOR(0, 1))
> print(XOR(1, 0))
> print(XOR(1, 1))
> ```

