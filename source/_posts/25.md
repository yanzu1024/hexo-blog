---
title: DL杂谈
data: 2025-07-12 15:24:00
updated: 2025-07-12 15:24:00
type: DL
top_img:
cover: http://picbed.yanzu.tech/img/post_cover/p25.png
tags:
  - DL
  - Learning
  - title-tattle
---


# 神经网络杂谈



### 什么是神经网络

> #### 神经网络跟函数一样，有输入、输出，本质上也是一种映射关系，当给出一个函数表达式和一些数据，如 $y = kx + b$ ，给出了一些点或者说数据 (1,2)、(2,4)、(3,6)、(4,8)，这几组数据可以轻易的求出k和b的值来
>
> #### 但当给出的数据很多时，求解k和b这两个参数的问题就会变得复杂，至少是不能一眼看出某种规律。另外，函数表达式越复杂，在数据量越多的情况下，更能找出与之尽可能拟合的函数，因为数据量越大，其某种规律就可能会越明显，求得的参数就会越接近真实值（精确解）
>
> #### 函数表达式越复杂，其对应的表达能力会更强，如 $y = ax^2 + bx + c$ ，显然，它涵盖了 $y = bx + c,a=0$ 
>
> #### 求出函数表达式中的参数，也就完成了神经网络的搭建，通过训练来求参数



#### 怎么求参数

> #### 直接求出参数的值是不太现实的，因为但表达式足够复杂，参数足够多，参数的求解是极为复杂的
>
> #### 但通过求数值解，使这个解尽可能的逼近真实值，当两者的误差达到了所能接受的范围，就把这个数值解近似的作为真实值，如 $y=kx$ ，可直接将k指定一个值，如 $k=1,\hat y=x$ 然后根据所给数据求数值解，然后计算误差 $L = (\hat y - y)^2$ 
>
> #### 而求数值解的一个经典算法就是梯度下降法（gradient decay）
>
> > #### 若预测值为 $\hat y=kx$ ，真实值是 y，那么误差 $L=(\hat y - y)^2$ ，误差L的值越小，说明 $\hat y$ 和 y 越接近
> >
> > $$
> > L=(\hat y - y)^2 = (kx - y)^2 = k^2x^2 -2kxy + y^2
> > $$
> >
> > #### 要使L值最小，使用梯度下降法来求解，对k求偏导
> >
> > $$
> > \frac {\partial L}{\partial k} = 2kx^2 - 2xy
> > $$
> >
> > #### 已知一组数据(x,y)=(2,4)，(x,y)=(4,8)，假设k1=1（初始值），那么带入上面的偏导中，求得偏导值（k的梯度）为-8
> >
> > #### 然后更新k的值，$ k = k1 - \frac {\partial L}{\partial k}=1-(-8)=9 $ ，可以看到更新后的k值确实有向着期望的值靠近，但是“靠的太近了”，为此，引入一个新的因子——学习率
> >
> > #### 学习率（learning rate），一般设为0.1、0.01，若这里的lr=0.1，则有$ k = k1 - \frac {\partial L}{\partial k} \cdot lr=1-(-8) \cdot 0.1=1.8 $ 
> >
> > #### 通过不断地训练，k会逼近真实值，训练所要得到的就是损失函数取最小值时对应的k值
>
> #### 判断一个神经网络能不能用来训练，就要看所求解的问题对应的表达式能否求导



#### 神经网络的表示形式

> #### 一般的形式写作 $y = Wx + b$ ，还可以是复杂一点的 $y=W_1(W_2x+b2)+b_1$ ，对于复杂形式的求偏导采用链式法则
>
> #### 事实上，形如 $y = Wx + b$ 、$y=W_1(W_2x+b2)+b_1$ 、$y=W_1(W_2(W_3x+b_3)+b2)+b_1$ 等形式，其表达能力都是有限的（对应的函数都是凸函数），其函数图像都是形如 $y=x^2$ 或 $y=x^3$ 的图像，其表达能力（函数图像的复杂程度）还不够多样化，为了“打破”其凸的性质，使其更具多样性（表达能力更强）
>
> #### 由此引入了一个新的概念——激活函数
>
> #### 常见的激活函数，sigmoid、ReLU、softmax、tanh等，它破坏了 $y=Wx+b$ 的线性，使之不再是凸函数
>
> #### 先是用矩阵乘法来表达信息，然后使用激活函数来破坏线性，最后用一些算法求出近似的参数



### 实际处理过程

> $$
> y_1 = W_1 \cdot x + b_1 \\
> y_2 = W_2 \cdot y_1 + b_2 \\
> y_3 = sigmoid(y_2) \\
> L = (y_3 - y)^2 \\
> $$
>
> #### 反向传播
>
> $$
> \begin{align}
> \frac {\partial L}{\partial W_1} & = \frac {\partial L}{\partial y_3} \cdot \frac {\partial y_3}{\partial W_1} \\
> & = \frac {\partial L}{\partial y_3} \cdot \frac {\partial y_3}{\partial y_2} \cdot \frac {\partial y_2}{\partial y_1} \cdot \frac {\partial y_1} {\partial W_1} \\
> & = 2(y_3 - y) \cdot \frac {exp(-y_2)}{(1 + exp(-y_2))^2} \cdot W_2 \cdot x
> \end{align}
> $$
>
> #### 其他参数的梯度也是按照这种方式来写
>
> $$
> \frac{\partial L}{\partial b_1} = \frac {\partial L}{\partial y_3} \cdot \frac {\partial y_3}{\partial y_2} \cdot \frac {\partial y_2}{\partial y_1} \cdot \frac{\partial y_1}{\partial b_1}
> $$
>
> #### 可以看到，前面部分的偏导是一模一样的，只有最后一项不同
>
> #### 所以说，反向传播，前一层（如第三层）的梯度计算需要传入后一层（如第四层）的梯度才能计算，以此类推
>
> 

